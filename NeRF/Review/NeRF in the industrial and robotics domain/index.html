<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/blog.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/blog.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/yqq/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Title Neural Radiance Fields in the Industrial and Robotics Domain: Applications, Research Opportunities and Use Cases     Author Eugen ˇSlapak, Enric Pardo, Mat ́uˇs Dopiriak, Taras Maksymyuk and">
<meta property="og:type" content="article">
<meta property="og:title" content="NeRF in the industrial and robotics domain">
<meta property="og:url" content="http://example.com/NeRF/Review/NeRF%20in%20the%20industrial%20and%20robotics%20domain/index.html">
<meta property="og:site_name" content="YunQi">
<meta property="og:description" content="Title Neural Radiance Fields in the Industrial and Robotics Domain: Applications, Research Opportunities and Use Cases     Author Eugen ˇSlapak, Enric Pardo, Mat ́uˇs Dopiriak, Taras Maksymyuk and">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821142517.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821142946.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821160422.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821161057.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821161801.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821161912.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821162121.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821162214.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821170702.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821170743.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821171943.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821171222.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821171347.png">
<meta property="article:published_time" content="2023-08-21T06:10:55.000Z">
<meta property="article:modified_time" content="2023-11-13T05:03:48.874Z">
<meta property="article:author" content="Qi Yun">
<meta property="article:tag" content="NeRF">
<meta property="article:tag" content="Review">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821142517.png">

<link rel="canonical" href="http://example.com/NeRF/Review/NeRF%20in%20the%20industrial%20and%20robotics%20domain/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>NeRF in the industrial and robotics domain | YunQi</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">YunQi</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Note</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/NeRF/Review/NeRF%20in%20the%20industrial%20and%20robotics%20domain/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Qi Yun">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YunQi">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NeRF in the industrial and robotics domain
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-08-21 14:10:55" itemprop="dateCreated datePublished" datetime="2023-08-21T14:10:55+08:00">2023-08-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-11-13 13:03:48" itemprop="dateModified" datetime="2023-11-13T13:03:48+08:00">2023-11-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NeRF-Review/" itemprop="url" rel="index"><span itemprop="name">NeRF/Review</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>19k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>1:07</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <div class="table-container">
<table>
<thead>
<tr>
<th>Title</th>
<th>Neural Radiance Fields in the Industrial and Robotics Domain: Applications, Research Opportunities and Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td>Author</td>
<td>Eugen ˇSlapak, Enric Pardo, Mat ́uˇs Dopiriak, Taras Maksymyuk and Juraj Gazda</td>
</tr>
<tr>
<td>Conf/Jour</td>
<td>cs.RO</td>
</tr>
<tr>
<td>Year</td>
<td>2023</td>
</tr>
<tr>
<td>Project</td>
<td><a target="_blank" rel="noopener" href="https://github.com/Maftej/iisnerf">Maftej/iisnerf (github.com)</a></td>
</tr>
<tr>
<td>Paper</td>
<td><a target="_blank" rel="noopener" href="https://readpaper.com/pdf-annotate/note?pdfId=4789767752768290817&amp;noteId=1925861245942862080">Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases (readpaper.com)</a></td>
</tr>
</tbody>
</table>
</div>
<p>探索了NeRF在工业和机器人领域的应用</p>
<ul>
<li>Instant-NGP 基于NeRF的视频压缩技术</li>
<li>D-NeRF 根据过去的arm位置来预测未来的arm运动</li>
</ul>
<span id="more"></span>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>在这项研究中，我们进行了全面的研究，并探讨了NeRF在工业部门的潜在应用。我们的研究表明，nerf可以克服传统3D表示和渲染方法的局限性，例如成本高、存储效率低、设备要求高和真实感有限。此外，我们研究了创新的NeRF在工业环境中的应用，并通过概念验证实验证实了它们的可行性。我们认为<strong>我们的工作是对不断扩大的关于nerf及其在各个领域的不同应用的文献的有价值的补充</strong>。此外，我们希望通过提供实用的指导和建议，帮助<strong>弥合学术研究与工业实践之间的差距</strong>，以利用nerf来应对现实世界的工业挑战。<br>作为一种新颖而有前途的方法，nerf为未来工业领域的研究提供了广泛的机会。</p>
<ul>
<li>我们的概念验证实验可以扩展到<strong>基于对未来机器人相机姿势的预测</strong>来<strong>探索视频帧的预测性推测生成</strong>，甚至可以预测使用这些帧进行其他下游任务，例如对象分类。推测性预测执行可以从根本上减少这类任务的延迟。</li>
<li>除了我们实验的扩展，还有许多未探索的主题。其中包括<strong>通过nerf对无线电频谱进行建模</strong>，以估计信号质量，或者<strong>通过静态外部摄像机观察蜂群及其周围环境的视频流</strong>，为机器人群中的单个机器人生成视图。<strong>嵌入语言的nerf</strong>，如[110]LERF中提出的nerf，也可以使基于语言模型的智能体能够在3D中进行正确的行动计划的常识性推理。</li>
</ul>
<h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>扩展现实(XR)等技术的激增增加了对高质量三维(3D)图形表示的需求。工业3D应用包括计算机辅助设计(CAD)、有限元分析(FEA)、扫描和机器人技术。然而，目前用于工业3D表示的方法存在实施成本高和依赖人工输入来进行精确的3D建模的问题。为了解决这些挑战，神经辐射场(nerf)已经成为一种很有前途的方法，可以基于提供的训练2D图像来学习3D场景表示。<strong>尽管人们对nerf的兴趣日益浓厚，但它们在各种工业子领域的潜在应用仍未得到探索</strong>。<br>在本文中，我们提供了NeRF工业应用的全面检查，同时也为未来的研究工作提供了方向。我们还提出了一系列概念验证实验，证明了nerf在工业领域的潜力。这些实验包括<strong>基于nerf的视频压缩技术</strong>，以及<strong>在避免碰撞的情况下使用nerf进行3D运动估计</strong>。在视频压缩实验中，我们的结果显示，在分辨率分别为1920x1080和300x168的情况下，压缩节省高达48%和74%。运动估计实验采用机械臂三维动画对动态nerf (D-NeRF)进行训练，得到视差图的平均峰值信噪比(PSNR)为23 dB，结构相似指数(SSIM)为0.97。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>人工智能(AI)应用于计算机视觉和机器人中的3D场景理解的早期研究主要集中在简单的算法上，例如导航的边缘识别[1]。到20世纪90年代末，大多数算法缺乏从视觉数据中提取特征的能力。机器人中图像特征提取能力的增长始于尺度不变特征变换(SIFT)[2]的出现和加速鲁棒特征(SURF)[3]方法的发展。<br>机器学习的最新发展加速了深度学习在机器人领域的应用。一个值得注意的突破是基于AlexNet卷积神经网络(CNN)的深度学习的成功实现[4]，在性能方面优于传统的计算机视觉方法。从那时起，深度学习被广泛应用于各种机器人场景，实现了计算机视觉任务与端到端训练机器人控制代理的集成。<br>在[5]中进行的一项全面调查发现，深度学习已经成功地应用于机器人的各个领域，包括抓取规划、路径规划、感觉整合以及广泛的深度强化学习方法。这些应用使机器人能够学习复杂的任务，包括联合规划和控制模型，如[6]等作品所示。<br>此外，随着近年来cpu、gpu和tpu计算能力的增长，我们在数字孪生和工业元应用上观察到更多的工业活动。一个突出的例子是<strong>宝马的数字工厂</strong>，在那里，全面的3D扫描被用来准确地表示所有的生产地点和数字孪生体，使实时，虚拟导航不受地理或时间的限制[7]。这种技术允许重建各种操作要素，如生产线、机器和人员。此外，开发的虚拟工厂可以使用虚拟现实耳机headsets来捕获、分析和体验虚拟领域的工作空间，从而实现沉浸式参与。一般来说，使用3D扫描或3D建模设计的数字孪生可以在整个工业过程中节省大量的运营费用。<br>[8]中的作者详细回顾了2017年之前基于深度学习的机器人3D计算机视觉技术。本文综述了两种主要的三维计算机视觉任务类别:<strong>三维分类和三维生成</strong>。分类任务包括对象类的存在和估计其3D边界框。生成任务包括将输入转换为3D空间的2D或3D表示。3D表示的例子包括深度图[9]和体素网格[10]，[11]。该综述还概述了各种神经结构，主要是cnn，它们使用2D图像深度图通道或3D表示(如体素)进行训练。这些基于深度学习的方法在从2D图像中提取深度和3D表示方面比以前的方法表现得更好<br><strong>然而，现有的方法有一定的局限性，例如需要人工输入3D建模</strong>，这导致了高昂的管理成本和实现时间。基于扫描对象点云的替代方法具有极高的存储和计算要求。这些缺点正在推动进一步的研究以改进现有的技术。<br>神经隐式表征在[12]中有正式描述，是深度学习中最近出现的一个值得注意的范式。与获取广义知识的传统模型不同，神经隐式表示被设计为过拟合训练样本。这使他们能够在推理过程中以低误差重建样本，使他们能够在神经网络权重内存储多媒体数据，如图像，音频和视频</p>
<p><strong>在这项工作中，我们专注于NeRF，一个隐式神经表征的例子</strong>。NeRF通过基于描绘场景的样本图像将3D坐标映射到相应的亮度和光密度来学习3D场景表示。我们探索了几种工业应用，其中nerf已被实验性地用于解决各种问题。此外，我们还就nerf的未来潜力提供了建议和见解，以应对现有的工业挑战。<br>到目前为止，已经发表了几篇关于nerf的review文章。值得注意的贡献包括Gao等[13]、Zhu等[14]和Tewari等[80]。然而，值得注意的是，这些评论主要集中在nerf及其衍生物上，忽略了利用图像空间2D推理的神经渲染技术的讨论。此外，一项额外的工作[15]在深入探索NeRF的同时，对神经领域的更广泛主题进行了全面的讨论。<br>我们的工作有几个独特的优势，填补了该领域现有综述论文的空白。<strong>首先，它特别关注NeRF在工业和机器人领域的应用</strong>，强调了在这一特定领域扩展NeRF应用的巨大潜力。这种有针对性的方法可以更深入地探索工业和机器人环境中存在的独特挑战和机遇。<strong>其次，我们的工作不仅涵盖了已建立的NeRF应用，还引入了有前途的创新想法</strong>，这些想法有可能推动进一步的高级研究。不像现有的评论可能只是简单地触及这些想法，我们的工作更广泛地探讨了它们，揭示了它们的潜在影响和好处<br>我们的工作超越了理论讨论，包括概念验证实验。这些实验验证了这些建议的可行性和潜力，可供进一步研究。包括这样的实验证据加强了我们关于提出的想法的适用性和有效性的论点。<br>总之，这项工作填补了两个主要的研究空白。<strong>一是缺乏对nerf在工业领域的综述和应用探索论文</strong>。<strong>其次是传统的应用三维图形和表示方法在该领域的不足</strong>，现有的和新的基于nerf的方法可以解决这些问题。我们的主要贡献可以概括如下:</p>
<ul>
<li>我们全面回顾了工业应用中计算机图形学和3D表示的现有方法，并概述了它们的弱点。</li>
<li>我们研究了NeRF方法在工业应用中的可行性，并与主要使用的非NeRF方法的缺点进行了比较。</li>
<li>我们对这项工作中研究的新型NeRF应用进行了实验评估。</li>
</ul>
<p><em>本文其余部分的结构概述如下。第二节解释了基本nerf的基本原则，特别是静态场景的操作。第三节讨论了NeRF衍生变体的主要家族的分类及其与感兴趣领域的相关性。第四部分概述了工业领域中3D渲染和3D场景表示应用的现状，以及nerf在这些应用中的潜在适用性。第V节介绍了nerf的数学描述和我们的概念验证实验所需的评估指标。第六节和第七节分别介绍了基于nerf的视频压缩和深度估计的避障实验。最后，论文在第八部分结束，总结了本文的主要发现和贡献。</em></p>
<h1 id="NEURAL-RADIANCE-FIELDS-NERFS-FUNDAMENTALS-AND-WORKFLOW"><a href="#NEURAL-RADIANCE-FIELDS-NERFS-FUNDAMENTALS-AND-WORKFLOW" class="headerlink" title="NEURAL RADIANCE FIELDS (NERFS): FUNDAMENTALS AND WORKFLOW"></a>NEURAL RADIANCE FIELDS (NERFS): FUNDAMENTALS AND WORKFLOW</h1><p>NeRFs [16]属于内隐神经表征模型家族。这种模型通常表示单个连续实体，例如，描述单个图像或单个3D场景属性的信号。它们也可以被认为是这种实体的压缩和信号函数近似技术。<strong>nerf使用完全连接的深度神经网络将3D静态场景表示集成到神经网络权重中</strong>。其目的是以图像的形式对信号进行参数化，并对数据进行过拟合。原始的NeRF实现使用基于基本多层感知器(MLP)体系结构的模型。这与大多数过度拟合是不必要现象的任务不同，因为神经网络通常用于对一组实体的泛化，而不是对单个学习实体的过度拟合。输入是一组描绘单个3D场景及其相应相机姿势的图像。可微分渲染函数计算亮度，用于获得2D图像渲染的结果像素。优化神经场景表示，然后通过最小化地面真实和生成的图像之间的指导。nerf完成了一项被广泛称为新视图合成的任务，用于从特定姿势生成2D图像或生成3D场景模型。</p>
<p>图1提供了NeRF工作原理的文本描述的视觉概述，以及整篇论文中使用的一些基本符号。与大多数其他方法相比，使用nerf生成高保真3D静态场景或对象所需的工作量更少，工作流程类似于立体摄影测量[17]。该工作流程包括训练NeRF，仅使用从不同视点捕获所需3D场景的输入图像集合。原始NeRF架构的一个限制是，它需要以相机变换矩阵的形式提供关于每个训练图像的相机位置和角度的额外信息。然而，它的一些衍生变体成功地解决了这个缺点。</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821142517.png" alt="image.png"></p>
<p>在训练过程中，NeRF通过近似3D空间中点的体积密度和亮度来增强其输出的准确性和真实感。这两个量对于定义给定场景中的对象属性至关重要。体积密度决定了物质在空间中某一点的不透明度。辐射度决定了由无限小的投影面积发射或反射的辐射功率的量，单位是每无限小的立体角的瓦数。图1a直观地表示了这些概念。为了显色，有必要分别考虑单个红、绿、蓝颜色通道的亮度。<br>当相机捕捉2D场景投影时，每个图像看起来可能完全不同。NeRF学习一个独立于任何视图转换的底层3D场景表示，可用于生成训练集中未见的新视图。多个训练图像允许NeRF学习所有训练摄像机视点一致的场景表示。<br>图像由NeRF使用传统的体绘制合成，如图1b所示。体积渲染将新视图图像的像素作为沿3D场景中穿越点的光线路径的数值集成的亮度，逐渐累积亮度并最终进入相机光圈。在渲染过程中，NeRF只查询相关光线路径的体积密度和亮度，这取决于相机的视点。<br>将训练集中摄像机视点合成的渲染结果与地面真值图像进行对比，得到梯度下降的损失信号，如图1c所示。这允许我们逐步优化MLP权重，以更好地表示场景。</p>
<h1 id="CLASSIFICATION-OF-DERIVATIVE-NERF-IMPLEMENTATIONS"><a href="#CLASSIFICATION-OF-DERIVATIVE-NERF-IMPLEMENTATIONS" class="headerlink" title="CLASSIFICATION OF DERIVATIVE NERF IMPLEMENTATIONS"></a>CLASSIFICATION OF DERIVATIVE NERF IMPLEMENTATIONS</h1><p>本节总结了衍生的NeRF变体，这些变体在效率或质量改进方面增强了原始NeRF架构的特定方面。已经开发了几个扩展来增强各种基本的NeRF架构属性，包括训练时间、质量、所需的样本和渲染时间。了解这些NeRF改进的知识对于解决不同科学和实际环境中固有的特定应用限制和要求至关重要。<br>每一项改进都是由衍生的NeRF变体引入的，这些变体解决了工业NeRF用例中的各种限制。例如，如果机器人在移动到未知环境时需要使用NeRF进行空间导航，直到从不同视点收集到环境中重要部分的足够多的图像，则样本效率可能是有用的。低渲染时间(以性能为导向)对于需要低反应时间的系统是有用的，例如，机器人在空间中高速移动。最后，动态NeRF变量可用于捕获运动，其应用的一个例子是学习机器人的预定义重复运动以进行时空协调。<br>我们对NeRF变体进行了分类，并根据其对原始NeRF的主要改进重点对以下子部分进行了构建。这些一般类别是NeRF变体，侧重于样本效率、性能和系统动力学。尽管这种分类有助于快速确定特定任务的最佳NeRF变体，但大多数论文同时关注多种改进。为了反映这一点，表1对这些作品进行了更细粒度的分类。</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821142946.png" alt="image.png"></p>
<h2 id="Latent-vectors-for-sample-efficiency-and-scene-modification"><a href="#Latent-vectors-for-sample-efficiency-and-scene-modification" class="headerlink" title="Latent vectors for sample efficiency and scene modification"></a>Latent vectors for sample efficiency and scene modification</h2><p>一些实现采样效率的NeRF变体利用潜在向量对从不同训练数据中学习到的对象类别的抽象特征进行编码。虽然潜在向量不是提高样本效率的唯一方法，如RegNeRF[36]所示，但它们也可以在一些NeRF架构中实现场景修改，如CodeNeRF[41]。</p>
<p>RegNeRF[36]从少至三张输入图像中增强场景重建。因此，它规范了新视图渲染图像补丁的几何形状和外观。几何正则化利用了三维几何平滑的先验假设，这适用于大多数具有低空间频率几何的现实世界场景。外观正则化基于一个新的视图图像补丁，利用其正确性的可能性来优化NeRF。这种似然是使用在非结构化图像数据集上训练的归一化流模型获得的<br>pixelNeRF[37]利用CNN提取的局部特征，在训练过程中学习2D和3D特征之间的对应关系。该方法还采用聚类算法将图像像素划分为NeRF子模块，从而实现并行训练。<br>LOLNeRF[38]使用NeRF的生成潜在优化(GLO)从给定对象类的单个图像执行3D结构重建。解码器网络使用从每个图像中提取的潜在代码表示进行训练，该编码表示编码与3D场景重建相关的抽象特征。在训练过程中，对潜在代码的提取进行逐步细化<br>CodeNeRF[41]在训练过程中学习对象三维结构和纹理的解纠缠潜码。它在单个帧上使用推理时间优化来调整潜在代码，这用于调节NeRF输出。这使得在不改变MLP权重的情况下，可以在推理时修改对象形状或纹理以匹配单个图像示例。此外，与类似的方法不同，执行此任务不需要了解相机姿势。<br>用于高质量视图合成的稀疏RGB-D图像的nerf[46]表明，通过对每个场景进行预训练和微调，它可以从少至6个输入图像中进行训练。<br>ActiveNeRF[47]为NeRF提供了不确定性感知的主动学习，并通过在训练过程中选择最能降低NeRF不确定性的特定样本来提高场景表示质量。通过这种方式，它可以通过关注最重要的样本来更有效地利用资源。<br>NeRDi[39]以语言引导扩散为一般图像先验的单视图NeRF合成, 利用2D扩散模型学习的多视图先验从单个图像重建3D场景。它使用图像字幕和输入图像的文本反转来分别引导粗和细物体的外观。<br><strong>SparseNeRF</strong>[48]利用来自低成本深度传感器或深度估计模型的不完全深度信息，以少量样本训练NeRF。它在局部光场融合(LLFF)[49]和数据库事务单元(DTU)[50]数据集上优于其他最先进的方法。</p>
<h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2><p>现有的研究工作改进了各种NeRF性能指标，如渲染速度、训练速度和模型内存占用。有些方法可以同时对多个指标进行改进。效率的增强还可以在资源受限的硬件上实现更好的性能和新的实际应用，降低部署成本并实现对机器人有用的更小的硬件尺寸。</p>
<p>NSVF: Neural sparse voxel fields[18]通过在体素八树中排列的体素内训练一组隐式场，将渲染性能提高了10倍。体素结构是在训练过程中学习的，通过跳过不相关的体素来加速渲染。<br>KiloNeRF[19]是一种面向渲染性能的方法，它将3D场景体分成许多较小的子体，每个子体由一个单独的小MLP服务，而不是在整个3D场景上训练单个大MLP。与原始NeRF相比，这导致渲染速度提高了三个数量级<br>Adaptive Voronoi NeRF[20]使用与KiloNeRF类似的方法，将单个NeRF函数划分为更小的函数。然而，它使用Voronoi图，通过一组近似器将要学习的几何图形均匀地划分到单个Voronoi细胞中。该方法不需要像KiloNeRF那样单独的神经网络蒸馏步骤，并且使用256个Voronoi细胞比使用512个规则间隔细胞的KiloNeRF获得更好的质量<br>Mega-NeRF[25]是一种设计<strong>用于大型3D场景训练的架构</strong>，例如无人机(UAV)飞越，可以捕获整个城市街区大小的区域。它使用一个稀疏神经网络，其参数专门用于场景的局部部分。当在Mega-NeRF架构上使用快速渲染NeRF变体时，它可以在基本NeRF的基础上实现40倍的加速，而质量损失可以忽略不计。<br>SNeRG[21]引入了一种称为稀疏神经辐射网格(SNeRG)的数据结构，它可以将预先计算的数据存储在体素的稀疏网格中。每个体素包含由NeRF计算的光密度，漫反射颜色和学习的镜面颜色特征向量，仅在训练期间使用。在推断时，漫射颜色和光密度沿射线进行alpha合成。高光颜色特征向量分别进行alpha合成，并馈送到输出高光颜色组件的MLP。然后将漫反射和镜面分量图像合并到最终图像中。这种方法可以将NeRF在普通硬件上的渲染速度提高到每秒30帧。它还实现了空间效率，只需要90mb的存储空间。<br>PlenOctrees[22]首先训练NeRF变体NeRFSH (SH代表球面谐波)，它使用封闭形式的球面基函数代替颜色。然后，NeRF-SH被预先计算成一个基于八叉树的3D结构，称为PlenOctrees。颜色可以通过将两个视角定义的给定方向的加权球谐基相加得到。全八叉树允许直接优化进一步的质量改进和三倍的训练加速。他们也实现了3000倍的渲染速度比原来的NeRF在150 FPS。<br>Plenoxels[28]是PlenOctrees的后续作品，它使用不同的体素结构来表示场景。它没有使用八叉树，而是使用指向体素的密集3D指针数组。每个体素边缘定义一个球谐函数，通过球谐函数的三线性插值得到每个体素内的全光函数。这与PlenOctree形成对比，PlenOctree使用单个常量来表示体素体积。Plenoxels可以在不使用NeRF的情况下进行端到端优化，实现比原始NeRF快约100倍的训练速度。因此，基于我们的NeRF分类方案，我们将Plenoxels主要分类为以训练绩效为重点的方法<br>FastNeRF[23]将原始NeRF MLP分解为两个MLP:一个依赖于位置，一个依赖于视图。位置相关的MLP将3D位置坐标作为输入，并输出具有D分量的深亮度图。视相关MLP以两个视角θ和φ作为输入，输出亮度贴图分量权重。这种分解使得高效的缓存和查询亮度字段，导致速度比原来的NeRF增加了大约3000倍，平均帧率约为200 FPS。<br>SqueezeNeRF[26]通过将位置相关的MLP拆分为三个独立的MLP(每个轴对一个MLP)进一步改进了FastNeRF。与FastNeRF相比，这将缓存内存需求减少了60倍。<br>深度监督NeRF[29]使用从多视图图像中提取的稀疏点云作为一种廉价的信号来指导NeRF进行场景重建。这提高了场景重建质量，并将训练速度提高了2×to 3倍。</p>
<h2 id="Dynamic-scene-reconstruction-with-NeRF"><a href="#Dynamic-scene-reconstruction-with-NeRF" class="headerlink" title="Dynamic scene reconstruction with NeRF"></a>Dynamic scene reconstruction with NeRF</h2><p>NeRF可以从多个视图生成逼真的静态场景重建。它还可以扩展到处理具有时间变化的动态场景。动态NeRF模型对于需要3D捕获重复过程的工业应用非常有用。大多数现有的动态NeRF模型专注于捕捉过去的运动，而不是预测未来的场景运动。<strong>它们中的许多都依赖于变形场的概念</strong>。本节中提到的一些方法主要是由人体动作捕捉引起的，其中获得多个完美静态视图是具有挑战性的<br>D-NeRF[44]使用学习函数$\Psi_{\mathrm{x}}(\mathbf{x},\mathbf{d})\mapsto(\mathbf{c},\sigma)$的MLP对参考时刻(通常为t = 0)的静态3D场景的规范外观进行编码，将3D位置和视图方向映射到颜色和光密度对。对于其他时刻，它使用另一个MLP来学习一个变形函数，该函数将场景中的每个规范点映射到其新位置Ψt(x, t)。该函数输出位移向量∆x，对应于每个场景点的运动。这种方法比单独学习每个时间瞬间的场景外观的单个时间条件MLP更有效。<br>TiNeuVox[31]具有时间感知神经体素的快速动态辐射场，采用小变形网络和多距离插值来实现与D-NeRF相当或更好的质量，但训练时间要快得多，通常不到10分钟，而D-NeRF需要数十小时。<br>Nerfies[45]通过NeRF解决人体捕捉的挑战，即使在多次自拍拍摄期间试图保持静止，由于不可避免的人体运动(例如，呼吸，肌肉抽搐等)，这也是困难的。当从不同角度投射光线时，这种运动导致σ和c在空间上的不对准。与D-NeRF使用时间潜在向量进行变形网络训练不同，Nerfies使用基于时间和视图变换的潜在向量，因为可能有来自相同视图但不同时间戳的多个样本。<br>HyperNeRF[42]扩展了Nerfies来处理拓扑变化，由于拓扑变化引入了不连续，变形场无法很好地捕捉到拓扑变化。HyperNeRF将NeRF投影到一个更高维度的空间中，其中每个样本代表该空间的一个切片。它学习了一个高维函数在切片之间进行插值。受水平集方法的启发，HyperNeRF使用弯曲的切割面而不是直的超平面，从而产生由高维函数描述的更简单的形状。HyperNeRF在插值(4.1%)和新颖视图合成(8.6%)方面优于Nerfies。<br>NeRF-DS[43]解决了基于变形场的方法的局限性，如Nerfies和HyperNeRF用于重建动态镜面物体(例如，闪亮的茶壶或玻璃器皿)。重新制定了NeRF函数，使其依赖于曲面的位置和方向，并增加了一个运动物体遮罩来引导变形场。<br><strong>DeVRF</strong>[32]与其他动态NeRF模型(如D-NeRF、Nerfies、HyperNeRF和神经场景流场(NSFF))相比，DeVRF[32]方法在训练中实现了100倍的加速。<strong>它的目标是面向内的场景，并使用多摄像头设置场景捕捉</strong>。<br>LFNs: LFNs(光场网络)是另一种基于神经光场而不是辐射场来探索神经表征的研究。动态LFN模型DyLiN[51]向HyperNeRF学习，HyperNeRF是一位高质量但速度慢的teacher。在PSNR视觉质量指标方面，DyLiN优于当前最先进的动态模型HyperNeRF和TiNeuVox。此外，DyLiN比这两种方法都快得多，实现了一个数量级的加速。<br>最近解决动态LFN问题的其他研究包括带限辐射场(blrf)[52]、RefiNeRF[53]、时间插值(temporal interpolation is all you need)[35]、D-Tensorf[34]和DyNeRF[33]。</p>
<h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><p>Mip-NeRF: Mip-NeRF[27]提出了一种抗混叠技术，该技术对3D场景的每个像素使用锥体而不是射线进行采样，就像在原始NeRF中一样。这提高了不同相机距离和视图分辨率的渲染质量，在原始NeRF数据集上平均误差减少了17%。此外，与蛮力NeRF超采样相比，该模型实现了60%的误差减少和22倍的渲染加速。与最初的NeRF实现相比，该模型还将模型大小减小了一半。<br>ICARUS: ICARUS[54]是在FPGA上测试的一种节能的NeRF推理和渲染加速器硬件架构。当前设计的ICARUS实现了每帧45.75 s的NeRF渲染速度，功耗仅为282.8 mW。该设计尚未实现并行化，可以进一步提高帧生成时间。</p>
<h1 id="CURRENT-STATUS-OF-INDUSTRY-AND-ADVANTAGES-OFNERF-ALTERNATIVES"><a href="#CURRENT-STATUS-OF-INDUSTRY-AND-ADVANTAGES-OFNERF-ALTERNATIVES" class="headerlink" title="CURRENT STATUS OF INDUSTRY AND ADVANTAGES OFNERF ALTERNATIVES"></a>CURRENT STATUS OF INDUSTRY AND ADVANTAGES OFNERF ALTERNATIVES</h1><p>在本节中，我们将介绍3D表示和可视化应用于此类应用的商业方法的主要领域，以及应用于这些领域的特定基于nerf的最新方法。</p>
<h2 id="CAE-Computer-aided-engineering"><a href="#CAE-Computer-aided-engineering" class="headerlink" title="CAE: Computer-aided engineering"></a>CAE: Computer-aided engineering</h2><p>计算机辅助工程(CAE)是指使用计算机软件和工具来协助工程和设计过程。它包含各种技术和方法，利用计算机技术来分析和模拟产品和系统的行为。<br>计算机辅助设计(CAD)作为CAE的一个特殊分支，使用计算机软件来创建和修改物理对象的几何模型。使用的主要建模方法是自由曲面建模。自由曲面通常使用基于非均匀有理b样条(NURBS)的边界表示(B-Rep)[55]。生成的模型可以作为产品尺寸分析、自动创建技术图纸、物料清单生成和其他任务的输入。<br>CAD模型可以重新用于各种应用，特别是当采用特定技术来促进其重用时[56]。一个值得注意的应用是通过有限元分析(FEA)进行仿真，它允许对产品或制造设备内的特性和相互作用进行计算和交互式可视化。这些模拟可以包含诸如零件应力、温度、失效模式、流体动力学和电磁相互作用等因素，并且它们通常会发现CAD模型中的缺陷或缺点，从而需要对其进行修改。因此，CAD模型也可以基于FEA仿真中使用的修改网格模型进行更新[57]。<br>CAE的更广泛领域包括一系列类似的方法，包括有限体积法(FVM)和有限差分法(FDM)、计算机辅助制造(CAM)和计算流体动力学(CFD)。这些技术共同使工程师和设计师能够分析和优化他们的设计，考虑各种因素和相互作用，从而改进产品和系统。<br>除了产品设计之外，制造过程需要通过尽可能少的昂贵的部署和重新部署迭代来设计和优化。3D计算机可视化允许对原型制造设施进行快速和无错误的视觉评估。</p>
<ul>
<li>新兴的<strong>生成式nerf</strong>可以从零开始生成所需的3D模型，可以部分补充甚至实现CAD工具在未来的作用。生成设计的应用包括产品设计，为其他模型生成训练数据，或环境设计，允许另一个训练好的模型更好地泛化。为NeRF开发的生成方法具有不同程度的复杂性和对生成结果的可用控制。这反映了现有2D屏幕空间图像生成方法的不同控制选项的类似情况，例如生成对抗网络(gan)、扩散模型和多模态变压器模型。纯粹由自然语言引导的生成式nerf可以产生多样化和创造性的输出。<strong>然而，它们对输出属性的控制有限，这使得它们只适合装饰和娱乐导向的设计</strong><ul>
<li>与仅以自然语言为条件的方法相比，文本和形状引导的latent-NeRF方法[58]对生成的结果提供了更好的控制。除了文本描述外，该方法还为自动纹理生成提供形状指导或精确的3D形状。潜在的NeRF方法是基于扩散的，这与传统的在红、绿、蓝(RGB)色彩空间中训练NeRF模型不同。相反，<strong>NeRF模型是在潜在空间中训练的</strong>。这种选择消除了在训练期间将图像转换为潜在空间的需要，从而减少了计算开销。</li>
<li>DreamFusion[59]是另一种将文本转换为NeRF 3D场景的基于扩散的方法。由于没有足够大的3D模型训练数据集与其文本描述配对，因此作者使用现有的Imagen[60]基于扩散的生成模型，该模型仅使用2D图像空间作为先验。<strong>Imagen模型将NeRF渲染的阴影图像与随机噪声线性结合</strong>，生成去噪后的图像，指导NeRF在像素空间进行优化。利用该方法对NeRF进行了优化，实现了随机角度NeRF视图的低损耗。</li>
<li>Magic3D[61]通过解决两个主要限制来改进DreamFusion:缓慢的NeRF优化和降低3D模型质量的低分辨率图像引导。它采用了两阶段的方法。首先，它使用低分辨率扩散模型作为先验，使用Instant-NGP NeRF创建粗略的3D表示。在第二阶段，它从NeRF中提取多边形网格及其纹理，并使用可微分渲染器和高分辨率扩散先验进一步优化它们。</li>
</ul>
</li>
<li>最近的一些工作也提出了有前途的方法，可以使NeRF表示用于工程模拟，如[62]和[63]。</li>
</ul>
<h2 id="SCADA-systems"><a href="#SCADA-systems" class="headerlink" title="SCADA systems"></a>SCADA systems</h2><p>supervisory control and data acquisition</p>
<p>建模制造过程的图形工具不仅在生产开始前的计划阶段是必不可少的，而且对于持续的监督也是必不可少的。在许多工业环境中，人为操作员的连续监测和控制是确保最佳运行的必要条件。这种监测和控制通常通过监控和数据采集(SCADA)系统来实现，SCADA系统采用图形界面，实现系统监测和控制功能。用于SCADA接口的图形范围从常用的2D交互式原理图到三维图形。在某些情况下，为了准确地理解系统的状态，或者至少，它有助于提高操作员控制系统的能力，结合三维视觉变得势在必行。<br>尽管前面提到了3D SCADA图形界面的优点，但2D界面比3D界面更容易设计。这是因为2D界面不需要创建复杂的3D多边形模型、控制器和动画。<br>使用nerf，可以大大简化创建描述受控系统的3D场景。由于SCADA系统呈现交互式图形，可以直观地反映受控系统状态的变化，因此应该使用具有<strong>可修改场景的NeRF变体</strong>。例如，当操作员远程开关一件工业机械时，其颜色，照明或类似的指示应该改变。</p>
<ul>
<li>一些NeRF变体可以对3D场景进行控制修改，一个值得注意的例子是CodeNeRF[41]。CodeNeRF允许在3D场景中精确地改变物体的纹理和形状。</li>
<li>CLIP-NeRF[64]通过调节文本或图像输入来促进NeRF场景的修改。它利用来自对比语言图像预训练(CLIP)模型的预先存在的嵌入，该模型为语言和图像学习联合嵌入空间。</li>
<li>CLIP-NeRF的一个潜在应用是能够使用参考图像快速将真实世界的变化引入NeRF场景。例如，新产品原型的照片可以提供比单独的文本更详细的信息，以准确描述新的对象属性。通过使用CLIP-NeRF，这些参考图像可以作为修改NeRF场景的宝贵输入，以纳入所需的变化。</li>
<li>动态nerf提供了一种替代方法来表示各种受控机械状态。他们通过显示3D物体运动序列的不同片段或单个冻结时间实例来实现这一点。例如，考虑风车的3D表示:使用动态NeRF，当查询单个时间瞬间的动态NeRF时，叶片可以被描述为静态或运动，反映现实世界的风车状态。</li>
</ul>
<h2 id="Training-simulations-for-workforce"><a href="#Training-simulations-for-workforce" class="headerlink" title="Training simulations for workforce"></a>Training simulations for workforce</h2><p>先进的3D模拟器用于在工业环境中培训工人。这些模拟器的范围从简单的键盘和鼠标控制，普通的计算机显示器到虚拟现实(VR)、增强现实(AR)和物理控制器，这些控制器模仿了反映现实世界交互的目标应用程序的物理控制。<br>AR和VR培训在工业维护和装配任务中的有效性在[65]中进行了评估，并与现实世界的培训进行了比较。<strong>研究发现，应该鼓励针对此类任务的AR培训，但VR需要进一步评估</strong>。然而，许多任务和情况过于昂贵、危险或罕见，无法在现实世界中进行训练。例如，化工行业的操作员在虚拟环境中接受培训[66]。在这种情况下，VR培训非常有用，因为没有有效的替代方法。</p>
<ul>
<li>结合精确控制的传统渲染图形和NeRF不仅可以用于训练机器学习模型，如NeRF2Real[67]所示，还可以用于训练人类。</li>
<li>人的中心视觉比低敏锐度的周边视觉更敏锐。这是由于视网膜的中心区域在一个叫做中央凹的区域含有密集排列的光锥。对于人类在VR环境中实时使用的nerf，传统的渲染和亮度场表示方法会不必要地渲染图像的所有区域，包括那些用于人类视觉外围部分的区域，从而浪费资源。FoV-NeRF[68]解决了这一问题，<strong>提出了将图像的质量调整到最高的中心视觉和最低的周边视觉</strong>，从而呈现出质量不均匀的图像。这种方法降低了劳动力培训或沉浸式SCADA系统的VR成本和延迟。</li>
</ul>
<h2 id="Training-simulations-for-machine-learning"><a href="#Training-simulations-for-machine-learning" class="headerlink" title="Training simulations for machine learning"></a>Training simulations for machine learning</h2><p><strong>Transfer of skills</strong> acquired in simulation:</p>
<ul>
<li>NeRF2Real[67]方法展示了在模拟环境中使用NeRF进行渲染。第一步是创建真实机器人环境的NeRF表示，其中提取障碍物进行碰撞建模，并与合成物体(包括动态球)集成。此外，利用NeRF作为机器人的视觉输入，并与合成的动态球目标相结合。随后，一个20自由度的人形机器人在这个环境中通过强化学习学会了推球的任务。</li>
</ul>
<p>Capture of environment for training of other <strong>navigation algorithms</strong>: </p>
<ul>
<li>在导航算法的背景下，经过训练的智能体从nerf中获得视图渲染输出，而不是与真实的3D环境交互或依赖于传统的3D环境多边形渲染。这是通过向nerf提供有关其模拟动作所导致的代理位置和视角变化的信息来实现的[69]。</li>
<li>通过将环境表示划分为重叠的NeRF块(称为block-NeRF)，这种能力甚至可以扩展到大规模环境，例如整个城市[70]。</li>
<li>使用nerf，还可以修改场景属性，例如实现任意场景重照明。这极大地扩展了为给定的3D场景生成的逼真2D图像样本的种类和数量。例如，以文本描述为条件的模型可以对场景中的对象材料进行任意更改，从而实现具有成本效益的训练环境修改以增强多样性[71]。</li>
</ul>
<p>Extracting quantitative physical properties:</p>
<ul>
<li>基于NeRF的场景中物体的物理参数提取得到了有趣的结果，如[72]所示。作者证明，<strong>通过NeRF应用可以获得摩擦角、粘度和杨氏模量等物体的动态特性</strong>。这些信息可以收集起来供以后在模拟中使用，机器人可以纯粹作为数据收集代理或直接用于实时机器人预测能力和决策过程。</li>
</ul>
<h2 id="Positioning-and-navigation"><a href="#Positioning-and-navigation" class="headerlink" title="Positioning and navigation"></a>Positioning and navigation</h2><p>有几种方法可用于工业定位和导航[73]。这些包括超宽带(UWB)定位、全球定位系统(GPS)、差分GPS和simultaneous localization and mapping(SLAM)。然而，这些方法都有缺点，如由于信号飞行时间计算不精确和误差累积而导致精度有限。此外，在基于激光雷达的SLAM情况下，点云数据的设备成本和数据存储/传输要求可能很高。</p>
<p>基于神经辐射场的SLAM方法包括束调节神经辐射场(BARFs)[74]。BARF能够对NeRF学习到的表示和训练样本(图像)的相机姿态进行从粗到精的配准，并同时优化。这使得NeRF训练不完美或缺失的相机姿势。<br>SLAM是一种需要对环境进行3D映射的技术。因此，可以重建3D场景的方法，如NeRF，通常对SLAM有用。Sun等人的研究是SLAM神经三维表面重建方法的一个例子[75]。他们提出了一种单目时间相干3D重建技术，该技术使用时间窗口来捕捉场景的时间一致性，而不是重建和合并单个帧<br>NICE-SLAM[76]对几何图形使用单独的粗、中、细分层特征网格，对外观使用另一个特征网格。受NeRF启发的SLAM方法的一个例子是NeRF-SLAM[77]，它使用基于即时ngp的分层体NeRF地图<br>在Zhu等人[78]的另一项工作中，为城市规模的NeRF引入了一种名为LATITUDE(截断动态低通滤波器的全球定位)的定位方法。该方法采用了从粗到精的定位策略。首先，使用nerf生成的样本图像训练的回归量来获得初始估计点，以优化位置估计。为了降低陷入局部极小值的风险，引入了截断动态低通滤波器(TDLF)作为解决方案。</p>
<h2 id="3D-scanning-and-tomography"><a href="#3D-scanning-and-tomography" class="headerlink" title="3D scanning and tomography"></a>3D scanning and tomography</h2><p>3D扫描是一项多功能技术，在不同的行业有不同的应用。一些例子是:分析采矿或制造场所的地形，监测和管理资源，如生物质[79]，检查问题和缺陷，以及执行SLAM。3D扫描还可以实现高价值的定制和与现有对象的兼容性。例如，它可以促进传统或第三方组件的更快逆向工程，定制假肢，以及为高价值产品或设备(例如，货船)的定制修复进行精确的损坏几何测量。<strong>然而，存储和处理激光雷达数据仍然是一个挑战</strong>。典型的激光雷达扫描可以包含数百万个离散点，在大面积上扫描产生的数据集可以被认为是大数据[80]，[81]。对于大规模体积估计，从感兴趣区域的一小部分获得的统计数据通常被外推到整个区域[79]。</p>
<p>3D场景几何知识具有广泛的应用，例如从小型组件到大型建筑物的物体的3D扫描，并使机器人能够与其环境进行交互(例如，避免障碍物和抓地力规划)。估计几何形状的一种方法是直接查询NeRF的体积密度，这已被证明与物质的存在有很好的相关性[69]。或者，可以使用立方体推进或基于符号距离场的方法提取障碍物网格，如<a target="_blank" rel="noopener" href="https://readpaper.com/paper/4734289236769914881">NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes</a> ，这可以提高提取几何形状的质量。</p>
<p>对于一些具有挑战性的情况，例如透明物体[83]，nerf在深度估计方面提供了显著的改进。这证明了与其他技术相比，神经网络在学习推断物体几何形状所需的微妙视觉线索方面的优势。此外，可以从模糊图像中获得3D场景，NeRF在训练过程中消除了模糊性[84]。这种方法可以为NeRF训练提供更快的数据采集，而不需要能够捕获运动中清晰图像的特殊设备，例如，允许通过在低空快速移动的无人机收集大面积图像。本小节中讨论的几何估计问题在地理空间行业中特别常见，并由NeRF研究解决(例如，[85])。</p>
<p>几种基于nerf的方法是专门为人体跟踪和姿态估计而设计的。与工业环境中的大多数其他物体不同，人类是高度移动的，他们的运动可能是不可预测的。这些人的流动性特点需要准确、快速的实时跟踪和理想的预测方法。[86]从单目视频中对人体3D姿势进行了细粒度跟踪，包括视频中不可见的几何部分。</p>
<p>机器人设计的一种具体方法是创建强调人与机器人之间协作的协作机器人(collaborative robots)[87]。这种协作机器人需要对人类运动进行更细致的跟踪和预测，而不是仅仅为了躲避或松散合作而跟踪人类。最近，一些工作集中在基于nerf的人体捕获和跟踪上，这有助于解决在协作机器人设置中实现精确人机交互的挑战。例如，[88]介绍了一种基于nerf的方法，通过从稀疏的多视图图像集重建给定的人体，来渲染新颖的人体姿势和新颖的3D场景视图。</p>
<p>断层扫描在工业环境中有各种应用，如探伤、失效分析、计量、装配分析和逆向工程。断层扫描也被用于新的任务，如无人机辅助扫描气体羽流的空间分布[89]。如文献[90]所示，将nerf应用于x射线扫描仪获得的sinograph数据可以获得最先进的3D重建结果，其中超过扫描仪分辨率的缩放称为超分辨率(分辨率提高8倍)，实现了从少量样本重建和从45度范围内的有限角度断层扫描重建。</p>
<h2 id="Other-areas"><a href="#Other-areas" class="headerlink" title="Other areas"></a>Other areas</h2><p>Dense object descriptors: 论文[91]提出了一种创建密集对象描述符的方法，该方法可用于点对应和关键点检测。这种方法对于薄物体和反射物体的6自由度拾取和放置操作特别有用，这在过去一直具有挑战性。所提出的方法明显优于最近的现成解决方案，如GLU-Net[92]、GOCor[93]和PDC-Net[94]。</p>
<p>Point-set registration : 在地理空间和医疗领域等不同行业中，经常需要对来自不同来源或时间的视觉数据进行对齐。这种对齐可以识别和比较不同图像中3D对象或场景上的相同点或区域。例如，多次断层扫描可用于评估随时间的生理变化。点配准是实现这种对齐的一种方法。Goli等[95]使用从nerf中提取的表面场来实现成对点配准。这些表面场不受摄像机视点变化的影响。然后，优化过程找到对齐表面场的刚性转换，确保相应视觉数据的对齐</p>
<p>Synthetic Perturbations for <strong>Augmenting Robot Trajectories</strong> via NeRF (SPARTN)[96]是一种机器人操作训练方法，它使用NeRF为手眼机器人操作任务的专家演示生成摄动和相应的纠正动作。这将模仿学习性能提高了2.8倍。</p>
<p>Synthetic dataset generation for supervised learning :Neural-Sim中提出了一种使用合成nerf生成的图像来训练视觉模型的方法[97]。合成图像的变化包括姿势、变焦和照明。实现了对用于训练的数据的精确控制，并且有目的地生成了最容易导致视觉模型损失的图像。因此，针对特定领域的图像调整的高质量视觉模型可以在不收集许多潜在冗余且变化有限的真实世界样本的情况下获得</p>
<p>Learned semantic relationships: 使用JacobiNeRF[98]中提出的方法的NeRF可以学习场景实体(如对象甚至单个像素)之间的底层抽象语义关系。当其权重沿场景中特定点的梯度扰动时，这样的NeRF在其输出中表现出语义关联共振。例如，如果墙壁上的单个点的亮度发生变化，则给定墙壁上其他点的亮度也会发生变化，而场景中的其他点保持不变。一个JacobiNeRF应用程序正在减少注释负担，因为注释可以传播到语义链接点。这种方法通常用于实体选择和场景修改。</p>
<h1 id="REQUIREMENTS-FOR-PROOF-OF-CONCEPTEXPERIMENTS"><a href="#REQUIREMENTS-FOR-PROOF-OF-CONCEPTEXPERIMENTS" class="headerlink" title="REQUIREMENTS FOR PROOF-OF-CONCEPTEXPERIMENTS"></a>REQUIREMENTS FOR PROOF-OF-CONCEPTEXPERIMENTS</h1><p>我们进行了定量实验，以测试我们的一些新颖的NeRF应用在我们感兴趣的领域的可行性，并研究实现所需的方法。<br><strong>第一个实验</strong>检查了不同设置(如分辨率和压缩质量预设)下的<strong>数据节省和压缩质量之间的权衡</strong>。我们在这个实验中使用了<strong>Instant-NGP</strong>[99]，一种快速的NeRF变体，因为它可以通过不同的实验配置进行快速迭代。<br><strong>第二个实验</strong>探讨了使用<strong>动态nerf进行预测导航</strong>。我们选择了<strong>D-NeRF</strong>[44]，这是一种动态NeRF变体，可以重建高质量的动态场景。它的显著优点是一个现有的开源实现，方便了动态场景实验的快速和可重复设置。</p>
<h2 id="A-Mathematical-representation"><a href="#A-Mathematical-representation" class="headerlink" title="A. Mathematical representation"></a>A. Mathematical representation</h2><p>我们提出了我们在实验中使用的两种算法——Instant-NGP and dynamic NeRF,的数学模型。我们还解释了这些NeRF变体中使用的轴对齐边界框(AABB)长方体，通过限制渲染体积来加快渲染过程。</p>
<p>1) 主要定义:在数学中，域是定义一些基本代数运算行为的集合。神经域是一个可以通过神经网络参数化的域。</p>
<p>NeRF用于从一组2D图像中建立3D场景重建模型。重建是一个神经场，表示为 $\Phi:X\rightarrow Y$，将每个$\mathbf{x}_{\mathrm{recon}}\in X$映射到对应的场量$\mathbf{y}_{\mathrm{recon}}\in Y.$。相机传感器图像观测也可以表示为一个field$\Omega:S\rightarrow T$，该field将传感器坐标S, $\mathbf{x}_{\mathrm{sens}}\in S$映射到测量值T, $\mathbf{t}_{\mathrm{sens}}\in T.$。正向映射是两个函数$F:(X\rightarrow Y)\rightarrow(S\rightarrow T).$之间的可微映射。</p>
<p>因此，如果forward map是可微的，我们可以求解以下优化问题[100]来寻找神经场Φ: $\operatorname{argmin}_W\int_{X\times S}||F(\Phi(\mathbf{x}_{\mathrm{recon}}))-\Omega(\mathbf{x}_{\mathrm{sens}})||.$<br>W为Φ的参数。神经域的一般定义也适用于各种非nerf表示，例如神经符号距离域</p>
<p>缩小到NeRF，从数学的角度来看，NeRF特定的$x_{recon}$是一个5元组(x, y, z， θ， φ)一个点的三维直角坐标x, y, z坐标沿着射线和视角θ， φ在球坐标系中采样，这决定了从相机发出的射线的方向。NeRF-specific $y_{recon}$是一个4元组(r, g, b， σ)，其中r, g和b是沿特定方向的射线采样的特定点的RGB亮度颜色分量，σ是该给定点的光密度。因此，NeRF是一个将5维xrecon向量转换为另一个4维xrecon向量的函数。这个映射可以表示为:    $F(W):(x,y,z,\theta,\phi)\implies(r,g,b,\sigma),$</p>
<p>在体绘制过程中，光线通过每个像素发射，以计算结果像素的颜色。只有落在AABB边界体内的射线段才用于渲染计算。在近、远射线界$t_n$和$t_f$内，由射线原点和方向d在射线上的点t处定义的相机射线r(t) = 0 + td的期望颜色C(r)定义为:$C(\mathbf{r})=\int_{t_n}^{t_f}T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t),\mathbf{d})dt,$</p>
<p>其中，c为射线线段上某一点的光线角度d所决定的发射颜色，参数化为参数t。函数T (t)为透射率，它决定了单个c值对总c (r)的贡献。</p>
<p>2) Instant Neural Graphics Primitives:</p>
<p>使用NeRF渲染完整图像的一个主要问题是计算时间。自从最初的NeRF文件采用NeRF方法以来，已经进行了不同的尝试来加快这一进程。即时神经图形原语(Instant neural graphics primitives, Instant NGPs)[101]试图减少层数，并以指数方式减少MLP计算中的计算次数。这背后的思想是考虑两个参数:参数的数量T和我们想要获得$N_{max}$的目标分辨率。所谓的多分辨率哈希编码不仅尝试像通常的MLP一样训练权重，还尝试训练参数。每一层都是独立的，网格顶点处的特征向量，其分辨率被选择为最差和最清晰分辨率之间的几何级数[Nmin, Nmax]<br>$N_{l}=N_{\mathrm{min}}bl,$<br>$b=\exp\left(\frac{\ln N_{\max}-\ln N_{\min}}{L-1}\right),$b取决于层数和attempted分辨率。</p>
<p>3) D-NeRF</p>
<p>动态NeRF (D-NeRF)是一种能够3D动态场景捕获的NeRF变体。由于额外的时间维度，动态NeRF变量的映射必须从Eq. 2修改为以下形式:</p>
<p>$F(W):(x,y,z,\theta,\phi,t)\implies(r,g,b,\sigma).$</p>
<p>虽然学习从6D到4D的直接映射的模型可以用于该任务，但[44]的结果表明，将映射分解为两个独立的函数可以减少计算时间。具体来说，一个MLP $Φ_x$作为动态场景的单个时间瞬间的参考静态场景表示，通常为t = 0。然后，另一个MLP $Φ_t$学习受时间限制的规范场景的动态场景变形。因此，两个mlp都在感兴趣的时间间隔内学习场景的外观。</p>
<p>4) AABB representation</p>
<p>一般来说，AABB是一个与坐标轴对齐的二维矩形边界区或三维长方体边界体。它用于简化模拟任务的计算，如碰撞检测或射线相交。在nerf中，使用长方体AABB来切断AABB边界处的射线传播。<br>AABB可以用两个参数来定义:一个中心点和一个半延伸向量。中心点具有三维笛卡尔坐标$(X_0,Y_0,Z_0)$并指定长方体的位置。半延伸向量具有分量(X,Y,Z)，并指定长方体在每个方向上从中心点延伸的距离。因此，长方体的宽度为2X单位，高度为2Y单位，深度为2Z单位。综上所述，范围向量及其中心唯一地定义了AABB，可以用于渲染加速。</p>
<h2 id="B-Evaluation-metrics"><a href="#B-Evaluation-metrics" class="headerlink" title="B. Evaluation metrics"></a>B. Evaluation metrics</h2><p>我们考虑完善的计算机视觉领域评估指标:PSNR和SSIM。</p>
<p>PSNR是最大信号功率与噪声功率之比，单位为dB。计算PSNR既需要噪声破坏前的原始图像，也需要噪声破坏后的图像。利用噪声图像的均方误差(MSE)来计算噪声功率分量。对于尺寸为m x n像素的灰度图像，MSE的计算方法如下[102]</p>
<p>$MSE=\frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i,j)-K(i,j)]^{2},$</p>
<p>其中I和K分别是描述原始图像和噪声图像的函数，通过将以i and j为索引的像素映射到信号值。对于灰度图像，信号值决定了整体像素亮度，对于彩色图像，信号值决定了RGB颜色通道亮度值。对于RGB颜色空间中的彩色图像，MSE只是在所有三个颜色通道上计算。</p>
<p>PSNR的计算公式如下:$PSNR=10\cdot\log_{10}\left(\frac{MAX_{I}^{2}}{MSE}\right),$<br>$MAX_{I}$表示原始无噪声图像中的峰值信号值。<br>PSNR通常以dB表示，可计算为: $PSNR(dB)=20log_{10}(MAX_I)-10log_{10}(MSE).$</p>
<p>SSIM是基于图像的亮度、对比度和结构来衡量两幅图像之间的结构相似性。与PSNR相反，该指标考虑了人类视觉感知i的具体情况，并提供了更接近普通人的质量评估函数。它是通过比较图像中像素的局部斑块(矩形窗口)来计算的。其计算公式如下式[102]<br>$\mathrm{SSIM}(x,y)=\frac{\left(2\mu_x\mu_y+c_1\right)\left(2\sigma_{xy}+c_2\right)}{\left(\mu_x^2+\mu_y^2+c_1\right)\left(\sigma_x^2+\sigma_y^2+c_2\right)},$</p>
<p>式中，x、y为两个大小为N × N的图像块，μx、μy为x、y的平均像素值，σ2x、σ2y为x、y的方差，σxy为x and y的协方差，c1、c2为分母趋近于零时避免不稳定的两个常数。SSIM的取值范围是-1 ~ 1,<strong>1表示完全相似，-1表示完全不相似</strong>。</p>
<h1 id="PROOF-OF-CONCEPT-I-UAV-VIDEO-COMPRESSION-USING-INSTANT-NGP"><a href="#PROOF-OF-CONCEPT-I-UAV-VIDEO-COMPRESSION-USING-INSTANT-NGP" class="headerlink" title="PROOF-OF-CONCEPT I.: UAV VIDEO COMPRESSION USING INSTANT-NGP"></a>PROOF-OF-CONCEPT I.: UAV VIDEO COMPRESSION USING INSTANT-NGP</h1><p>我们提出的第一个新颖的NeRF应用是基于NeRF的视频压缩技术。这种方法只对nerf生成的具有给定相机姿势的场景的预期2D表示与实时捕获的实际帧之间的差异进行编码。这种增强的视频压缩降低了传输所需的比特率，因为只有关于摄像机姿态的数据和真实帧与生成帧之间转换的信息会通过网络发送。我们在虚拟工业3D场景中进行了实验，并测量了在这种情况下使用我们的压缩方法传输视频流的无人机所能实现的压缩节省。</p>
<h2 id="A-Proposed-NeRF-based-compression"><a href="#A-Proposed-NeRF-based-compression" class="headerlink" title="A. Proposed NeRF-based compression"></a>A. Proposed NeRF-based compression</h2><p>虽然广泛使用的视频压缩算法使用光流来编码帧间的差异，但由于一些帧的变化不能用前一帧的视觉流变换来描述，因此限制增加了需要添加到每一帧的信息量[103]。另一方面，nerf可以提取通过广泛的3D转换获得的视图信息。<br>由于NeRF对完整3D场景的了解，与传统的视频压缩相比，它具有之前所有编码帧中未见的像素知识。例如，当视图挫折移动到视频中显示这些时，视觉流无法访问视图边缘以外的未见像素的信息。同样，当物体在旋转过程中被遮挡的部分变得可见时，它也没有关于这些部分的信息。<br>重要的是，经过训练的NeRF的权重比为学习的3D场景生成的单个2D视图图像占用的数据存储空间更少[16]。<br>我们所有的实验都使用了H.264压缩算法。在H.264中，压缩帧流包括所谓的I帧和P帧。I帧包含完整的图像信息，P帧仅编码I帧中编码的图像与其时间相邻图像之间的差异数据[104]。在压缩视频中，每n帧是一个I帧，在压缩开始前n通常被确定为一个常数。<br>在给定场景上对NeRF进行预训练后，压缩视频中通常存储在第一帧中的整帧信息可以被省略，NeRF根据需要仅从摄像机姿态信息中生成，所需数据少于1kb。对于网络上的视频流，发送方和接收方都使用NeRF在感兴趣的场景上训练的副本，因此两者都可以使用NeRF生成的图像而不是发送I帧。<br>适合无人机用例场景的网络架构包括在MEC网络中分配计算负载，以便将nerf相关的计算从无人机卸载到边缘服务器。我们的压缩方法和MEC网络架构的设计如图2所示。</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821160422.png" alt="image.png"><br><em>(a)无人机摄像机捕捉环境。相机的真实帧和姿态被无线传输到附近的多址边缘计算(MEC)服务器。(b) MEC服务器采用NeRF模型进行基于相机姿态的新视图合成。H.264编解码器对real帧和NeRF帧进行编码，得到包含它们差异的P帧，并随姿态通过网络传输。(c)接收器使用H.264编解码器从P帧和从相机姿态生成的本地NeRF帧重建真实帧。</em></p>
<h2 id="B-Experimental-design-and-results"><a href="#B-Experimental-design-and-results" class="headerlink" title="B. Experimental design and results"></a>B. Experimental design and results</h2><p>首先，我们在表2中列出了所有重要的仿真参数。作为NeRF模型，使用的是Instant-NGP[99]，没有任何超参数改变或架构修改。我们准备了一个实验来证明新型基于NeRF的压缩算法在图2所示的工业3D场景中的性能，并通过在Blender 3D建模软件中<strong>渲染工业3D场景来生成NeRF的训练数据集</strong>。</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821161057.png" alt="image.png"></p>
<p>实验示意图如图3所示，其中一架无人机带着相机在虚拟环境中飞行，并对沿其轨迹捕获的帧进行索引方案。该图还说明了NeRF在帧重建过程中引入的一些质量退化伪影。</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821161801.png" alt="image.png"><br><em>一架无人机摄像机在虚线无人机轨迹上的点捕捉图像。图像的索引从0到158，这种索引也在图5-7中使用。索引为10的帧SSIM最差，如图5所示。从这个框架裁剪的区域，通过NeRF重建，被显示为突出SSIM退化的原因。</em></p>
<p>图4显示了代表PSNR指标的四条测量曲线，它评估了NeRF重建3D场景视图的质量与原始视图质量的比较。这些测量是从模拟无人机轨迹期间捕获的单个帧中获得的，每个帧分配一个index范围从0到158。PSNR曲线对应于表2中列出的不同分辨率。值得注意的是，不同指标的PSNR值不同，表明重建质量不同。有趣的是，当尝试更高的分辨率时，可实现的PSNR会降低。然而，值得一提的是，<strong>在500和720分辨率下，PSNR值是相似的，在这个分辨率范围内，PSNR和分辨率之间表现出更好的权衡</strong>。</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821161912.png" alt="image.png"><br><em>用PSNR (dB)测量了无人机沿三维场景轨迹的NeRF帧重建质量。</em></p>
<p>图5显示了对应的一组分辨率目标的SSIM。SSIM测量值不同;然而，整体行为保持一致，所有分辨率都遵循与PSNR相似的趋势。此外，<strong>在500和720分辨率下</strong>实现的SSIM也有相似之处，这进一步支持了这样一个概念，<strong>即在SSIM中无需做出重大妥协就可以实现更高的分辨率</strong>。</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821162121.png" alt="image.png"><br><em>利用SSIM测量了无人机三维场景沿轨迹的NeRF帧重构质量</em></p>
<p>图6显示了不同分辨率下压缩节省的百分比。我们的方法的压缩节省是相对于H.264压缩得到的一组帧对I到P帧来计算的。我们使用公式100 <em> Isize/(Isize + Psize)来计算压缩节省，其中Isize和Psize分别是I帧和P帧的大小。由于我们不通过网络传输I帧，而是使用NeRF生成它，因此<em>*Isize占Isize + Psize总数的百分比表示节省</em></em></p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821162214.png" alt="image.png"><br><em>相对于H.264的压缩节省实现了单个图像沿无人机轨迹通过3D场景。</em></p>
<p>我们绘制了通过所提出的压缩方法沿无人机轨迹对不同帧分辨率从300 ×168到1920 × 1080(全高清)的159个连续帧所获得的压缩增益。主要结果是压缩节省与帧分辨率成反比。对于1920x1080分辨率，压缩节省百分比的值从45- 48%到300x168分辨率时的66- 74%不等。<strong>低分辨率节省的传输数据更多</strong></p>
<p><strong>分辨率和压缩节省百分比的反比关系到nerf生成图像的缺陷</strong>。已经观察到，原始和nerf生成的图像的高频信号成分在降尺度期间丢失。这种现象可以看作是有损压缩的一种形式，通过去除NeRF引入的高频伪影，使NeRF生成的图像更接近地面真实视图。SSIM的行为支持这些观察结果。它以一种似乎与压缩节省相关的方式随着分辨率的降低而提高。对比，分别见图5和图6对应的SSIM和压缩特性。<br>我们使用的分辨率范围低端的相对较低的值适用于多个重要场景。如果视频用于视觉机器学习下游模型，例如cnn，由于内存限制，目前最先进的预训练模型通常只能以相对较低的分辨率工作，最高可达800 × 600。</p>
<h2 id="C-Computational-and-energy-tradeoffs-of-NeRF-based-video-compression"><a href="#C-Computational-and-energy-tradeoffs-of-NeRF-based-video-compression" class="headerlink" title="C. Computational and energy tradeoffs of NeRF-based video compression"></a>C. Computational and energy tradeoffs of NeRF-based video compression</h2><p>我们的实验表明，与传统的压缩技术相比，<strong>使用NeRF可以实现显著的压缩节省，但代价是更高的计算和能源需求</strong>。这可以通过设计合适的网络体系结构(如MEC)来减少，该体系结构将计算资源放置在通信端点附近<br>虽然这样的配置要求无人机在没有我们的压缩方法的情况下向MEC服务器发送单个无人机视频流，但从服务器发出的流量仍然明显较低。这offloads了工厂网络节点，这些节点聚合了来自多个链接的流量，通常会经历最高的网络负载，例如，许多这样的无人机摄像头流。在网络边缘本地提供高吞吐量和低延迟通常成本更低，也更容易，在那里不存在这种流量聚合的问题。<br>根据发送方和接收方的计算能力，有多种可能的计算卸载配置。NeRF计算可以卸载到靠近压缩视频发送方、接收方或两者的边缘服务器。如果发送方或接收方不受电池或计算资源的限制，则不需要卸载。然而，由于在我们的模拟场景中，发送方是一个电池受限的无人机，<strong>因此将基于nerf的压缩工作负载卸载到边缘服务器MEC并发送高吞吐量流是有益的</strong>。</p>
<h1 id="PROOF-OF-CONCEPT-II-OBSTACLE-AVOIDANCE-USING-D-NERF"><a href="#PROOF-OF-CONCEPT-II-OBSTACLE-AVOIDANCE-USING-D-NERF" class="headerlink" title="PROOF-OF-CONCEPT II. : OBSTACLE AVOIDANCE USING D-NERF"></a>PROOF-OF-CONCEPT II. : OBSTACLE AVOIDANCE USING D-NERF</h1><p>关于动态NeRF概念验证实验，我们在重复机械手臂运动的3D动画图像上训练<strong>D-NeRF</strong>。由于这些动作的重复性，D-NeRF可以根据过去的手臂位置来预测未来的手臂运动。</p>
<h2 id="A-Proposed-D-NeRF-based-obstacle-avoidance"><a href="#A-Proposed-D-NeRF-based-obstacle-avoidance" class="headerlink" title="A. Proposed D-NeRF-based obstacle avoidance"></a>A. Proposed D-NeRF-based obstacle avoidance</h2><p>对于D-NeRF中动态场景的单个时间瞬间，有多种场景几何提取方法，可用于避障。由于诸如光线行进之类的方法相对较慢，我们对一组光线使用视差图。视差图是对一对立体图像之间的视距离或像素运动的度量，可用于估计场景的深度。<strong>视差图因此帮助我们估计与给定像素相交的光线与最近障碍物表面相交的距离</strong>。由于使用这些信息来检测或预测与无人机边界体的碰撞是微不足道的，因此我们从实验中省略了碰撞检测，并直接关注生成的视差图的效用和质量。<br>运动预测需要与实时手臂3D姿势同步，并将其与学习到的动态场景时间轴上的过去姿势相匹配。几种超出我们工作范围的方法可用于姿态确定，例如[105]。我们的目标是展示机器人手臂和移动机器人(在我们的实验中，无人机)之间避免碰撞的可能性。这个用例显然可以扩展到其他具有重复移动模式的工业过程。我们基于d - nerf的避障设计如图7所示。</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821170702.png" alt="image.png"><br><em>a)具有特定相机姿态的无人机在t时刻捕获目标b) D-NeRF模型以RGB图、不透明度图和视差图的形式输出指定时间t相机姿态的新视图，如图8所示。c)不透明度图和视差图是无人机进行避障或路径规划的关键组件。</em></p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821170743.png" alt="image.png"><br><em>从代表机器人手臂的D-NeRF模型中提取的所有地图，在t1 = 0.05和t3 = 0.95两个时间瞬间进行渲染。a) RGB地图描述了新的视图合成。b)不透明度图显示光线通过环境的传输，环境可以是透明的、部分不透明的或完全不透明的。c)处理后的视差图以热图的形式捕捉这些时刻重复运动时的机械臂表面深度。</em></p>
<h2 id="B-Experimental-design-and-results-1"><a href="#B-Experimental-design-and-results-1" class="headerlink" title="B. Experimental design and results"></a>B. Experimental design and results</h2><p>首先，我们在III中列出了所有重要的仿真参数。在D-NeRF训练中，利用环绕机械臂的圆形轨迹的虚拟摄像机收集绘制的机械臂动画视图，模拟无人机的飞行轨迹。为了获得可用于碰撞检测的描述机械臂表面的深度数据，考虑了D-NeRF生成的视差图。由训练D-NeRF生成的原始视差图包含一定数量的视觉噪声，可见为不规则点。它们离机械臂的轮廓既远又近。有几种可能的方法来去除这种噪声[106]，[107]，[108]，对于这个特殊的用例，我们采用了[109]中提出的图像处理技术。我们演示了训练好的D-NeRF渲染，噪声去除蒙版，并在t1 = 0.05和t3 = 0.95两个时间瞬间获得深度数据。</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821171943.png" alt="image.png"></p>
<p>图8a显示了这些时间瞬间的D-NeRF渲染。为了得到最终的清理后的视差图，如图8c所示，我们首先生成了一个不透明度图，它比视差图提供了更高水平和质量的手臂轮廓细节，但也包含了较小的噪声。然后，我们对不透明度图应用简单的侵蚀和扩张OpenCV操作序列，以创建如图8b所示的干净的不透明度蒙版，该蒙版可用作裁剪蒙版来裁剪原始的噪声视差图。不透明度地图侵蚀去除了大部分不属于机械臂轮廓的噪声，<strong>因为机械臂是场景中最大的连续物体</strong>。在膨胀步骤中读取了机械臂轮廓上的大部分像素。我们把去噪后的不透明度贴图裁剪出来的视差贴图称为处理后的视差贴图。</p>
<p>D-NeRF生成的视差图的质量以PSNR衡量，相对于通过动态场景的连续帧的训练、测试和验证轨迹的地面真实视差图，如图9所示。为了可视化多模态数据分布，采用核密度估计，训练、测试和验证数据集分别用灰色、蓝色和黄色表示。可视化中较宽的部分表示存在某些值的可能性较高。蓝点表示中位数，水平线表示平均平均值，箱形图描述了四分位数范围，可以深入了解分布的中间部分的分布情况。我们可以看到，<strong>包含在D-NeRF训练集中的帧对两个测量的质量指标偏离基本事实的程度较小</strong>。</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821171222.png" alt="image.png"><br><em>使用PSNR度量比较Blender和D-NeRF图像之间的视差图。</em></p>
<p>图10所示的结果表明，与PSNR相比，SSIM测量的深度重建平均质量明显更高。对于上下文中，重要的是要注意哪些值被认为对SSIM和PSNR有利。SSIM的取值范围是0.97到1.0，而PSNR的取值范围是30 dB到50 dB。</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821171347.png" alt="image.png"><br><em>使用SSIM度量比较Blender和D-NeRF图像之间的视差图。</em></p>
<p>在避免碰撞的背景下，SSIM度量对大型结构重建质量的关注变得至关重要。这是由于碰撞边界体积，它只能粗略地近似障碍物的几何形状，因此提供了一个可容忍的误差幅度，以应对较小的深度估计偏差。PSNR指标对这种小尺度偏差更为敏感;然而，避免碰撞很少需要完美的精度。此外，未来的研究可以探索PSNR和SSIM之间的差异如何影响其他特定的下游任务，从而为进一步的研究提供有价值的见解。</p>

    </div>

    
    
    
        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://t.me/+1ZnNxFWnEbk5YzRl">
            <span class="icon">
              <i class="fab fa-telegram"></i>
            </span>

            <span class="label">Telegram</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/NeRF/" rel="tag"><i class="fa fa-tag"></i> NeRF</a>
              <a href="/tags/Review/" rel="tag"><i class="fa fa-tag"></i> Review</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/NeRF/PointCloud/3D%20Gaussian%20Splatting/" rel="prev" title="3D Gaussian Splatting">
      <i class="fa fa-chevron-left"></i> 3D Gaussian Splatting
    </a></div>
      <div class="post-nav-item">
    <a href="/Learn/Learn-Colmap/" rel="next" title="Learn-Colmap">
      Learn-Colmap <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Conclusion"><span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AIR"><span class="nav-text">AIR</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-text">Introduction</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NEURAL-RADIANCE-FIELDS-NERFS-FUNDAMENTALS-AND-WORKFLOW"><span class="nav-text">NEURAL RADIANCE FIELDS (NERFS): FUNDAMENTALS AND WORKFLOW</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CLASSIFICATION-OF-DERIVATIVE-NERF-IMPLEMENTATIONS"><span class="nav-text">CLASSIFICATION OF DERIVATIVE NERF IMPLEMENTATIONS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Latent-vectors-for-sample-efficiency-and-scene-modification"><span class="nav-text">Latent vectors for sample efficiency and scene modification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Performance"><span class="nav-text">Performance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dynamic-scene-reconstruction-with-NeRF"><span class="nav-text">Dynamic scene reconstruction with NeRF</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Other"><span class="nav-text">Other</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CURRENT-STATUS-OF-INDUSTRY-AND-ADVANTAGES-OFNERF-ALTERNATIVES"><span class="nav-text">CURRENT STATUS OF INDUSTRY AND ADVANTAGES OFNERF ALTERNATIVES</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CAE-Computer-aided-engineering"><span class="nav-text">CAE: Computer-aided engineering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SCADA-systems"><span class="nav-text">SCADA systems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-simulations-for-workforce"><span class="nav-text">Training simulations for workforce</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-simulations-for-machine-learning"><span class="nav-text">Training simulations for machine learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Positioning-and-navigation"><span class="nav-text">Positioning and navigation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3D-scanning-and-tomography"><span class="nav-text">3D scanning and tomography</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Other-areas"><span class="nav-text">Other areas</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#REQUIREMENTS-FOR-PROOF-OF-CONCEPTEXPERIMENTS"><span class="nav-text">REQUIREMENTS FOR PROOF-OF-CONCEPTEXPERIMENTS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Mathematical-representation"><span class="nav-text">A. Mathematical representation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-Evaluation-metrics"><span class="nav-text">B. Evaluation metrics</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PROOF-OF-CONCEPT-I-UAV-VIDEO-COMPRESSION-USING-INSTANT-NGP"><span class="nav-text">PROOF-OF-CONCEPT I.: UAV VIDEO COMPRESSION USING INSTANT-NGP</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Proposed-NeRF-based-compression"><span class="nav-text">A. Proposed NeRF-based compression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-Experimental-design-and-results"><span class="nav-text">B. Experimental design and results</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-Computational-and-energy-tradeoffs-of-NeRF-based-video-compression"><span class="nav-text">C. Computational and energy tradeoffs of NeRF-based video compression</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PROOF-OF-CONCEPT-II-OBSTACLE-AVOIDANCE-USING-D-NERF"><span class="nav-text">PROOF-OF-CONCEPT II. : OBSTACLE AVOIDANCE USING D-NERF</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Proposed-D-NeRF-based-obstacle-avoidance"><span class="nav-text">A. Proposed D-NeRF-based obstacle avoidance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-Experimental-design-and-results-1"><span class="nav-text">B. Experimental design and results</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Qi Yun"
      src="/images/avatar.jpeg">
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">116</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/qiyun71" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qiyun71" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/29010355" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;29010355" rel="noopener" target="_blank"><i class="fa fa-star fa-fw"></i>Bilibili</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qi Yun</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">432k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">26:12</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/yqq/anime.min.js"></script>
  <script src="/yqq/velocity/velocity.min.js"></script>
  <script src="/yqq/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="//cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/three.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
