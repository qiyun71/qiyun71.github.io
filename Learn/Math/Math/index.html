<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/blog.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/blog.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/yqq/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="概率&#x2F;微积分&#x2F;矩阵&#x2F;拓扑&#x2F;泛函&#x2F;复变… MATH $e^{i\pi}+1&#x3D;0$">
<meta property="og:type" content="article">
<meta property="og:title" content="Math">
<meta property="og:url" content="http://example.com/Learn/Math/Math/index.html">
<meta property="og:site_name" content="QiYun">
<meta property="og:description" content="概率&#x2F;微积分&#x2F;矩阵&#x2F;拓扑&#x2F;泛函&#x2F;复变… MATH $e^{i\pi}+1&#x3D;0$">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20241001123745.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240925100202.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240921212004.png">
<meta property="og:image" content="https://iknow-pic.cdn.bcebos.com/f9198618367adab46e65f0ea87d4b31c8601e4e6">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20250805145812.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20250805145843.png">
<meta property="og:image" content="https://pbs.twimg.com/media/GZsfHmbaAAE1ZFD?format=jpg&amp;name=large">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20240316151511.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240328193106.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240328193446.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240401085715.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240921142810.png">
<meta property="og:image" content="https://img-blog.csdn.net/20161109194405936?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810162118.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240503135053.png">
<meta property="article:published_time" content="2024-03-29T14:24:11.000Z">
<meta property="article:modified_time" content="2026-01-02T09:58:45.135Z">
<meta property="article:author" content="Qi Yun">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20241001123745.png">

<link rel="canonical" href="http://example.com/Learn/Math/Math/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Math | QiYun</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">QiYun</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Note</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/Learn/Math/Math/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Qi Yun">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QiYun">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Math
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-03-29 22:24:11" itemprop="dateCreated datePublished" datetime="2024-03-29T22:24:11+08:00">2024-03-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2026-01-02 17:58:45" itemprop="dateModified" datetime="2026-01-02T17:58:45+08:00">2026-01-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Learn/" itemprop="url" rel="index"><span itemprop="name">Learn</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>7k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>25 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>概率/微积分/矩阵/拓扑/泛函/复变…</p>
<p>MATH $e^{i\pi}+1=0$</p>
<span id="more"></span>
<h1 id="泛函"><a href="#泛函" class="headerlink" title="泛函"></a>泛函</h1><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/21938224">(72 封私信 / 80 条消息) 「泛函」究竟是什么意思？ - 知乎</a><br><a target="_blank" rel="noopener" href="https://www.pynumerical.com/archives/48/">Calculus of Variations：变分计算 - 分享我的学习心得</a> 变分：自变量x不变，函数$y(\cdot)$改变</p>
</blockquote>
<p>研究不同的函数，对输出的影响。<br>例如MLP要拟合一个函数，让预测的输出与标签非常相近。又如有限元求解偏微分方程，是要根据<strong>最小作用量原理</strong>，求得一个满足边界条件的、相对准确的近似解</p>
<h2 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u013066730/article/details/83013885">L0,L1,L2范数（双竖线，有下标）_数学公式两对竖线右下角加个2-CSDN博客</a></p>
</blockquote>
<h1 id="复变函数"><a href="#复变函数" class="headerlink" title="复变函数"></a>复变函数</h1><h2 id="拉普拉斯变换"><a href="#拉普拉斯变换" class="headerlink" title="拉普拉斯变换"></a>拉普拉斯变换</h2><p>(传递函数必用)</p>
<p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A2">拉普拉斯变换 - 维基百科，自由的百科全书</a></p>
<p>$F(s)=\int_0^\infty e^{-st}f(t)\mathrm{d}t$</p>
<h2 id="柯西积分公式"><a href="#柯西积分公式" class="headerlink" title="柯西积分公式"></a>柯西积分公式</h2><p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/zh-cn/%E6%9F%AF%E8%A5%BF%E7%A9%8D%E5%88%86%E5%85%AC%E5%BC%8F">柯西积分公式 - 维基百科，自由的百科全书</a></p>
<p>$f(a)=\frac{1}{2\pi i}\oint_\gamma\frac{f(z)}{z-a}dz.$</p>
<h2 id="留数"><a href="#留数" class="headerlink" title="留数"></a>留数</h2><p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%95%99%E6%95%B0">留数 - 维基百科，自由的百科全书</a></p>
<p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%95%99%E6%95%B0%E5%AE%9A%E7%90%86">zh.wikipedia.org/wiki/留数定理</a><br>计算解析函数沿着闭曲线的路径积分的一个有力的工具，也可以用来计算实函数的积分</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/708571624">从零开始的留数定理 - Monsoon的文章 - 知乎</a></p>
<h1 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h1><h2 id="帕斯瓦定理"><a href="#帕斯瓦定理" class="headerlink" title="帕斯瓦定理"></a>帕斯瓦定理</h2><p>信号的能量在时域和频域是一样的<br>$\int_{-\infty}^{+\infty}|x(t)|^2dt=\frac{1}{2\pi}\int_{-\infty}^{+\infty}|F_x(\omega)|^2d\omega=\int_{-\infty}^{+\infty}|F_x(2\pi f)|^2df$</p>
<h2 id="偏微分方程"><a href="#偏微分方程" class="headerlink" title="偏微分方程"></a>偏微分方程</h2><p>拉普拉斯算子 $\Delta = \frac{\partial^{2}}{\partial x^{2}} + \frac{\partial^{2}}{\partial y^{2}}$</p>
<p>Laplace eqn: $\Delta u = u_{xx}+u_{yy}=\nabla \cdot \nabla u = \nabla ^{2} u = 0$<br>Possion eqn: $\Delta u = F(x,y)$</p>
<p>二维平面D，边界为$\partial D$：$u(x,y)$<br>边界条件：$(x,y) \in \partial D$</p>
<ul>
<li>Dirichlet：$u(x,y)=g(x,y)$</li>
<li>Neumann：$\partial n u(x,y)=g(x,y)$ $where \partial n = \hat n \cdot \nabla，\hat{n}$为表面法向量</li>
<li>Robin：$u(x,y)+\alpha(x,y)\partial n u(x,y)=g(x,y)$</li>
</ul>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=KuXjwB4LzSA">但什么是卷积呢？ - YouTube</a></p>
</blockquote>
<p>$f\left(t\right)*g\left(t\right)=\int_{0}^{t}f\left(\tau\right)g\left(t-\tau\right)d\tau$ 时间信号的时间一般是从0开始的</p>
<p>Example1：已知$f(x) = a_{0}+a_{1}x+\dots+a_{n}x^{n}$ 和 $g(x)=b_{0}+b_{1}x+\dots b_{n}x^{n}$，求$h(x)=f(x) \cdot g(x)$</p>
<ul>
<li>$h(x)$的系数c是ab两系数的卷积结果：直接计算的话时间复杂度为$\mathcal{O}(n^{2})$</li>
</ul>
<script type="math/tex; mode=display">\left.\mathbf{a}*\mathbf{b}=\left[\begin{array}{c}a_0b_0,\\a_0b_1+a_1b_0,\\a_0b_2+a_1b_1+a_2b_0,\\\vdots\\a_{n-1}b_{m-1}\end{array}\right.\right]</script><ul>
<li>另一种思路是先将$f(x)$于$g(x)$进行FFT$f(\omega),g(\omega)$，频域的系数$\hat{\mathbf{a}}=[\hat{a}_0,\hat{a}_1,\hat{a}_2,\ldots,\hat{a}_{m+n-1}]$ 和$\hat{\mathbf{b}}=[\hat{b}_0,\hat{b}_1,\hat{b}_2,\ldots,\hat{b}_{m+n-1}]$，两者直接相乘得到$\hat{\mathbf{a}}\cdot\hat{\mathbf{b}}=[\hat{a}_0\hat{b}_0,\hat{a}_1\hat{b}_1,\hat{a}_2\hat{b}_2,\ldots,]$，然后进行逆FFT，得到的$h(x)$系数即想要的结果，时间复杂度为$\mathcal{O}(n\log n)$</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Yk4y1K7Az/?spm_id_from=333.999.0.0&amp;vd_source=1dba7493016a36a32b27a14ed2891088">【官方双语】卷积的两种可视化|概率论中的X+Y既美妙又复杂_哔哩哔哩_bilibili</a> 最好的动画⭐</p>
</blockquote>
<h3 id="自相关"><a href="#自相关" class="headerlink" title="自相关"></a>自相关</h3><p><a href="../Finite%20Element/Modal%20Testing.md">Modal Testing</a></p>
<h2 id="摄动法"><a href="#摄动法" class="headerlink" title="摄动法"></a>摄动法</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://kexue.fm/archives/1878">轻微的扰动——摄动法简介(1) - 科学空间|Scientific Spaces</a> </p>
</blockquote>
<h2 id="泰勒"><a href="#泰勒" class="headerlink" title="泰勒"></a>泰勒</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://dezeming.top/wp-content/uploads/2021/06/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B0%EF%BC%88%E5%8F%8A%E5%90%91%E9%87%8F%E5%87%BD%E6%95%B0%EF%BC%89%E7%9A%84%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80.pdf">dezeming.top/wp-content/uploads/2021/06/多元函数（及向量函数）的泰勒展开.pdf</a></p>
</blockquote>
<p>泰勒展开本质上是求近似</p>
<h2 id="Domain-Transform"><a href="#Domain-Transform" class="headerlink" title="Domain Transform"></a>Domain Transform</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=3gjJDuCAEQQ">The intuition behind Fourier and Laplace transforms I was never taught in school - YouTube</a></p>
</blockquote>
<h3 id="Fourier-Transform"><a href="#Fourier-Transform" class="headerlink" title="Fourier Transform"></a>Fourier Transform</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://yangwc.com/2019/10/24/FFT/">频域处理Frequency domain processing：傅里叶变换 | YangWC’s Blog</a></p>
</blockquote>
<p>$Fourier: F(\omega)=\int_{-\infty}^{\infty}f(t)e^{-i\omega t}dt$<br>$F(\omega)=\int_{-\infty}^{\infty}f(t)\cos{(\omega t)}dt-i\int_{-\infty}^{\infty}f(t)\sin{(\omega t)}dt$ </p>
<p>傅里叶变换可以看作使用不同频率$\omega$的sin和cos对原始时域信号进行积分，sin和cos积分得到的值越大，则虚部和实部的值越大，直观的模/振幅 magnitude 也越大，该频率$\omega$成分下的值也越大</p>
<p>对于信号$cos\pi t$ 进行FT，其虚部$\cos \pi t \cdot \sin \omega t$一直为0，实部$\sin \pi t \cdot \sin \omega t$在其他地方为0，在$\omega =\pi$时达到正无穷</p>
<h4 id="DFT"><a href="#DFT" class="headerlink" title="DFT"></a>DFT</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.demofox.org/2016/08/11/understanding-the-discrete-fourier-transform/">Understanding The Discrete Fourier Transform « The blog at the bottom of the sea</a></p>
</blockquote>
<p>$X_{k}=\sum_{n=0}^{N-1}x_{n}\cdot e^{-2\pi ikn/N}$</p>
<h4 id="FFT"><a href="#FFT" class="headerlink" title="FFT"></a>FFT</h4><h4 id="STFT"><a href="#STFT" class="headerlink" title="STFT"></a>STFT</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/26673889/answer/3292923354">短时傅里叶变换和小波变换有何不同？ - Mr.看海的回答 - 知乎</a></p>
</blockquote>
<p>可见当信号的频率成分随时间显著变化时，即所谓的非平稳信号，传统频谱分析就不太合适了。<br>举几个例子：</p>
<ul>
<li>研究信号的局部特性：如爆炸声、机器的故障噪声等，这些信号的特性在短时间内变化很大。</li>
<li>语音处理：在自动语音识别和语音合成中，语音的特征如音高和音量随着时间而变化。</li>
<li>雷达和无线电信号分析：雷达信号的特性依赖于时间和观测的角度，需分析这些信号随时间的变化。</li>
<li>生物医学信号分析：比如心电图（ECG）和脑电图（EEG）这样的生理信号，它们的频率特性随时间改变。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20241001123745.png" alt="image.png|333"></p>
<p>在短时傅里叶变换（STFT）中，窗口函数及其大小选择是分析的关键。窗口函数决定了在任何给定时间点，信号的哪一部分被用于分析。窗口大小的选择直接影响了分析结果的时间分辨率和频率分辨率，这是进行有效STFT分析的最重要的权衡。</p>
<p>窗口大小的时间分辨率影响：时间分辨率与窗口的宽度密切相关。一个窄窗口提供较高的时间分辨率，因为它捕捉了信号在很短时间内的变化。这对于分析包含快速变化的瞬态事件，如敲击声或爆炸声，是非常有用的。然而，较小的窗口将限制频率分辨率，因为频率分析需要足够的周期来准确估计。</p>
<p>窗口大小的频率分辨率影响：频率分辨率与窗口的宽度呈反比。一个宽窗口覆盖了信号的较长时间段，提供了较高的频率分辨率。这是因为更多的周期可以在窗口内被分析，从而更准确地确定低频成分。但是，这会牺牲时间分辨率，因为窗口中的信号被假定在这段时间内是平稳的。</p>
<p><strong>那有没有一种可能，窗口大小是可调的呢？</strong></p>
<h4 id="Wavelet-Transform"><a href="#Wavelet-Transform" class="headerlink" title="Wavelet Transform"></a>Wavelet Transform</h4><p>可以发现其特点：高频部分具有较高的时间分辨率和较低的频率分辨率，而低频部分具有较高的频率分辨率和较低的时间分辨率，这就恰好解决了STFT的痛点</p>
<h3 id="Laplace-Transform"><a href="#Laplace-Transform" class="headerlink" title="Laplace Transform"></a>Laplace Transform</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=n2y7n6jw5d0">What does the Laplace Transform really tell us? A visual explanation (plus applications) - YouTube</a></p>
</blockquote>
<p>$X(s)=\int_0^\infty x(t)e^{-st}dt$</p>
<p>$Laplace:F(s)=\int_0^\infty f(t)e^{-st}dt$</p>
<p>$s=\alpha+i\omega$</p>
<p>$F(s)=\int_0^\infty f(t)e^{-i\omega t}e^{-\alpha t}dt$</p>
<ul>
<li>$e^{-i\omega t}$ scans for sinusoids</li>
<li>$e^{-\alpha t}$ scans for exponentials</li>
</ul>
<p>时域的微积分在s域中可以很好地计算</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240925100202.png" alt="image.png|666"></p>
<h2 id="复数-Complex-Number"><a href="#复数-Complex-Number" class="headerlink" title="复数 Complex Number"></a>复数 Complex Number</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.longluo.me/blog/2021/12/29/fourier-transform/">傅里叶变换(Fourier Transform) | Long Luo’s Life Notes</a></p>
</blockquote>
<p>在复平面上，1 有 n 个不同的 n 次方根，它们位于复平面的单位圆上，构成<strong>正多边形的顶点</strong>，但最多只可有两个顶点同时标在实数线上</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240921212004.png" alt="image.png|666"></p>
<h2 id="KL-散度"><a href="#KL-散度" class="headerlink" title="KL 散度"></a>KL 散度</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.vectorexplore.com/tech/loss-functions/kl-divergence/">KL 散度（Kullback-Leibler Divergence）：图示+公式+代码</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/45131536">机器学习中的散度 - 知乎</a></p>
</blockquote>
<p>概率分布P的概率密度函数为 $p(x_i)$<br>信息理论的主要目标是量化数据中的信息量。信息理论中最重要的度量标准称为熵（Entropy），通常表示为 H其信息熵：$H(X)=-\sum_{i=1}^np(x_i)\log p(x_i)$ , <strong><em>可以将-log(p)看成权重，概率p越大，权重越小</em></strong> (如果有人告诉我们一个相当不可能的事件发生了，我们收到的信息要多于我们被告知某个很可能发生的事件发生时收到的信息。如果我们知道某件事情一定会发生，那么我们就不会接收到信息。)</p>
<ul>
<li>如果我们在计算中使用 $log_{2}$​ 我们可以将熵解释为“编码我们的信息所需的最小比特数”。<ul>
<li>【例1】A、B、C、D 四个字母分别占 1/2（4096个），1/4（2048个），1/8（1024个），1/8（1024个）。那么最有效的一种编码方式为 A(0)，B（10），C（110），D(111)。整个语料库的长度 4096 x 1 + 2048 x 2 + 1024 x 3 x 2 = 14336，平均长度为 14336/8192 = 1.75。和下面代码中的【结果1】一致。<em>如果使用另一种编码方式(例2)即  A（110），B（111），C（0），D（10），则熵为(4096 </em> 3 + 2048 <em> 3 + 1024 </em> 1 + 1024 <em> 2 )/8192 = 2.625</em><ul>
<li>其中如果p(x)=0.5，则只需要$-log_{2}(0.5)=1$个bit的编码；如果p(x)=0.25，则需要2 bit的编码，以此类推</li>
<li>依照这种规律的编码，可以保证信息中的熵最小</li>
</ul>
</li>
<li>【例2】ABCD 四个字母占比变成了1/8（1024 个），1/8（1024 个），1/2（4096 个），1/4（2048 个），这样最有效的一种编码方式为 A（110），B（111），C（0），D（10），计算平均长度为1.75，和代码中的【结果3】一致。 <em>如果使用另一种编码方式(例1)即  A(0)，B（10），C（110），D(111)，则熵为(1024 </em> 1 + 1024 <em> 2 + 4096 </em> 3 + 2048 <em> 3 )/8192 = 2.625</em></li>
<li>相对熵(KL散度)：<ul>
<li>例1 相对于 例2 的相对熵为 $p(x)\log_{2}\left( \frac{p(x)}{q(x)} \right)$=(4096 <em> -(1-3) + 2048 </em> -(2-3) + 1024 <em> -(3-1) + 1024 </em> -(3-2) )/8192 = 0.875 = 2.625 - 1.75。 这说明针对例子中的这一分布，使用法1编码相对于法2来说，平均可以省下0.875bit</li>
<li>例2 相对于 例1 的相对熵为 $q(x)\log_{2}\left( \frac{q(x)}{p(x)} \right)$=(1024 <em> -(3-1) + 1024 </em> -(3-2) + 4096 <em> -(1-3) + 2048 </em> -(2-3) )/8192 =0.875 = 2.625 - 1.75</li>
<li>两个例子尽管编码方式不同，但字母的频率分布是一致的。因此，在计算KL散度时，由于 p(x) 和 q(x) 实际上是相同的，导致 KL散度的两个方向相等。<strong>为什么是0.875而不是0？</strong> 这说明即使两种编码方案都能描述相同的概率分布，<strong>但由于它们在信息表示方式上的差异</strong>，导致了每种编码方式在另一种分布下都有额外的冗余。KL散度反映了这种冗余：即使分布相同，编码方式的差异会导致你用一种方式来编码另一种信息时需要额外的0.875 bits。<strong>KL散度真正衡量的是概率分布之间的差异，而不是编码方案之间的差异。</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>KL散度的值越大，表示用一个分布近似另一个分布时引入的信息损失或误差越大：</p>
<ul>
<li>非对称性：KL散度是非对称的，即从 P 分布到 Q 分布的 KL 散度与从 Q 分布到 P 分布的 KL 散度可能不同。<ul>
<li>虽然KL散度通常是非对称的，但在特定条件下<strong>KL散度从 P 到 Q</strong> 与 <strong>从 Q 到 P</strong> 的值可以相等：<ul>
<li><strong>在某些特定的离散分布情况下</strong>，当 P(x)和 Q(x)的概率质量函数具有某种对称结构时，它们的KL散度可能相等（但这仍是概率分布的特殊情况，而不是普遍现象）。</li>
<li>。当且仅当两个概率分布完全相同时，KL散度的值为零</li>
</ul>
</li>
</ul>
</li>
<li>非负性：KL散度的值始终为非负数。KL散度值越大，表示两个概率分布越不相似。</li>
<li>非度量性：KL散度并不满足度量空间的性质，特别是三角不等式。由于非对称性和非度量性，KL 散度不能用于计算两个分布之间的“距离”或“相似度”。</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://wuli.wiki/online/KLD.html">KL 散度（相对熵） - 小时百科 (wuli.wiki)</a></p>
</blockquote>
<p><strong>KL 散度</strong>（Kullback–Leibler divergence，缩写 KLD）是一种统计学度量，<strong>表示的是一个概率分布相对于另一个概率分布的差异程度</strong>，在信息论中又称为<strong>相对熵</strong>（Relative entropy）。对于随机变量Q的概率分布，相对于随机变量P的概率分布的KL散度定义为： $D_{KL}(P||Q)=H(P,Q)-H(P)$</p>
<script type="math/tex; mode=display">\begin{equation}
D_{KL}(P||Q)=\sum_{x\in X}P(x)ln(\frac{P(x)}{Q(x)})=\sum_{x\in X}P(x)(ln(P(x))-ln(Q(x)))~.
\end{equation}</script><p>对于连续型随机变量，设概率空间 X 上有两个概率分布 P 和 Q，其概率密度分别为 p 和 q，那么，P 相对于 Q 的 KL 散度定义如下：</p>
<script type="math/tex; mode=display">\begin{equation}
D_{KL}(P||Q)=\int_{-\infty}^{+\infty}p(x)ln(\frac{p(x)}{q(x)})dx~.
\end{equation}</script><p> 显然，当 P=Q 时，$D_{KL}=0$</p>
<p>两个一维高斯分布的 KL 散度公式：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/">KL散度(Kullback-Leibler Divergence)介绍及详细公式推导 | HsinJhao’s Blogs</a></p>
</blockquote>
<script type="math/tex; mode=display">\begin{aligned}
KL(p,q)& =\int[\left.p(x)\log(p(x))-p(x)\log(q(x))\right]dx  \\
&=-\frac12\left[1+\log(2\pi\sigma_1^2)\right]-\left[-\frac12\log(2\pi\sigma_2^2)-\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}\right] \\
&=\log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac12
\end{aligned}</script><h2 id="凸函数与Jensen不等式"><a href="#凸函数与Jensen不等式" class="headerlink" title="凸函数与Jensen不等式"></a>凸函数与Jensen不等式</h2><p>凸函数是一个定义在某个向量空间的凸子集 C（区间）上的实值函数 f，如果在其定义域 C 上的任意两点$x_{1},x_{2}$，$0\leq t\leq 1$，有：<br>满足$tf(x_1)+(1-t)f(x_2)\geq f\left(tx_1+(1-t)x_2\right)$<br>也就是说凸函数任意两点的割线位于函数图形上方， <strong>这也是Jensen不等式的两点形式</strong></p>
<p>若对于任意点集 $\{x_{i}\}$，若 $\lambda_i≥0$且 $∑_{i}\lambda_{i}=1$ ，使用<strong>数学归纳法</strong>，可以证明凸函数 f (x) 满足：<br>$f(\sum_{i=1}^M\lambda_ix_i)\leq\sum_{i=1}^M\lambda_if(x_i)$ 即为Jesen不等式</p>
<p><strong>在概率论中</strong>，如果把 $\lambda_i$ 看成取值为 $x_{i}$的离散变量 x 的概率分布$p_{i}$，那么公式(2)就可以写成<br>$f(E[x])\leq E[f(x)]$ , $E[\cdot]$代表期望</p>
<p>对于连续变量，Jensen不等式给出了积分的凸函数值和凸函数的积分值间的关系：<br>$f(\int xp(x)dx)\leq\int f(x)p(x)dx$</p>
<h2 id="定积分求导"><a href="#定积分求导" class="headerlink" title="定积分求导"></a>定积分求导</h2><p><a target="_blank" rel="noopener" href="https://zhidao.baidu.com/question/940652843635700092.html">老师对定积分的求导怎么求，能给点例子吗_百度知道</a></p>
<p><img src="https://iknow-pic.cdn.bcebos.com/f9198618367adab46e65f0ea87d4b31c8601e4e6" alt="f9198618367adab46e65f0ea87d4b31c8601e4e6 (559×302)"></p>
<h1 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h1><h2 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/shyjhyp11/article/details/133969095">概率论_概率公式中的逗号( , ) 竖线( | ) 分号( ； )及其优先级_概率p里面的逗号-CSDN博客</a></p>
<h3 id="概率-似然"><a href="#概率-似然" class="headerlink" title="概率/似然"></a>概率/似然</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/334890990">通俗理解“极大似然估计” - 知乎</a><br><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/zh-cn/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0">似然函数 - 维基百科，自由的百科全书</a> 例子比较好理解</p>
</blockquote>
<p>概率函数：由因到果，已知参数(概率)，根据真实参数（或已经发生的观测结果）去推测未来的观测结果<br>似然函数：由果到因，根据已经发生的观测结果去猜想真实参数，这个过程叫做<strong>估计</strong>；估计正确的可能性叫做<strong>似然性</strong>。估计参数的似然性，其目的是帮助我们根据已观测的结果，推测出最符合观测结果、最合理的参数。</p>
<p>假设一个参数p，则在这一参数基础上出现结果的概率即为似然</p>
<script type="math/tex; mode=display">\begin{aligned}
&L(p)=L(p|x_1,\ldots,x_n) \\
&=P(X_1=x_1|p)•\ldots•P(X_n=x_n|p) \\
&=\prod_{i=1}^nP(X_i=x_i|p)
\end{aligned}</script><p>极大似然估计（maximum likelihood estimation，缩写为MLE），也称<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=162550522&amp;content_type=Article&amp;match_order=2&amp;q=%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1&amp;zhida_source=entity">最大似然估计</a>。<br>$\arg\max_pL(p)=\arg\max_p\prod_{i=1}^nP(X_i=x_i|p)$<br>将$L(p)$看作p的函数，对其求导，导数为0，即可得到最大的似然值对应的参数p</p>
<p>求导前为什么要取对数？：<strong>（1）避免下溢出</strong> <strong>（2）便于计算</strong> 将累积乘法转换成累加</p>
<h3 id="均值方差"><a href="#均值方差" class="headerlink" title="均值方差"></a>均值方差</h3><p>为什么样本估计方差要除以n-1 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/550427703">【浅谈】样本方差的分母“n”为什么要改为“n-1” - 知乎</a></p>
<p>其实就是自由度，当你算标准差的时候，已经知道均值了，那么就只有n-1个数字是自由的，第n个数值可以由前面的n-1个数字和均值算出来了，所以他实际上不包含任何关于数据波动的信息</p>
<p>无偏估计的方差：<br>$\left\{\begin{array}{c}M_n=\frac{X_1+X_2+\cdots+X_n}n\\\hat{S}_n^2=\frac{\sum_{i=1}^n(X_i-M_n)^2}{n-1}\end{array}\right.$</p>
<p>$Var(X)=E[X-E(X)]^{2}=E\{X^{2}-2XE(X)+[E(X)]^{2}\}=E(X^{2})-2[E(X)]^{2}+[E(X)]^{2}$<br>$Var(X)=E(X^{2})-[E(X)]^{2}$</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20250805145812.png" alt="image.png|666"></p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20250805145843.png" alt="image.png|666"></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/64859161">【AP统计】期望E(X)与方差Var(X) - 知乎</a></p>
<h3 id="联合概率密度"><a href="#联合概率密度" class="headerlink" title="联合概率密度"></a>联合概率密度</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/670185196">概率论(二)——二维随机变量 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/340502318">两个随机变量的函数的分布 - 知乎</a></p>
<h3 id="置信区间"><a href="#置信区间" class="headerlink" title="置信区间"></a>置信区间</h3><blockquote>
<p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/653263495">置信区间全攻略 - 知乎</a></p>
</blockquote>
<p>正态分布置信区间：<br>$P(\overline{X}-Z^<em>\frac{\sigma}{\sqrt{n}}\leq\mu\leq\overline{X}+Z^</em>\frac{\sigma}{\sqrt{n}})=1-\alpha$</p>
<ul>
<li>$n$为样本数量</li>
<li>$Z^{*}$为z-score，被定义为一种标准分数，当我们确定我们的置信水平时，可以通过查表的方式获取</li>
<li>$\alpha$为置信水平</li>
</ul>
<p>如果使用 $p &lt; 0.05$ 的 $\alpha$ 值来表示统计显著性，那么置信水平就是 1 − 0.05 = 0.95，即置信度$1-\alpha$为95%</p>
<ul>
<li>区间估计的置信度越高，置信区间越宽，估计精度越低。</li>
<li><strong>样本数越多，置信区间也就越窄，其实也符合常识，即数据量越多，参数估计的值就越接近于真值。</strong></li>
</ul>
<h2 id="Bayes"><a href="#Bayes" class="headerlink" title="Bayes"></a>Bayes</h2><blockquote>
<p>⭐<a target="_blank" rel="noopener" href="https://liaoxuefeng.com/blogs/all/2023-08-27-bayes-explain/index.html">一文搞懂贝叶斯定理（原理篇） - Blogs - 廖雪峰的官方网站</a></p>
</blockquote>
<p><img src="https://pbs.twimg.com/media/GZsfHmbaAAE1ZFD?format=jpg&amp;name=large" alt="GZsfHmbaAAE1ZFD (1264×1128)|555"></p>
<p>Follow:</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/people/chenxran0916/posts">KERO - 知乎</a></p>
</blockquote>
<ul>
<li>最大似然</li>
</ul>
<p>最大似然估计(Maximum Likelihood Estimation, MLE)，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！样本从某一个客观存在的模型中抽样得来，然后根据样本来计算该模型的数学参数，即：模型已定，参数未知！</p>
<script type="math/tex; mode=display">\begin{aligned}
\widehat{\theta}_{\mathrm{MLE}}& =\arg\max P(X;\theta)  \\
&=\arg\max P(x_1;\theta)P(x_2;\theta)\cdot\cdots P(x_n;\theta) \\
&=\arg\max\log\prod_{i=1}^nP(x_i;\theta) \\
&=\arg\max\sum_{i=1}^n\log P(x_i;\theta) \\
&=\arg\min-\sum_{i=1}^n\log P(x_i;\theta)
\end{aligned}</script><ul>
<li>最大后验</li>
</ul>
<script type="math/tex; mode=display">\begin{aligned}
\hat{\theta}_{\mathrm{MAP}}& =\arg\max P(\theta|X)  \\
&=\arg\min-\log P(\theta|X) \\
&=\arg\min-\log P(X|\theta)-\log P(\theta)+\log P(X) \\
&=\arg\min-\log P(X|\theta)-\log P(\theta)
\end{aligned}</script><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=HZGCoVF3YvM">贝叶斯定理，改变信念的几何学 - YouTube</a> ——数形结合<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/401258319">走进贝叶斯统计（一）—— 先验分布与后验分布 - 知乎</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/USTC-ZCC/p/12786860.html">超详细讲解贝叶斯网络(Bayesian network) - USTC丶ZCC - 博客园</a></p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20240316151511.png" alt="image.png|666"></p>
<p>全概率公式：$\mathbf{P(H)}=\mathbf{P(H|A)P(A)}+\mathbf{P(H|B)P(B)}$，结果H发生的概率</p>
<p>贝叶斯公式：$\mathbf{P}(\mathbf{A}|\mathbf{H})=\frac{P(A)P(H|A)}{P(H)}$，H结果发生时，是由A导致的概率</p>
<ul>
<li>连续$p(y_0|x)=\frac{p(x|y_0)p(y_0)}{\int_{-\infty}^{+\infty}p(x|y)p(y)dy}$</li>
<li>离散$p(y_j|x)=\frac{p(x|y_j)p(y_j)}{\sum_{i=0}^np(x|y_i)p(y_i)}$</li>
</ul>
<p>在使用数据估计参数$\theta$之前，我们需要给这个参数设定一个分布，即先验分布$p(\theta)$（根据经验得到）</p>
<p>$p(\theta|X)=\frac{p(\theta,X)}{p(X)}=\frac{p(X|\theta)p(\theta)}{\int_{-\infty}^{+\infty}p(X|\theta)p(\theta)d\theta}.$</p>
<ul>
<li>$p(\theta|X)$是$\theta$的后验分布</li>
<li>$p(X|\theta)$是在给定$\theta$下关于数据样本的似然函数</li>
<li>$\int_{-\infty}^{+\infty}p(X|\theta)p(\theta)d\theta$ 为常数c，可以写为$p(\theta|X)\propto p(X|\theta)p(\theta).$</li>
</ul>
<h2 id="Distribution"><a href="#Distribution" class="headerlink" title="Distribution"></a>Distribution</h2><h3 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a>泊松分布</h3><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Ta4y1j7YK/?spm_id_from=333.788.recommend_more_video.-1">泊松分布，无穷分割玩的魔术_哔哩哔哩_bilibili</a></p>
<p>$P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!}$</p>
<ul>
<li>$\lambda$是随机事件发生次数的数学期望。</li>
<li>分布律函数或概率密度函数是通过二项分布推导 (当n很大，p很小时，λ=np，二项分布可用泊松分布近似)。</li>
</ul>
<h3 id="核密度估计"><a href="#核密度估计" class="headerlink" title="核密度估计"></a>核密度估计</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://viruspc.github.io/blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2021/09/27/kde.html">核密度估计 (KDE, Kernel Density Estimation) | Blog</a></p>
</blockquote>
<h3 id="Gamma-分布"><a href="#Gamma-分布" class="headerlink" title="Gamma 分布"></a>Gamma 分布</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gamma_distribution">Gamma distribution - Wikipedia</a></p>
</blockquote>
<p>Gamma Function：$\Gamma(z)=\int_0^\infty t^{z-1}e^{-t}\mathrm{d}t,\quad\Re(z)&gt;0.$ or $\Gamma(n)=(n-1)!.$</p>
<p>$f(x)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$</p>
<p>图中k对应$\alpha$，$\theta$对应$\beta$</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240328193106.png" alt="image.png|333"></p>
<h3 id="Beta-分布"><a href="#Beta-分布" class="headerlink" title="Beta 分布"></a>Beta 分布</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution - Wikipedia</a></p>
</blockquote>
<p>通常是概率分布的分布</p>
<p>$\mathbf{B}(\alpha,\beta)={\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}}$</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240328193446.png" alt="image.png|333"></p>
<h3 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h3><p>$X\sim N(\mu,\sigma^2)$</p>
<p>PDF：$f(x)=\frac1{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$</p>
<h3 id="多元高斯分布"><a href="#多元高斯分布" class="headerlink" title="多元高斯分布"></a>多元高斯分布</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/kailugaji/p/15542845.html">多元/多维高斯/正态分布概率密度函数推导 (Derivation of the Multivariate/Multidimensional Normal/Gaussian Density) - 凯鲁嘎吉 - 博客园</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/bingjianing/p/9117330.html">多元高斯分布（The Multivariate normal distribution） - bingjianing - 博客园</a></p>
</blockquote>
<p>概率密度函数<br>$p(x)=p(x_{1},x_{2},\ldots,x_{D})=\frac{1}{(2\pi)^{D/2}}\exp\left(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right)$</p>
<h3 id="混合高斯分布"><a href="#混合高斯分布" class="headerlink" title="混合高斯分布"></a>混合高斯分布</h3><p>GMM</p>
<h3 id="联合高斯分布"><a href="#联合高斯分布" class="headerlink" title="联合高斯分布"></a>联合高斯分布</h3><p>如何构造两个协方差标准分布</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/kdazhe/article/details/104599229">生成一定相关性的二元正态分布_怎么产生二元正态分布的随机数-CSDN博客</a></p>
</blockquote>
<p>AB为两个独立的标准正态分布：</p>
<p>$\begin{aligned}\mathrm{X}&amp;=\alpha\mathrm{A}+\beta\mathrm{B},\\\\\mathrm{Y}&amp;=\gamma\mathrm{A}+\delta\mathrm{B}\end{aligned}$</p>
<p>$\alpha,\beta,\gamma,\delta$是四个待确定的参数，希望找到这四个参数，使得X和Y也服从标准正态分布$N(0,1)$，并且相关系数$Cor(X,Y)=\rho$。</p>
<p>由标准正态分布性质可得：$\mathrm{X}\sim\mathrm{N}(0,\alpha^2+\beta^2),\mathrm{Y}\sim\mathrm{N}(0,\gamma^2+\delta^2)。$<br>要想：$\bar{\alpha}^{2}+\beta^{2}=1,\gamma^{2}+\delta^{2}=1$<br>则可以假设：$\alpha=\cos\theta,\beta=\sin\theta;\gamma=\sin\theta,\delta=\cos\theta,$</p>
<p>由相关系数：$\mathrm{Cor(X,~Y)}=\frac{\mathrm{Cov(X,~Y)}}{\sqrt{\mathrm{Var(X)Var(Y)}}}$，$\operatorname{Var}(\mathrm{X})=1$，$\operatorname{Var}(\mathrm{Y})=1$<br>因此使得：</p>
<script type="math/tex; mode=display">\begin{aligned}
\mathrm{Cov}(\mathrm{X,Y})& =\mathrm{Cov}\Big((\cos\theta)\mathrm{A}+(\sin\theta)\mathrm{B},(\sin\theta)\mathrm{A}+(\cos\theta)\mathrm{B}\Big)  \\
&=\cos\theta\sin\theta+\sin\theta\cos\theta  \\
&=\sin2\theta 
\end{aligned}</script><ul>
<li>（$Cov(A,A)=1,Cov(A,B)=0$）</li>
<li>$\text{Cov}(x_1 + x_2, y_1 + y_2) = \text{Cov}(x_1, y_1) + \text{Cov}(x_2, y_1) + \text{Cov}(x_1, y_2) + \text{Cov}(x_2, y_2)$</li>
<li>$\mathrm{Cov}(aX,Y)=a\mathrm{Cov}(X,Y)$</li>
</ul>
<p>则有：$\theta=\frac{\arcsin\rho}2$，然后令$\begin{cases}\mathrm X=(\cos\theta)\mathrm A+(\sin\theta)\mathrm B\\\mathrm Y=(\sin\theta)\mathrm A+(\cos\theta)\mathrm B\end{cases}$，则计算出来的X和Y就是相关性为$\rho$的标准正态分布</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">bivariateNormal</span>:</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rho: <span class="string">&#x27;float&#x27;</span>, m: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Suppose we want to generate a pair of </span></span><br><span class="line"><span class="string">        random variables X, Y, with X ~ N(0, 1), </span></span><br><span class="line"><span class="string">        Y ~ N(0, 1), and Cor(X, Y) = rho. m is </span></span><br><span class="line"><span class="string">        the number of data pairs we want to generate.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.rho = rho</span><br><span class="line">        self.m = m</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generateBivariate</span>(<span class="params">self</span>) -&gt; <span class="string">&#x27;tuple(np.array, np.array)&#x27;</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Generate two random variables X, Y, with X ~ N(0, 1), </span></span><br><span class="line"><span class="string">        Y ~ N(0, 1), and Cor(X, Y) = rho. </span></span><br><span class="line"><span class="string">        self.m is the number of sample points we generated.</span></span><br><span class="line"><span class="string">        We return a tuple (X, Y). </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        theta = np.arcsin(self.rho) / <span class="number">2</span></span><br><span class="line">        A = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, self.m)</span><br><span class="line">        B = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, self.m)</span><br><span class="line">        X = np.cos(theta) * A + np.sin(theta) * B</span><br><span class="line">        Y = np.sin(theta) * A + np.cos(theta) * B</span><br><span class="line">        <span class="keyword">return</span> X, Y</span><br><span class="line"></span><br><span class="line">m = <span class="number">10</span> ** <span class="number">3</span></span><br><span class="line">rho = -<span class="number">0.4</span></span><br><span class="line">a = bivariateNormal(rho, m)</span><br><span class="line">X, Y = a.generateBivariate()</span><br><span class="line">np.corrcoef(X, Y)</span><br></pre></td></tr></table></figure>
<h3 id="P-box"><a href="#P-box" class="headerlink" title="P-box"></a>P-box</h3><h2 id="Markov-Chains"><a href="#Markov-Chains" class="headerlink" title="Markov Chains"></a>Markov Chains</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://setosa.io/ev/markov-chains/">Markov Chains explained visually</a> 入门</p>
</blockquote>
<h2 id="Sampling-Method"><a href="#Sampling-Method" class="headerlink" title="Sampling Method"></a>Sampling Method</h2><p><a href="Sampling%20Method.md">Sampling Method</a></p>
<h2 id="置信区间-1"><a href="#置信区间-1" class="headerlink" title="置信区间"></a>置信区间</h2><p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7%E6%B3%95%E5%89%87">68–95–99.7法则 - 维基百科，自由的百科全书</a></p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240401085715.png" alt="image.png|444"></p>
<h2 id="图像展示"><a href="#图像展示" class="headerlink" title="图像展示"></a>图像展示</h2><h3 id="箱型图"><a href="#箱型图" class="headerlink" title="箱型图"></a>箱型图</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/347067055">如何深刻理解箱线图（boxplot） - 知乎</a></p>
</blockquote>
<h3 id="t-SNE-数据降维"><a href="#t-SNE-数据降维" class="headerlink" title="t-SNE 数据降维"></a>t-SNE 数据降维</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/426068503">降维方法之t-SNE - 知乎</a><br><a target="_blank" rel="noopener" href="https://medium.com/swlh/everything-about-t-sne-dde964f0a8c1">Everything About t-SNE. t-SNE means t-distribution Stochastic… | by Ram Thiagu | The Startup | Medium</a></p>
</blockquote>
<p>PCA是一种常用的降维方式，但是不方便展示结果。例如对两类数据进行降维(二维)，PCA和t-SNE两种方法的展示结果为：</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240921142810.png" alt="image.png|666"></p>
<h1 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h1><h2 id="矩阵的范数"><a href="#矩阵的范数" class="headerlink" title="矩阵的范数"></a>矩阵的范数</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_29540745/article/details/53102095">矩阵范数详解-CSDN博客</a></p>
<p><img src="https://img-blog.csdn.net/20161109194405936?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="20161109194405936 (733×384)"></p>
<h2 id="特殊矩阵定义"><a href="#特殊矩阵定义" class="headerlink" title="特殊矩阵定义"></a>特殊矩阵定义</h2><p><strong>正交矩阵</strong> <a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%AD%A3%E4%BA%A4%E7%9F%A9%E9%98%B5">正交矩阵 - 维基百科，自由的百科全书</a></p>
<ul>
<li>$Q^T=Q^{-1}\Leftrightarrow Q^TQ=QQ^T=I.$ 其中$I$为单位矩阵</li>
<li>正交矩阵的行列式值必定为+1或−1。</li>
<li>行列式值为+1的正交矩阵，称为<strong>特殊正交矩阵</strong>，它是一个旋转矩阵。</li>
<li>行列式值为-1的正交矩阵，称为瑕旋转矩阵。瑕旋转是旋转加上镜射。镜射也是一种瑕旋转。</li>
</ul>
<p><strong>正定矩阵</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/234967628">【线性代数】详解正定矩阵、实对称矩阵、矩阵特征值分解、矩阵 SVD 分解 - 知乎</a></p>
<ul>
<li>任意非零向量$\mathbf{x}$，若$\mathbf{x}^{T}\mathbf{A}\mathbf{x}&gt;0$恒成立，则$\mathbf{A}$为正定矩阵。若$\mathbf{x}^{T}\mathbf{A}\mathbf{x}\geq0$恒成立，则$\mathbf{A}$为半正定矩阵</li>
<li></li>
</ul>
<h2 id="特征分解"><a href="#特征分解" class="headerlink" title="特征分解"></a>特征分解</h2><p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3">特征分解 - 维基百科，自由的百科全书</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解（SVD） - 知乎</a></p>
<h2 id="变换"><a href="#变换" class="headerlink" title="变换"></a>变换</h2><h3 id="2D"><a href="#2D" class="headerlink" title="2D"></a>2D</h3><p>仿射变换</p>
<h3 id="3D"><a href="#3D" class="headerlink" title="3D"></a>3D</h3><p>像素Pixel | 相机Camera | 世界World</p>
<p>内参矩阵 = c2p<br>外参矩阵 = w2c<br>根据世界坐标计算像素坐标 = <code>c2p * w2c * world_position</code></p>
<h3 id="Homography"><a href="#Homography" class="headerlink" title="Homography"></a>Homography</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/74597564">单应性Homography估计：从传统算法到深度学习 - 你再好好想想的文章 - 知乎</a></p>
</blockquote>
<p>单应性不严谨的定义：用 <strong>[无镜头畸变]</strong> 的相机从不同位置拍摄 <strong>[同一平面物体]</strong> 的图像之间存在单应性，可以用 <strong>[透视变换]</strong> 表示 。</p>
<p>刚体变换：平移+旋转，只改变物体位置，不改变物体形状。</p>
<script type="math/tex; mode=display">\begin{pmatrix}x'\\y'\\1\end{pmatrix}=\begin{bmatrix}\mathrm{cos}\theta&-\mathrm{sin}\theta&t_x\\\mathrm{sin}\theta&\mathrm{cos}\theta&t_y\\0&0&1\end{bmatrix}\begin{pmatrix}x\\y\\1\end{pmatrix}=\begin{bmatrix}R_{2\times2}&T_{2\times1}\\0^T&1\end{bmatrix}\begin{pmatrix}x\\y\\1\end{pmatrix}</script><p>仿射变换：改变物体位置和形状，但是原来平行的边依然平行。</p>
<script type="math/tex; mode=display">\begin{pmatrix}x'\\y'\\1\end{pmatrix}=\begin{bmatrix}A_{2\times2}&T_{2\times1}\\0^T&1\end{bmatrix}\begin{pmatrix}x\\y\\1\end{pmatrix}</script><p>透视变换（也称投影变换）：彻底改变物体位置和形状</p>
<script type="math/tex; mode=display">\begin{pmatrix}x'\\y'\\1\end{pmatrix}=\begin{bmatrix}A_{2\times2}&T_{2\times1}\\V^T&s\end{bmatrix}\begin{pmatrix}x\\y\\1\end{pmatrix}=H_{3\times3}\begin{pmatrix}x\\y\\1\end{pmatrix}</script><h1 id="Computer-Graphics"><a href="#Computer-Graphics" class="headerlink" title="Computer Graphics"></a>Computer Graphics</h1><h2 id="SDF计算与求导"><a href="#SDF计算与求导" class="headerlink" title="SDF计算与求导"></a>SDF计算与求导</h2><p>空间中的子集$\partial\Omega$，SDF值定义为：$\left.f(x)=\left\{\begin{array}{ll}d(x,\partial\Omega)&amp;\mathrm{~if~}x\in\Omega\-d(x,\partial\Omega)&amp;\mathrm{~if~}x\in\Omega^c\end{array}\right.\right.$<br>其中$d(x,\partial\Omega):=\inf_{y\in\partial\Omega}d(x,y)$表示x到表面子集上一点的距离，inf表示infimum最大下界</p>
<h1 id="Computer-Vision"><a href="#Computer-Vision" class="headerlink" title="Computer Vision"></a>Computer Vision</h1><h2 id="卷积图像大小计算公式"><a href="#卷积图像大小计算公式" class="headerlink" title="卷积图像大小计算公式"></a>卷积图像大小计算公式</h2><p>图像卷积后的大小计算公式： $N=\left\lfloor\frac{W-F+2P}{Step}\right\rfloor+1$</p>
<ul>
<li>输入图片大小 $W \times W$</li>
<li>Filter（卷积核）大小 $F \times F$</li>
<li>步长 Step</li>
<li>padding（填充）的像素数 $P$</li>
<li>输出图片的大小为$N \times N$</li>
</ul>
<h2 id="linearColor-2-sRGB"><a href="#linearColor-2-sRGB" class="headerlink" title="linearColor 2 sRGB"></a>linearColor 2 sRGB</h2><p>(Why)为什么要将线性RGB转换成sRGB</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.zhangxinxu.com/wordpress/2017/12/linear-rgb-srgb-js-convert/">小tip: 了解LinearRGB和sRGB以及使用JS相互转换 « 张鑫旭-鑫空间-鑫生活 (zhangxinxu.com)</a></p>
</blockquote>
<p><strong>人这种动物，对于真实世界的颜色感受，并不是线性的，而是曲线的</strong></p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810162118.png" alt="image.png|444"></p>
<p>(How)线性RGB与sRGB相互转化</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/bby1987/article/details/109522126">RGB与Lab转换_rgb转lab-CSDN博客</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear_to_srgb</span>(<span class="params">linear</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(linear, torch.Tensor):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Assumes `linear` is in [0, 1], see https://en.wikipedia.org/wiki/SRGB.&quot;&quot;&quot;</span></span><br><span class="line">        eps = torch.finfo(torch.float32).eps</span><br><span class="line">        srgb0 = <span class="number">323</span> / <span class="number">25</span> * linear</span><br><span class="line">        srgb1 = (<span class="number">211</span> * torch.clamp(linear, <span class="built_in">min</span>=eps)**(<span class="number">5</span> / <span class="number">12</span>) - <span class="number">11</span>) / <span class="number">200</span></span><br><span class="line">        <span class="keyword">return</span> torch.where(linear &lt;= <span class="number">0.0031308</span>, srgb0, srgb1)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(linear, np.ndarray):</span><br><span class="line">        eps = np.finfo(np.float32).eps</span><br><span class="line">        srgb0 = <span class="number">323</span> / <span class="number">25</span> * linear</span><br><span class="line">        srgb1 = (<span class="number">211</span> * np.maximum(eps, linear) ** (<span class="number">5</span> / <span class="number">12</span>) - <span class="number">11</span>) / <span class="number">200</span></span><br><span class="line">        <span class="keyword">return</span> np.where(linear &lt;= <span class="number">0.0031308</span>, srgb0, srgb1)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">srgb_to_linear</span>(<span class="params">srgb</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(srgb, torch.Tensor):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Assumes `srgb` is in [0, 1], see https://en.wikipedia.org/wiki/SRGB.&quot;&quot;&quot;</span></span><br><span class="line">        eps = torch.finfo(torch.float32).eps</span><br><span class="line">        linear0 = <span class="number">25</span> / <span class="number">323</span> * srgb</span><br><span class="line">        linear1 = torch.clamp(((<span class="number">200</span> * srgb + <span class="number">11</span>) / (<span class="number">211</span>)), <span class="built_in">min</span>=eps)**(<span class="number">12</span> / <span class="number">5</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.where(srgb &lt;= <span class="number">0.04045</span>, linear0, linear1)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(srgb, np.ndarray):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Assumes `srgb` is in [0, 1], see https://en.wikipedia.org/wiki/SRGB.&quot;&quot;&quot;</span></span><br><span class="line">        eps = np.finfo(np.float32).eps</span><br><span class="line">        linear0 = <span class="number">25</span> / <span class="number">323</span> * srgb</span><br><span class="line">        linear1 = np.maximum(((<span class="number">200</span> * srgb + <span class="number">11</span>) / (<span class="number">211</span>)), eps)**(<span class="number">12</span> / <span class="number">5</span>)</span><br><span class="line">        <span class="keyword">return</span> np.where(srgb &lt;= <span class="number">0.04045</span>, linear0, linear1)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
<h1 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h1><p><a href="DeepLearning.md">DeepLearning</a></p>
<h2 id="数据处理方法"><a href="#数据处理方法" class="headerlink" title="数据处理方法"></a>数据处理方法</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/73534694">SOM(自组织映射神经网络)——理论篇 - 知乎</a><br><a target="_blank" rel="noopener" href="https://nice-ai.top/posts/SelfOrganizingMaps-SOM.html">自组织映射（SOM）理论基础与Python NumPy实现 | 美美智能博客站</a><br>针对数据量大的情况</p>
</blockquote>
<p>自组织映射(Self-organizing map, SOM)通过学习输入空间中的数据，生成一个低维、离散的映射(Map)，从某种程度上也可看成一种降维算法。（<em>降维或者升为可以由输入和输出尺寸决定</em>，SOM输入为1E6，输出为70x70，本质上数据量变少，因此为降维算法）</p>
<h2 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240503135053.png" alt="image.png|666"></p>

    </div>

    
    
    
        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://t.me/+1ZnNxFWnEbk5YzRl">
            <span class="icon">
              <i class="fab fa-telegram"></i>
            </span>

            <span class="label">Telegram</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Learn/Python/Linux%20OS/" rel="prev" title="Linux">
      <i class="fa fa-chevron-left"></i> Linux
    </a></div>
      <div class="post-nav-item">
    <a href="/3DReconstruction/Multi-view/Explicit%20Volumetric/GS-based/2DGS/" rel="next" title="2DGS">
      2DGS <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B3%9B%E5%87%BD"><span class="nav-text">泛函</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8C%83%E6%95%B0"><span class="nav-text">范数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%8D%E5%8F%98%E5%87%BD%E6%95%B0"><span class="nav-text">复变函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%8F%98%E6%8D%A2"><span class="nav-text">拉普拉斯变换</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9F%AF%E8%A5%BF%E7%A7%AF%E5%88%86%E5%85%AC%E5%BC%8F"><span class="nav-text">柯西积分公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%95%99%E6%95%B0"><span class="nav-text">留数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80"><span class="nav-text">数学基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%95%E6%96%AF%E7%93%A6%E5%AE%9A%E7%90%86"><span class="nav-text">帕斯瓦定理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B"><span class="nav-text">偏微分方程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF"><span class="nav-text">卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E7%9B%B8%E5%85%B3"><span class="nav-text">自相关</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%84%E5%8A%A8%E6%B3%95"><span class="nav-text">摄动法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%B0%E5%8B%92"><span class="nav-text">泰勒</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Domain-Transform"><span class="nav-text">Domain Transform</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Fourier-Transform"><span class="nav-text">Fourier Transform</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DFT"><span class="nav-text">DFT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FFT"><span class="nav-text">FFT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#STFT"><span class="nav-text">STFT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Wavelet-Transform"><span class="nav-text">Wavelet Transform</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Laplace-Transform"><span class="nav-text">Laplace Transform</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%8D%E6%95%B0-Complex-Number"><span class="nav-text">复数 Complex Number</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KL-%E6%95%A3%E5%BA%A6"><span class="nav-text">KL 散度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%87%B8%E5%87%BD%E6%95%B0%E4%B8%8EJensen%E4%B8%8D%E7%AD%89%E5%BC%8F"><span class="nav-text">凸函数与Jensen不等式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E7%A7%AF%E5%88%86%E6%B1%82%E5%AF%BC"><span class="nav-text">定积分求导</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA"><span class="nav-text">概率论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic"><span class="nav-text">Basic</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E7%8E%87-%E4%BC%BC%E7%84%B6"><span class="nav-text">概率&#x2F;似然</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE"><span class="nav-text">均值方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6"><span class="nav-text">联合概率密度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4"><span class="nav-text">置信区间</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bayes"><span class="nav-text">Bayes</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Distribution"><span class="nav-text">Distribution</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83"><span class="nav-text">泊松分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1"><span class="nav-text">核密度估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gamma-%E5%88%86%E5%B8%83"><span class="nav-text">Gamma 分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Beta-%E5%88%86%E5%B8%83"><span class="nav-text">Beta 分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="nav-text">高斯分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="nav-text">多元高斯分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="nav-text">混合高斯分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%94%E5%90%88%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="nav-text">联合高斯分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#P-box"><span class="nav-text">P-box</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Markov-Chains"><span class="nav-text">Markov Chains</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sampling-Method"><span class="nav-text">Sampling Method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4-1"><span class="nav-text">置信区间</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%B1%95%E7%A4%BA"><span class="nav-text">图像展示</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%B1%E5%9E%8B%E5%9B%BE"><span class="nav-text">箱型图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#t-SNE-%E6%95%B0%E6%8D%AE%E9%99%8D%E7%BB%B4"><span class="nav-text">t-SNE 数据降维</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="nav-text">线性代数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E7%9A%84%E8%8C%83%E6%95%B0"><span class="nav-text">矩阵的范数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E6%AE%8A%E7%9F%A9%E9%98%B5%E5%AE%9A%E4%B9%89"><span class="nav-text">特殊矩阵定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3"><span class="nav-text">特征分解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%98%E6%8D%A2"><span class="nav-text">变换</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2D"><span class="nav-text">2D</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3D"><span class="nav-text">3D</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Homography"><span class="nav-text">Homography</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Computer-Graphics"><span class="nav-text">Computer Graphics</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SDF%E8%AE%A1%E7%AE%97%E4%B8%8E%E6%B1%82%E5%AF%BC"><span class="nav-text">SDF计算与求导</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Computer-Vision"><span class="nav-text">Computer Vision</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%9B%BE%E5%83%8F%E5%A4%A7%E5%B0%8F%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F"><span class="nav-text">卷积图像大小计算公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#linearColor-2-sRGB"><span class="nav-text">linearColor 2 sRGB</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Other"><span class="nav-text">Other</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95"><span class="nav-text">数据处理方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%8C%E8%85%8A%E5%AD%97%E6%AF%8D"><span class="nav-text">希腊字母</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Qi Yun"
      src="/images/avatar.jpeg">
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">173</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">59</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/qiyun71" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qiyun71" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/29010355" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;29010355" rel="noopener" target="_blank"><i class="fa fa-star fa-fw"></i>Bilibili</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qi Yun</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">545k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">33:03</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/yqq/anime.min.js"></script>
  <script src="/yqq/velocity/velocity.min.js"></script>
  <script src="/yqq/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="//cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/three.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
