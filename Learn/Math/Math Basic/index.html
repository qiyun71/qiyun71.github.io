<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/blog.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/blog.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/yqq/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="MATH $e^{i\pi}+1&#x3D;0$">
<meta property="og:type" content="article">
<meta property="og:title" content="Math">
<meta property="og:url" content="http://example.com/Learn/Math/Math%20Basic/index.html">
<meta property="og:site_name" content="QiYun">
<meta property="og:description" content="MATH $e^{i\pi}+1&#x3D;0$">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20241001123745.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240925100202.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240921212004.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20240316151511.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240328193106.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240328193446.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/v2-eb0945aa2185df958f4568e58300e77a_1440w.gif">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230801135729.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230801135800.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240722141841.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240401085715.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240921142810.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810162118.png">
<meta property="og:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240503135053.png">
<meta property="article:published_time" content="2024-03-29T14:24:11.000Z">
<meta property="article:modified_time" content="2024-10-03T08:46:07.525Z">
<meta property="article:author" content="Qi Yun">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20241001123745.png">

<link rel="canonical" href="http://example.com/Learn/Math/Math%20Basic/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Math | QiYun</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">QiYun</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Note</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/Learn/Math/Math%20Basic/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Qi Yun">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QiYun">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Math
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-03-29 22:24:11" itemprop="dateCreated datePublished" datetime="2024-03-29T22:24:11+08:00">2024-03-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-03 16:46:07" itemprop="dateModified" datetime="2024-10-03T16:46:07+08:00">2024-10-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Learn/" itemprop="url" rel="index"><span itemprop="name">Learn</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>7.2k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>26 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>MATH $e^{i\pi}+1=0$</p>
<span id="more"></span>
<h1 id="泛函"><a href="#泛函" class="headerlink" title="泛函"></a>泛函</h1><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/21938224">(72 封私信 / 80 条消息) 「泛函」究竟是什么意思？ - 知乎</a><br><a target="_blank" rel="noopener" href="https://www.pynumerical.com/archives/48/">Calculus of Variations：变分计算 - 分享我的学习心得</a> 变分：自变量x不变，函数$y(\cdot)$改变</p>
</blockquote>
<p>研究不同的函数，对输出的影响。<br>例如MLP要拟合一个函数，让预测的输出与标签非常相近。又如有限元求解偏微分方程，是要根据<strong>最小作用量原理</strong>，求得一个满足边界条件的、相对准确的近似解</p>
<h1 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h1><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=KuXjwB4LzSA">但什么是卷积呢？ - YouTube</a></p>
</blockquote>
<p>$f\left(t\right)*g\left(t\right)=\int_{0}^{t}f\left(\tau\right)g\left(t-\tau\right)d\tau$</p>
<p>Example1：已知$f(x) = a_{0}+a_{1}x+\dots+a_{n}x^{n}$ 和 $g(x)=b_{0}+b_{1}x+\dots b_{n}x^{n}$，求$h(x)=f(x) \cdot g(x)$</p>
<ul>
<li>$h(x)$的系数c是ab两系数的卷积结果：直接计算的话时间复杂度为$\mathcal{O}(n^{2})$</li>
</ul>
<script type="math/tex; mode=display">\left.\mathbf{a}*\mathbf{b}=\left[\begin{array}{c}a_0b_0,\\a_0b_1+a_1b_0,\\a_0b_2+a_1b_1+a_2b_0,\\\vdots\\a_{n-1}b_{m-1}\end{array}\right.\right]</script><ul>
<li>另一种思路是先将$f(x)$于$g(x)$进行FFT$f(\omega),g(\omega)$，频域的系数$\hat{\mathbf{a}}=[\hat{a}_0,\hat{a}_1,\hat{a}_2,\ldots,\hat{a}_{m+n-1}]$ 和$\hat{\mathbf{b}}=[\hat{b}_0,\hat{b}_1,\hat{b}_2,\ldots,\hat{b}_{m+n-1}]$，两者直接相乘得到$\hat{\mathbf{a}}\cdot\hat{\mathbf{b}}=[\hat{a}_0\hat{b}_0,\hat{a}_1\hat{b}_1,\hat{a}_2\hat{b}_2,\ldots,]$，然后进行逆FFT，得到的$h(x)$系数即想要的结果，时间复杂度为$\mathcal{O}(n\log n)$</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Yk4y1K7Az/?spm_id_from=333.999.0.0&amp;vd_source=1dba7493016a36a32b27a14ed2891088">【官方双语】卷积的两种可视化|概率论中的X+Y既美妙又复杂_哔哩哔哩_bilibili</a> 最好的动画⭐</p>
</blockquote>
<h2 id="摄动法"><a href="#摄动法" class="headerlink" title="摄动法"></a>摄动法</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://kexue.fm/archives/1878">轻微的扰动——摄动法简介(1) - 科学空间|Scientific Spaces</a> </p>
</blockquote>
<h2 id="泰勒"><a href="#泰勒" class="headerlink" title="泰勒"></a>泰勒</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://dezeming.top/wp-content/uploads/2021/06/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B0%EF%BC%88%E5%8F%8A%E5%90%91%E9%87%8F%E5%87%BD%E6%95%B0%EF%BC%89%E7%9A%84%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80.pdf">dezeming.top/wp-content/uploads/2021/06/多元函数（及向量函数）的泰勒展开.pdf</a></p>
</blockquote>
<p>泰勒展开本质上是求近似</p>
<h2 id="Domain-Transform"><a href="#Domain-Transform" class="headerlink" title="Domain Transform"></a>Domain Transform</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=3gjJDuCAEQQ">The intuition behind Fourier and Laplace transforms I was never taught in school - YouTube</a></p>
</blockquote>
<h3 id="Fourier-Transform"><a href="#Fourier-Transform" class="headerlink" title="Fourier Transform"></a>Fourier Transform</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://yangwc.com/2019/10/24/FFT/">频域处理Frequency domain processing：傅里叶变换 | YangWC’s Blog</a></p>
</blockquote>
<p>$Fourier: F(\omega)=\int_{-\infty}^{\infty}f(t)e^{-i\omega t}dt$<br>$F(\omega)=\int_{-\infty}^{\infty}f(t)\cos{(\omega t)}dt-i\int_{-\infty}^{\infty}f(t)\sin{(\omega t)}dt$ </p>
<p>傅里叶变换可以看作使用不同频率$\omega$的sin和cos对原始时域信号进行积分，sin和cos积分得到的值越大，则虚部和实部的值越大，直观的模/振幅 magnitude 也越大，该频率$\omega$成分下的值也越大</p>
<p>对于信号$cos\pi t$ 进行FT，其虚部$\cos \pi t \cdot \sin \omega t$一直为0，实部$\sin \pi t \cdot \sin \omega t$在其他地方为0，在$\omega =\pi$时达到正无穷</p>
<h4 id="DFT"><a href="#DFT" class="headerlink" title="DFT"></a>DFT</h4><h4 id="FFT"><a href="#FFT" class="headerlink" title="FFT"></a>FFT</h4><h4 id="STFT"><a href="#STFT" class="headerlink" title="STFT"></a>STFT</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/26673889/answer/3292923354">短时傅里叶变换和小波变换有何不同？ - Mr.看海的回答 - 知乎</a></p>
</blockquote>
<p>可见当信号的频率成分随时间显著变化时，即所谓的非平稳信号，传统频谱分析就不太合适了。<br>举几个例子：</p>
<ul>
<li>研究信号的局部特性：如爆炸声、机器的故障噪声等，这些信号的特性在短时间内变化很大。</li>
<li>语音处理：在自动语音识别和语音合成中，语音的特征如音高和音量随着时间而变化。</li>
<li>雷达和无线电信号分析：雷达信号的特性依赖于时间和观测的角度，需分析这些信号随时间的变化。</li>
<li>生物医学信号分析：比如心电图（ECG）和脑电图（EEG）这样的生理信号，它们的频率特性随时间改变。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20241001123745.png" alt="image.png|333"></p>
<p>在短时傅里叶变换（STFT）中，窗口函数及其大小选择是分析的关键。窗口函数决定了在任何给定时间点，信号的哪一部分被用于分析。窗口大小的选择直接影响了分析结果的时间分辨率和频率分辨率，这是进行有效STFT分析的最重要的权衡。</p>
<p>窗口大小的时间分辨率影响：时间分辨率与窗口的宽度密切相关。一个窄窗口提供较高的时间分辨率，因为它捕捉了信号在很短时间内的变化。这对于分析包含快速变化的瞬态事件，如敲击声或爆炸声，是非常有用的。然而，较小的窗口将限制频率分辨率，因为频率分析需要足够的周期来准确估计。</p>
<p>窗口大小的频率分辨率影响：频率分辨率与窗口的宽度呈反比。一个宽窗口覆盖了信号的较长时间段，提供了较高的频率分辨率。这是因为更多的周期可以在窗口内被分析，从而更准确地确定低频成分。但是，这会牺牲时间分辨率，因为窗口中的信号被假定在这段时间内是平稳的。</p>
<p><strong>那有没有一种可能，窗口大小是可调的呢？</strong></p>
<h4 id="Wavelet-Transform"><a href="#Wavelet-Transform" class="headerlink" title="Wavelet Transform"></a>Wavelet Transform</h4><p>可以发现其特点：高频部分具有较高的时间分辨率和较低的频率分辨率，而低频部分具有较高的频率分辨率和较低的时间分辨率，这就恰好解决了STFT的痛点</p>
<h3 id="Laplace-Transform"><a href="#Laplace-Transform" class="headerlink" title="Laplace Transform"></a>Laplace Transform</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=n2y7n6jw5d0">What does the Laplace Transform really tell us? A visual explanation (plus applications) - YouTube</a></p>
</blockquote>
<p>$X(s)=\int_0^\infty x(t)e^{-st}dt$</p>
<p>$Laplace:F(s)=\int_0^\infty f(t)e^{-st}dt$</p>
<p>$s=\alpha+i\omega$</p>
<p>$F(s)=\int_0^\infty f(t)e^{-i\omega t}e^{-\alpha t}dt$</p>
<ul>
<li>$e^{-i\omega t}$ scans for sinusoids</li>
<li>$e^{-\alpha t}$ scans for exponentials</li>
</ul>
<p>时域的微积分在s域中可以很好地计算</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240925100202.png" alt="image.png|666"></p>
<h2 id="复数-Complex-Number"><a href="#复数-Complex-Number" class="headerlink" title="复数 Complex Number"></a>复数 Complex Number</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.longluo.me/blog/2021/12/29/fourier-transform/">傅里叶变换(Fourier Transform) | Long Luo’s Life Notes</a></p>
</blockquote>
<p>在复平面上，1 有 n 个不同的 n 次方根，它们位于复平面的单位圆上，构成<strong>正多边形的顶点</strong>，但最多只可有两个顶点同时标在实数线上</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240921212004.png" alt="image.png|666"></p>
<h2 id="KL-散度"><a href="#KL-散度" class="headerlink" title="KL 散度"></a>KL 散度</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.vectorexplore.com/tech/loss-functions/kl-divergence/">KL 散度（Kullback-Leibler Divergence）：图示+公式+代码</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/45131536">机器学习中的散度 - 知乎</a></p>
</blockquote>
<p>概率分布P的概率密度函数为 $p(x_i)$<br>信息理论的主要目标是量化数据中的信息量。信息理论中最重要的度量标准称为熵（Entropy），通常表示为 H其信息熵：$H(X)=-\sum_{i=1}^np(x_i)\log p(x_i)$ , <strong><em>可以将-log(p)看成权重，概率p越大，权重越小</em></strong> (如果有人告诉我们一个相当不可能的事件发生了，我们收到的信息要多于我们被告知某个很可能发生的事件发生时收到的信息。如果我们知道某件事情一定会发生，那么我们就不会接收到信息。)</p>
<ul>
<li>如果我们在计算中使用 $log_{2}$​ 我们可以将熵解释为“编码我们的信息所需的最小比特数”。<ul>
<li>【例1】A、B、C、D 四个字母分别占 1/2（4096个），1/4（2048个），1/8（1024个），1/8（1024个）。那么最有效的一种编码方式为 A(0)，B（10），C（110），D(111)。整个语料库的长度 4096 x 1 + 2048 x 2 + 1024 x 3 x 2 = 14336，平均长度为 14336/8192 = 1.75。和下面代码中的【结果1】一致。<em>如果使用另一种编码方式(例2)即  A（110），B（111），C（0），D（10），则熵为(4096 </em> 3 + 2048 <em> 3 + 1024 </em> 1 + 1024 <em> 2 )/8192 = 2.625</em><ul>
<li>其中如果p(x)=0.5，则只需要$-log_{2}(0.5)=1$个bit的编码；如果p(x)=0.25，则需要2 bit的编码，以此类推</li>
<li>依照这种规律的编码，可以保证信息中的熵最小</li>
</ul>
</li>
<li>【例2】ABCD 四个字母占比变成了1/8（1024 个），1/8（1024 个），1/2（4096 个），1/4（2048 个），这样最有效的一种编码方式为 A（110），B（111），C（0），D（10），计算平均长度为1.75，和代码中的【结果3】一致。 <em>如果使用另一种编码方式(例1)即  A(0)，B（10），C（110），D(111)，则熵为(1024 </em> 1 + 1024 <em> 2 + 4096 </em> 3 + 2048 <em> 3 )/8192 = 2.625</em></li>
<li>相对熵(KL散度)：<ul>
<li>例1 相对于 例2 的相对熵为 $p(x)\log_{2}\left( \frac{p(x)}{q(x)} \right)$=(4096 <em> -(1-3) + 2048 </em> -(2-3) + 1024 <em> -(3-1) + 1024 </em> -(3-2) )/8192 = 0.875 = 2.625 - 1.75。 这说明针对例子中的这一分布，使用法1编码相对于法2来说，平均可以省下0.875bit</li>
<li>例2 相对于 例1 的相对熵为 $q(x)\log_{2}\left( \frac{q(x)}{p(x)} \right)$=(1024 <em> -(3-1) + 1024 </em> -(3-2) + 4096 <em> -(1-3) + 2048 </em> -(2-3) )/8192 =0.875 = 2.625 - 1.75</li>
<li>两个例子尽管编码方式不同，但字母的频率分布是一致的。因此，在计算KL散度时，由于 p(x) 和 q(x) 实际上是相同的，导致 KL散度的两个方向相等。<strong>为什么是0.875而不是0？</strong> 这说明即使两种编码方案都能描述相同的概率分布，<strong>但由于它们在信息表示方式上的差异</strong>，导致了每种编码方式在另一种分布下都有额外的冗余。KL散度反映了这种冗余：即使分布相同，编码方式的差异会导致你用一种方式来编码另一种信息时需要额外的0.875 bits。<strong>KL散度真正衡量的是概率分布之间的差异，而不是编码方案之间的差异。</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>KL散度的值越大，表示用一个分布近似另一个分布时引入的信息损失或误差越大：</p>
<ul>
<li>非对称性：KL散度是非对称的，即从 P 分布到 Q 分布的 KL 散度与从 Q 分布到 P 分布的 KL 散度可能不同。<ul>
<li>虽然KL散度通常是非对称的，但在特定条件下<strong>KL散度从 P 到 Q</strong> 与 <strong>从 Q 到 P</strong> 的值可以相等：<ul>
<li><strong>在某些特定的离散分布情况下</strong>，当 P(x)和 Q(x)的概率质量函数具有某种对称结构时，它们的KL散度可能相等（但这仍是概率分布的特殊情况，而不是普遍现象）。</li>
<li>。当且仅当两个概率分布完全相同时，KL散度的值为零</li>
</ul>
</li>
</ul>
</li>
<li>非负性：KL散度的值始终为非负数。KL散度值越大，表示两个概率分布越不相似。</li>
<li>非度量性：KL散度并不满足度量空间的性质，特别是三角不等式。由于非对称性和非度量性，KL 散度不能用于计算两个分布之间的“距离”或“相似度”。</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://wuli.wiki/online/KLD.html">KL 散度（相对熵） - 小时百科 (wuli.wiki)</a></p>
</blockquote>
<p><strong>KL 散度</strong>（Kullback–Leibler divergence，缩写 KLD）是一种统计学度量，<strong>表示的是一个概率分布相对于另一个概率分布的差异程度</strong>，在信息论中又称为<strong>相对熵</strong>（Relative entropy）。对于随机变量Q的概率分布，相对于随机变量P的概率分布的KL散度定义为： $D_{KL}(P||Q)=H(P,Q)-H(P)$</p>
<script type="math/tex; mode=display">\begin{equation}
D_{KL}(P||Q)=\sum_{x\in X}P(x)ln(\frac{P(x)}{Q(x)})=\sum_{x\in X}P(x)(ln(P(x))-ln(Q(x)))~.
\end{equation}</script><p>对于连续型随机变量，设概率空间 X 上有两个概率分布 P 和 Q，其概率密度分别为 p 和 q，那么，P 相对于 Q 的 KL 散度定义如下：</p>
<script type="math/tex; mode=display">\begin{equation}
D_{KL}(P||Q)=\int_{-\infty}^{+\infty}p(x)ln(\frac{p(x)}{q(x)})dx~.
\end{equation}</script><p> 显然，当 P=Q 时，$D_{KL}=0$</p>
<p>两个一维高斯分布的 KL 散度公式：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/">KL散度(Kullback-Leibler Divergence)介绍及详细公式推导 | HsinJhao’s Blogs</a></p>
</blockquote>
<script type="math/tex; mode=display">\begin{aligned}
KL(p,q)& =\int[\left.p(x)\log(p(x))-p(x)\log(q(x))\right]dx  \\
&=-\frac12\left[1+\log(2\pi\sigma_1^2)\right]-\left[-\frac12\log(2\pi\sigma_2^2)-\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}\right] \\
&=\log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac12
\end{aligned}</script><h2 id="凸函数与Jensen不等式"><a href="#凸函数与Jensen不等式" class="headerlink" title="凸函数与Jensen不等式"></a>凸函数与Jensen不等式</h2><p>凸函数是一个定义在某个向量空间的凸子集 C（区间）上的实值函数 f，如果在其定义域 C 上的任意两点$x_{1},x_{2}$，$0\leq t\leq 1$，有：<br>满足$tf(x_1)+(1-t)f(x_2)\geq f\left(tx_1+(1-t)x_2\right)$<br>也就是说凸函数任意两点的割线位于函数图形上方， <strong>这也是Jensen不等式的两点形式</strong></p>
<p>若对于任意点集 $\{x_{i}\}$，若 $\lambda_i≥0$且 $∑_{i}\lambda_{i}=1$ ，使用<strong>数学归纳法</strong>，可以证明凸函数 f (x) 满足：<br>$f(\sum_{i=1}^M\lambda_ix_i)\leq\sum_{i=1}^M\lambda_if(x_i)$ 即为Jesen不等式</p>
<p><strong>在概率论中</strong>，如果把 $\lambda_i$ 看成取值为 $x_{i}$的离散变量 x 的概率分布$p_{i}$，那么公式(2)就可以写成<br>$f(E[x])\leq E[f(x)]$ , $E[\cdot]$代表期望</p>
<p>对于连续变量，Jensen不等式给出了积分的凸函数值和凸函数的积分值间的关系：<br>$f(\int xp(x)dx)\leq\int f(x)p(x)dx$</p>
<h1 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h1><h2 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h2><h3 id="均值方差"><a href="#均值方差" class="headerlink" title="均值方差"></a>均值方差</h3><p>为什么样本估计方差要除以n-1 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/550427703">【浅谈】样本方差的分母“n”为什么要改为“n-1” - 知乎</a></p>
<p>其实就是自由度，当你算标准差的时候，已经知道均值了，那么就只有n-1个数字是自由的，第n个数值可以由前面的n-1个数字和均值算出来了，所以他实际上不包含任何关于数据波动的信息</p>
<p>无偏估计的方差：<br>$\left\{\begin{array}{c}M_n=\frac{X_1+X_2+\cdots+X_n}n\\\hat{S}_n^2=\frac{\sum_{i=1}^n(X_i-M_n)^2}{n-1}\end{array}\right.$</p>
<h3 id="概率-似然"><a href="#概率-似然" class="headerlink" title="概率/似然"></a>概率/似然</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/334890990">通俗理解“极大似然估计” - 知乎</a></p>
</blockquote>
<p>概率函数：由因到果，已知参数(概率)，根据真实参数（或已经发生的观测结果）去推测未来的观测结果<br>似然函数：由果到因，根据已经发生的观测结果去猜想真实参数，这个过程叫做<strong>估计</strong>；估计正确的可能性叫做<strong>似然性</strong>。估计参数的似然性，其目的是帮助我们根据已观测的结果，推测出最符合观测结果、最合理的参数。</p>
<p>假设一个参数p，则在这一参数基础上出现结果的概率即为似然</p>
<script type="math/tex; mode=display">\begin{aligned}
&L(p)=L(p|x_1,\ldots,x_n) \\
&=P(X_1=x_1|p)•\ldots•P(X_n=x_n|p) \\
&=\prod_{i=1}^nP(X_i=x_i|p)
\end{aligned}</script><p>极大似然估计（maximum likelihood estimation，缩写为MLE），也称<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=162550522&amp;content_type=Article&amp;match_order=2&amp;q=%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1&amp;zhida_source=entity">最大似然估计</a>。<br>$\arg\max_pL(p)=\arg\max_p\prod_{i=1}^nP(X_i=x_i|p)$<br>将$L(p)$看作p的函数，对其求导，导数为0，即可得到最大的似然值对应的参数p</p>
<p>求导前为什么要取对数？：<strong>（1）避免下溢出</strong> <strong>（2）便于计算</strong> 将累积乘法转换成累加</p>
<h2 id="Bayes"><a href="#Bayes" class="headerlink" title="Bayes"></a>Bayes</h2><p>Follow:</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/people/chenxran0916/posts">KERO - 知乎</a></p>
</blockquote>
<ul>
<li>最大似然</li>
</ul>
<p>最大似然估计(Maximum Likelihood Estimation, MLE)，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！样本从某一个客观存在的模型中抽样得来，然后根据样本来计算该模型的数学参数，即：模型已定，参数未知！</p>
<script type="math/tex; mode=display">\begin{aligned}
\widehat{\theta}_{\mathrm{MLE}}& =\arg\max P(X;\theta)  \\
&=\arg\max P(x_1;\theta)P(x_2;\theta)\cdot\cdots P(x_n;\theta) \\
&=\arg\max\log\prod_{i=1}^nP(x_i;\theta) \\
&=\arg\max\sum_{i=1}^n\log P(x_i;\theta) \\
&=\arg\min-\sum_{i=1}^n\log P(x_i;\theta)
\end{aligned}</script><ul>
<li>最大后验</li>
</ul>
<script type="math/tex; mode=display">\begin{aligned}
\hat{\theta}_{\mathrm{MAP}}& =\arg\max P(\theta|X)  \\
&=\arg\min-\log P(\theta|X) \\
&=\arg\min-\log P(X|\theta)-\log P(\theta)+\log P(X) \\
&=\arg\min-\log P(X|\theta)-\log P(\theta)
\end{aligned}</script><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=HZGCoVF3YvM">贝叶斯定理，改变信念的几何学 - YouTube</a> ——数形结合<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/401258319">走进贝叶斯统计（一）—— 先验分布与后验分布 - 知乎</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/USTC-ZCC/p/12786860.html">超详细讲解贝叶斯网络(Bayesian network) - USTC丶ZCC - 博客园</a></p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20240316151511.png" alt="image.png|666"></p>
<p>全概率公式：$\mathbf{P(H)}=\mathbf{P(H|A)P(A)}+\mathbf{P(H|B)P(B)}$，结果H发生的概率</p>
<p>贝叶斯公式：$\mathbf{P}(\mathbf{A}|\mathbf{H})=\frac{P(A)P(H|A)}{P(H)}$，H结果发生时，是由A导致的概率</p>
<ul>
<li>连续$p(y_0|x)=\frac{p(x|y_0)p(y_0)}{\int_{-\infty}^{+\infty}p(x|y)p(y)dy}$</li>
<li>离散$p(y_j|x)=\frac{p(x|y_j)p(y_j)}{\sum_{i=0}^np(x|y_i)p(y_i)}$</li>
</ul>
<p>在使用数据估计参数$\theta$之前，我们需要给这个参数设定一个分布，即先验分布$p(\theta)$（根据经验得到）</p>
<p>$p(\theta|X)=\frac{p(\theta,X)}{p(X)}=\frac{p(X|\theta)p(\theta)}{\int_{-\infty}^{+\infty}p(X|\theta)p(\theta)d\theta}.$</p>
<ul>
<li>$p(\theta|X)$是$\theta$的后验分布</li>
<li>$p(X|\theta)$是在给定$\theta$下关于数据样本的似然函数</li>
<li>$\int_{-\infty}^{+\infty}p(X|\theta)p(\theta)d\theta$ 为常数c，可以写为$p(\theta|X)\propto p(X|\theta)p(\theta).$</li>
</ul>
<h2 id="Distribution"><a href="#Distribution" class="headerlink" title="Distribution"></a>Distribution</h2><h3 id="核密度估计"><a href="#核密度估计" class="headerlink" title="核密度估计"></a>核密度估计</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://viruspc.github.io/blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2021/09/27/kde.html">核密度估计 (KDE, Kernel Density Estimation) | Blog</a></p>
</blockquote>
<h3 id="Gamma-分布"><a href="#Gamma-分布" class="headerlink" title="Gamma 分布"></a>Gamma 分布</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gamma_distribution">Gamma distribution - Wikipedia</a></p>
</blockquote>
<p>Gamma Function：$\Gamma(z)=\int_0^\infty t^{z-1}e^{-t}\mathrm{d}t,\quad\Re(z)&gt;0.$ or $\Gamma(n)=(n-1)!.$</p>
<p>$f(x)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$</p>
<p>图中k对应$\alpha$，$\theta$对应$\beta$</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240328193106.png" alt="image.png|333"></p>
<h3 id="Beta-分布"><a href="#Beta-分布" class="headerlink" title="Beta 分布"></a>Beta 分布</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution - Wikipedia</a></p>
</blockquote>
<p>通常是概率分布的分布</p>
<p>$\mathbf{B}(\alpha,\beta)={\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}}$</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240328193446.png" alt="image.png|333"></p>
<h3 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h3><p>$X\sim N(\mu,\sigma^2)$</p>
<p>PDF：$f(x)=\frac1{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$</p>
<h3 id="多元高斯分布"><a href="#多元高斯分布" class="headerlink" title="多元高斯分布"></a>多元高斯分布</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/kailugaji/p/15542845.html">多元/多维高斯/正态分布概率密度函数推导 (Derivation of the Multivariate/Multidimensional Normal/Gaussian Density) - 凯鲁嘎吉 - 博客园</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/bingjianing/p/9117330.html">多元高斯分布（The Multivariate normal distribution） - bingjianing - 博客园</a></p>
</blockquote>
<p>概率密度函数<br>$p(x)=p(x_{1},x_{2},\ldots,x_{D})=\frac{1}{(2\pi)^{D/2}}\exp\left(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right)$</p>
<h3 id="混合高斯分布"><a href="#混合高斯分布" class="headerlink" title="混合高斯分布"></a>混合高斯分布</h3><p>GMM</p>
<h3 id="联合高斯分布"><a href="#联合高斯分布" class="headerlink" title="联合高斯分布"></a>联合高斯分布</h3><p>如何构造两个协方差标准分布</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/kdazhe/article/details/104599229">生成一定相关性的二元正态分布_怎么产生二元正态分布的随机数-CSDN博客</a></p>
</blockquote>
<p>AB为两个独立的标准正态分布：</p>
<p>$\begin{aligned}\mathrm{X}&amp;=\alpha\mathrm{A}+\beta\mathrm{B},\\\\\mathrm{Y}&amp;=\gamma\mathrm{A}+\delta\mathrm{B}\end{aligned}$</p>
<p>$\alpha,\beta,\gamma,\delta$是四个待确定的参数，希望找到这四个参数，使得X和Y也服从标准正态分布$N(0,1)$，并且相关系数$Cor(X,Y)=\rho$。</p>
<p>由标准正态分布性质可得：$\mathrm{X}\sim\mathrm{N}(0,\alpha^2+\beta^2),\mathrm{Y}\sim\mathrm{N}(0,\gamma^2+\delta^2)。$<br>要想：$\bar{\alpha}^{2}+\beta^{2}=1,\gamma^{2}+\delta^{2}=1$<br>则可以假设：$\alpha=\cos\theta,\beta=\sin\theta;\gamma=\sin\theta,\delta=\cos\theta,$</p>
<p>由相关系数：$\mathrm{Cor(X,~Y)}=\frac{\mathrm{Cov(X,~Y)}}{\sqrt{\mathrm{Var(X)Var(Y)}}}$，$\operatorname{Var}(\mathrm{X})=1$，$\operatorname{Var}(\mathrm{Y})=1$<br>因此使得：</p>
<script type="math/tex; mode=display">\begin{aligned}
\mathrm{Cov}(\mathrm{X,Y})& =\mathrm{Cov}\Big((\cos\theta)\mathrm{A}+(\sin\theta)\mathrm{B},(\sin\theta)\mathrm{A}+(\cos\theta)\mathrm{B}\Big)  \\
&=\cos\theta\sin\theta+\sin\theta\cos\theta  \\
&=\sin2\theta 
\end{aligned}</script><ul>
<li>（$Cov(A,A)=1,Cov(A,B)=0$）</li>
<li>$\text{Cov}(x_1 + x_2, y_1 + y_2) = \text{Cov}(x_1, y_1) + \text{Cov}(x_2, y_1) + \text{Cov}(x_1, y_2) + \text{Cov}(x_2, y_2)$</li>
<li>$\mathrm{Cov}(aX,Y)=a\mathrm{Cov}(X,Y)$</li>
</ul>
<p>则有：$\theta=\frac{\arcsin\rho}2$，然后令$\begin{cases}\mathrm X=(\cos\theta)\mathrm A+(\sin\theta)\mathrm B\\\mathrm Y=(\sin\theta)\mathrm A+(\cos\theta)\mathrm B\end{cases}$，则计算出来的X和Y就是相关性为$\rho$的标准正态分布</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">bivariateNormal</span>:</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rho: <span class="string">&#x27;float&#x27;</span>, m: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Suppose we want to generate a pair of </span></span><br><span class="line"><span class="string">        random variables X, Y, with X ~ N(0, 1), </span></span><br><span class="line"><span class="string">        Y ~ N(0, 1), and Cor(X, Y) = rho. m is </span></span><br><span class="line"><span class="string">        the number of data pairs we want to generate.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.rho = rho</span><br><span class="line">        self.m = m</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generateBivariate</span>(<span class="params">self</span>) -&gt; <span class="string">&#x27;tuple(np.array, np.array)&#x27;</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Generate two random variables X, Y, with X ~ N(0, 1), </span></span><br><span class="line"><span class="string">        Y ~ N(0, 1), and Cor(X, Y) = rho. </span></span><br><span class="line"><span class="string">        self.m is the number of sample points we generated.</span></span><br><span class="line"><span class="string">        We return a tuple (X, Y). </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        theta = np.arcsin(self.rho) / <span class="number">2</span></span><br><span class="line">        A = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, self.m)</span><br><span class="line">        B = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, self.m)</span><br><span class="line">        X = np.cos(theta) * A + np.sin(theta) * B</span><br><span class="line">        Y = np.sin(theta) * A + np.cos(theta) * B</span><br><span class="line">        <span class="keyword">return</span> X, Y</span><br><span class="line"></span><br><span class="line">m = <span class="number">10</span> ** <span class="number">3</span></span><br><span class="line">rho = -<span class="number">0.4</span></span><br><span class="line">a = bivariateNormal(rho, m)</span><br><span class="line">X, Y = a.generateBivariate()</span><br><span class="line">np.corrcoef(X, Y)</span><br></pre></td></tr></table></figure>
<h3 id="P-box"><a href="#P-box" class="headerlink" title="P-box"></a>P-box</h3><h2 id="Markov-Chains"><a href="#Markov-Chains" class="headerlink" title="Markov Chains"></a>Markov Chains</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://setosa.io/ev/markov-chains/">Markov Chains explained visually</a> 入门</p>
</blockquote>
<h2 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h2><p>如何在不知道目标概率密度函数的情况下，抽取所需数量的样本，使得这些样本符合目标概率密度函数。这个问题简称为抽样</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/full/10.1155/2018/8980756">Finite Element Model Updating in Bridge Structures Using Kriging Model and Latin Hypercube Sampling Method - Wu - 2018 - Advances in Civil Engineering - Wiley Online Library</a> 不同采样方法的讨论(simple random sampling (SRS), stratified sampling method, cluster sampling method, and systematic sampling) <strong>Latin Hypercube Sampling 属于 =分层采样</strong><br><a target="_blank" rel="noopener" href="https://huangc.top/2019/03/24/sampling-2019/">It’s all about Sampling - 子淳的博客 | Just Me</a></p>
</blockquote>
<h3 id="Monte-Carlo"><a href="#Monte-Carlo" class="headerlink" title="Monte Carlo"></a>Monte Carlo</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338103692">一文看懂蒙特卡洛采样方法 - 知乎 (zhihu.com)</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/39628670">简明例析蒙特卡洛（Monte Carlo）抽样方法 - 知乎 (zhihu.com)</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/406256344">走进贝叶斯统计（四）—— 蒙特卡洛方法 - 知乎</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/barwe/p/14140681.html">逆变换采样和拒绝采样 - barwe - 博客园 (cnblogs.com)</a><br><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo method - Wikipedia</a><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=c2WrJY8tnGE">【数之道 22】巧妙使用”接受-拒绝”方法，玩转复杂分布抽样 - YouTube</a></p>
</blockquote>
<p>MC Sampling<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/v2-eb0945aa2185df958f4568e58300e77a_1440w.gif" alt="v2-eb0945aa2185df958f4568e58300e77a_1440w.gif|222"></p>
<p>对某一种概率分布p(x)进行蒙特卡洛采样的方法主要分为直接采样、拒绝采样与重要性采样三种:</p>
<ul>
<li>Naive Method<ul>
<li>根据概率分布进行采样。对一个已知概率密度函数与累积概率密度函数的概率分布，我们可以直接从累积分布函数（cdf）进行采样（类似逆变换采样）</li>
</ul>
</li>
<li>Acceptance-Rejection Method <ul>
<li>逆变换采样虽然简单有效，但是当累积分布函数或者反函数难求时却难以实施，可使用MC的接受拒绝采样</li>
<li>对于累积分布函数未知的分布，我们可以采用接受-拒绝采样。如下图所示，p(z)是我们希望采样的分布，q(z)是我们提议的分布(proposal distribution)，令kq(z)&gt;p(z)，我们首先在kq(z)中按照直接采样的方法采样粒子，接下来判断这个粒子落在途中什么区域，对于落在灰色区域的粒子予以拒绝，落在红线下的粒子接受，最终得到符合p(z)的N个粒子</li>
<li><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230801135729.png" alt="image.png|333"></li>
<li>数学推导：<ul>
<li><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230801135800.png" alt="image.png|500"></li>
</ul>
<ol>
<li>从 $f_r(x)$ 进行一次采样 $x_i$</li>
<li>计算 $x_i$ 的 <strong>接受概率</strong> $\alpha$（Acceptance Probability）:$\alpha=\frac{f\left(x_i\right)}{f_r\left(x_i\right)}$</li>
<li>从 (0,1) 均匀分布中进行一次采样 u</li>
<li>如果 $\alpha$≥u，接受 $x_i$ 作为一个来自 f(x) 的采样；否则，重复第1步</li>
</ol>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">N=<span class="number">1000</span> <span class="comment">#number of samples needed</span></span><br><span class="line">i = <span class="number">1</span></span><br><span class="line">X = np.array([])</span><br><span class="line"><span class="keyword">while</span> i &lt; N:</span><br><span class="line">    u = np.random.rand()</span><br><span class="line">    x = (np.random.rand()-<span class="number">0.5</span>)*<span class="number">8</span></span><br><span class="line">    res = u &lt; <span class="built_in">eval</span>(x)/ref(x)</span><br><span class="line">    <span class="keyword">if</span> res:</span><br><span class="line">        X = np.hstack((X,x[res])) <span class="comment">#accept</span></span><br><span class="line">        ++i</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Importance Sampling</strong><ul>
<li>接受拒绝采样完美的解决了累积分布函数不可求时的采样问题。但是接受拒绝采样非常依赖于提议分布(proposal distribution)的选择，如果提议分布选择的不好，可能采样时间很长却获得很少满足分布的粒子。</li>
<li>$E_{p(x)}[f(x)]=\int_a^bf(x)\frac{p(x)}{q(x)}q(x)dx=E_{q(x)}[f(x)\frac{p(x)}{q(x)}]$</li>
<li>我们从提议分布q(x)中采样大量粒子$x_1,x_2,…,x_n$，每个粒子的权重是 $\frac{p(x_i)}{q(x_i)}$，通过加权平均的方式可以计算出期望:</li>
<li>$E_{p(x)}[f(x)]=\frac{1}{N}\sum f(x_i)\frac{p(x_i)}{q(x_i)}$<ul>
<li>q提议的分布，p希望的采样分布</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">N=<span class="number">100000</span></span><br><span class="line">M=<span class="number">5000</span></span><br><span class="line">x = (np.random.rand(N)-<span class="number">0.5</span>)*<span class="number">16</span></span><br><span class="line">w_x = <span class="built_in">eval</span>(x)/ref(x)</span><br><span class="line">w_x = w_x/<span class="built_in">sum</span>(w_x)</span><br><span class="line">w_xc = np.cumsum(w_x) <span class="comment">#accumulate</span></span><br><span class="line"></span><br><span class="line">X=np.array([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">    u = np.random.rand()</span><br><span class="line">    X = np.hstack((X,x[w_xc&gt;u][<span class="number">0</span>])) <span class="comment"># 其中，w_xc是对归一化后的权重计算的累计分布概率。每次取最终样本时，都会先随机一个(0,1)之间的随机数，并使用这个累计分布概率做选择。样本的权重越大，被选中的概率就越高。</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h3 id="分层采样"><a href="#分层采样" class="headerlink" title="分层采样"></a>分层采样</h3><h4 id="分层抖动采样"><a href="#分层抖动采样" class="headerlink" title="分层抖动采样"></a>分层抖动采样</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://yangwc.com/2020/04/11/Sampling2/">Physically Based Rendering：采样和重建（二） | YangWC’s Blog</a></p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240722141841.png" alt="image.png|666"></p>
<ul>
<li>随机采样</li>
<li>分层均匀采样</li>
<li>分层抖动采样。理想的分层抖动采样很容易陷入<strong>维数灾难</strong>，因此有人提出了高维转低维采样+随机串联(or随机配对)的方法</li>
</ul>
<h4 id="Latin-Hypercube-Sampling"><a href="#Latin-Hypercube-Sampling" class="headerlink" title="Latin Hypercube Sampling"></a>Latin Hypercube Sampling</h4><ol>
<li>将每个维度的区间划分为m个不重叠的区间，每个区间概率相等（取均匀分布，区间大小应相等）</li>
<li>从均匀分布中随机采样，每个维度的每个间隔中的一个点</li>
<li>将每个维度的点随机配对（相同可能的组合）</li>
</ol>
<p>相较于Simple Random Sampling，LHS的方法更加分散，且不存在聚类效应</p>
<h3 id="MCMC"><a href="#MCMC" class="headerlink" title="MCMC"></a>MCMC</h3><p>(Markov Chain Monte Carlo)</p>
<p>Blog:</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/37121528">马尔可夫链蒙特卡罗算法（MCMC） - 知乎</a><br>动画 <a target="_blank" rel="noopener" href="https://chi-feng.github.io/mcmc-demo/">The Markov-chain Monte Carlo Interactive Gallery</a> | 代码 <a target="_blank" rel="noopener" href="https://github.com/chi-feng/mcmc-demo">Javascript demos</a><br><a target="_blank" rel="noopener" href="https://prappleizer.github.io/Tutorials/MCMC/MCMC_Tutorial.html">MCMC</a> MCMC: A (very) Beginnner’s Guide</p>
</blockquote>
<p>Paper:</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.10145">An effective introduction to the Markov Chain Monte Carlo method</a> <strong>For physics</strong><br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.12313">A Conceptual Introduction to Markov Chain Monte Carlo Methods</a><br><a target="_blank" rel="noopener" href="https://www.taylorfrancis.com/books/mono/10.1201/b14835/markov-chain-monte-carlo-practice-david-spiegelhalter-gilks-richardson">Markov Chain Monte Carlo in Practice | W.R. Gilks, S. Richardson, Davi</a> MCMC需要小心地初始化，早期阶段需要warm-up time</p>
</blockquote>
<h4 id="M-H采样"><a href="#M-H采样" class="headerlink" title="M-H采样"></a>M-H采样</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/411689417">走进贝叶斯统计（五）—— Metropolis-Hasting 算法 - 知乎</a></p>
</blockquote>
<h4 id="Gibbs采样"><a href="#Gibbs采样" class="headerlink" title="Gibbs采样"></a>Gibbs采样</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/416670115">走进贝叶斯统计（六）—— 吉布斯抽样 （Gibbs Sampling） - 知乎</a></p>
</blockquote>
<h4 id="TMCMC"><a href="#TMCMC" class="headerlink" title="TMCMC"></a>TMCMC</h4><blockquote>
<p><a href="Transitional%20Markov%20Chain%20Monte%20Carlo%20Method%20for%20Bayesian%20Model%20Updating,%20Model%20Class%20Selection,%20and%20Model%20Averaging.md">Transitional Markov Chain Monte Carlo Method for Bayesian Model Updating, Model Class Selection, and Model Averaging</a></p>
</blockquote>
<h4 id="拉丁超立方采样-Latin-hypercube-sampling-LHS"><a href="#拉丁超立方采样-Latin-hypercube-sampling-LHS" class="headerlink" title="拉丁超立方采样(Latin hypercube sampling, LHS)"></a>拉丁超立方采样(Latin hypercube sampling, LHS)</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/385966408">拉丁超立方采样(Latin hypercube sampling, LHS)及蒙特卡洛模拟简介 - 知乎</a></p>
</blockquote>
<p>拉丁超立方采样先把样本空间分层，在此问题下要分为5层，于是便有了 [1,20],[21,40],[41,60],[61,80],[81,100] 共5个样本空间，在各样本空间内进行随机抽样，然后再打乱顺序，得到结果。这样就结束了~</p>
<p>可以看出拉丁超立方采样分为了三步——<strong>分层、采样、乱序</strong>。</p>
<h4 id="Langevin-Monte-Carlo-LMC"><a href="#Langevin-Monte-Carlo-LMC" class="headerlink" title="Langevin Monte Carlo(LMC)"></a>Langevin Monte Carlo(LMC)</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/613579202/answer/3310826408">什么是diffusion model? 它为什么好用？ - Zephyr的回答 - 知乎</a></p>
</blockquote>
<h4 id="Hamiltonian-Monte-Carlo-HMC"><a href="#Hamiltonian-Monte-Carlo-HMC" class="headerlink" title="Hamiltonian Monte Carlo(HMC)"></a>Hamiltonian Monte Carlo(HMC)</h4><h2 id="置信区间"><a href="#置信区间" class="headerlink" title="置信区间"></a>置信区间</h2><p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7%E6%B3%95%E5%89%87">68–95–99.7法则 - 维基百科，自由的百科全书</a></p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240401085715.png" alt="image.png|444"></p>
<h2 id="图像展示"><a href="#图像展示" class="headerlink" title="图像展示"></a>图像展示</h2><h3 id="箱型图"><a href="#箱型图" class="headerlink" title="箱型图"></a>箱型图</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/347067055">如何深刻理解箱线图（boxplot） - 知乎</a></p>
</blockquote>
<h3 id="t-SNE-数据降维"><a href="#t-SNE-数据降维" class="headerlink" title="t-SNE 数据降维"></a>t-SNE 数据降维</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/426068503">降维方法之t-SNE - 知乎</a><br><a target="_blank" rel="noopener" href="https://medium.com/swlh/everything-about-t-sne-dde964f0a8c1">Everything About t-SNE. t-SNE means t-distribution Stochastic… | by Ram Thiagu | The Startup | Medium</a></p>
</blockquote>
<p>PCA是一种常用的降维方式，但是不方便展示结果。例如对两类数据进行降维(二维)，PCA和t-SNE两种方法的展示结果为：</p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240921142810.png" alt="image.png|666"></p>
<h1 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h1><h2 id="特殊矩阵定义"><a href="#特殊矩阵定义" class="headerlink" title="特殊矩阵定义"></a>特殊矩阵定义</h2><p><strong>正交矩阵</strong> <a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%AD%A3%E4%BA%A4%E7%9F%A9%E9%98%B5">正交矩阵 - 维基百科，自由的百科全书</a></p>
<ul>
<li>$Q^T=Q^{-1}\Leftrightarrow Q^TQ=QQ^T=I.$ 其中$I$为单位矩阵</li>
<li>正交矩阵的行列式值必定为+1或−1。</li>
<li>行列式值为+1的正交矩阵，称为<strong>特殊正交矩阵</strong>，它是一个旋转矩阵。</li>
<li>行列式值为-1的正交矩阵，称为瑕旋转矩阵。瑕旋转是旋转加上镜射。镜射也是一种瑕旋转。</li>
</ul>
<p><strong>正定矩阵</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/234967628">【线性代数】详解正定矩阵、实对称矩阵、矩阵特征值分解、矩阵 SVD 分解 - 知乎</a></p>
<ul>
<li>任意非零向量$\mathbf{x}$，若$\mathbf{x}^{T}\mathbf{A}\mathbf{x}&gt;0$恒成立，则$\mathbf{A}$为正定矩阵。若$\mathbf{x}^{T}\mathbf{A}\mathbf{x}\geq0$恒成立，则$\mathbf{A}$为半正定矩阵</li>
<li></li>
</ul>
<h2 id="变换"><a href="#变换" class="headerlink" title="变换"></a>变换</h2><h3 id="2D"><a href="#2D" class="headerlink" title="2D"></a>2D</h3><p>仿射变换</p>
<h3 id="3D"><a href="#3D" class="headerlink" title="3D"></a>3D</h3><p>像素Pixel | 相机Camera | 世界World</p>
<p>内参矩阵 = c2p<br>外参矩阵 = w2c<br>根据世界坐标计算像素坐标 = <code>c2p * w2c * world_position</code></p>
<h1 id="Computer-Graphics"><a href="#Computer-Graphics" class="headerlink" title="Computer Graphics"></a>Computer Graphics</h1><h2 id="SDF计算与求导"><a href="#SDF计算与求导" class="headerlink" title="SDF计算与求导"></a>SDF计算与求导</h2><p>空间中的子集$\partial\Omega$，SDF值定义为：$\left.f(x)=\left\{\begin{array}{ll}d(x,\partial\Omega)&amp;\mathrm{~if~}x\in\Omega\-d(x,\partial\Omega)&amp;\mathrm{~if~}x\in\Omega^c\end{array}\right.\right.$<br>其中$d(x,\partial\Omega):=\inf_{y\in\partial\Omega}d(x,y)$表示x到表面子集上一点的距离，inf表示infimum最大下界</p>
<h1 id="Computer-Vision"><a href="#Computer-Vision" class="headerlink" title="Computer Vision"></a>Computer Vision</h1><h2 id="卷积图像大小计算公式"><a href="#卷积图像大小计算公式" class="headerlink" title="卷积图像大小计算公式"></a>卷积图像大小计算公式</h2><p>图像卷积后的大小计算公式： $N=\left\lfloor\frac{W-F+2P}{Step}\right\rfloor+1$</p>
<ul>
<li>输入图片大小 $W \times W$</li>
<li>Filter（卷积核）大小 $F \times F$</li>
<li>步长 Step</li>
<li>padding（填充）的像素数 $P$</li>
<li>输出图片的大小为$N \times N$</li>
</ul>
<h2 id="linearColor-2-sRGB"><a href="#linearColor-2-sRGB" class="headerlink" title="linearColor 2 sRGB"></a>linearColor 2 sRGB</h2><p>(Why)为什么要将线性RGB转换成sRGB</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.zhangxinxu.com/wordpress/2017/12/linear-rgb-srgb-js-convert/">小tip: 了解LinearRGB和sRGB以及使用JS相互转换 « 张鑫旭-鑫空间-鑫生活 (zhangxinxu.com)</a></p>
</blockquote>
<p><strong>人这种动物，对于真实世界的颜色感受，并不是线性的，而是曲线的</strong></p>
<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810162118.png" alt="image.png|444"></p>
<p>(How)线性RGB与sRGB相互转化</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/bby1987/article/details/109522126">RGB与Lab转换_rgb转lab-CSDN博客</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear_to_srgb</span>(<span class="params">linear</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(linear, torch.Tensor):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Assumes `linear` is in [0, 1], see https://en.wikipedia.org/wiki/SRGB.&quot;&quot;&quot;</span></span><br><span class="line">        eps = torch.finfo(torch.float32).eps</span><br><span class="line">        srgb0 = <span class="number">323</span> / <span class="number">25</span> * linear</span><br><span class="line">        srgb1 = (<span class="number">211</span> * torch.clamp(linear, <span class="built_in">min</span>=eps)**(<span class="number">5</span> / <span class="number">12</span>) - <span class="number">11</span>) / <span class="number">200</span></span><br><span class="line">        <span class="keyword">return</span> torch.where(linear &lt;= <span class="number">0.0031308</span>, srgb0, srgb1)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(linear, np.ndarray):</span><br><span class="line">        eps = np.finfo(np.float32).eps</span><br><span class="line">        srgb0 = <span class="number">323</span> / <span class="number">25</span> * linear</span><br><span class="line">        srgb1 = (<span class="number">211</span> * np.maximum(eps, linear) ** (<span class="number">5</span> / <span class="number">12</span>) - <span class="number">11</span>) / <span class="number">200</span></span><br><span class="line">        <span class="keyword">return</span> np.where(linear &lt;= <span class="number">0.0031308</span>, srgb0, srgb1)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">srgb_to_linear</span>(<span class="params">srgb</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(srgb, torch.Tensor):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Assumes `srgb` is in [0, 1], see https://en.wikipedia.org/wiki/SRGB.&quot;&quot;&quot;</span></span><br><span class="line">        eps = torch.finfo(torch.float32).eps</span><br><span class="line">        linear0 = <span class="number">25</span> / <span class="number">323</span> * srgb</span><br><span class="line">        linear1 = torch.clamp(((<span class="number">200</span> * srgb + <span class="number">11</span>) / (<span class="number">211</span>)), <span class="built_in">min</span>=eps)**(<span class="number">12</span> / <span class="number">5</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.where(srgb &lt;= <span class="number">0.04045</span>, linear0, linear1)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(srgb, np.ndarray):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Assumes `srgb` is in [0, 1], see https://en.wikipedia.org/wiki/SRGB.&quot;&quot;&quot;</span></span><br><span class="line">        eps = np.finfo(np.float32).eps</span><br><span class="line">        linear0 = <span class="number">25</span> / <span class="number">323</span> * srgb</span><br><span class="line">        linear1 = np.maximum(((<span class="number">200</span> * srgb + <span class="number">11</span>) / (<span class="number">211</span>)), eps)**(<span class="number">12</span> / <span class="number">5</span>)</span><br><span class="line">        <span class="keyword">return</span> np.where(srgb &lt;= <span class="number">0.04045</span>, linear0, linear1)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
<h1 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h1><p><a href="DeepLearning.md">DeepLearning</a></p>
<h2 id="数据处理方法"><a href="#数据处理方法" class="headerlink" title="数据处理方法"></a>数据处理方法</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/73534694">SOM(自组织映射神经网络)——理论篇 - 知乎</a><br><a target="_blank" rel="noopener" href="https://nice-ai.top/posts/SelfOrganizingMaps-SOM.html">自组织映射（SOM）理论基础与Python NumPy实现 | 美美智能博客站</a><br>针对数据量大的情况</p>
</blockquote>
<p>自组织映射(Self-organizing map, SOM)通过学习输入空间中的数据，生成一个低维、离散的映射(Map)，从某种程度上也可看成一种降维算法。（<em>降维或者升为可以由输入和输出尺寸决定</em>，SOM输入为1E6，输出为70x70，本质上数据量变少，因此为降维算法）</p>
<h2 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240503135053.png" alt="image.png|666"></p>

    </div>

    
    
    
        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://t.me/+1ZnNxFWnEbk5YzRl">
            <span class="icon">
              <i class="fab fa-telegram"></i>
            </span>

            <span class="label">Telegram</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Learn/Python/Linux%20OS/" rel="prev" title="Linux">
      <i class="fa fa-chevron-left"></i> Linux
    </a></div>
      <div class="post-nav-item">
    <a href="/3DReconstruction/Multi-view/Explicit%20Volumetric/GS-based/2DGS/" rel="next" title="2DGS">
      2DGS <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B3%9B%E5%87%BD"><span class="nav-text">泛函</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80"><span class="nav-text">数学基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF"><span class="nav-text">卷积</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%84%E5%8A%A8%E6%B3%95"><span class="nav-text">摄动法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%B0%E5%8B%92"><span class="nav-text">泰勒</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Domain-Transform"><span class="nav-text">Domain Transform</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Fourier-Transform"><span class="nav-text">Fourier Transform</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DFT"><span class="nav-text">DFT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FFT"><span class="nav-text">FFT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#STFT"><span class="nav-text">STFT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Wavelet-Transform"><span class="nav-text">Wavelet Transform</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Laplace-Transform"><span class="nav-text">Laplace Transform</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%8D%E6%95%B0-Complex-Number"><span class="nav-text">复数 Complex Number</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KL-%E6%95%A3%E5%BA%A6"><span class="nav-text">KL 散度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%87%B8%E5%87%BD%E6%95%B0%E4%B8%8EJensen%E4%B8%8D%E7%AD%89%E5%BC%8F"><span class="nav-text">凸函数与Jensen不等式</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA"><span class="nav-text">概率论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic"><span class="nav-text">Basic</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE"><span class="nav-text">均值方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E7%8E%87-%E4%BC%BC%E7%84%B6"><span class="nav-text">概率&#x2F;似然</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bayes"><span class="nav-text">Bayes</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Distribution"><span class="nav-text">Distribution</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1"><span class="nav-text">核密度估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gamma-%E5%88%86%E5%B8%83"><span class="nav-text">Gamma 分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Beta-%E5%88%86%E5%B8%83"><span class="nav-text">Beta 分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="nav-text">高斯分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="nav-text">多元高斯分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="nav-text">混合高斯分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%94%E5%90%88%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="nav-text">联合高斯分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#P-box"><span class="nav-text">P-box</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Markov-Chains"><span class="nav-text">Markov Chains</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sampling"><span class="nav-text">Sampling</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Monte-Carlo"><span class="nav-text">Monte Carlo</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%B1%82%E9%87%87%E6%A0%B7"><span class="nav-text">分层采样</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%B1%82%E6%8A%96%E5%8A%A8%E9%87%87%E6%A0%B7"><span class="nav-text">分层抖动采样</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Latin-Hypercube-Sampling"><span class="nav-text">Latin Hypercube Sampling</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MCMC"><span class="nav-text">MCMC</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#M-H%E9%87%87%E6%A0%B7"><span class="nav-text">M-H采样</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gibbs%E9%87%87%E6%A0%B7"><span class="nav-text">Gibbs采样</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TMCMC"><span class="nav-text">TMCMC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8B%89%E4%B8%81%E8%B6%85%E7%AB%8B%E6%96%B9%E9%87%87%E6%A0%B7-Latin-hypercube-sampling-LHS"><span class="nav-text">拉丁超立方采样(Latin hypercube sampling, LHS)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Langevin-Monte-Carlo-LMC"><span class="nav-text">Langevin Monte Carlo(LMC)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hamiltonian-Monte-Carlo-HMC"><span class="nav-text">Hamiltonian Monte Carlo(HMC)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4"><span class="nav-text">置信区间</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%B1%95%E7%A4%BA"><span class="nav-text">图像展示</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%B1%E5%9E%8B%E5%9B%BE"><span class="nav-text">箱型图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#t-SNE-%E6%95%B0%E6%8D%AE%E9%99%8D%E7%BB%B4"><span class="nav-text">t-SNE 数据降维</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="nav-text">线性代数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E6%AE%8A%E7%9F%A9%E9%98%B5%E5%AE%9A%E4%B9%89"><span class="nav-text">特殊矩阵定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%98%E6%8D%A2"><span class="nav-text">变换</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2D"><span class="nav-text">2D</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3D"><span class="nav-text">3D</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Computer-Graphics"><span class="nav-text">Computer Graphics</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SDF%E8%AE%A1%E7%AE%97%E4%B8%8E%E6%B1%82%E5%AF%BC"><span class="nav-text">SDF计算与求导</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Computer-Vision"><span class="nav-text">Computer Vision</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%9B%BE%E5%83%8F%E5%A4%A7%E5%B0%8F%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F"><span class="nav-text">卷积图像大小计算公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#linearColor-2-sRGB"><span class="nav-text">linearColor 2 sRGB</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Other"><span class="nav-text">Other</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95"><span class="nav-text">数据处理方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%8C%E8%85%8A%E5%AD%97%E6%AF%8D"><span class="nav-text">希腊字母</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Qi Yun"
      src="/images/avatar.jpeg">
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">143</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/qiyun71" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qiyun71" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/29010355" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;29010355" rel="noopener" target="_blank"><i class="fa fa-star fa-fw"></i>Bilibili</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qi Yun</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">496k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">30:02</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/yqq/anime.min.js"></script>
  <script src="/yqq/velocity/velocity.min.js"></script>
  <script src="/yqq/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="//cdn.jsdelivr.net/gh/theme-next/theme-next-three@1/three.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
