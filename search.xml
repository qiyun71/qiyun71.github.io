<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Vibration-Review</title>
      <link href="/Vibration/Vibration-Review/"/>
      <url>/Vibration/Vibration-Review/</url>
      
        <content type="html"><![CDATA[<span id="more"></span><p>基本概念：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/23320350">什么是固有频率？ - 知乎 (zhihu.com)</a><ul><li>通常一个结构有很多个<strong>固有频率</strong>。固有频率与外界激励没有关系，是结构的一种固有属性。<strong>只与材料的刚度k和质量m有关</strong>：$\omega_{n} =  \sqrt{\frac{k}{m}}$</li></ul></li><li>模态分析是一种处理过程：根据结构的固有特性，包括频率、阻尼和模态振型，这些动力学属性去描述结构。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Vibration </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SuGaR</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/GS-based/SuGaR/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/GS-based/SuGaR/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>SuGaR</th></tr></thead><tbody><tr><td>Author</td><td></td></tr><tr><td>Conf/Jour</td><td></td></tr><tr><td>Year</td><td></td></tr><tr><td>Project</td><td><a href="https://github.com/Anttwo/SuGaR">Anttwo/SuGaR: Official implementation of SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering (github.com)</a></td></tr><tr><td>Paper</td></tr></tbody></table></div><span id="more"></span><h1 id="实验-win10"><a href="#实验-win10" class="headerlink" title="实验(win10)"></a>实验(win10)</h1><p><del>conda env create -f environment.yml</del><br>复制environment.yml中pip包到requirements.txt中，通过pip安装<br><code>conda create -n sugar</code><br><code>pip install -r requirements.txt</code><br>add:</p><ul><li>plyfile</li><li>tqdm</li><li>rich</li></ul><p><code>pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118</code></p><p>pytorch3d: <a href="https://blog.csdn.net/m0_70229101/article/details/127196699">pytorch3D Windows下安装经验总结_windows安装pytorch3d-CSDN博客</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> gaussian_splatting/submodules/diff-gaussian-rasterization/</span><br><span class="line">pip install -e .</span><br><span class="line"><span class="built_in">cd</span> ../simple-knn/</span><br><span class="line">pip install -e .</span><br><span class="line"><span class="built_in">cd</span> ../../../</span><br></pre></td></tr></table></figure><h2 id="run"><a href="#run" class="headerlink" title="run"></a>run</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GS</span></span><br><span class="line">python gaussian_splatting/train.py -s &lt;path to COLMAP or NeRF Synthetic dataset&gt; --iterations 7000 -m &lt;path to the desired output directory&gt;</span><br><span class="line"></span><br><span class="line">-</span><br><span class="line"></span><br><span class="line"><span class="comment">## eg</span></span><br><span class="line">python gaussian_splatting/train.py -s inputs/dtu_scan114 --iterations 7000 -m exp/dtu_scan114</span><br><span class="line">python gaussian_splatting/train.py -s inputs/Miku --iterations 7000 -m exp/Miku</span><br><span class="line"></span><br><span class="line"><span class="comment"># SuGaR</span></span><br><span class="line">python train.py -s &lt;path to COLMAP or NeRF Synthetic dataset&gt; -c &lt;path to the Gaussian Splatting checkpoint&gt; -r &lt;<span class="string">&quot;density&quot;</span> or <span class="string">&quot;sdf&quot;</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">##eg</span></span><br><span class="line">python train.py -s inputs/dtu_scan114 -c exp/dtu_scan114/ -r sdf</span><br><span class="line">python train.py -s inputs/Miku -c exp/Miku/ -r sdf</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/GS-based </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>MonoSDF</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Loss/MonoSDF/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Loss/MonoSDF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction</th></tr></thead><tbody><tr><td>Author</td><td>Zehao Yu1     Songyou Peng2,3     Michael Niemeyer1,3     Torsten Sattler4     Andreas Geiger1,3</td></tr><tr><td>Conf/Jour</td><td>NeurIPS</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://niujinshuchong.github.io/monosdf/">MonoSDF (niujinshuchong.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4678115703736238081&amp;noteId=2084231918355671808">MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231219132905.png" alt="image.png|666"></p><p>深度+法向量监督</p><span id="more"></span>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Loss </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
            <tag> NeRF </tag>
            
            <tag> Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RegNeRF</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Loss/RegNeRF/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Loss/RegNeRF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>RegNeRF</th></tr></thead><tbody><tr><td>Author</td><td></td></tr><tr><td>Conf/Jour</td><td></td></tr><tr><td>Year</td><td></td></tr><tr><td>Project</td><td><a href="https://m-niemeyer.github.io/regnerf/">RegNeRF (m-niemeyer.github.io)</a></td></tr><tr><td>Paper</td></tr></tbody></table></div><p>深度先验</p><span id="more"></span>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Loss </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SDFStudio</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/SDFStudio/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/SDFStudio/</url>
      
        <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231211203625.png" alt="image.png|666"></p><span id="more"></span><h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><h3 id="Create-environment"><a href="#Create-environment" class="headerlink" title="Create environment"></a>Create environment</h3><p>SDFStudio requires <code>python &gt;= 3.7</code>. We recommend using conda to manage dependencies. Make sure to install <a href="https://docs.conda.io/en/latest/miniconda.html">Conda</a> before proceeding.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create --name sdfstudio -y python=3.8</span><br><span class="line">conda activate sdfstudio</span><br><span class="line">python -m pip install --upgrade pip</span><br></pre></td></tr></table></figure><h3 id="Dependencies"><a href="#Dependencies" class="headerlink" title="Dependencies"></a>Dependencies</h3><p>Install pytorch with CUDA (this repo has been tested with CUDA 11.3) and <a href="https://github.com/NVlabs/tiny-cuda-nn">tiny-cuda-nn</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html</span><br><span class="line"><span class="comment"># 以下win10需要在vs的x64 Native Tools Command Prompt for VS 2019中进行编译和安装PyTorch extension</span></span><br><span class="line">pip install git+https://github.com/NVlabs/tiny-cuda-nn/<span class="comment">#subdirectory=bindings/torch</span></span><br></pre></td></tr></table></figure><h3 id="Installing-SDFStudio"><a href="#Installing-SDFStudio" class="headerlink" title="Installing SDFStudio"></a>Installing SDFStudio</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/autonomousvision/sdfstudio.git</span><br><span class="line"><span class="built_in">cd</span> sdfstudio</span><br><span class="line">pip install --upgrade pip setuptools</span><br><span class="line">pip install -e .</span><br><span class="line"><span class="comment"># install tab completion</span></span><br><span class="line">ns-install-cli</span><br></pre></td></tr></table></figure><h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><p><code>ns-download-data sdfstudio</code> 如果网络无法连接，直接使用amazonaws地址下载</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ns-download-data sdfstudio --dataset-name DATASET_NAME</span><br><span class="line"></span><br><span class="line"><span class="comment"># pylint: disable=line-too-long</span></span><br><span class="line">sdfstudio_downloads = &#123;</span><br><span class="line">    <span class="string">&quot;sdfstudio-demo-data&quot;</span>: <span class="string">&quot;https://s3.eu-central-1.amazonaws.com/avg-projects/monosdf/data/sdfstudio-demo-data.tar&quot;</span>,</span><br><span class="line">    <span class="string">&quot;dtu&quot;</span>: <span class="string">&quot;https://s3.eu-central-1.amazonaws.com/avg-projects/monosdf/data/DTU.tar&quot;</span>,</span><br><span class="line">    <span class="string">&quot;replica&quot;</span>: <span class="string">&quot;https://s3.eu-central-1.amazonaws.com/avg-projects/monosdf/data/Replica.tar&quot;</span>,</span><br><span class="line">    <span class="string">&quot;scannet&quot;</span>: <span class="string">&quot;https://s3.eu-central-1.amazonaws.com/avg-projects/monosdf/data/scannet.tar&quot;</span>,</span><br><span class="line">    <span class="string">&quot;tanks-and-temple&quot;</span>: <span class="string">&quot;https://s3.eu-central-1.amazonaws.com/avg-projects/monosdf/data/tnt_advanced.tar&quot;</span>,</span><br><span class="line">    <span class="string">&quot;tanks-and-temple-highres&quot;</span>: <span class="string">&quot;https://s3.eu-central-1.amazonaws.com/avg-projects/monosdf/data/highresTNT.tar&quot;</span>,</span><br><span class="line">    <span class="string">&quot;heritage&quot;</span>: <span class="string">&quot;https://s3.eu-central-1.amazonaws.com/avg-projects/monosdf/data/Heritage-Recon.tar&quot;</span>,</span><br><span class="line">    <span class="string">&quot;neural-rgbd-data&quot;</span>: <span class="string">&quot;http://kaldir.vc.in.tum.de/neural_rgbd/neural_rgbd_data.zip&quot;</span>,</span><br><span class="line">    <span class="string">&quot;all&quot;</span>: <span class="literal">None</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中RGBD数据需要转换为SDFStudio数据格式<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kitchen scene for example, replca the scene path to convert other scenes</span></span><br><span class="line">python scripts/datasets/process_neuralrgbd_to_sdfstudio.py --input_path data/neural-rgbd-data/kitchen/ --output_path data/neural_rgbd/kitchen_sensor_depth --<span class="built_in">type</span> sensor_depth</span><br></pre></td></tr></table></figure></p><h2 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">└── scan65</span><br><span class="line">  └── meta_data.json</span><br><span class="line">  ├── pairs.txt</span><br><span class="line">  ├── 000000_rgb.png</span><br><span class="line">  ├── 000000_normal.npy</span><br><span class="line">  ├── 000000_depth.npy</span><br><span class="line">  ├── .....</span><br></pre></td></tr></table></figure><p><code>meta_data.json like:</code></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  &#x27;camera_model&#x27;<span class="punctuation">:</span> &#x27;OPENCV&#x27;<span class="punctuation">,</span> # camera model (currently only OpenCV is supported)</span><br><span class="line">  &#x27;height&#x27;<span class="punctuation">:</span> <span class="number">384</span><span class="punctuation">,</span> # height of the images</span><br><span class="line">  &#x27;width&#x27;<span class="punctuation">:</span> <span class="number">384</span><span class="punctuation">,</span> # width of the images</span><br><span class="line">  &#x27;has_mono_prior&#x27;<span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span> # use monocular cues or not</span><br><span class="line">  &#x27;pairs&#x27;<span class="punctuation">:</span> &#x27;pairs.txt&#x27;<span class="punctuation">,</span> # pairs file used for multi-view photometric consistency loss</span><br><span class="line">  &#x27;worldtogt&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">]</span><span class="punctuation">,</span> # world to gt transformation (useful for evauation)</span><br><span class="line">      <span class="punctuation">[</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">[</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">[</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  &#x27;scene_box&#x27;<span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      &#x27;aabb&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">          <span class="punctuation">[</span><span class="number">-1</span><span class="punctuation">,</span> <span class="number">-1</span><span class="punctuation">,</span> <span class="number">-1</span><span class="punctuation">]</span><span class="punctuation">,</span> # aabb for the bbox</span><br><span class="line">          <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      &#x27;near&#x27;<span class="punctuation">:</span> <span class="number">0.5</span><span class="punctuation">,</span> # near plane for each image</span><br><span class="line">      &#x27;far&#x27;<span class="punctuation">:</span> <span class="number">4.5</span><span class="punctuation">,</span> # far plane for each image</span><br><span class="line">      &#x27;radius&#x27;<span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span> # radius of ROI region in scene</span><br><span class="line">      &#x27;collider_type&#x27;<span class="punctuation">:</span> &#x27;near_far&#x27;<span class="punctuation">,</span></span><br><span class="line">      # collider_type can be <span class="string">&quot;near_far&quot;</span><span class="punctuation">,</span> <span class="string">&quot;box&quot;</span><span class="punctuation">,</span> <span class="string">&quot;sphere&quot;</span><span class="punctuation">,</span></span><br><span class="line">      # it indicates how do we determine the near and far for each ray</span><br><span class="line">      # <span class="number">1.</span> near_far means we use the same near and far value for each ray</span><br><span class="line">      # <span class="number">2.</span> box means we compute the intersection with the bounding box</span><br><span class="line">      # <span class="number">3.</span> sphere means we compute the intersection with the sphere</span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  &#x27;frames&#x27;<span class="punctuation">:</span> <span class="punctuation">[</span> # this contains information for each image</span><br><span class="line">      <span class="punctuation">&#123;</span></span><br><span class="line">        # note that all paths are relateive path</span><br><span class="line">        # path of rgb image</span><br><span class="line">        &#x27;rgb_path&#x27;<span class="punctuation">:</span> &#x27;<span class="number">000000</span>_rgb.png&#x27;<span class="punctuation">,</span></span><br><span class="line">        # camera to world transform</span><br><span class="line">        &#x27;camtoworld&#x27;<span class="punctuation">:</span></span><br><span class="line">          <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">[</span></span><br><span class="line">              <span class="number">0.9702627062797546</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">-0.014742869883775711</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">-0.2416049987077713</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.6601868867874146</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">[</span></span><br><span class="line">              <span class="number">0.007479910273104906</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.9994929432868958</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">-0.03095100075006485</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.07803472131490707</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">[</span></span><br><span class="line">              <span class="number">0.2419387847185135</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.028223417699337006</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">0.9698809385299683</span><span class="punctuation">,</span></span><br><span class="line">              <span class="number">-2.6397712230682373</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">[</span><span class="number">0.0</span><span class="punctuation">,</span> <span class="number">0.0</span><span class="punctuation">,</span> <span class="number">0.0</span><span class="punctuation">,</span> <span class="number">1.0</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">          <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        # intrinsic of current image</span><br><span class="line">        &#x27;intrinsics&#x27;<span class="punctuation">:</span></span><br><span class="line">          <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">[</span><span class="number">925.5457763671875</span><span class="punctuation">,</span> <span class="number">-7.8512319305446e-05</span><span class="punctuation">,</span> <span class="number">199.4256591796875</span><span class="punctuation">,</span> <span class="number">0.0</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">[</span><span class="number">0.0</span><span class="punctuation">,</span> <span class="number">922.6160278320312</span><span class="punctuation">,</span> <span class="number">198.10269165039062</span><span class="punctuation">,</span> <span class="number">0.0</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">[</span><span class="number">0.0</span><span class="punctuation">,</span> <span class="number">0.0</span><span class="punctuation">,</span> <span class="number">1.0</span><span class="punctuation">,</span> <span class="number">0.0</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">[</span><span class="number">0.0</span><span class="punctuation">,</span> <span class="number">0.0</span><span class="punctuation">,</span> <span class="number">0.0</span><span class="punctuation">,</span> <span class="number">1.0</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">          <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        # path of monocular depth prior</span><br><span class="line">        &#x27;mono_depth_path&#x27;<span class="punctuation">:</span> &#x27;<span class="number">000000</span>_depth.npy&#x27;<span class="punctuation">,</span></span><br><span class="line">        # path of monocular normal prior</span><br><span class="line">        &#x27;mono_normal_path&#x27;<span class="punctuation">:</span> &#x27;<span class="number">000000</span>_normal.npy&#x27;<span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      ...<span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p><code>pairs.txt like:</code><br>for multi-view photometric consistency loss</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># ref image, source image 1, source image 2, ..., source image N, note source image are listed in ascending order, which means last image has largest score</span><br><span class="line">000000.png 000032.png 000023.png 000028.png 000031.png 000029.png 000030.png 000024.png 000002.png 000015.png 000025.png ...</span><br><span class="line">000001.png 000033.png 000003.png 000022.png 000016.png 000027.png 000023.png 000007.png 000011.png 000026.png 000024.png ...</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="自定义数据集"><a href="#自定义数据集" class="headerlink" title="自定义数据集"></a>自定义数据集</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/datasets/process_scannet_to_sdfstudio.py --input_path /your_path/datasets/scannet/scene0050_00 --output_path data/custom/scannet_scene0050_00</span><br></pre></td></tr></table></figure><p>如果需要，可以使用<a href="https://github.com/EPFL-VILAB/omnidata">omnidata:</a>提取单目深度和法向用于监督<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python scripts/datasets/extract_monocular_cues.py --task normal --img_path data/custom/scannet_scene0050_00/ --output_path data/custom/scannet_scene0050_00 --omnidata_path YOUR_OMNIDATA_PATH --pretrained_models PRETRAINED_MODELS</span><br><span class="line"></span><br><span class="line">python scripts/datasets/extract_monocular_cues.py --task depth --img_path data/custom/scannet_scene0050_00/ --output_path data/custom/scannet_scene0050_00 --omnidata_path YOUR_OMNIDATA_PATH --pretrained_models PRETRAINED_MODELS</span><br></pre></td></tr></table></figure></p><h1 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h1><p>eg: sdfstudio-demo-data</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train model on the dtu dataset scan65</span></span><br><span class="line">ns-train neus-facto --pipeline.model.sdf-field.inside-outside False --vis viewer --experiment-name neus-facto-dtu65 sdfstudio-data --data data/sdfstudio-demo-data/dtu-scan65</span><br><span class="line"></span><br><span class="line"><span class="comment"># Or you could also train model on the Replica dataset room0 with monocular priors</span></span><br><span class="line">ns-train neus-facto --pipeline.model.sdf-field.inside-outside True --pipeline.model.mono-depth-loss-mult 0.1 --pipeline.model.mono-normal-loss-mult 0.05 --vis viewer --experiment-name neus-facto-replica1 sdfstudio-data --data data/sdfstudio-demo-data/replica-room0 --include_mono_prior True</span><br><span class="line"></span><br><span class="line"><span class="comment"># resume checkpoint 根据模型参数恢复训练</span></span><br><span class="line">ns-train neus-facto --trainer.load-dir &#123;outputs/neus-facto-dtu65/neus-facto/XXX/sdfstudio_models&#125; sdfstudio-data --data data/sdfstudio-demo-data/dtu-scan65 </span><br></pre></td></tr></table></figure><h2 id="图片-mask"><a href="#图片-mask" class="headerlink" title="图片(+mask)"></a>图片(+mask)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ns-train unisurf --pipeline.model.sdf-field.inside-outside False sdfstudio-data --data data/sdfstudio-demo-data/dtu-scan65</span><br><span class="line"></span><br><span class="line">ns-train volsdf --pipeline.model.sdf-field.inside-outside False sdfstudio-data --data data/sdfstudio-demo-data/dtu-scan65</span><br><span class="line"></span><br><span class="line">ns-train neus --pipeline.model.sdf-field.inside-outside False sdfstudio-data --data data/sdfstudio-demo-data/dtu-scan65</span><br></pre></td></tr></table></figure><h2 id="mono-prior"><a href="#mono-prior" class="headerlink" title="mono-prior"></a>mono-prior</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ns-train monosdf --pipeline.model.sdf-field.inside-outside True sdfstudio-data --data data/sdfstudio-demo-data/replica-room0 --include-mono-prior True</span><br><span class="line"></span><br><span class="line">ns-train mono-unisurf --pipeline.model.sdf-field.inside-outside True sdfstudio-data --data data/sdfstudio-demo-data/replica-room0 --include-mono-prior True</span><br><span class="line"></span><br><span class="line">ns-train mono-neus --pipeline.model.sdf-field.inside-outside True sdfstudio-data --data data/sdfstudio-demo-data/replica-room0 --include-mono-prior True</span><br></pre></td></tr></table></figure><h2 id="Geo-Neus’s-pairs"><a href="#Geo-Neus’s-pairs" class="headerlink" title="Geo-Neus’s pairs"></a>Geo-Neus’s pairs</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ns-train geo-neus --pipeline.model.sdf-field.inside-outside False sdfstudio-data -data data/dtu/scan24 --load-pairs True</span><br><span class="line"></span><br><span class="line">ns-train geo-unisurf --pipeline.model.sdf-field.inside-outside False sdfstudio-data -data data/dtu/scan24 --load-pairs True</span><br><span class="line"></span><br><span class="line">ns-train geo-volsdf --pipeline.model.sdf-field.inside-outside False sdfstudio-data -data data/dtu/scan24 --load-pairs True</span><br></pre></td></tr></table></figure><h2 id="Neus"><a href="#Neus" class="headerlink" title="Neus"></a>Neus</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ns-train neus-acc --pipeline.model.sdf-field.inside-outside False sdfstudio-data -data data/dtu/scan65</span><br><span class="line"></span><br><span class="line">ns-train neus-facto --pipeline.model.sdf-field.inside-outside False sdfstudio-data -data data/dtu/scan65</span><br></pre></td></tr></table></figure><h2 id="NeuralReconW"><a href="#NeuralReconW" class="headerlink" title="NeuralReconW"></a>NeuralReconW</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ns-train neusW --pipeline.model.sdf-field.inside-outside False heritage-data --data data/heritage/brandenburg_gate</span><br></pre></td></tr></table></figure><h2 id="其他命令"><a href="#其他命令" class="headerlink" title="其他命令"></a>其他命令</h2><h3 id="Representations"><a href="#Representations" class="headerlink" title="Representations"></a>Representations</h3><p>MLP、HashGrid、Tri-plane</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Representations</span></span><br><span class="line"><span class="comment">## MLPs</span></span><br><span class="line">ns-train volsdf --pipeline.model.sdf-field.use-grid-feature False sdfstudio-data --data YOUR_DATA</span><br><span class="line"></span><br><span class="line"><span class="comment">## Multi-res Feature Grids</span></span><br><span class="line">ns-train volsdf --pipeline.model.sdf-field.use-grid-feature True --pipeline.model.sdf-field.encoding-type <span class="built_in">hash</span> sdfstudio-data --data YOUR_DATA</span><br><span class="line"></span><br><span class="line"><span class="comment">## Tri-plane</span></span><br><span class="line">ns-train volsdf --pipeline.model.sdf-field.use-grid-feature True  --pipeline.model.sdf-field.encoding-type tri-plane sdfstudio-data --data YOUR_DATA</span><br><span class="line"></span><br><span class="line"><span class="comment">## Geometry Initialization</span></span><br><span class="line">ns-train volsdf  --pipeline.model.sdf-field.geometric-init True --pipeline.model.sdf-field.bias 0.5 --pipeline.model.sdf-field.inside-outside False</span><br><span class="line">ns-train volsdf --pipeline.model.sdf-field.geometric-init True --pipeline.model.sdf-field.bias 0.8 --pipeline.model.sdf-field.inside-outside True</span><br><span class="line"></span><br><span class="line"><span class="comment">## Color Network</span></span><br><span class="line">ns-train volsdf --pipeline.model.sdf-field.num-layers-color 2 --pipeline.model.sdf-field.hidden-dim-color 512</span><br></pre></td></tr></table></figure><h3 id="Supervision"><a href="#Supervision" class="headerlink" title="Supervision"></a>Supervision</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## RGB Loss 默认使用L1损失</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Mask Loss</span></span><br><span class="line">--pipeline.model.fg-mask-loss-mult 0.001</span><br><span class="line"></span><br><span class="line"><span class="comment">## Eikonal Loss</span></span><br><span class="line">--pipeline.model.eikonal-loss-mult 0.01</span><br><span class="line"></span><br><span class="line"><span class="comment">## Smoothness Loss</span></span><br><span class="line">--pipeline.model.smooth-loss-multi 0.01</span><br><span class="line"></span><br><span class="line"><span class="comment">## Monocular Depth Consistency</span></span><br><span class="line">--pipeline.model.mono-depth-loss-mult 0.1</span><br><span class="line"></span><br><span class="line"><span class="comment">## Monocular Normal Consistency</span></span><br><span class="line">--pipeline.model.mono-normal-loss-mult 0.05</span><br><span class="line"></span><br><span class="line"><span class="comment">## Multi-view Photometric Consistency（Geo-NeuS.）</span></span><br><span class="line">--pipeline.model.patch-size 11 --pipeline.model.patch-warp-loss-mult 0.1 --pipeline.model.topk 4</span><br><span class="line"></span><br><span class="line"><span class="comment">## Sensor Depth Loss（for RGBD data）</span></span><br><span class="line"><span class="comment"># truncation is set to 5cm with a rough scale value 0.3 (0.015 = 0.05 * 0.3)</span></span><br><span class="line">--pipeline.model.sensor-depth-truncation 0.015 --pipeline.model.sensor-depth-l1-loss-mult 0.1 --pipeline.model.sensor-depth-freespace-loss-mult 10.0 --pipeline.model.sensor-depth-sdf-loss-mult 6000.0</span><br></pre></td></tr></table></figure><h3 id="vis可视化跟踪"><a href="#vis可视化跟踪" class="headerlink" title="vis可视化跟踪"></a>vis可视化跟踪</h3><p><code>--vis &#123;viewer, tensorboard, wandb&#125;</code></p><h1 id="提取和渲染mesh"><a href="#提取和渲染mesh" class="headerlink" title="提取和渲染mesh"></a>提取和渲染mesh</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ns-extract-mesh --load-config outputs/neus-facto-dtu65/neus-facto/XXX/config.yml --output-path meshes/neus-facto-dtu65.ply</span><br><span class="line"></span><br><span class="line">ns-render-mesh --meshfile meshes/neus-facto-dtu65.ply --traj interpolate  --output-path renders/neus-facto-dtu65.mp4 sdfstudio-data --data data/sdfstudio-demo-data/dtu-scan65</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
            <tag> SurfaceReconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DVR</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/DVR/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/DVR/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision</th></tr></thead><tbody><tr><td>Author</td><td>Michael Niemeyer1,2 Lars Mescheder1,2,3† Michael Oechsle1,2,4 Andreas Geiger1,2</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2020</td></tr><tr><td>Project</td><td><a href="https://github.com/autonomousvision/differentiable_volumetric_rendering">autonomousvision/differentiable_volumetric_rendering: This repository contains the code for the CVPR 2020 paper “Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision” (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4556205053120094209&amp;noteId=1967822994669366784">Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231208100556.png" alt="image.png|666"></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>基于学习的三维重建方法已经显示出令人印象深刻的结果。然而，大多数方法需要3D监督，这通常很难获得真实世界的数据集。最近，一些研究提出了可微分渲染技术来训练RGB图像的重建模型。不幸的是，这些方法目前仅限于基于体素和网格的表示，受到离散化或低分辨率的影响。在这项工作中，我们提出了一种用于隐式形状和纹理表示的可微分渲染公式。隐式表征最近越来越受欢迎，因为它们连续地表示形状和纹理。我们的关键观点是深度梯度可以使用隐式微分的概念解析地推导出来。这允许我们直接从RGB图像中学习隐含的形状和纹理表示。我们的实验表明，我们的单视图重建可以与那些完全3D监督的学习相媲美。此外，我们发现我们的方法可以用于多视图三维重建，直接产生水密网格。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>近年来，基于学习的三维重建方法取得了令人瞩目的成果[12,13,17,24,41,48,49,56,64,80]。通过使用在训练过程中获得的丰富的先验知识，他们能够从单个图像中推断出3D模型。然而，大多数基于学习的方法都局限于合成数据，主要是因为它们需要精确的三维地面真值模型作为训练的监督。</p><p>为了克服这一障碍，最近的研究工作已经研究了只需要以深度图或多视图图像形式进行2D监督的方法。大多数现有方法通过修改渲染过程使其可微来实现这一点[4,11,15,21,33,36,43,44,47,50,58,59,62,75,76,79,88]。虽然产生令人信服的结果，但它们仅限于特定的3D表示(例如体素或网格)，这些表示受到离散化伪影的影响，并且计算成本将它们限制在小分辨率或变形固定模板网格。同时，已经提出了形状和纹理的隐式表示[12,48,56][54,66]，它们在训练过程中不需要离散化，并且具有恒定的内存占用。然而，使用隐式表示的现有方法需要3D ground truth进行训练，并且如何仅从图像数据中学习隐式表示仍然不清楚</p><p>贡献:在这项工作中，我们引入了可微分体积渲染(DVR)。我们的关键见解是，我们可以根据隐式形状和纹理表示的网络参数推导出预测深度图的分析梯度(见图1)。这一见解使我们能够为隐式形状和纹理表示设计一个可微分的渲染器，并允许我们仅从多视图图像和对象蒙版中学习这些表示。由于我们的方法不需要在前向传递中存储体积数据，因此其内存占用与深度预测步骤的采样精度无关。我们表明，我们的公式可以用于各种任务，如单视图和多视图重建，并适用于合成和真实数据。与[54]相反，我们不需要将纹理表示限制在几何上，而是学习一个具有表示几何和纹理的共享参数的单一模型。我们的代码和数据提供在<a href="https://github.com/autonomousvision/可微分体绘制。">https://github.com/autonomousvision/可微分体绘制。</a></p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>3D Representations<br>基于学习的三维重建方法可以分为三类。它们使用的表示方式有基于体素的[8,13,19,61,64,73,82,83]，基于点的[2,17,31,40,77,85]，基于网格的[24,32,41,45,55,80]，或者隐式表示[3,12,22,30,48,49,56,66,81]。体素可以通过标准深度学习架构轻松处理，但即使在稀疏数据结构上操作[23,64,74]，它们也仅限于相对较小的分辨率。虽然基于点的方法[2,17,40,77,85]的内存效率更高，但由于缺少连接信息，它们需要密集的后处理。大多数基于网格的方法不进行后处理，但它们通常需要一个可变形的模板网格[80]或将几何形状表示为3D补丁的集合[24]，这会导致自相交和非水密网格。<br>为了缓解这些问题，隐式表示越来越受欢迎[3,12,22,30,48,49,53,54,56,66,81]。通过隐式描述三维几何和纹理，例如，作为二值分类器的决策边界[12,48]，它们不会离散空间，并且具有固定的内存占用。<br>在这项工作中，我们证明了隐式表示的体积渲染步骤是固有可微的。与之前的作品相比，这使我们能够使用2D监督学习隐含的3D形状和纹理表示。</p><p>3D Reconstruction<br>恢复在图像捕获过程中丢失的3D信息是计算机视觉的长期目标之一[25]。经典的多视图立体(MVS)方法[5 - 7,20,37,60,68 - 70]通常在相邻视图之间匹配特征[5,20,68]或在体素网格中重建三维形状[6,7,37,60,70]。虽然前一种方法产生深度图作为输出，但必须在有损的后处理步骤中进行融合，例如使用体积融合[14]，后一种方法受到3D体素网格过度内存要求的限制。与这些高度工程化的方法相比，我们的通用方法直接在3D空间中输出一致的表示，可以很容易地转换为水密网格，同时具有恒定的内存占用。<br>最近，基于学习的方法[16,29,39,58,63,86,87]被提出，要么学习匹配图像特征[39]，精炼或融合深度图[16,63]，优化经典MVS管道的部分[57]，要么用端到端训练的神经网络替换整个MVS管道[29,86,87]。与这些基于学习的方法相比，我们的方法可以仅从2D图像进行监督，并输出一致的3D表示。</p><p>Differentiable Rendering<br>我们专注于通过可微渲染学习3D几何的方法，而不是最近的神经渲染方法[42,51,52,71]，后者合成高质量的新视图，但不推断3D物体。它们也可以根据它们所使用的3D几何图形的底层表示进行分类。<br>Loper等人[47]提出了OpenDR，它近似于传统的基于网格的图形管道的向后传递，并启发了后续的一些工作[11,21,27,28,33,44,88]。Liu等人[44]用软版本替换栅格化步骤，使其可微。虽然在重建任务中产生令人信服的结果，但这些方法需要一个可变形的模板网格进行训练，限制了输出的拓扑结构。<br>另一条工作线在体素网格上运行[46,50,57,79]。Paschalidou等人[57]和Tulsiani等人[79]提出了一种概率射线势公式。虽然提供了一个坚实的数学框架，但所有中间评估都需要保存以进行反向传播，这将这些方法限制在相对小分辨率的体素网格中。<br>Liu等人[45]提出通过在具有稀疏数量支持区域的光线交叉点上执行max-pooling来推断多视图轮廓的隐式表示。相比之下，我们使用纹理信息使我们能够改善视觉船体和重建凹形状。Sitzmann等人[72]通过基于lstm的可微分渲染器从RGB图像中推断出隐含的场景表示。在生成高质量的渲染图时，不能直接提取几何图形，需要存储中间结果以便计算梯度。相反，我们证明了体绘制对于隐式表示是固有可微的。因此，不需要为向后传递保存中间结果。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>在本节中，我们描述了我们的可微分体积渲染(DVR)方法。我们首先定义用于表示三维形状和纹理的隐式神经表示。接下来，我们提供了DVR的正式描述和所有相关的实现细节。图2概述了我们的方法</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>在这项工作中，我们提出了可微分体积渲染(DVR)。观察到体绘制对于隐式表示是固有可微的，这使我们能够为相对于网络参数的深度梯度制定一个解析表达式。我们的实验表明，DVR使我们能够在没有3D监督的情况下从多视图图像中学习隐式3D形状表示，与完全3D监督学习的模型相媲美。此外，我们发现我们的模型也可以用于多视图三维重建。我们认为DVR是一种有用的技术，它拓宽了隐式形状和纹理表示的应用范围。在未来，我们计划研究如何规避对物体蒙版和相机信息的需求，例如，通过预测软蒙版，以及如何不仅估计纹理，还估计更复杂的材料属性。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Image-Based 3D Object Reconstruction</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Review/Image-Based%203D%20Object%20Reconstruction/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Review/Image-Based%203D%20Object%20Reconstruction/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Image-based 3D Object Reconstruction: State-of-the-Art and Trends in the Deep Learning Era</th></tr></thead><tbody><tr><td>Author</td><td>Xian-Feng Han<em>, Hamid Laga</em>, Mohammed Bennamoun Senior Member, IEEE</td></tr><tr><td>Conf/Jour</td><td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td></tr><tr><td>Year</td><td>2019</td></tr><tr><td>Project</td><td></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4500339722630619137&amp;noteId=2082170288520054016">Image-Based 3D Object Reconstruction: State-of-the-Art and Trends in the Deep Learning Era (readpaper.com)</a></td></tr></tbody></table></div><span id="more"></span><p><a href="https://cloud.tencent.com/developer/article/2127922">最新综述：深度学习图像三维重建最新方法及未来趋势-腾讯云开发者社区-腾讯云 (tencent.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Review </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Structured Light Review</title>
      <link href="/3DReconstruction/Multi-view/Structured%20Light/Structured%20Light%20Review/"/>
      <url>/3DReconstruction/Multi-view/Structured%20Light/Structured%20Light%20Review/</url>
      
        <content type="html"><![CDATA[<p>三维重建是计算机视觉和计算机图像图形学相结合的一个热门研究方向。根据测量时是否与被测物体接触，可分为接触式测量和非接触式测量。</p><ul><li>接触式测量方法虽然测量精度高，但测量效率低，速度慢，操作不当很容易损坏被测物体表面，而且由于探头有一定表面积，对表面复杂的物体难以测量，不具备普遍性和通用性。</li><li>非接触式三维测量方式又可以分为两大类：主动式测量和被动式测量。非接触式测量方式以其无损坏、测量速度高、简单等优点已成为三维轮廓测量的研究趋势。<ul><li>主动式测量是向目标物体表面投射设计好的图案，该图案由于物体的高度起伏引起一定的畸变，通过匹配畸变的图案获得目标物体的。<strong>TOF、结构光三维重建</strong></li><li>被动式测量是通过周围环境光对目标物体进行照射，然后检测目标物体的特征点以得到其数据。<strong>双目视觉法、SFM、MVS、NeRF</strong></li></ul></li></ul><span id="more"></span><p>参考：<br><a href="https://zhuanlan.zhihu.com/p/54761392">结构光综述 - 知乎 (zhihu.com)</a></p><ul><li><a href="http://www.rtbasics.com/Downloads/IEEE_structured_light.pdf">http://www.rtbasics.com/Downloads/IEEE_structured_light.pdf</a></li><li><a href="http://www.sci.utah.edu/~gerig/CS6320-S2015/CS6320_3D_Computer_Vision.html">CS6320 3D Computer Vision (utah.edu)</a></li><li><a href="http://mesh.brown.edu/byo3d/source.html">Build Your Own 3D Scanner: Optical Triangulation for Beginners (brown.edu)</a><br><a href="https://www.oakchina.cn/2023/05/16/3_depth_cams/">双目、结构光、tof，三种深度相机的原理区别看这一篇就够了！ - (oakchina.cn)</a><br><a href="https://github.com/jiayuzhang128/FourStepPhaseShifting/blob/master/support/%E7%BB%93%E6%9E%84%E5%85%89%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA.pdf">FourStepPhaseShifting/support/结构光三维重建.pdf</a><br><a href="https://zhuanlan.zhihu.com/p/29971801">结构光简史 - 知乎 (zhihu.com)</a></li></ul><h1 id="结构光原理"><a href="#结构光原理" class="headerlink" title="结构光原理"></a>结构光原理</h1><p>结构光主要可以分为两类</p><ol><li>线扫描结构光；线扫描结构光较之面阵结构光较为简单，精度也比较高，在工业中广泛用于物体体积测量、三维成像等领域。</li><li>面阵结构光；</li></ol><h2 id="线扫描结构光"><a href="#线扫描结构光" class="headerlink" title="线扫描结构光"></a>线扫描结构光</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231202200426.png" alt="image.png|666"></p><p>由小孔成像模型有 $\frac Xx=\frac Zf=\frac Yy$<br>由三角测量原理又有 $\tan\alpha=\frac Z{b-X}$<br>两式联立则有<br>$\begin{aligned}&amp;Z=\frac Xx\cdot f=\tan\alpha\cdot(b-X)\\&amp;X\cdot(\frac fx+\tan\alpha)=\tan\alpha\cdot b\end{aligned}$</p><p>可得，所测物体点的三维坐标与俯仰角 $\gamma$ 无关<br>$\begin{aligned}X&amp;=\frac{\tan\alpha\cdot b\cdot x}{f+x\cdot\tan\alpha}\\Y&amp;=\frac{\tan\alpha\cdot b\cdot y}{f+x\cdot\tan\alpha}\\Z&amp;=\frac{\tan\alpha\cdot b\cdot f}{f+x\cdot\tan\alpha}\end{aligned}$</p><h2 id="面阵结构光"><a href="#面阵结构光" class="headerlink" title="面阵结构光"></a>面阵结构光</h2><p>面阵结构光大致可以分为两类：<strong>随机结构光</strong>和<strong>编码结构光</strong>。</p><ul><li>随机结构光较为简单，也更加常用。通过投影器向被测空间中投射<strong>亮度不均</strong>和<strong>随机分布</strong>的点状结构光，通过双目相机成像，所得的双目影像经过极线校正后再进行双目稠密匹配，即可重建出对应的深度图。如下图为某种面阵的红外结构光。(和普通双目算法很相似)</li><li>编码结构光可分为：<ul><li>时序编码：高精度，<strong>但只适用于静态场景且需要拍摄大量影像</strong></li><li>空间编码：无需多张照片，只需要一对影像即可进行三维重建。可以满足实时处理，用在动态环境中，<strong>但易受噪声干扰</strong>：由于反光、照明等原因可能导致成像时部分区域等编码信息缺失；<strong>对于空间中的遮挡比较敏感</strong>；相较于时序编码结构光<strong>精度较低</strong></li></ul></li></ul><p><strong>时序编码结构光</strong><br>在一定时间范围内，通过投影器向被测空间投射<strong>一系列</strong>明暗不同的结构光，每次投影都通过相机进行成像。则通过查找具有相同编码值的像素，来进行双目匹配</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231202204148.png" alt="image.png|666"></p><p><strong>空间编码结构光</strong><br>为满足动态场景的需要，可以采用空间编码结构光。空间编码结构光特指向被测空间中投影经过数学编码的、一定范围内的光斑不具备重复性的结构光。由此，某个点的编码值可以通过其临域获得。其中，包含一个完整的空间编码的像素数量（窗口大小）就决定了重建的精度</p><p>De Bruijn sequence</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231202204800.png" alt="image.png|666"></p><p>2D Spatial Grid Patterns<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231202204925.png" alt="image.png|666"></p><h1 id="结构光三维重建项目"><a href="#结构光三维重建项目" class="headerlink" title="结构光三维重建项目"></a>结构光三维重建项目</h1><p>几个 Github 项目：</p><ul><li><a href="https://github.com/Tang1705/Happy-Reconstruction">Tang1705/Happy-Reconstruction</a></li><li><a href="https://github.com/jiayuzhang128/FourStepPhaseShifting">jiayuzhang128/FourStepPhaseShifting</a></li><li><a href="https://github.com/timbrist/structure-light">3D reconstruction with Structure Light</a></li><li><a href="https://github.com/casparji1018921/-Structured-Light-3D-Reconstruction-">Structured-Light-3D-Reconstruction分享和交流</a></li></ul><h2 id="FourStepPhaseShifting"><a href="#FourStepPhaseShifting" class="headerlink" title="FourStepPhaseShifting"></a>FourStepPhaseShifting</h2><p><a href="https://github.com/jiayuzhang128/FourStepPhaseShifting">jiayuzhang128/FourStepPhaseShifting</a></p><p>使用”互补格雷码+相移码”方法获取被测物体的三维信息<br>相机标定获得相机内外参</p><h3 id="硬件设备搭建"><a href="#硬件设备搭建" class="headerlink" title="硬件设备搭建"></a>硬件设备搭建</h3><ul><li>DLP 投影仪：闻亭 PRO6500</li><li>灰度相机：FLIR BFS-U3-50S5</li><li>旋转平台</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231203175408.png" alt="image.png|666"></p><h3 id="投影仪-相机系统标定"><a href="#投影仪-相机系统标定" class="headerlink" title="投影仪-相机系统标定"></a>投影仪-相机系统标定</h3><p><a href="https://blog.csdn.net/qq_40918859/article/details/122503156">投影仪-相机系统标定方法_投影仪标定-CSDN博客</a><br><a href="http://mesh.brown.edu/calibration/">Projector-Camera Calibration / 3D Scanning Software (brown.edu)</a></p><h4 id="一般相机标定"><a href="#一般相机标定" class="headerlink" title="一般相机标定"></a>一般相机标定</h4><p>—&gt;得到精确的相机内外参和畸变参数<br><a href="https://zhuanlan.zhihu.com/p/136827980">张正友标定法-完整学习笔记-从原理到实战 - 知乎 (zhihu.com)</a></p><ul><li>相机标定的目的是：<strong>建立相机成像几何模型并矫正透镜畸变</strong><ul><li>建立物体从三维世界映射到相机成像平面这一过程中的几何模型非常重要，而这一过程最关键的部分就是要得到相机的内参和外参</li><li>由于小孔成像只有小孔部分能透过光线就会导致物体的成像亮度很低，因此发明了透镜，但由于透镜的制造工艺，会使成像产生多种形式的畸变</li></ul></li></ul><p>透镜的畸变主要分为径向畸变和切向畸变，我们一共需要 5 个 3 个畸变参数（k1、k2、k3、p1 和 p2 ）来描述透镜畸变</p><ul><li>径向畸变是由于透镜形状的制造工艺导致。且越向透镜边缘移动径向畸变越严重</li><li>切向畸变是由于透镜和 CMOS 或者 CCD 的安装位置误差导致。因此，如果存在切向畸变，一个矩形被投影到成像平面上时，很可能会变成一个梯形</li></ul><p>标定过程：固定相机，改变棋盘标定板的位姿，一般拍摄 20 组以上照片</p><ul><li>根据两张图片中棋盘特征点的世界坐标位置和像素坐标位置，可以得到单应性矩阵(特征点从一张图片变换到另一张图片的<strong>变换矩阵</strong>，单应性矩阵<strong>H 是内参矩阵和外参矩阵的混合体</strong>)</li><li>先不考虑镜头畸变，根据旋转向量之间的两个约束关系和单应性矩阵，得到相机的内参<ul><li>如果图片数量 n&gt;=3，就可以得到唯一解 b(相机内参)</li></ul></li><li>上述只是理论过程，在实际标定过程中，一般使用最大似然估计进行优化</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231203104443.png" alt="image.png|666"></p><p>标定实战<br>MATLAB 自带相机标定应用程序，有 camera calibrator 和 stereo camera calibrator 两类相机标定应用程序。其操作简单、直观，能够获得相机的内、外参数以及畸变参数等</p><h4 id="投影仪-相机系统标定-1"><a href="#投影仪-相机系统标定-1" class="headerlink" title="投影仪-相机系统标定"></a>投影仪-相机系统标定</h4><p><a href="http://mesh.brown.edu/calibration/">Projector-Camera Calibration / 3D Scanning Software (brown.edu)</a></p><p>用带有径向和切向畸变的<strong>小孔模型</strong>描述相机和投影仪 <a href="https://blog.csdn.net/qq_40918859/article/details/122271381">相机模型</a></p><h3 id="基于相移法的结构光三维重建"><a href="#基于相移法的结构光三维重建" class="headerlink" title="基于相移法的结构光三维重建"></a>基于相移法的结构光三维重建</h3><p>互补格雷码+相移码 <a href="https://github.com/jiayuzhang128/FourStepPhaseShifting/blob/master/support/%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D.pdf">FourStepPhaseShifting/support/原理介绍.pdf</a> or <a href="https://blog.csdn.net/qq_40918859/article/details/120575820">CSDN1</a> + <a href="https://blog.csdn.net/qq_40918859/article/details/127763190">CSDN2</a></p><p>相移法的结构光通过投影仪投射一系列正弦编码的条纹图案到被测物体表面，然后通过相机采集发生形变的条纹图像，继而根据相移算法进行解码获得待测物体表面的深度信息</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231203111427.png" alt="image.png|666"></p><h4 id="生成四步相移图像"><a href="#生成四步相移图像" class="headerlink" title="生成四步相移图像"></a>生成四步相移图像</h4><p>N 步相移码：首先相移码的原理是利用 N 幅正弦条纹图通过投影仪投射到物体表面再通过相机拍摄获取图像，通过所得图像计算每个位置的相位差，然后通过相位—深度的映射关系获取物体的深度信息。</p><p>投影光栅的光强函数<br>$\begin{aligned}&amp;I_n(x,y)=A(x,y)+B(x,y)cos[\varphi(x,y)+\Delta\varphi_n]\\{}\\&amp;\Delta\varphi_n=2\pi(n-1)/N(n\in[1,N])\end{aligned}$</p><ul><li>A(x,y)表示背景光强，B(x,y)表示调制幅值</li><li>$\varphi(x,y)$ 表示包裹相位（相对相位）</li><li>$\Delta\varphi_n$ 表示平移相位</li></ul><p>由于选用 4 位格雷码+四步相移，编码区域可以分为 16，因此相移码的周期数，周期 $T=Width/f$<br>$\varphi(x,y)=\frac{2\pi fx}{Width}$ Width 表示图像宽度(单位:像素)</p><p>相移条纹图(下)生成公式，$u_p,v_p$ 表示投影仪像素坐标；T 表示单根条纹在一个周期内的像素数量<br>$\begin{gathered}\begin{aligned}&amp;I_0(u_p,\nu_p)=0.5+0.5\cos(2\pi\frac{u_p}T) \\&amp;I_{1}(u_{p},\nu_{p})=0.5+0.5\cos(2\pi\frac{u_{p}}{T}+\frac{\pi}{2}) \\&amp;I_2(u_p,\nu_p)=0.5+0.5\cos(2\pi\frac{u_p}T+\pi) \\&amp;I_{3}(u_{p},\nu_{p})=0.5+0.5\cos(2\pi\frac{u_{p}}{T}+\frac{3\pi}{2})\end{aligned} \end{gathered}$</p><p>代码生成：</p><ul><li>第一步：生成一个 1920 维的行向量； </li><li>第二步：利用公式 $I(x,y)=128+127cos[2\pi(\frac{fx}{Width}+\frac{n-1}N)]$ 对每一个向量元素进行填充； </li><li>第三步：利用 <code>np.Tile()</code> 函数生成 1080 行，得到 <code>1920*1080</code> 的矩阵； </li><li>第四步：利用 <code>cv2.imshow()</code> 函数显示。</li></ul><h4 id="格雷码-中"><a href="#格雷码-中" class="headerlink" title="格雷码(中)"></a>格雷码(中)</h4><p>一种二进制码制，是一种无权码，它的特点是前后相邻码值只改变一位数，这样可以减小错位误差，因此又称为最小错位误差码。</p><div class="table-container"><table><thead><tr><th>十进制数</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th></tr></thead><tbody><tr><td>格雷码</td><td>0000</td><td>0001</td><td>0011</td><td>0010</td><td>0110</td><td>0111</td><td>0101</td><td>0100</td><td>1100</td><td>1101</td><td>1111</td><td>1110</td><td>1010</td><td>1011</td><td>1001</td><td>1000</td></tr></tbody></table></div><p>生成 n 位格雷码:</p><ul><li>传统方法<ul><li>第一步，生成 n 位全零码 </li><li>第二步，改变最右端的码值 </li><li>第三步，改变自右起第一个“1”码元左边的码元 </li><li>重复第二、三步直至得到 2^n 个格雷码</li></ul></li><li>递归法：n 位格雷码可以由（n-1）位格雷码得到<ul><li>第一步：（n-1）位格雷码正序排列最左侧（前缀）补 0 </li><li>第二步：（n-1）位格雷码逆序排列最左侧（前缀）补 1 </li><li>第三步：一、二步得到结果依次排列得到 n 位格雷码</li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">递归法：</span><br><span class="line">1位：0 1 </span><br><span class="line">正序 00 01 </span><br><span class="line">逆序 11 10 </span><br><span class="line">2位：00 01 11 10 </span><br><span class="line">正序 000 001 011 010 </span><br><span class="line">逆序 110 111 101 100 </span><br><span class="line">3位：000 001 011 010 110 111 101 100</span><br><span class="line">正序 0000 0001 0011 0010 0110 0111 0101 0100</span><br><span class="line">逆序 1100 1101 1111 1110 1010 1011 1001 1000</span><br><span class="line">4位：0000 0001 0011 0010 0110 0111 0101 0100 1100 1101 1111 1110 1010 1011 1001 1000</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>格雷码与普通二进制码的转换</p><ul><li>传统方法：<ul><li>二进制码—&gt;格雷码：二进制码与其右移一位高位补零后的数码异或后得到格雷码</li><li>格雷码—&gt;二进制码：最左边的一位不变，从左边第二位起，将每位与左边一位解码后的值异或，作为该位解码后的值。依次异或，直到最低位。依次异或转换后的值（二进制数）就是格雷码转换后二进制码的值</li></ul></li></ul><p>在生成格雷码的同时，将每一位格雷码与其对应的十进制数组成键值对储存在字典中，这样在进行二进制码、格雷码、十进制相互转换时可以直接查询字典完成比较方便<br>本项目采用的互补格雷码，需要 4 位格雷码图和 5 位格雷码的最后一张，详细代码可以查看 python 版本代码。</p><h4 id="投影获得图像"><a href="#投影获得图像" class="headerlink" title="投影获得图像"></a>投影获得图像</h4><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231203111447.png" alt="image.png|666"></p><h4 id="求解相对相位-包裹相位"><a href="#求解相对相位-包裹相位" class="headerlink" title="求解相对相位(包裹相位)"></a>求解相对相位(包裹相位)</h4><p><a href="https://blog.csdn.net/qq_44408326/article/details/114838649?spm=1001.2014.3001.5501">数字条纹投影~标准N步相移主值相位计算式推导过程_up六月上的博客-CSDN博客</a></p><p>$\begin{aligned}I_2(u_c,\nu_c)-I_0(u_c,\nu_c)=&amp;0.5\left(\cos[\phi(u_c,\nu_c)+\pi]-\cos[\phi(u_c,\nu_c)]\right)=-\cos[\phi(u_c,\nu_c)]\\I_3(u_c,\nu_c)-I_1(u_c,\nu_c)=&amp;0.5\left\{\cos\left[\phi(u_c,\nu_c)+\frac{3\pi}2\right]-\cos\left[\phi(u_c,\nu_c)+\frac\pi2\right]\right\}=\sin[\phi(u_c,\nu_c)]\end{aligned}$</p><p>$\phi(u_c,\nu_c)=-\arctan\frac{I_3(u_c,\nu_c)-I_1(u_c,\nu_c)}{I_2(u_c,v_c)-I_0(u_c,\nu_c)},\phi(u_c,\nu_c)\in\left[-\pi,\pi\right]$<br>$𝑢_𝑐$、$𝑣_𝑐$ 表示相机获取图像的像素标, $\phi(u_c, v_c)$ 表示该像素点的包裹相位</p><p>将每一个像素利用上述方法求得包裹相位并储存在对应位置，可以得到所有对应位置的数值大小都在 $[0,2\pi]$,然后对其进行线性放缩到 $[0,255]$</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231203165120.png" alt="image.png|666"></p><h4 id="求绝对相位"><a href="#求绝对相位" class="headerlink" title="求绝对相位"></a>求绝对相位</h4><p>相位展开获得绝对相位(得到了投影仪像素坐标与相机像素坐标的关系)<br><strong>GC 表示格雷码图，k1、k2 表示对应的编码值</strong><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231203165131.png" alt="image.png|666"></p><p>包裹相位ϕ如上图，要想将上面的包裹相位还原成连续的绝对相位，只要在每一个截断处加上2kπ(k 表示周期的级次),就可以恢复成连续的相位。($\phi(x,y) = \varphi(x,y) + 2 \pi k_1(x,y)$)因此我们用四幅格雷码图像将整个有效视区分成 16 份并分别编码，因此这里的<strong>周期级次 K 就等于格雷码的编码值（k1），但是由于实际过程中，由于投影仪和相机的畸变效应，所投的格雷码图像与相移码图像会产生错位：</strong><br>由于错位发生在包裹相位的<strong>截断处</strong>，为了解决错位问题，我们引入一张5位格雷码，与4位格雷码形成互补，k2的计算公式：$K2=INT[(V2+1)/2]$，INT:向下取整，V2：GC0-GC5 格雷码对应的十进制数。</p><p>利用以下公式就可以避免截断处产生错位：<br>$\phi(x,y)=\begin{cases}\varphi(x,y)+2\pi k_2(x,y),~\varphi(x,y)\leq\frac\pi2\\\\\varphi(x,y)+2\pi k_1(x,y),~\frac\pi2&lt;\varphi(x,y)&lt;\frac{3\pi}2\\\\\varphi(x,y)+2\pi[k_2(x,y)-1],~\varphi(x,y)\geq\frac{3\pi}2&amp;\end{cases}$</p><p>在相机实际拍摄的图片中由于环境光的影响，拍摄到的格雷码值并不是标准的二值图，需要二值化:</p><ul><li>首先要将格雷码图像进行二值化处理</li><li>然后计算 k1、k2 的值</li><li>最后带入公式求解绝对相位 $\phi(x,y)$</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231203175500.png" alt="image.png|666"></p><p><strong>然后求解三维坐标</strong>，获得三维点云(根据各坐标系的关系以及相机和投影仪的参数)</p><h4 id="获得相机-投影仪像素坐标之间的对应关系"><a href="#获得相机-投影仪像素坐标之间的对应关系" class="headerlink" title="获得相机-投影仪像素坐标之间的对应关系"></a>获得相机-投影仪像素坐标之间的对应关系</h4><p>由相机像素坐标 $u_c,v_c$，投影仪像素坐标 $(u_p,v_p)$<br>$\Phi(\operatorname{u_P},\operatorname{v_P})=\Phi(\operatorname{u_C},\operatorname{v_C})$<br>$\Phi(\operatorname{u_P},\operatorname{v_P})=\frac{2\pi\operatorname{u_p}}{\operatorname{T}}$<br>可得：$\mathrm{u_p~=~\frac{\Phi(u_p,v_p)<em>T}{2\pi}~=~\frac{\Phi(u_c,v_c)</em>T}{2\pi}}$</p><h4 id="根据标定参数获得重建点云信息"><a href="#根据标定参数获得重建点云信息" class="headerlink" title="根据标定参数获得重建点云信息"></a>根据标定参数获得重建点云信息</h4><p>由相机内外参矩阵，可以得到像素坐标与世界坐标之间的关系：</p><p>$\left.\left\{\begin{array}{c}\mathrm{X_c=x_c<em>Z_c}\\\mathrm{Y_c=y_c</em>Z_c}\\\mathrm{Z_c=Z_c}\end{array}\right.\right.$<br>$\left.\left\{\begin{array}{rl}\mathrm{x_c~=~(u_c~-u_{c0}~)/f_{cx}}\\\mathrm{y_c~=~(v_c~-v_{c0}~)/f_{cy}}\end{array}\right.\right.$<br>$x_c,y_c$ 为 $u_c,v_c$ 的相机坐标，世界坐标 $X_w,Y_w,Z_w$ 旋转平移得到 $X_c,Y_c,Z_c$</p><p>同理投影仪：<br>$\left.\left\{\begin{array}{l}X_p=x_p<em>Z_p\\Y_p=y_p</em>Z_p\\Z_p=Z_p\end{array}\right.\right.$<br>$\left.\left\{\begin{array}{l}x_p=(u_p-u_{p0})/f_{px}\\y_p=(v_p-v_{p0})/f_{py}\end{array}\right.\right.$</p><p>由相机和投影仪外参关系：<br>$\left.\left[\begin{array}{c}X_p\\Y_p\\Z_p\end{array}\right.\right]=R_{c\to p}\left[\begin{array}{c}X_c\\Y_c\\Z_c\end{array}\right]+t_{c\to p}$<br>可得：<br>$\left.\left\{\begin{array}{l}X_p=r_{11}X_c+r_{12}Y_c+r_{13}Z_c+t_x=(r_{11}x_c+r_{12}y_c+r_{13})Z_c+t_x\\Y_p=r_{21}X_c+r_{22}Y_c+r_{23}Z_c+t_y=(r_{21}x_c+r_{22}y_c+r_{23})Z_c+t_y\\Z_p=r_{31}X_c+r_{32}Y_c+r_{33}Z_c+t_z=(r_{31}x_c+r_{32}y_c+r_{33})Z_c+t_z\end{array}\right.\right.$</p><p>由相机投影仪像素坐标关系：<br>$\left.\left\{\begin{array}{l}u_p=f_{px}<em>x_p+u_{p0}\\u_p=\frac{\Phi(u_c,v_c)</em>T}{2\pi}\end{array}\right.\right.\Rightarrow f_{px}<em>x_p+u_{p0}=\frac{\Phi(u_c,v_c)</em>T}{2\pi}$</p><p>联立上述两式：<br>$\left.\left\{\begin{array}{l}x_p<em>Z_p=(r_{11}x_c+r_{12}y_c+r_{13})Z_c+t_x\\Z_p=(r_{31}x_c+r_{32}y_c+r_{33})Z_c+t_z\\f_{px}</em>x_p+u_{p0}=\frac{\Phi(u_c,v_c)<em>T}{2\pi}\end{array}\right.\right.$<br>可得：$Z_{c}=\frac{x_{p}t_{z}-t_{x}}{J_{x}-J_{z}x_{p}}$<br>其中<br>$\begin{aligned}&amp;\begin{aligned}J_x=(r_{11}x_c+r_{12}y_c+r_{13})\end{aligned}\text{;} \\&amp;\begin{aligned}J_z=(r_{31}x_c+r_{32}y_c+r_{33})\end{aligned}{;} \\&amp;x_{p}=(\frac{\Phi(u_{c},v_{c})</em>T}{2\pi}-u_{p0})/f_{px}。\end{aligned}$</p><p>则相机坐标系下，每个像素的世界坐标为：<br>$\left.\left\{\begin{array}{l}X_c=x_c<em>Z_c\\Y_c=y_c</em>Zc\\Z_c=\frac{x_pt_z-t_x}{J_x-J_zx_p}\end{array}\right.\right.$</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Structured Light </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Voxurf</title>
      <link href="/3DReconstruction/Multi-view/Explicit%20Volumetric/Voxurf/"/>
      <url>/3DReconstruction/Multi-view/Explicit%20Volumetric/Voxurf/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Voxurf: Voxel-based Efficient and Accurate Neural Surface Reconstruction</th></tr></thead><tbody><tr><td>Author</td><td>Tong Wu and Jiaqi Wang and Xingang Pan and Xudong Xu and Christian Theobalt and Ziwei Liu and Dahua Lin</td></tr><tr><td>Conf/Jour</td><td>ICLR</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://github.com/wutong16/Voxurf">wutong16/Voxurf: [ ICLR 2023 Spotlight ] Pytorch implementation for “Voxurf: Voxel-based Efficient and Accurate Neural Surface Reconstruction” (github.Com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4789011193549291521&amp;noteId=2073894198108795136">Voxurf: Voxel-based Efficient and Accurate Neural Surface Reconstruction (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231201171502.png" alt="image.png|666"></p><p>依赖 mask 即 rembg 技术</p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>神经表面重建的目的是基于多视图图像重建精确的三维表面。以前基于神经体绘制的方法大多是用 mlp 训练一个完全隐式的模型，这通常需要数小时的训练来处理单个场景。最近的研究探索了显式体积表示，通过记忆具有可学习体素网格的重要信息来加速优化。然而，现有的基于体素的方法往往难以重建细粒度的几何形状，即使与基于 sdf 的体绘制方案相结合也是如此。我们发现这是因为 1)体素网格倾向于打破有利于精细几何学习的颜色几何依赖;2)约束不足的体素网格缺乏空间一致性，容易受到局部极小值的影响。在这项工作中，我们提出了 Voxurf，一种基于体素的表面重建方法，既高效又准确。Voxurf 通过几个关键设计解决了上述问题，包括 1)一个两阶段的训练过程，获得连贯的粗形状并连续恢复精细细节，2)一个保持颜色几何依赖性的双色网络，以及 3)一个分层几何特征，以鼓励信息跨体素传播。大量的实验表明，Voxurf 同时实现了高效率和高质量。在 DTU 基准测试中，与之前的全隐式方法相比，Voxurf 以 20 倍的训练加速实现了更高的重建质量。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><strong>基于多视图图像的神经表面重建 + 完全隐式 MLP</strong><br>近年来，基于多视图图像的神经表面重建取得了巨大进展。受 Neural Radiance Fields (NeRF) (Mildenhall et al.， 2020)在 Novel View Synthesis (NVS)上的成功启发，最近的作品遵循神经体渲染方案，通过完全隐式模型(Oechsle et al.， 2021)使用有符号距离函数(SDF)或占用场表示 3D 几何形状。Yariv 等，2021;Wang 等人，2021)。这些方法训练了一个深度多层感知器(MLP)，它在每个相机光线上采集数百个采样点，并输出相应的颜色和几何信息。然后通过测量每条光线上累积的颜色与地面真实值之间的差异来应用逐像素监督。使用纯基于 mlp 的框架学习所有几何和颜色细节，这些方法需要对单个场景进行数小时的训练，这大大限制了它们在现实世界中的应用。</p><p><strong>NeRF 显示体积方法</strong><br>NeRF 的最新进展借助显式体积表示加速了训练过程(Sun et al.， 2022a;Yu et al.， 2022a;Chen et al.， 2022)。这些作品通过显式体素网格直接存储和优化几何和颜色信息。例如，查询点的密度可以很容易地从八个相邻点中插值出来，并且与视图相关的颜色要么用球面谐波系数表示(Yu 等人，2022a)，要么用将可学习的网格特征作为辅助输入的浅 mlp 预测(Sun 等人，2022a)。这些方法以更低的训练成本(&lt; 20 分钟)获得具有竞争力的渲染性能。<strong>然而，它们的三维表面重建结果不能忠实地反映精确的几何形状，存在明显的噪声和孔洞</strong>(图 1 (a))。这是由于基于密度的体绘制方案固有的模糊性，而明确的体表示引入了额外的挑战。</p><p><strong>本文 Voxurf</strong><br>在这项工作中，我们的目标是利用显式体积表示进行有效的训练，并提出定制设计来获得高质量的表面重建。为此，一个简单的想法是嵌入基于 sdf 的体绘制方案(Wang et al.， 2021;Yariv 等人，2021)将其转化为明确的体积表示框架(Sun 等人，2022a)。然而，我们发现这个 naive 基线模型不能很好地工作，因为它丢失了大部分几何细节并产生了不希望的噪声(图 1 (c))。我们揭示了这个框架的几个关键问题如下。</p><ul><li>首先，在完全隐式模型中，颜色网络将表面法线作为输入，有效地建立颜色几何依赖关系，促进精细几何学习。然而，在基线模型中，颜色网络倾向于更多地依赖于额外的欠约束体素特征网格输入，从而打破了颜色几何依赖性。</li><li>其次，由于优化体素网格的自由度很高，如果没有额外的约束，很难保持全局一致的形状。每个体素点的单独优化阻碍了整个体素网格的信息共享，这损害了表面的平滑性并引入了局部最小值。</li></ul><p>为了应对挑战，我们引入了 Voxurf，这是一种高效的基于体素的精确表面重建管道:<br>1)我们利用两阶段的训练过程，获得连贯的粗形状并连续恢复精细细节。<br>2)我们设计了一个双色网络，它能够通过体素网格表示复杂的颜色场，并与协同工作的两个子网络保持颜色几何依赖性。<br>3)我们还提出了基于 SDF 体素网格的分层几何特征，以促进更大区域的信息共享，从而实现稳定的优化。<br>4)我们引入了几个有效的正则化项来提高平滑性和降低噪声。</p><p><strong>具体实施+更好的结果</strong><br>我们对 DTU (Jensen et al.， 2014)和 BlendedMVS (Yao et al.， 2020)数据集进行了实验，以进行定量和定性评估。实验结果表明，Voxurf 在 DTU (Jensen et al.， 2014)基准上实现了比竞争性全隐式方法 news (Wang et al.， 2021)更低的倒角距离，加速速度约为 20 倍。在 NVS 的辅助任务上也取得了显著的效果。如图 1 所示，与之前的方法相比，我们的方法在几何重建和图像渲染中都具有保留高频细节的优势。综上所述，我们的贡献如下:</p><ul><li>与 SOTA 方法相比，我们的方法使训练速度提高了约 20 倍，在单个 Nvidia A100 GPU 上将训练时间从 5 个多小时减少到 15 分钟。</li><li>我们的方法实现了更高的表面重建保真度和新颖的视图合成质量，与以前的方法相比，在表面恢复和图像渲染方面都具有更好的细节表现。</li><li>我们的研究为表面重建的显式体表示框架的架构设计提供了深刻的观察和分析。</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>Multi-view 3D reconstruction<br>最近，通过神经网络编码 3D 场景的几何形状和外观的隐式表示引起了人们的关注(Park 等人，2019;Chen &amp; Zhang, 2019;Lombardi 等人，2019;Mescheder 等人，2019;Sitzmann et al.， 2019b;Saito 等人，2019;Atzmon 等人，2019;Jiang et al.， 2020;Zhang et al.， 2021;Toussaint et al.， 2022)。<br>其中，大量的论文探讨了从多视图图像中重建神经表面。基于表面渲染的方法(Niemeyer et al.， 2020;Yariv 等，2020;刘等，2020b;Kellnhofer et al.， 2021)将光线与表面交点的颜色作为最终渲染的颜色，同时要求精确的对象蒙版和仔细的权重初始化。<br>最近的研究方法(Wang et al.， 2021;Yariv 等，2021;Oechsle 等人，2021;Darmon et al.， 2022;Zhang et al.， 2022;刘等，2020a;Sitzmann et al.， 2019a)基于体绘制(Max, 1995)在统一模型中制定了辐射场和隐式表面表示，从而实现了两种技术的优点。<br><strong>然而，在纯 MLP 网络中对整个场景进行编码需要很长的训练时间</strong>。与这些作品不同，我们利用可学习的体素网格和浅颜色网络进行快速收敛，并在表面和渲染图像中追求精细细节。</p><p>Explicit volumetric representation<br>尽管隐式神经表征在 3D 建模中取得了成功，但最近的进展已经集成了显式 3D 表征，例如点云、体素和 MPIs (Mildenhall 等人，2019)，并受到越来越多的关注(Wizadwongsa 等人，2021;Xu et al.， 2022;Lombardi 等人，2019;Wang et al.， 2022;Fang 等人，2022)。<br>Instant-ngp 使用多分辨率哈希进行高效编码，并实现了完全融合的 CUDA 内核。<br>Plenoxels (Yu et al.， 2022a)将场景表示为具有球面谐波的稀疏 3D 网格，其优化速度比 NeRF (Mildenhall et al.， 2020)快两个数量级。<br>TensoRF (Chen et al.， 2022)将全体积场视为 4D 张量，并将其分解为多个紧凑的低秩张量分量以提高效率。<br>与我们最相关的方法是 DVGO (Sun et al.， 2022a)，它采用混合架构设计，包括体素网格和浅 MLP。尽管它们在新视图合成方面取得了显著的成果，<strong>但它们都不是为了忠实地重建场景的几何形状而设计的</strong>。相比之下，我们的目标不仅是从新颖的视点绘制逼真的图像，而且还重建具有精细细节的高质量表面。</p><h2 id="PRELIMINARIES"><a href="#PRELIMINARIES" class="headerlink" title="PRELIMINARIES"></a>PRELIMINARIES</h2><p>Volume rendering with SDF representation: <strong>NeuS</strong><br>Eq1,2<br>$\hat{C}(r)=\sum_{i=1}^NT_i\alpha_ic_i,T_i=\prod_{j=1}^{i-1}(1-\alpha_j),$<br>$\alpha_i=\max\left(\frac{\Phi_s(f(p(t_i)))-\Phi_s(f(p(t_{i+1})))}{\Phi_s(f(p(t_i)))},0\right)$</p><p>Explicit volumetric representation: <strong>DVGO</strong><br>Eq3,4<br>$\sigma=\operatorname{interp}(p,V^{(density)}),$ 三线性插值<br>$c=\mathrm{MLP}_{\Theta}(\mathrm{interp}(p,V^{(feat)}),p,v),$</p><p>Naive 组合。这两种技术的直接结合是用基于 sdf 的体绘制方案取代 DVGO 中的体绘制，如 Eqn. 1 和 Eqn. 2 所示。在这项工作中，它作为 naive 基线，很难产生令人满意的结果，如图 1 (c)所示。在下一节中，我们将通过实证研究来阐明这一现象。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231202102731.png" alt="image.png|666"></p><h1 id="STUDY-ON-ARCHITECTURE-DESIGN-FOR-GEOMETRY-LEARNING"><a href="#STUDY-ON-ARCHITECTURE-DESIGN-FOR-GEOMETRY-LEARNING" class="headerlink" title="STUDY ON ARCHITECTURE DESIGN FOR GEOMETRY LEARNING"></a>STUDY ON ARCHITECTURE DESIGN FOR GEOMETRY LEARNING</h1><p>几何学习的结构设计研究</p><p>在本节中，我们对基线模型的变体进行了一些先前的实验，旨在找出本任务中架构设计的关键因素。具体来说，我们使用一个 SDF 体素网格 V (SDF)，并应用 Eqn. 2 进行α计算，手动定义 s 的调度。我们从一个浅 MLP 作为颜色网络开始，其中 1)从 $V^{(feat)}$ 插值的局部特征 f 和 2) $V^{(sdf)}$ 计算的法向量 n 都是可选的输入。良好的表面重建应具有<strong>连贯的粗结构</strong>、<strong>精确的精细细节</strong>和<strong>光滑的表面</strong>。接下来我们将关注这些因素，并分析不同架构设计的影响。</p><p>The key to maintaining a <strong>coherent coarse shape</strong>.<br>直观上，浅层 MLP 的容量是有限的，它很难表示具有不同材质、高频纹理和视依赖光照信息的复杂场景。当 ground truth 图像遇到快速的颜色移动时，在未拟合的颜色场上进行体绘制集成会导致几何结构损坏，如图 2 案例(1)(a)和(b)所示。结合局部特征 f 可以实现快速的颜色学习并提高网络的表示能力，并且问题得到明显缓解，如图 2 案例(1)所示，(a)和(c)， (b)和(d)之间的差异。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231202104332.png" alt="image.png|666"></p><p>The key to reconstructing <strong>accurate geometry details</strong>.<br>然后，我们在图 2 中引入另一种情况(2)。它的纹理变化适度，由于漫反射，颜色与表面法线主要相关。尽管在图 2 情形(2)(a)中，在没有法向 n 或特征 fas 输入的情况下，几何图形仍然会崩溃，但在图 2 情形(2)(b)中，即使只有 n 作为输入，我们也可以观察到一些几何细节的合理重构。加入特征 f 不会进一步减小倒角距离(CD);相反，由于可学习特征干扰了几何-颜色依赖关系，即颜色与表面法线之间建立的关系，如图 2 案例(2)所示，(b)与(d)之间的差异。考虑到可学习局部特征的优缺点，设计一个框架是有价值的，利用它来实现一致的形状，并<strong>保持颜色-几何依赖于精细的细节</strong>。</p><p><strong>作者观点</strong>：局部特征利于得到连贯的粗结构，但同时也会破坏颜色与表面法线之间的关系，导致精确的细节被破坏</p><p>The reason for <strong>noisy surfaces</strong>.<br>对于上述所有情况，结果都受到表面明显噪声的影响。与全局学习隐式表示相比，<strong>约束不足的体素网格缺乏空间一致性，容易受到局部极小值的影响</strong>，从而损害了表面的连续性和光滑性。一个直观的想法是利用来自一个区域而不是局部点的几何线索，这可以引入模型输入、网络组件和损失函数中</p><h1 id="METHODOLOGY"><a href="#METHODOLOGY" class="headerlink" title="METHODOLOGY"></a>METHODOLOGY</h1><p>受第 4 节中揭示的见解的启发，我们提出了几个关键设计:<br>1)我们采用两阶段训练程序，依次获得连贯的粗形状(第 5.1 节)和恢复精细细节(第 5.2 节);<br>2)提出了一种双颜色网络，以保持颜色几何依赖性，恢复精确表面和新视图图像;<br>3)设计分层几何特征，促进信息跨体素传播，实现稳定优化;<br>4)我们还引入了平滑先验，包括梯度平滑损失，以获得更好的视觉质量(第 5.3 节)。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231201171502.png" alt="image.png|666"></p><h2 id="COARSE-SHAPE-INITIALIZATION"><a href="#COARSE-SHAPE-INITIALIZATION" class="headerlink" title="COARSE SHAPE INITIALIZATION"></a>COARSE SHAPE INITIALIZATION</h2><p>我们初始化我们的 SDF 体素网格 $V^{(sdf)}$，在一个准备好的区域内使用椭球状零水平集进行重建，然后，我们在第 4 节中介绍的 $V^{(feat)}$ 的帮助下执行粗形状优化。具体来说，我们用法向量 n 和局部特征 f 作为输入，以及嵌入的位置和观看方向 V 来训练一个浅 MLP，为了促进稳定的训练过程和光滑的表面，<strong>我们建议在光滑的体素网格上进行插值</strong>，而不是 $V^{(sdf)}$ 的原始数据。</p><p> $\mathcal{G}(V,k_g,\sigma_g)$ 表示用高斯核对体素网格 V 进行三维卷积，权重矩阵遵循高斯分布：<br> $K_{i,j,k}~=~1/Z~\times \exp\left(-((\mathrm{i}-\lfloor\mathrm{k}_\mathrm{g}/2\rfloor)^2+(\mathrm{j}-\lfloor\mathrm{k}_\mathrm{g}/2\rfloor)^2+(\mathrm{k}-\lfloor\mathrm{k}_\mathrm{g}/2\rfloor)^2)/2\sigma_\mathrm{g}^2\right),\mathrm{i},\mathrm{j},\mathrm{k}\in\{0,1,…,\mathrm{k}_\mathrm{g}-1\},$<br> 其中 Z 表示归一化项，$k_g$ 表示核大小，$σ_g$ 表示标准差。因此，查询任意点 p 的光滑 SDF 值 $d ‘$ 变为: $d^{\prime}=\mathrm{interp}(p,\mathcal{G}(V^{(sdf)},k_{g},\sigma_{g})).$</p><p>我们在 following Eqn1,2使用 $d ‘$ 进行射线行进积分，并计算重建损失。我们还应用了几个平滑先验，如第 5.3 节所述</p><h2 id="FINE-GEOMETRY-OPTIMIZATION"><a href="#FINE-GEOMETRY-OPTIMIZATION" class="headerlink" title="FINE GEOMETRY OPTIMIZATION"></a>FINE GEOMETRY OPTIMIZATION</h2><p>在此阶段，我们的目标是在粗初始化的基础上恢复精确的几何细节。我们注意到挑战是双重的:<br>1) 第 4 节的研究揭示了特征体素网格引入的权衡，即以牺牲颜色几何依赖性为代价提高了颜色场的表示能力。<br>2) SDF 体素网格的优化是基于三线插值来查询三维点。该操作带来了快速收敛，但也限制了不同位置之间的信息共享，这可能导致退化解的局部最小值和次优平滑度。<strong>我们提出了一种双色网络和一种分层几何特征来分别解决这两个问题</strong>。</p><h3 id="Dual-color-network"><a href="#Dual-color-network" class="headerlink" title="Dual color network."></a>Dual color network.</h3><p>第4节的观察鼓励我们设计一个双色网络，利用从可学习的特征体素网格 $V^{(feat)}$ 插值的局部特征 $f_i^{feat}$，而不失去颜色几何依赖性。如图 3 所示，除了嵌入位置和视图方向外，我们还训练了两个具有不同附加输入的浅 mlp。第一个 MLPggeo 采用 geo 的分层几何特征(稍后将介绍)来构建颜色几何依赖;第二种方法同时采用简单的几何特征(即表面法线 ni)和局部特征 f I 作为输入，以实现更快、更精确的颜色学习，这反过来又有利于几何优化。这两个网络通过分离操作以残差方式组合:在输入 togf 之前分离 ggeo 的输出，表示为 c0，并将输出添加回 c0 的分离副本，以获得最终的颜色预测 c</p><p>第4节的观察结果鼓励我们设计一个双色网络，利用从可学习的特征体素网格 $V^{(feat)}$ 插值的局部特征 $f_i^{feat}$，而不失去颜色几何依赖性。如图3所示，除了嵌入位置和视图方向外，我们还训练了两个具有不同附加输入的浅 mlp。第一个 MLP $g_{geo}$ 采用分层几何特征 $f_i^{geo}$ (稍后将介绍)来构建 color-geometry 依赖关系;第二个 MLP $g_{feat}$ 同时使用一个简单的几何特征 (即表面法线 $n_i)$ 和局部特征 $f_i^{feat}$ 作为输入，以实现更快，更精确的颜色学习，从而有利于几何优化。这两个网络通过分离操作以残差方式组合在一起: $g_{geo}$ 的输出(表示为 $c_0$)在输入 $g_{feat}$ 之前被分离，然后将输出添加回 $c_{0}$ 的分离副本，以获得最终的颜色预测 $c$。</p><p>$g_{geo}$ 和 $g_{feat}$ 的输出都由地面真值图像和沿射线的集成颜色之间的重建损失来监督。具体来说，将它们的渲染颜色表示为 $C^0(r)$ 和 $C(r)$，整体重构损失表示为:<br>$\mathcal{L}_{recon}=\frac1{\mathcal{R}}\sum_{r\in\mathcal{R}}\left(||C(r)-\hat{C}(r)||_2^2+\lambda_0||C_0(r)-\hat{C}(r)||_2^2\right),$</p><p>式中 $\hat{C}(r)$ 表示真色，$\lambda_{0}$ 表示损失权值。$V^{(feat)}$ 和 MLP $g_{feat}$ 对场景的拟合速度较快，而 MLP $g_{geo}$ 对场景的拟合速度相对较慢。分离操作促进了 $g_{geo}$ 在自身重构损失的指导下的稳定优化，这有助于保持颜色几何依赖性。</p><h3 id="Hierarchical-geometry-feature"><a href="#Hierarchical-geometry-feature" class="headerlink" title="Hierarchical geometry feature."></a>Hierarchical geometry feature.</h3><p>分层几何特征。使用表面法线 $n$ 作为颜色网络的几何特征是一个简单的选择，而它只从相邻的 $V^{(SDF)}$ 的网格中获取信息。为了扩大感知区域并鼓励信息跨体素传播，我们建议查看更大的 SDF 场区域，并将相应的 SDF 值和梯度作为颜色网络的辅助条件。具体来说，对于给定的 3D 位置 $p=(x,y,z)$，我们取体素大小 $v_s$ 的一半作为步长，并沿着 $x,y,z$ 坐标轴两侧定义它的邻居。以 X 轴为例,邻近的坐标定义为 $p_x ^ {l -} = (x ^ {l -}, y, z)$ 和 $p_x ^ {l +} = (x ^ {l +}, y, z)$,在 $X ^ {l -} = \max(x-l <em> v_s, 0)$, $X ^ {l +} = \min (x + l </em> v_s ,v_x ^ m), l \in[0.5, 1.0, 1.5,…]$ 表示相邻区域的“水平”，$v_x^m$ 表示体素网格在 $x$ 轴上的最大值。然后，我们通过将不同级别的邻居连接在一起，将定义扩展为分层方式，如下所示:<br>$\begin{aligned}d_k^l=[d_k^{l-},d_k^{l+}]&amp;=[\text{interp}(p_k^{l-},V^{(sdf)}),\text{interp}(p_k^{l+},V^{(sdf)})],k\in\{x,y,z\},\\f_p^{sdf}(l)&amp;=[d^0,d_x^{0.5},d_y^{0.5},d_z^{0.5},\cdots,d_x^{l},d_y^{l},d_z^{l}]^T,\end{aligned}$</p><p>其中 $d_x^l$ 表示在 $p_x^{l-}$ 和 $p_x^{l+}$ 位置从 $V^{(SDF)}$ 查询到的 SDF 值。当 $l= 0$ 时，f $_p^{sdf}(0) = d^0$，这正是位置 $p$ 本身的 sdf 值。然后，我们还将梯度信息合并到几何特征中。具体来说，我们可以得到梯度向量 $\delta_x^l=(d_x^{l+}-d_x^{l-})/(2<em>l</em>v_s)$。我们将 $[\delta_x^l，\delta_y^l，\delta_z^l]$ 归一化为 l 的 l2 范数，表示为 $n^l\in\mathbb{R}^3。$ normal 的分层版本公式为: $\begin{aligned}f_p^{normal}(l)=[n^{0.5},\cdots,n^l].\end{aligned}$<br>最后，在点 p 处的分层几何特征为预定义的水平 $l\in[0.5,1.0,1.5，…]$ 是 $f_p^{geo}(l)=[f_p^{sdf}(l),f_p^{normal}(l)].$，将 $f_p^{geo}(l)$ 输入进 MLP $g_{geo}$ 中辅助几何学习</p><h2 id="SMOOTHNESS-PRIORS"><a href="#SMOOTHNESS-PRIORS" class="headerlink" title="SMOOTHNESS PRIORS"></a>SMOOTHNESS PRIORS</h2><p>我们结合了两个有效的正则化项来提高训练过程中的表面平滑度。<br>(1)首先，我们采用了总变分(TV)正则化(Rudin &amp; Osher, 1994):<br>$\mathcal{L}_{TV}(V)=\sum_{d\in[D]}\sqrt{\Delta_{x}^{2}(V,d)+\Delta_{y}^{2}(V,d)+\Delta_{z}^{2}(V,d)},$<br>其中 $\Delta_x^2(V,d)$ 表示 voxel $v:=(i;j;k)$ 和 voxel $(i+1;j;k)$ 中第 d 通道值之间的平方差，可以类似地扩展到 $\Delta_y^2(V,d)$ 和 $\Delta_z^2(V,d)$。我们将上面的 TV 项应用于 SDF 体素网格，表示为 $\mathcal{L}_{TV}(V^{(SDF)})$，这鼓励了连续和紧凑的几何结构。</p><p>(2)我们也假设表面在局部区域是光滑的，我们遵循第 5.1 节中高斯卷积的定义，引入光滑正则化，公式为:<br>$\mathcal{L}_{smooth}(V)=||\mathcal{G}(V,k_{g},\sigma_{g})-V||_{2}^{2},$<br>我们将上述平滑项应用于 SDF 体素网格的梯度，得到梯度平滑损失，记为 $\mathcal{L}_{smooth}(\nabla V^{(sdf)})$。它鼓励光滑的表面，减轻了自由空间中嘈杂点的问题。注意，由于 SDF 的显式表示，我们也可以在训练后自然地对 SDF 字段进行后处理。例如，在提取几何图形之前应用高斯核可以进一步提高表面的平滑度，从而获得更好的可视化效果。</p><p>最后，将整体训练损失表示为: $\mathcal{L}=\mathcal{L}_{recon}+\lambda_{tv}\mathcal{L}_{TV}(V^{(sdf)})+\lambda_{s}\mathcal{L}_{smooth}(\nabla V^{(sdf)}),$</p><h1 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h1><p>使用 DTU 数据集定性定量的比较，再 BlenderMVS 数据集上定性的比较<br>比较的方法：IDR、NeuS、NeRF、DVGO、Point-NeRF，为所有方法提供了一个干净的背景，以便公平比较。</p><h1 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h1><p>本文提出了一种基于体素的神经表面重构方法 Voxurf。它包括几个关键设计:两阶段框架获得连贯的粗形状，并依次恢复精细细节;双色网络有助于保持颜色几何依赖性，分层几何特征鼓励信息跨体素传播;有效平滑先验包括梯度平滑损失进一步提高视觉质量。大量的实验表明，Voxurf 同时实现了高效率和高质量。</p><h1 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h1><p>本机：只能使用 cuda11.1 版本</p><ul><li>Win10，torchvision 0.11.0+cu111 没有 windows 版本<ul><li>换 cu113 版本</li></ul></li><li>Wsl2，ubuntu 版本无法使用 cuda11.1</li></ul><p>作者没有做好多版本 torch 和 cuda 的适配</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Explicit Volumetric </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
            <tag> Voxel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VolSDF</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/VolSDF/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/VolSDF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Volume Rendering of Neural Implicit Surfaces</th></tr></thead><tbody><tr><td>Author</td><td>Lior Yariv, Jiatao Gu, Yoni Kasten, Yaron Lipman</td></tr><tr><td>Conf/Jour</td><td>NeurIPS 2021 Oral presentation</td></tr><tr><td>Year</td><td>2021</td></tr><tr><td>Project</td><td><a href="https://lioryariv.github.io/volsdf/">Volume Rendering of Neural Implicit Surfaces (lioryariv.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4545341065975717889&amp;noteId=2073471740411162624">Volume Rendering of Neural Implicit Surfaces (readpaper.com)</a></td></tr></tbody></table></div><p>将 NeRF 的密度场替换为 SDF，提出了详细的采样算法</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231201160653.png" alt="image.png|666"></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>神经体绘制最近越来越流行，因为它成功地从一组稀疏的输入图像合成了一个场景的新视图。到目前为止，通过神经体绘制技术学习的几何图形是使用通用密度函数建模的。此外，<strong>使用密度函数的任意水平集提取几何形状本身，导致有噪声</strong>，通常是低保真度的重建。本文的目标是改进神经体绘制中的几何表示和重建。我们通过将体积密度建模为几何形状的函数来实现这一点。这与之前将几何形状建模为体积密度函数的工作形成对比。更详细地说，<strong>我们将体积密度函数定义为应用于有符号距离函数(SDF)表示的拉普拉斯累积分布函数(CDF)</strong>。这种简单的密度表示有三个好处:<br>(i)它为在神经体绘制过程中学习到的几何图形提供了有用的归纳偏置<em>inductive bias</em>;<br>(ii)它有利于不透明度近似误差的限制，从而导致观察光线的精确采样。准确的采样是重要的，以提供几何和辐射的精确耦合;并且<br>(iii)它允许在体积渲染中有效地无监督地解除形状和外观的纠缠。将这种新的密度表示应用于具有挑战性的场景多视图数据集，产生高质量的几何重建，优于相关基线。此外，由于两者的解除纠缠，在场景之间切换形状和外观是可能的。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><strong>体渲染定义 &amp; 最新的神经体渲染优点+缺点</strong><br>体渲染[21]是通过所谓的体渲染积分在光场中渲染体积密度的一组技术。最近有研究表明，将密度和光场都表示为神经网络，可以通过仅从稀疏的输入图像集中学习，从而实现对新视图的出色预测。这种神经体绘制方法在[24]中提出，并由其后续研究[38,3]发展，以一种可微的方式将积分近似为 alpha-composition，允许同时从输入图像中学习。虽然这种耦合确实可以很好地概括新的观看方向，但密度部分在忠实地预测场景的实际几何形状方面并不成功，通常会产生嘈杂的、低保真的几何近似。</p><p><strong>本文 VolSDF 方法主要思想+优点</strong><br>我们提出 VolSDF 为神经体渲染中的密度设计一个不同的模型，从而在保持视图合成质量的同时更好地近似场景的几何形状。关键思想是将密度表示为到场景表面的带符号距离的函数，参见图 1。这样的密度函数有几个好处。首先，它保证存在一个定义良好的表面来产生密度。这为解纠缠密度和光场提供了有用的归纳偏置，从而提供了更精确的几何近似。其次，我们表明这种密度公式允许沿每条射线的不透明度的近似误差的边界。该边界用于对观察光线进行采样，以便在体渲染积分中提供密度和光场的忠实耦合。例如，如果没有这样的界限，沿射线(像素颜色)计算的亮度可能会遗漏或扩展表面部分，导致不正确的亮度近似。</p><p><strong>并行研究方向——基于表面渲染方法的缺点</strong><br>一个密切相关的研究，通常被称为神经隐式曲面[25,42,15]，一直专注于使用神经网络隐式地表示场景的几何形状，使表面渲染过程可微分。这些方法的主要缺点是它们需要遮罩将物体与背景分开。此外，学习直接渲染表面往往会由于优化问题而产生多余的部分，这可以通过体绘制来避免。从某种意义上说，我们的工作结合了两个世界的优点:体积渲染和神经隐式表面。</p><p><strong>具体实施+更好的结果</strong><br>我们通过重建 DTU[13]和 BlendedMVS[41]数据集的表面来证明 VolSDF 的有效性。与 NeRF[24]和 NeRF++[44]相比，VolSDF 产生了更精确的表面重建，与 IDR[42]相比，VolSDF 产生了更精确的重建，同时避免了使用对象掩模。此外，我们展示了用我们的方法解除纠缠的结果，即切换不同场景的密度和光场，这在基于 nerf 的模型中被证明是失败的。</p><h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p>Neural Scene Representation &amp; Rendering<br>传统上，三维场景建模采用隐式函数[27,12,5]。由于多层感知器(MLP)的表达能力强且占用内存少，最近的研究主要集中在基于 MLP 的模型隐式函数上，包括场景(几何和外观)表示[10,23,22,26,28,33,40,31,39]和自由视图渲染[37,18,34,29,19,24,17,44,38,3]。特别是，<strong>NeRF</strong>[24]开辟了一条研究路线(见[7]概述)，<strong>将神经隐式函数与体绘制结合起来，以实现逼真的渲染结果</strong>。<strong>然而，从预测密度中寻找合适的阈值提取曲面并非易事，并且恢复的几何形状远不能令人满意</strong>。此外，沿着光线对点进行采样以呈现像素是使用从另一个网络近似的不透明度函数完成的，而不保证正确的近似。</p><p>Multi-view 3D Reconstruction<br>在过去的几十年里，基于图像的三维表面重建(多视点立体)一直是一个长期存在的问题。经典的多视图立体方法通常是基于深度的[2,35,9,8]或基于体素的[6,4,36]。<br>例如，在 COLMAP [35]一种典型的基于深度的方法中，提取图像特征并在不同视图之间进行匹配以估计深度。然后对预测的深度图进行融合，得到密集的点云。为了获得曲面，采用了额外的网格划分步骤，例如泊松曲面重构[14]。然而，<strong>这些具有复杂管道的方法可能会在每个阶段积累错误，并且通常会导致不完整的 3D 模型，特别是对于非朗伯曲面，因为它们不能处理视图相关的颜色</strong>。相反，尽管它通过直接在一个体积中建模对象来生成完整的模型，但由于内存消耗高，基于体素的方法的分辨率很低。<br>最近，基于神经的方法如 DVR[25]、IDR[42]、NLR[15]也被提出用于从多视图图像中重建场景几何。然而，由于梯度难以传播，<strong>这些方法需要精确的对象掩码和适当的权值初始化</strong>。<br>与我们在这里的工作相独立，[30]UNISURF 也使用隐式表面表示合并到体绘制中。特别是，<strong>他们用占用网络代替了局部透明函数</strong>[22]。这允许在损失中添加表面平滑项，从而提高所得表面的质量。与他们的方法不同，我们使用带符号距离表示，用 Eikonal 损失进行正则化[42,11]，没有任何显式平滑项。此外，我们表明，使用带符号距离的选择允许不透明度近似误差的边界，促进对密度 suggested family 的体积渲染积分的近似。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>在本节中，<strong>我们将介绍一种新的体积密度参数化，定义为转换的有符号距离函数</strong>。然后我们将展示这个定义如何促进体渲染过程。特别是，我们推导了不透明度近似的误差范围，<strong>并因此设计了一个近似体渲染积分的采样程序</strong>。</p><h2 id="Density-as-transformed-SDF"><a href="#Density-as-transformed-SDF" class="headerlink" title="Density as transformed SDF"></a>Density as transformed SDF</h2><p>将密度转换为与 SDF 有关的函数：$\sigma(\boldsymbol{x})=\alpha\Psi_\beta\left(-d_\Omega(\boldsymbol{x})\right),$</p><ul><li>α、β &gt; 0 为可学习参数</li><li>$\Psi_{\boldsymbol{\beta}}$ 为零均值β标度拉普拉斯分布的累积分布函数(CDF)：$\Psi_\beta(s)=\begin{cases}\frac{1}{2}\exp\left(\frac{s}{\beta}\right)&amp;\text{if }s\leq0\\1-\frac{1}{2}\exp\left(-\frac{s}{\beta}\right)&amp;\text{if }s&gt;0\end{cases}$</li><li>SDF 值定义：$1_\Omega(\boldsymbol{x})=\begin{cases}1&amp;\mathrm{if~}\boldsymbol{x}\in\Omega\\0&amp;\mathrm{if~}\boldsymbol{x}\notin\Omega\end{cases},\quad\text{and} d_\Omega(\boldsymbol{x})=(-1)^{\mathbf{1}_\Omega(\boldsymbol{x})}\min_{\boldsymbol{y}\in\mathcal{M}}\left|x-\boldsymbol{y}\right|_2,$ $\mathcal{M}=\partial\Omega$ 为边界表面</li></ul><p><strong>这样定义可以看出</strong>：当 $\beta$ 接近0时，所有点的密度 $\sigma$ 趋近于 $\alpha 1_\Omega(\boldsymbol{x})$ </p><p>从直观上看，密度σ模拟了一个均匀的固体，密度α恒定，在固体边界附近平滑减小，其中平滑量由β控制。</p><h2 id="Volume-rendering-of-σ"><a href="#Volume-rendering-of-σ" class="headerlink" title="Volume rendering of σ"></a>Volume rendering of σ</h2><p>$I(\boldsymbol{c},\boldsymbol{v})=\int_0^\infty L(\boldsymbol{x}(t),\boldsymbol{n}(t),\boldsymbol{v})\tau(t)dt,$ 体渲染过程即沿着一条光线的期望像素</p><p>在 NeRF 提出的体渲染公式中，有两个重要的量：</p><ul><li>The volume’s opacity O ， or its transperancy T<ul><li>transperancy T 表示光粒子成功穿过 <code>[c, x(t)]</code> 段而没有碰撞的概率 $T(t)=\exp\left(-\int_0^t\sigma(\boldsymbol{x}(s))ds\right),$</li><li>opacity O 是补概率 $O(t)=1-T(t).$</li><li>这种情况可以将 O 看作 CDF ，则 PDF $\tau(t)=\frac{dO}{dt}(t)=\sigma(\boldsymbol{x}(t))T(t)$</li></ul></li><li>the light field L. 与点坐标，法向量以及观察方向有关<ul><li>增加光场对法线方向的依赖性是因为普通材料的 brdf 通常是相对于表面法线进行编码的，这有助于在表面渲染中进行解纠缠</li></ul></li></ul><p>体渲染公式离散形式：$I(c,v)\approx\hat{I}_{\mathcal{S}}(c,v)=\sum_{i=1}^{m-1}\hat{\tau}_iL_i,$</p><ul><li>S 为光线上的采样点集</li><li>$\hat{\tau}_i\approx\tau(s_i)\Delta s$ 使用采样点处的密度 + 区间长度来近似</li></ul><p><strong>采样很重要</strong>，PDF 集中在物体边界附近<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231201111115.png" alt="image.png|666"></p><h2 id="Bound-on-the-opacity-approximation-error"><a href="#Bound-on-the-opacity-approximation-error" class="headerlink" title="Bound on the opacity approximation error"></a>Bound on the opacity approximation error</h2><p>目的：通过控制一些超参数，使得透明的近似误差存在一个上界</p><p>给定一系列采样点，根据某个 $t\in(0,M],\text{assume }t\in[t_k,t_{k+1}],$ 得到：<br>$\int_0^t\sigma(x(s))ds=\widehat{R}(t)+E(t),\quad\mathrm{~where~}\widehat{R}(t)=\sum_{i=1}^{k-1}\delta_i\sigma_i+(t-t_k)\sigma_k$ ，其中 E(t)指近似的误差</p><p>则相应的不透明度函数的近似： $\widehat{O}(t)=1-\exp\left(-\widehat{R}(t)\right).$ ，目的是得到 <code>[0,M]</code> 上近似于 $\widehat{O}\approx O.$ 的 uniform bound</p><p>采样策略：给定一个 $\epsilon$</p><ul><li>引理 1：足够密集的采样点，保证误差上界 $B_{T,\epsilon} &lt; \epsilon$</li><li>引理 2：固定的采样点个数，如果要想得到的误差上界&lt; $\epsilon$，则需要满足 $\beta\geq\frac{\alpha M^2}{4(n-1)\log(1+\epsilon)}$ , M 为采样 t 的上界</li></ul><h2 id="Sampling-algorithm"><a href="#Sampling-algorithm" class="headerlink" title="Sampling algorithm"></a>Sampling algorithm</h2><blockquote><p><a href="https://longtimenohack.com/posts/nerf/nerf_on_surface/#%E4%BE%9D%E6%8D%AE-error-bound-%E7%9A%84%E9%87%87%E6%A0%B7%E7%AE%97%E6%B3%95">Survey: nerf + surface enhancements - Jianfei Guo (longtimenohack.com)</a></p></blockquote><ul><li>初始化采样：均匀采 $n=128$ 个点；然后根据引理 2 选取一个 $\beta_+$ (大于当前 $\beta)$ 保证 $\epsilon$ 被满足，迭代最大5次上采样点，直到采样足够充足使得网络现在的 $\beta$ 就能够满足上界 $&lt;\epsilon;$ (如果5次上采样以后还是不行，就选取最后一次送代的那个 $\beta_+$,而不是现在网络自己的 $\beta$<ul><li>为了能够减小 $\beta_+$,同时保证 $\epsilon$ 被满足，上采样添加 $n$ 个采样点<ul><li>每个区间采的点的个数和该区间当前 error bound 值成比例</li></ul></li><li>由均值定理，区间 $(\beta,\beta_+)$ 中一定存在一个 $\beta_{*}$ 刚好让 error bound 等于 $\epsilon$ <ul><li>所以用二分法(最大10个迭代)来找这样的一个 $\beta_{*}$ 来更新 $\beta_{+}$</li></ul></li></ul></li><li>用最后的上采样点和选取的 $\beta$ 来估计一个 $\hat{O}$ 序列</li><li>用估计出来的 $\hat{O}$ 序列来 inverse CDF 采样一组 ($m=64$) fresh new 的点来算 $\hat{O}$</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231201154656.png" alt="image.png|333"></p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p><strong>描述网络结构及如何训练</strong><br>可学习参数：$\theta=(\varphi,\psi,\beta).$<br>损失函数：$\mathcal{L}(\theta)=\mathcal{L}_{\mathrm{RGB}}(\theta)+\lambda\mathcal{L}_{\mathrm{SDF}}(\varphi)$</p><ul><li>$\mathcal{L}_{\mathrm{RGB}}(\theta)=\mathbb{E}_p\left|I_p-\hat{I}_{\mathcal{S}}(c_p,v_p)\right|_1$</li><li>$\mathcal{L}_{\mathrm{SDF}}(\varphi)=\mathbb{E}_{\boldsymbol{y}}\left(\left|\nabla d(\boldsymbol{y})\right|_2-1\right)^2$</li></ul><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>我们在具有挑战性的多视图三维表面重建任务中评估了我们的方法。我们使用两个数据集:DTU[13]和 BlendedMVS[41]，它们都包含从多个视图捕获的具有不同材料的真实物体。在第 4.1 节中，我们展示了 VolSDF 的定性和定量三维表面重建结果，与相关基线进行了比较。在 4.2 节中，我们证明，与 NeRF[24]相比，我们的模型能够成功地解开捕获物体的几何形状和外观。</p><h2 id="Multi-view-3D-reconstruction"><a href="#Multi-view-3D-reconstruction" class="headerlink" title="Multi-view 3D reconstruction"></a>Multi-view 3D reconstruction</h2><p>描述如何在某一数据集上与其他方法进行对比<br>数据集包含什么？数据集选取哪几个物体？其他方法如何设定参数？定性定量结果（图表）？所提出方法在哪方面更好？</p><p>也可以拎出来单独与 SOTA 比较一下</p><h2 id="Disentanglement-of-geometry-and-appearance"><a href="#Disentanglement-of-geometry-and-appearance" class="headerlink" title="Disentanglement of geometry and appearance"></a>Disentanglement of geometry and appearance</h2><p>两个版本的 NeRF 在这些场景中都不能产生正确的解纠缠，而 VolSDF 成功地切换了两个物体的材料。我们将此归因于使用方程 2 中的密度注入的特定 inductive 偏置。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231201160111.png" alt="image.png|666"></p><h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>我们介绍了 VolSDF，一个隐式神经表面的体绘制框架。我们将体积密度表示为学习到的表面几何的带符号距离函数的转换版本。这个看似简单的定义提供了一个有用的归纳偏置，允许几何(即密度)和光场的解纠缠，并且比以前的神经体渲染技术改进了几何近似。此外，它允许约束不透明度近似误差<em>opacity approximation error</em>，导致高保真采样的体积渲染积分。</p><p>未来有几个有趣的工作方向。</p><ul><li>首先，尽管在实践中工作得很好，<strong>但我们没有证明采样算法的正确性</strong>。我们相信提供这样的证明，或者找到一个有证明的算法版本将是一个有用的贡献。总的来说，我们相信在体绘制中使用边界可以改善学习和解纠缠，并推动该领域向前发展。</li><li>其次，<strong>我们目前的公式假设密度均匀</strong>;将其扩展到更一般的密度模型是一个有趣的未来工作方向。</li><li>第三，既然可以以无监督的方式学习高质量的几何形状，<strong>那么直接从图像集合中学习动态几何形状和形状空间将是很有趣的</strong>。</li><li>最后，尽管我们没有看到我们的工作对社会产生直接的负面影响，但我们确实注意到，从图像中精确重建几何形状可能被用于恶意目的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Plenoxels</title>
      <link href="/3DReconstruction/Multi-view/Explicit%20Volumetric/Plenoxels/"/>
      <url>/3DReconstruction/Multi-view/Explicit%20Volumetric/Plenoxels/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Plenoxels: Radiance Fields without Neural Networks</th></tr></thead><tbody><tr><td>Author</td><td>Sara Fridovich-Keil and Alex Yu and Matthew Tancik and Qinhong Chen and Benjamin Recht and Angjoo Kanazawa</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://github.com/sxyu/svox2">sxyu/svox2: Plenoxels: Radiance Fields without Neural Networks (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4567253676347891713&amp;noteId=2057865157560867840">Plenoxels: Radiance Fields without Neural Networks (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231120154626.png" alt="image.png|666"></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们介绍了 Plenoxels(plenoptic voxels)，一个用于真实感视图合成的系统。Plenoxels 将场景表示为带有球面谐波的稀疏3D 网格。这种表示可以通过梯度方法和正则化从校准的图像中优化，而不需要任何神经成分。在标准的基准任务中，Plenoxels 的优化速度比 Neural Radiance Fields 快两个数量级，而视觉质量没有损失。有关视频和代码，请参阅 <a href="https://alexyu.net/plenoxels">https://alexyu.net/plenoxels</a> 。</p><p><strong>我们的实验表明，神经辐射场的关键元素不是神经网络，而是可微体积渲染器</strong></p><p>Classical Volume Reconstruction<br>使用一个更比八叉树简单的稀疏数组结构。将这些基于网格的表示与某种形式的插值相结合，产生一个连续的表示，可以使用标准信号处理方法任意调整大小</p><p>Neural Volume Reconstruction<br>Neural Volumes[20]与我们的方法最相似，因为它使用带有插值的体素网格，但通过卷积神经网络优化该网格，并应用学习的扭曲函数来提高(1283 网格)的有效分辨率。我们证明了体素网格可以直接优化，并且可以通过修剪和粗到细的优化来实现高分辨率，而不需要任何神经网络或扭曲函数。</p><p>Accelerating NeRF.<br>我们的方法扩展了 PlenOctrees，对具有球面谐波的稀疏体素表示进行端到端优化，提供了更快的训练(与 NeRF 相比加速了两个数量级)。我们的 Plenoxel 模型是 PlenOctrees 的泛化，以支持任意分辨率(不需要 2 的幂)的稀疏全光学体素网格，并具有执行三线性插值的能力，使用这种稀疏体素结构更容易实现</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>我们的模型是一个稀疏体素网格，其中每个被占用的体素角存储一个标量不透明度σ和每个颜色通道的球谐系数向量。从这里开始，我们把这种表现称为 Plenoxel。在任意位置和观察方向上的不透明度和颜色是通过对相邻体素存储的值进行三线性插值，并在适当的观察方向上计算球面谐波来确定的。给定一组校准图像，我们直接使用训练光线的渲染损失来优化我们的模型。我们的模型如图 2 所示，并在下面进行详细描述。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231120154626.png" alt="image.png|666"></p><ul><li><strong>Volume Rendering</strong> 与 NeRF 相同</li><li><strong>Voxel Grid with Spherical Harmonics</strong> 与 PlenOctrees 类似<ul><li>我们不使用八叉树作为我们的数据结构。相反，我们将一个密集的 3D 索引数组与指针存储到一个单独的数据数组中，该数据数组仅包含已占用体素的值</li><li>与 pleenoctrees 类似，每个占用的体素为每个颜色通道存储一个标量不透明度σ和一个球谐系数向量。球面谐波形成了在球面上定义的函数的正交基，低次谐波编码平滑(更朗伯)的颜色变化，高次谐波编码更高频率(更镜面)的效果</li><li>样本 ci 的颜色简单地是每个颜色通道的这些谐波基函数的和，由相应的优化系数加权，并在适当的观看方向上进行评估。我们使用 2 degree 的球面谐波，每个颜色通道需要 9 个系数，每个体素总共需要 27 个谐波系数。我们使用 2 次谐波，因为 PlenOctrees 发现高次谐波只带来最小的好处。</li><li>我们的 Plenoxel 网格使用三线性插值来定义整个体积的连续全光函数。这与 PlenOctrees 相反，PlenOctrees 假设不透明度和球谐系数在每个体素内保持恒定。这种差异是成功优化容量的一个重要因素，我们将在下面讨论。所有系数(不透明度和球面谐波)都是直接优化的，没有任何特殊的初始化或神经网络的预训练</li></ul></li><li><strong>Interpolation</strong><ul><li>通过存储在最近的 8 个体素处的不透明度和谐波系数的三线性插值计算每条射线上每个样本点的不透明度和颜色。我们发现三线性插值明显优于最近邻插值</li><li>Plenoxel 在固定分辨率下缩小了最近邻和三线性插值之间的大部分差距，但由于优化不连续模型的困难，仍然存在一些差距。事实上，我们发现与最近邻插值相比，三线性插值在学习率变化方面更稳定</li></ul></li><li><strong>Coarse to Fine</strong><ul><li>我们通过一种从粗到细的策略来实现高分辨率，该策略从低分辨率的密集网格开始，优化修剪不必要的体素，通过在每个维度上将每个体素细分为两半来细化剩余的体素，并继续优化</li><li>由于三线性插值，天真的修剪会对表面附近的颜色和密度产生不利影响，因为这些点的值会与直接外部的体素进行插值。为了解决这个问题，我们执行一个扩张操作，这样一个体素只有在它自己和它的邻居都被认为是未被占用时才会被修剪。</li></ul></li><li><strong>Optimization</strong>：$\mathcal{L}=\mathcal{L}_{recon}+\lambda_{TV}\mathcal{L}_{TV}$ 根据渲染像素颜色的均方误差(MSE)优化了体素不透明度和球谐系数<ul><li>$\mathcal{L}_{recon}=\frac1{|\mathcal{R}|}\sum_{\mathbf{r}\in\mathcal{R}}|C(\mathbf{r})-\hat{C}(\mathbf{r})|_2^2$</li><li>total variation (TV) $\mathcal{L}_{TV}=\frac1{|\mathcal{V}|}\sum_{\underset{d\in[D]}{\operatorname*{v\in\mathcal{V}}}}\sqrt{\Delta_x^2(\mathbf{v},d)+\Delta_y^2(\mathbf{v},d)+\Delta_z^2(\mathbf{v},d)}$</li><li>为了更快的迭代，我们在每个优化步骤中使用 raysR 的随机样本来评估 MSE 项，并使用体素 V 的随机样本来评估 TV 项</li></ul></li><li><strong>Unbounded Scenes</strong><ul><li>对于前向场景，我们使用与原始 NeRF 论文[26]中定义的归一化设备坐标相同的稀疏体素网格结构。</li><li>对于 360 场景，我们用多球体图像(MSI)背景模型增强我们的稀疏体素网格前景表示，该模型还使用球体内部和球体之间的三线性插值来学习体素颜色和不透明度。注意，这实际上与我们的前景模型相同，除了使用简单的等矩形投影将体素扭曲成球体(体素指数超过球体角度θ和φ)。我们将 64 个球体线性地放置在从 1 到∞的逆半径上(我们预缩放内部场景以近似包含在单位球体中)。为了节省内存，我们只为颜色存储 rgb 通道(只有零阶 SH)，并通过使用不透明度阈值来稀疏存储所有图层，就像我们的主模型一样。这类似于 nerf++[57]中的背景模型。</li></ul></li><li><strong>Regularization</strong> 对于某些类型的场景，我们还使用额外的正则化器<ul><li>在真实的、正向的和360°的场景中，我们使用基于柯西损失的稀疏先验 $\mathcal{L}_s=\lambda_s\sum_{i,k}\log\left(1+2\sigma(\mathrm{r}_i(t_k))^2\right)$，类似于 PlenOctrees[56]中使用的稀疏度损失，并<strong>鼓励体素为空</strong>，这有助于节省内存并减少上采样时的质量损失</li><li>在真实的 360 度场景中，我们还对每个 minibatch 中每条光线的累积前景透射率使用 beta 分布正则化器。这个损失项，在神经体积[20]之后，通过鼓励前景完全不透明或空来<strong>促进清晰的前景-背景分解</strong>。这个损失是: $\mathcal{L}_{\beta}=\lambda_{\beta}\sum_{\mathbf{r}}\left(\log(T_{FG}(\mathbf{r}))+\log(1-T_{FG}(\mathbf{r}))\right)$</li></ul></li><li><strong>Implementation</strong><ul><li>由于稀疏体素体渲染在现代自动 diff 库中不受很好的支持，我们创建了一个自定义 PyTorch CUDA[29]扩展库来实现快速可微体渲染;我们希望从业者会发现这个实现在他们的应用程序中很有用。我们还提供了一个更慢、更高级的 JAX[4]实现。这两个实现都将向公众发布</li><li>我们实现的速度在很大程度上是可能的，因为我们的 Plenoxel 模型的梯度变得非常稀疏，非常快，如图 4 所示。在优化的前 1-2 分钟内，只有不到 10%的体素具有非零梯度。</li></ul></li></ul><h1 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h1><p>我们提出了一种用于逼真场景建模和新颖视点渲染的方法，该方法产生的结果具有与最新技术相当的保真度，同时减少了训练时间的数量级。我们的方法也非常简单，阐明了解决 3D 逆问题所需的核心要素:可微分正演模型，连续表示(在我们的情况下，通过三线性插值)和适当的正则化。我们承认，这种方法的成分已经存在了很长时间，但是具有数千万变量的非线性优化直到最近才被计算机视觉从业者所接受。</p><p>Limitations and Future Work<br>与任何未确定的逆问题一样，我们的方法容易受到人为因素的影响。我们的方法显示了与神经方法不同的伪影，如图 9 所示，但两种方法在标准度量方面都达到了相似的质量(如第 4 节所示)。未来的工作可能能够通过研究不同的正则化先验和/或更精确的物理可微分渲染函数来调整或减轻这些剩余的伪影。<br>尽管我们用一组固定的超参数报告了每个数据集的所有结果，但没有权重 $λ_{TV}$ 的最佳先验设置。在实践中，<strong>通过在逐场景的基础上调整该参数可以获得更好的结果</strong>，这是可能的，因为我们的训练时间很快。这是意料之中的，因为不同场景的训练视图的规模、平滑度和数量是不同的。我们注意到 NeRF 也有超参数需要设置，如位置编码的长度、学习率和层数，并且调整这些也可以在逐场景的基础上提高性能。<br>我们的方法应该自然地扩展到支持多尺度渲染，并通过体素 cone tracing 提供适当的抗锯齿，类似于 Mip-NeRF[2]中的修改。另一个简单的补充是色调映射，以说明白平衡和曝光的变化，我们希望这将有助于特别是在真实的 360 度场景。与我们的稀疏数组实现相比，分层数据结构(如八叉树)可以提供额外的加速，前提是保留了可微分插值。<br>由于我们的方法比 NeRF 快两个数量级，我们相信它可能会使下游应用程序目前受到 NeRF 性能的瓶颈，例如，跨大型场景数据库的多重反弹照明和 3D 生成模型。通过将我们的方法与其他组件(如相机优化和大规模体素哈希)相结合，它可能为端到端逼真的 3D 重建提供实用的管道。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Explicit Volumetric </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
            <tag> Voxel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Adaptive Shells</title>
      <link href="/3DReconstruction/Single-view/Implicit%20Function/Adaptive%20Shells/"/>
      <url>/3DReconstruction/Single-view/Implicit%20Function/Adaptive%20Shells/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Adaptive Shells for Efficient Neural Radiance Field Rendering</th></tr></thead><tbody><tr><td>Author</td><td>Zian Wang and Tianchang Shen and Merlin Nimier-David and Nicholas Sharp and Jun Gao and Alexander Keller and Sanja Fidler and Thomas M\”uller and Zan Gojcic</td></tr><tr><td>Conf/Jour</td><td>ACM Trans. On Graph. (SIGGRAPH Asia 2023)</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://research.nvidia.com/labs/toronto-ai/adaptive-shells/">Adaptive Shells for Efficient Neural Radiance Field Rendering (nvidia.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=2053478190589858048&amp;noteId=2053479143116332288">Adaptive Shells for Efficient Neural Radiance Field Rendering (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231117150239.png" alt="image.png|666"></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>神经辐射场在新视图合成中实现了前所未有的质量，但它们的体积公式仍然昂贵，需要大量的样本来渲染高分辨率图像。体积编码对于表示模糊几何(如树叶和毛发)是必不可少的，它们非常适合于随机优化。然而，许多场景最终主要由固体表面组成，这些表面可以通过每个像素的单个样本精确地渲染。基于这一见解，我们提出了一种神经辐射公式，可以在基于体积和基于表面的渲染之间平滑过渡，大大加快渲染速度，甚至提高视觉保真度。<br>我们的方法构建了一个明确的网格包络，该包络在空间上约束了神经体积表示。在固体区域，包络几乎收敛到一个表面，通常可以用单个样本渲染。为此，我们推广了 NeuS [Wang et al. 2021]公式，<strong>使用学习的空间变化核大小来编码密度的传播，将宽核拟合到类体积区域，将紧核拟合到类表面区域</strong>。然后，我们在表面周围提取一个窄带的显式网格，宽度由核尺寸决定，并微调该波段内的辐射场。在推断时，我们将光线投射到网格上，并仅在封闭区域内评估辐射场，大大减少了所需的样本数量。实验表明，我们的方法可以在非常高的保真度下实现高效的渲染。我们还演示了提取的包络支持下游应用程序，如动画和仿真。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>使用 InstantNGP 加速：nerf 已经可以渲染每条射线具有不同数量的样本的图像</p><ul><li>首先，基于网格的加速结构的内存占用随着分辨率的增加而减少</li><li>其次，mlp 的平滑感应偏置阻碍了学习体积密度的尖锐脉冲或阶跃函数，即使学习了这样的脉冲，也很难有效地对其进行采样</li><li>最后，由于缺乏约束，隐式体积密度场不能准确地表示下表面 underlying surfaces—— NeuS [Wang et al. 2021]，这往往限制了它们在依赖网格提取的下游任务中的应用</li></ul><p>为了弥补最后一点，[Wang et al. 2021, 2022a;Yariv 等人]提出优化带符号距离函数(SDF)以及编码密度分布的核大小，而不是直接优化密度。虽然这对于改善表面表示是有效的，但使用<strong>全局核大小</strong>与场景的不同区域需要自适应处理的观察相矛盾。</p><p>为了解决上述挑战，我们提出了一种新的体积神经辐射场表示方法。特别是:<br>I)我们概括了 NeuS[Wang et al .2021]的公式，该公式具有空间变化的核宽度，对于模糊表面收敛为宽核，而对于没有额外监督的固体不透明表面则坍缩为脉冲函数。在我们的实验中，<strong>仅这一改进就可以提高所有场景的渲染质量</strong>。<br>Ii)我们使用学习到的空间变化核宽度来提取表面周围窄带的网格包络。提取的包络的宽度可以适应场景的复杂性，并作为一种有效的辅助加速数据结构。<br>Iii)在推断时，我们将光线投射到包络层上，以便跳过空白区域，并仅在对渲染有重要贡献的区域对辐射场进行采样。在类表面区域，窄带可以从单个样本进行渲染，而对于模糊表面则可以进行更宽的核和局部体渲染。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>Neural Radiance Fields (NeRFs).</p><p>Implicit surface representation.<br>我们的方法是建立在 NeuS 公式之上，我们的主要目标不是提高提取表面的准确性。相反，我们利用 SDF 提取一个狭窄的外壳，使我们能够适应场景的局部复杂性，从而加速渲染</p><p>Accelerating neural volume rendering.<br>我们研究了一种加速(体积)渲染的替代方法，通过调整渲染每个像素所需的样本数量来适应场景的潜在局部复杂性。请注意，我们的公式是对“烘焙”方法的补充，我们认为两者的结合是未来研究的有趣途径。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>我们的方法(见图 3)建立在 NeRF 和 NeuS 的基础上。具体来说，我们概括了 NeuS 使用新的空间变化核(章节 3.2)，提高了质量并指导窄带壳的提取(章节 3.3)。然后，在 shell 内对神经表示进行微调(第 3.5 节)，从而显著加速渲染(第 3.4 节)。</p><h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><p>直观上，<strong>一个小的𝑠会得到一个具有模糊密度的宽核</strong>，而在限界 $\lim_{\mathfrak{s}\to0}d\Phi_\mathbf{s}/d\tau$ 中，则近似于一个尖锐的脉冲函数(见插图)。这种基于 sdf 的公式允许在训练期间使用 Eikonal 正则化器，这鼓励学习的𝑓成为实际的距离函数，从而产生更准确的表面重建。相关的损失将在第 3.5 节中讨论。</p><p>(NeuS 对 $\Phi_s(f)=(1+\exp(-f/s))^{-1},$ 中 s 的修改即核大小，修改密度变化 $\sigma=\max\left(-\frac{\frac{d\Phi_s}{d\tau}(f)}{\Phi_s(f)},0\right)$) <a href="https://www.desmos.com/calculator/owxqvpotdc">趋势</a><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231117160846.png" alt="image.png|333"></p><p>==后面的 s 都是核大小==</p><h2 id="Spatially-Varying-Kernel-Size"><a href="#Spatially-Varying-Kernel-Size" class="headerlink" title="Spatially-Varying Kernel Size"></a>Spatially-Varying Kernel Size</h2><p>NeuS SDF 公式是非常有效的，然而，它依赖于全局核大小。结合 Eikonal 正则化，这意味着在整个场景中体积密度的恒定分布。然而，<strong>一刀切的方法并不能很好地适应</strong>包含“尖锐”表面(如家具或汽车)和“模糊”体积区域(如头发或草)混合的场景。</p><p>我们的第一个贡献是用一个空间变化的、局部学习的核大小𝑠作为依赖于输入 3D 位置 x 的额外神经输出来增强 NeuS 公式。扩展的网络变成 $(\mathbf{c},f,s)=\mathrm{NN}_{\theta}(\mathbf{x},\mathbf{d})$ (参见第 4.1 节中的实现细节)。在训练过程中，我们还加入了一个正则化器来提高核大小 field 的平滑度(第 3.5 节)。该神经场仍然可以仅从彩色图像监督中进行拟合，并且由此产生的随空间变化的核大小会自动适应场景内容的清晰度(图 7)。这种增强的表示本身是有价值的，可以提高困难场景中的重建质量，但重要的是它将指导我们在 3.3 节中明确的 Shell 提取，从而大大加快渲染速度。</p><h2 id="Extracting-an-Explicit-Shell"><a href="#Extracting-an-Explicit-Shell" class="headerlink" title="Extracting an Explicit Shell"></a>Extracting an Explicit Shell</h2><p>自适应壳划分了对渲染外观有重要影响的空间区域，并由两个显式三角形网格表示。当𝑠较大时，外壳较厚，对应于体积场景内容，当𝑠较小时，外壳较薄，对应于表面。在隐式 field 𝑠和𝑓按照 3.2 节的描述进行拟合之后，我们作为后处理提取这个自适应 shell 一次。</p><p>在方程3中 $\Phi_s(f)=(1+\exp(-f/s))^{-1},$ $\sigma=\max\left(-\frac{\frac{d\Phi_s}{d\tau}(f)}{\Phi_s(f)},0\right)$<br>S 形指数中数量𝑓/𝑠的大小决定了沿着一条射线的渲染贡献(参见第 3.1 节的插图)。简单地提取|𝑓/𝑠| &lt;𝜂(对于某些𝜂)作为对呈现有重要贡献的区域是很有诱惑力的。然而，学习到的函数很快就会在远离𝑓= 0 水平集的地方变得有噪声，并且在不破坏精细细节的情况下无法充分正则化。我们的解决方案是分别提取内部边界作为𝑓= 0 水平集的侵蚀，并将外部边界作为其膨胀(图 4)，两者都通过针对任务定制的正则化约束水平集进化来实现</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231117160902.png" alt="image.png|666"></p><p>详细地说，我们首先对规则网格顶点处的 field 𝑓和𝑠进行采样。然后，我们将水平集进化应用于𝑓，产生新的侵蚀场 SDF-，并通过 marching cubes 提取 SDF- = 0 水平集作为内壳边界。一个独立的、类似的演化产生了膨胀场 SDF+，而 SDF+ = 0 的能级集形成了外壳边界。<br>我们分别定义这两个 level 集:</p><ul><li>膨胀的外表面应该是光滑的，以避免可见的边界伪影，</li><li>而侵蚀的内表面不需要光滑，但必须只排除那些肯定对渲染外观没有贡献的区域。</li></ul><p>Recall field 𝑎的基本水平集演化由 $\partial a/\partial t=-\left|\nabla a\right|v$ 给定，其中𝑣是水平集的所需标量向外法向速度。我们在𝑓上的限制的正则化流是：$\frac{\partial f}{\partial t}=-|\nabla f|\left(v(f_{0},s)+\lambda_{\mathrm{curv}}\nabla\cdot\frac{\nabla f}{|\nabla f|}\right)\omega(f),$<br>其中 $𝑓_{0}$ 表示初始学习的 SDF，散度项是一个权值为 $𝜆_{curv}$ 的曲率平滑正则化器。软衰减𝜔(见插图)将流量限制在水平集周围的窗口:<br>窗口宽度𝜁。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231117163131.png" alt="image.png|333"></p><p>为了 dilate 水平集，对于法线方向入射的射线，<strong>选择速率 v</strong>用密度为 $\sigma&gt;\sigma_{\min}$ 填充所有区域</p><p>$v_\text{dilate}(f_0,s)=\begin{cases}\beta_d\sigma(f_0,s)&amp;\sigma(f_0,s)&gt;\sigma_{\min}\\0&amp;\sigma(f_0,s)\leq\sigma_{\min}\end{cases},$</p><p>$\beta_{d}$ 是 scaling coefficient。我们使用𝜁= 0.1，$𝜆_{curv} = 0.01$。</p><p>为了侵蚀水平集，速度与密度成反比，因此在低密度区域，壳向内膨胀得快，而在高密度区域，壳向内膨胀得慢</p><p>$v_{\mathrm{erode}}(f_0,s)=\min{(v_{\mathrm{max}},\beta_e}\frac{1}{\sigma(f_0,s)}),$</p><p>这里我们使用𝜁= 0.05，$𝜆_{curv}$ = 0。These velocities 导致了短距离的流动，因此形成了一个狭窄的壳，其中𝑠很小，内容物呈表面状。它们导致长距离流动，因此形成一个宽壳，其中𝑠很大，内容物呈体积状。</p><p>我们通过前向欧拉积分法在网格上对该流进行50步积分，通过空间有限差分计算导数，分别计算膨胀场 SDF+和侵蚀场 SDF-。我们认为没有必要进行数值上的距离调整。最后，我们将结果 $SDF−←max(𝑓_{0},SDF−)$ 和 $SDF+←min(𝑓_0,SDF+)$ 夹紧，以确保侵蚀场只缩小水平集，而膨胀流只增大水平集。SDF+ = 0 和 SDF−= 0 的水平集通过 MC 分别作为外壳外边界网格 M+和内壳边界网格 M−提取。图 5 显示了结果字段。进一步详情载于附录的程序1及2。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231120093710.png" alt="image.png|666"></p><h2 id="Narrow-Band-Rendering"><a href="#Narrow-Band-Rendering" class="headerlink" title="Narrow-Band Rendering"></a>Narrow-Band Rendering</h2><p>提取的自适应壳作为辅助加速数据结构，引导点沿着射线采样(公式 2)，使我们能够有效地跳过空白空间，只在需要高感知质量的地方采样点。对于每条射线，我们使用硬件加速射线跟踪来有效地枚举由射线和自适应壳的交点定义的有序间隔。在每个间隔内，我们查询等间隔的样本。我们的渲染器不需要任何动态自适应采样或依赖于样本的终止标准，这有利于高性能并行评估。</p><p>详细地说，我们首先为外部网格 M+和内部网格 M-<br>构建光线跟踪加速数据结构，然后将每条光线投射到网格上，产生一系列光线进入或退出网格的交叉点，将光线划分为零或更多包含在外壳中的间隔(见插图)。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231120094106.png" alt="image.png|222"></p><p>采样数量计算方法：$ceil(max(w-w_s,0)/{\delta_{s}})+1$，将最大采样数量限制为 $N_{max}$，并等距离采样<br>这个过程可以通过首先生成所有间隔内的所有样本，然后执行单个批处理 MLP 推理传递来实现，这可以提高吞吐量</p><h2 id="Losses-and-Training"><a href="#Losses-and-Training" class="headerlink" title="Losses and Training"></a>Losses and Training</h2><p>在第一阶段<br>$\mathcal{L}=\mathcal{L}_{\mathbf{c}}+\lambda_{e}\mathcal{L}_{e}+\lambda_{s}\mathcal{L}_{s}+\lambda_{\mathbf{n}}\mathcal{L}_{\mathbf{n}}$</p><ul><li>所有实验的权值分别为𝜆c = 1， 𝜆𝑒= 0.1，𝜆n = 0.1，𝜆𝑠= 0.01。</li><li>$\mathcal{L_{c}}$ 是对校准的真地图像的标准逐像素色彩损失：$\mathcal{L}_{\mathrm{c}}=\frac{1}{|\mathcal{R}|}\sum_{\mathrm{r}\in\mathcal{R}}|\mathbf{c}(\mathbf{r})-\mathbf{c}_{\mathrm{gt}}(\mathbf{r})|$</li><li>$\mathcal{L_{e}}$ 是 Eikonal 正则器 $\mathcal{L}_{e}=\frac{1}{|X|}\sum_{\mathbf{x}\in\mathcal{X}}\left(||\nabla f(\mathbf{x})||_{2}-1\right)^{2},$ 本文使用有限差分得到</li><li>损失 $\mathcal{L_{s}}$ 正则化在我们的公式中引入的空间变化的核大小以实现平滑: $\mathcal{L}_{s}=\frac{1}{\mathcal{X}}\sum_{\mathbf{x}\in\mathcal{X}}|||\log\big[s(\mathbf{x})\big]-\log\big[s(\mathbf{x}+\mathcal{N}(0,\varepsilon^{2}))\big]||_{2},$ 式中 $\mathcal{N}(0,\varepsilon^{2})$ 为标准差为 $\varepsilon$ 的正态分布样本。最后，我们把损失算进去</li><li>$\mathcal{L}_{\mathbf{n}}=\frac{1}{|\mathcal{X}|}\big|\mathbf{n}(\mathbf{x})-\frac{\nabla f(\mathbf{x})}{||\nabla f(\mathbf{x})||_{2}}\big|_{2},$ 与 NeuS 一样，我们将利用几何法线作为着色子网络的输入，但我们发现使用网络预测这些法线可以提高推理性能，而不是梯度评估</li></ul><p>在隐式域拟合之后，我们提取自适应壳，如第3.3节所示。虽然最初的训练需要沿着射线密集采样，但我们的显式外壳现在允许窄带渲染只在重要区域集中样本。因此，我们在窄带内微调表示，现在只有 $\mathcal{L_{c}}$ -不再需要鼓励几何上很好的表示，因为我们已经提取了外壳并将采样限制在场景内容周围的小带内。禁用正则化使网络能够将其全部容量用于拟合视觉外观，从而提高视觉保真度(表 4)。在附录的过程 4 中，我们还提供了带有算法细节的训练管道。</p><ul><li>第二阶段只需要优化 $\mathcal{L_{c}}$ </li></ul><h1 id="APPLICATIONS"><a href="#APPLICATIONS" class="headerlink" title="APPLICATIONS"></a>APPLICATIONS</h1><ul><li>Cage-based deformation methods 动态物体（变形网格）</li><li>physical simulation and animation</li></ul><h1 id="DISCUSSION"><a href="#DISCUSSION" class="headerlink" title="DISCUSSION"></a>DISCUSSION</h1><p>最近的工作开发了加速和提高 nerf 类场景表示质量的方案。第 4 节提供了对选定的，特别是相关方法的比较。请注意，由于该领域的高研究活动，不可能对所有技术进行比较，并且许多方法的实现是不可用的。因此，我们对一些相关工作提出补充意见:</p><ul><li>MobileNeRF、nerfmesh 和 nerf2mesh 后处理类似 nerf 的模型并提取网格以加速推理，类似于这项工作。然而，这些方法将外观限制在表面，牺牲了质量。相反，我们的方法保留了完整的体积表示和几乎完全的 nerf 质量，代价是稍微昂贵一些的推理(尽管在现代硬件上仍然是实时的)。</li><li>DuplexRF 也从底层神经领域提取了一个显式外壳，并使用它来加速渲染，尽管它使用了非常不同的神经表示，优先考虑性能。它们的壳直接从辐射场的两个阈值中提取，这需要仔细选择阈值，并导致与我们的方法相反的噪声壳不适应场景的局部复杂性。</li><li>VMesh 基于类似的见解，即场景的不同部分需要不同的处理。然而，它们的公式假设了一个额外的体素网格数据结构来标记有助于最终渲染的体积区域。与InstantNGP的辅助加速数据结构一样，这种方法的复杂性缩放能力较差。相反，我们的方法使用显式的自适应shell来划分有助于呈现的区域。除了较低的复杂性外，我们的配方无缝地支持第5节APPLICATIONS中讨论的进一步应用</li></ul><h1 id="CONCLUSION-AND-FUTURE-WORK"><a href="#CONCLUSION-AND-FUTURE-WORK" class="headerlink" title="CONCLUSION AND FUTURE WORK"></a>CONCLUSION AND FUTURE WORK</h1><p>在这项工作中，我们专注于有效地渲染nerf。我们的第一阶段训练(第3.2节)在很大程度上类似于NNeuralangelo，并且可能通过算法进步和类似于我们的推理管道的低级调优来加速NeuS。<br>尽管我们的方法为高保真神经渲染提供了很大的加速，并在现代硬件上以实时速率运行(表1)，但它仍然比MeRF等方法昂贵得多，其预先计算神经场输出，并将其烘烤成离散的网格表示。我们的公式与MeRF的公式是互补的，我们假设结合这两种方法将导致进一步的加速，潜在地达到性能-高质量的方法，将体积表示烘烤到显式网格，甚至可以在商品硬件上实时运行(例如MobileNeRF)。<br>我们的方法<strong>不能保证捕获薄结构</strong>——如果提取的自适应壳忽略了一个几何区域，它将永远无法在微调期间恢复，并且将始终缺席重建。这种形式的工件在一些mipnerf360场景中是可见的。未来的工作将探索一个迭代过程，在这个过程中，我们交替调整我们的重建和适应外壳，以确保没有重要的几何形状丢失。在我们的重建中偶尔出现的其他人工制品包括虚假的浮动几何形状和低分辨率背景;这两者都是神经重建中常见的挑战，我们的方法可以借鉴该领域其他工作的解决方案(例如RegNeRF)。<br>更广泛地说，将最近的神经表示与计算机图形学中实时性能的高性能技术结合起来有很大的潜力。在这里，我们展示了如何使用光线跟踪和自适应shell来极大地提高性能</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Implicit Function </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DMV3D</title>
      <link href="/3DReconstruction/Multi-view/Hybrid%20Methods/DMV3D/"/>
      <url>/3DReconstruction/Multi-view/Hybrid%20Methods/DMV3D/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model</th></tr></thead><tbody><tr><td>Author</td><td>Xu, Yinghao and Tan, Hao and Luan, Fujun and Bi, Sai and Wang Peng and Li, Jihao and Shi, Zifan and Sunkavalli, Kaylan and Wetzstein Gordon and Xu, Zexiang and Zhang Kai}</td></tr><tr><td>Conf/Jour</td><td>arxiv</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://justimyhxu.github.io/projects/dmv3d/">DMV3D: Denoising Multi-View Diffusion Using 3D Large Reconstruction Mode (justimyhxu.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=2051819500442962176&amp;noteId=2052104116852710656">DMV3D: DENOISING MULTI-VIEW DIFFUSION USING 3D LARGE RECONSTRUCTION MODEL (readpaper.com)</a></td></tr></tbody></table></div><p><strong>需要相机位姿</strong> + 多视图 + Diffusion Model + NeRF Triplane2MLP</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231116161909.png" alt="image.png|666"></p><p>不足：</p><ul><li>对未见视图的重建质量不高</li><li>只支持低分辨率图像和三平面</li><li>只支持输入没有背景的物体图像</li><li>没用到任何先验知识</li></ul><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们提出了一种新的三维生成方法 DMV3D，它使用基于 transformer 的三维大重建模型来去噪多视图扩散。我们的重建模型采用三平面 NeRF 表示，可以通过 NeRF 重建和渲染去噪多视图图像，在单个 A100 GPU 上实现 30 秒内的单阶段 3D 生成。我们<strong>只使用图像重建损失</strong>，而不访问 3D 资产，在<strong>高度多样化对象的大规模多视图图像数据集</strong>上训练 DMV3D。我们展示了最先进的单图像重建问题的结果，其中需要对未见物体部分进行概率建模，以生成具有尖锐纹理的多种重建。我们还展示了高质量的文本到 3D 生成结果，优于以前的 3D 扩散模型。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>我们现在展示我们的单阶段 3D 扩散模型。特别是，我们引入了一种新的扩散框架，该框架使用基于重建的去噪器来去噪 3D 生成的多视图图像(第 3.1 节)。在此基础上，我们提出了一种新的基于 lrm 的(Hong et al.， 2023)基于扩散时间步长的多视点去噪器，通过 3D NeRF 重建和渲染逐步去噪多视点图像(第 3.2 节)。我们进一步扩展我们的模型以支持文本和图像调节，从而实现可控生成(第 3.3 节)。</p><h2 id="MULTI-VIEW-DIFFUSION-AND-DENOISING"><a href="#MULTI-VIEW-DIFFUSION-AND-DENOISING" class="headerlink" title="MULTI-VIEW DIFFUSION AND DENOISING"></a>MULTI-VIEW DIFFUSION AND DENOISING</h2><p>Diffusion. Denoising Diffusion Probabilistic Models (DDPM)在正向扩散过程中使用高斯噪声调度对数据分布 $x_{0} \sim q(x)$ 进行变换。生成过程是图像逐渐去噪的反向过程。</p><p>Multi-view diffusion.</p><p>Reconstruction-based denoising.<br>$\mathbf{I}_{r,t}=\mathrm{R}(\mathrm{S}_t,\boldsymbol{c}),\quad\mathrm{S}_t=\mathrm{E}(\mathcal{I}_t,t,\mathcal{C})$</p><ul><li>使用重建模块 E(·) 从有噪声的多视图图像中重建三维表示 S<ul><li>基于 lrm 的重构器 E(·)</li></ul></li><li>使用可微渲染模块 R(·)渲染去噪图像</li></ul><p>仅在输入视点监督 $\mathcal{I}_0$ 预测并不能保证高质量的3D 生成, 也监督来自3D 模型 $S_{t}$ 的新颖视图渲染<br>$\mathrm{L}_{recon}(t)=\mathbb{E}_{\mathbf{I},\boldsymbol{c}\sim\mathcal{I}_{full},\mathcal{C}_{full}}\ell\big(\mathbf{I},\mathrm{R}(\mathrm{E}(\mathcal{I}_t,t,\mathcal{C}),\boldsymbol{c})\big)$ , $\mathcal{I}_{full}\mathrm{~and~}\mathcal{C}_{full}$ 表示图像和姿态的完整集合(来自随机选择的输入和新视图)</p><h2 id="RECONSTRUCTOR-BASED-MULTI-VIEW-DENOISER"><a href="#RECONSTRUCTOR-BASED-MULTI-VIEW-DENOISER" class="headerlink" title="RECONSTRUCTOR-BASED MULTI-VIEW DENOISER"></a>RECONSTRUCTOR-BASED MULTI-VIEW DENOISER</h2><p>我们在 LRM (Hong et al.， 2023)上构建了我们的多视图去噪器，并使用大型 transformer 模型从嘈杂的稀疏视图图像中重建干净的三平面 NeRF (Chan et al.， 2022)。然后将重建的三平面 NeRF 的渲染图用作去噪输出。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231116161909.png" alt="image.png|666"></p><p>Reconstruction and Rendering.</p><p>与 LRM 工作一样，transformer 变压器模型由一系列三平面到图像的交叉注意层和三平面到三平面的自注意层组成</p><p>Time Conditioning.</p><p>与基于 cnn 的 DDPM 相比，我们基于变压器的模型<strong>需要不同的时间调节设计</strong>(Ho et al.， 2020)。受 DiT (Peebles &amp; Xie, 2022)的启发，我们通过将 adaLN-Zero 块(Ho et al.， 2020)注入模型的自注意层和交叉注意层来对时间进行调节，以有效处理不同噪声水平的输入。</p><p>Camera Conditioning.</p><p><em>在具有高度多样化的相机特性和外部特性的数据集上训练我们的模型，例如 MVImgNet (Yu et al.， 2023)，需要有效地设计输入相机调节，以促进模型对用于 3D 推理的相机的理解</em> 。<br>一种基本策略是，在时间调节的情况下，对相机参数使用 adaLN-Zero 块(Peebles &amp; Xie, 2022)(如 Hong 等人(2023)所做的那样);Li et al.(2023)。<strong>然而，我们发现，同时使用相同的策略对相机和时间进行调节往往会削弱这两个条件的效果，并且往往导致训练过程不稳定和收敛缓慢</strong>。<br>相反，我们提出了一种新的方法——用像素对齐的射线集参数化相机。特别是继 Sitzmann et al. (2021);Chen et al. (2023a)，我们使用 Plucker 坐标 r = (o × d, d)参数化射线，其中 o 和 d 是由相机参数计算的像素射线的原点和方向，x 表示叉积。我们将 Plucker 坐标与图像像素连接，并将其发送到 ViT 转换器进行二维图像标记化，实现有效的相机调理</p><ul><li><a href="https://banbao991.github.io/2021/10/07/Math/Pl%C3%BCcker-Coordinates/">普吕克坐标(Plücker Coordinates) 简介 | Banbao (banbao991.github.io)</a></li></ul><h2 id="CONDITIONING-ON-SINGLE-IMAGE-OR-TEXT"><a href="#CONDITIONING-ON-SINGLE-IMAGE-OR-TEXT" class="headerlink" title="CONDITIONING ON SINGLE IMAGE OR TEXT"></a>CONDITIONING ON SINGLE IMAGE OR TEXT</h2><p>迄今为止所描述的方法使我们的模型能够作为无条件生成模型运行。我们现在介绍如何用条件去噪器 $\operatorname{E}(\mathcal{I}_t,t,\mathcal{C},y),$ 对条件概率分布建模，其中 y 是文本或图像，从而实现可控的 3D 生成。</p><p>Image Conditioning.<br>不改变模型结构，我们保持第一个视图 I1(在去噪器输入中)无噪声作为条件图像，同时对其他视图应用扩散和去噪。在这种情况下，去噪器本质上是学习使用从第一个输入视图中提取的线索来填充嘈杂的未见视图中缺失的像素，类似于 2D dm 可寻址的图像绘制任务(Rombach et al.， 2022a)。此外，为了提高我们的图像条件模型的可泛化性，我们在与条件视图对齐的坐标框架中生成三平面，并使用相对于条件视图的姿态渲染其他图像。我们在训练过程中以与 LRM (Hong et al.， 2023)相同的方式规范化输入视图的姿态，并在推理过程中以相同的方式指定输入视图的姿态</p><p>Text Conditioning.<br>CLIP 文本编码器</p><h2 id="TRAINING-AND-INFERENCE"><a href="#TRAINING-AND-INFERENCE" class="headerlink" title="TRAINING AND INFERENCE"></a>TRAINING AND INFERENCE</h2><p>Training：<br>$\mathrm{L}=\mathbb{E}_{t\sim U[1,T],(\mathbf{I},\boldsymbol{c})\sim(\mathcal{I}_{full},\mathcal{C}_{fuu)}}\ell\big(\mathbf{I},\mathrm{R}(\mathrm{E}(\mathcal{I}_t,t,\mathcal{D},y),c)\big)$</p><p>Inference：</p><p>为了进行推断，我们选择了四个视点，它们均匀地围绕在一个圆圈中，以确保生成的 3D 资产的良好覆盖。我们将相机的视角设置为 50 度，用于四个视图。由于我们预测三平面 NeRF 与条件反射图像的相机帧对齐，因此我们也将条件反射图像的相机外饰件固定为具有相同的方向和(0，−2,0)位置，遵循 LRM 的实践(Hong et al.， 2023)。我们从最后的去噪步骤输出三平面 NeRF 作为生成的 3D 模型。我们利用 DDIM (Song et al.， 2020a)算法来提高推理速度。</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了一种新的单阶段扩散模型，该模型通过去噪多视图图像扩散来生成 3D 资产。我们的多视图去噪器基于大型 transformer模型(Hong et al.， 2023)，该模型采用带噪的多视图图像来重建干净的三平面 NeRF，并通过体渲染输出去噪图像。我们的框架支持文本和图像调节输入，通过直接扩散推理实现快速3D 生成，而无需进行资产优化。我们的方法在文本到3D 生成方面优于以前的3D 扩散模型，并在各种测试数据集上实现了最先进的单视图重建质量。</p><p><strong>Limitations</strong><br>尽管我们在这项工作中展示了高质量的图像或文本条件下的3D生成结果，但未来的工作仍有一些限制可以探索:<br>1)首先，<strong>我们为物体未见部分生成的纹理似乎缺乏高频细节</strong>，颜色略有褪色。这将是有趣的进一步提高纹理保真度;<br>2)我们的输入图像和三平面目前是<strong>低分辨率</strong>的。将我们的方法扩展到从高分辨率输入图像生成高分辨率NeRF也是非常可取的;<br>3)<strong>我们的方法只支持输入没有背景的物体图像</strong>;直接生成具有3D背景的对象NeRF (Zhang et al.， 2020;Barron et al.， 2022)在许多应用中也非常有价值;<br>4)我们的图像和文本条件模型都是从头开始训练的，而不需要利用2D基础模型(如Stable diffusion)中的强图像先验(Rombach et al.， 2022b)。考虑如何<strong>在我们的框架中利用这些强大的2D图像先验</strong>可能会有所帮助</p><p>道德声明。我们的生成模型是在Objaverse数据和MVImgNet数据上训练的。该数据集(1M左右)小于训练2D扩散模型的数据集(100M ~ 1000M左右)。数据的缺乏会引起两方面的考虑。首先，它可能会偏向训练数据的分布。其次，它可能不够强大，无法涵盖测试图像和测试文本的所有巨大多样性。我们的模型具有一定的泛化能力，但可能不能像二维扩散模型那样覆盖那么多的模式。鉴于我们的模型不具备识别超出其知识范围的内容的能力，它可能会引入不令人满意的用户体验。此外，如果文本提示或图像输入与某些数据样本高度一致，我们的模型可能会泄漏训练数据。这种潜在的泄漏引起了法律和安全方面的考虑，并在所有生成模型(如LLM和2D扩散模型)中共享。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Hybrid Methods </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> 3DReconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RayDF</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/RayDF/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/RayDF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency</th></tr></thead><tbody><tr><td>Author</td><td>Zhuoman Liu, Bo Yang</td></tr><tr><td>Conf/Jour</td><td>NeurIPS</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://vlar-group.github.io/RayDF.html">RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency (vlar-group.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=2037229054391691776&amp;noteId=2047746094923644416">RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231113155552.png" alt="image.png|666"></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文研究了连续三维形状表示问题。现有的成功方法大多是基于坐标的隐式神经表示。然而，<strong>它们在呈现新视图或恢复显式表面点方面效率低下</strong>。一些研究开始将三维形状表述为基于光线的神经函数，但由于缺乏多视图几何一致性，学习到的结构较差。<br>为了应对这些挑战，我们提出了一个名为 RayDF 的新框架。它包括三个主要部分:<br>1)简单的射线-表面距离场，<br>2)新颖的双射线可见性分类器，<br>3)多视图一致性优化模块，以驱动学习的射线-表面距离在多视图几何上一致。<br>我们在三个公共数据集上广泛评估了我们的方法，证明了在合成和具有挑战性的现实世界3D 场景中3D 表面点重建的显着性能，明显优于现有的基于坐标和基于光线的基线。最值得注意的是，我们的方法在渲染800 × 800深度的图像时，速度比基于坐标的方法快1000倍，显示了我们的方法在3D 形状表示方面的优势。我们的代码和数据可在 <a href="https://github.com/vLAR-group/RayDF上获得">https://github.com/vLAR-group/RayDF上获得</a></p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231114103030.png" alt="image.png|666"></p><p>两个网络+一个优化模块</p><ul><li>主网络 ray-surface distance field $f_{\mathbf{\Theta}}$ <ul><li>输入：单条定向射线 r 使用球形化参数射线，每条射线穿过球体有两个交点，每个交点有两个变量参数化(相对于球体中心的角度)，i.e. $r=(\theta^{in},\phi^{in},\theta^{out},\phi^{out}),$</li><li>输出：射线起点与表面落点之间的距离 d</li></ul></li><li>辅助网络 dual-ray visibility classifier $h_{\Phi}$<ul><li>输入：一对射线</li><li>输出：相互可见性，旨在显式地建模任意一对射线之间的相互空间关系。主要用于第三部分的多视图一致性优化</li></ul></li></ul><h2 id="Dual-ray-Visibility-Classifier"><a href="#Dual-ray-Visibility-Classifier" class="headerlink" title="Dual-ray Visibility Classifier"></a>Dual-ray Visibility Classifier</h2><p>单独的 ray-surface distance field 也可以拟合输入输出，但没有机制驱动其输出距离，即表面几何。i.e.缺乏多视图一致性</p><p>下图中的 $r_1$ 和 $r_2$ 相互可见，则两条光线同时击中一个表面点。则应该满足：<br>$\left.r_1^{in}+d_1r_1^d=\left(\begin{smallmatrix}x_1\\y_1\\z_1\end{smallmatrix}\right.\right)=r_2^{in}+d_2r_2^d,\mathrm{~where~}r^d=\frac{r^{out}-r^{in}}{\left|\boldsymbol{r}^{out}-\boldsymbol{r}^{in}\right|}$ $\left.\mathrm{and~}r^<em>=\left(\begin{array}{c}\sin\theta^</em>\cos\phi^<em>\\\sin\theta^</em>\sin\phi^<em>\\\cos\phi^</em>\end{array}\right.\right),*\in\{in,out\}$</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231114104017.png" alt="image.png|666"></p><p>任意两条射线之间的相互可见性 <em>mutual visibility</em> 对于 ray-surface distance field 的多视图一致性至关重要</p><p><strong>dual-ray visibility classifier</strong> 二元分类器设计需要保证即便两个射线的顺序调换，也可以得到相同的结果</p><p>$h_{\Phi}:\quad MLPS\Big[\frac{g(\theta_1^{in},\phi_1^{in},\theta_1^{out},\phi_1^{out})+g(\theta_2^{in},\phi_2^{in},\theta_2^{out},\phi_2^{out})}2\oplus k(x_1,y_1,z_1)\Big]\to0/1$</p><ul><li>G(): 一个共享的单一全连接层</li></ul><h2 id="Multi-view-Consistency-Optimization"><a href="#Multi-view-Consistency-Optimization" class="headerlink" title="Multi-view Consistency Optimization"></a>Multi-view Consistency Optimization</h2><p>给定一个静态 3D 场景的 K 张深度图像(H × W)作为整个训练数据，训练模块由两个阶段组成</p><p>Stage 1 - Training Dual-ray Visibility Classifier<br>首先，将所有原始深度值转换为射线表面距离值。对于第 K 张图像中的特定第 i 条射线(像素)，我们将其射线表面点投影回剩余的(K−1)次扫描，获得相应的(K−1)个距离值。我们设置 10 毫米作为接近阈值，以确定投影(K−1)射线在(K−1)图像中是否可见。总的来说，我们生成了 K∗H∗W∗(K−1)对带有 0/1 标记的射线。采用标准交叉熵损失函数对双射线可见性分类器进行优化</p><p>请注意，这个分类器是以<strong>特定于场景的方式</strong>进行训练的。一旦网络得到良好的训练，它基本上会将特定场景的任意两条光线之间的关系编码为网络权重。</p><p>Stage 2 - Training Ray-surface Distance Network<br>我们整个管道的最终目标是优化 Ray-surface Distance Network，并使其具有多视图几何一致性。然而，这不是微不足道的，因为<strong>简单地用射线表面数据点拟合网络不能推广到看不见的射线</strong>，这可以在我们第 4.5 节的消融研究中看到。在这方面，我们充分利用训练良好的可见性分类器来帮助我们训练射线表面距离网络。具体而言，这一阶段包括以下关键步骤:</p><ul><li>所有深度图像都转换为射线表面距离，为特定的 3D 场景生成 K <em> H </em> W 训练射线距离对。</li><li>如图4所示，对于特定的训练射线 $(r,d)$，称为主射线，我们在以表面点 p 为中心的球内均匀采样 M 条射线 $\{r^1\cdots r^m\cdots r^M\}$，称为多视图射线。然后，我们沿着每条 M rays 计算表面点 p 与边界球之间的距离，得到多视图距离 $\{\tilde{d}^{1}\cdots\tilde{d}^{m}\cdots\tilde{d}^{M}\}.$ <strong>根据训练集中给定的距离 d</strong>，这很容易实现。M 简单地设置为20，更多细节见附录A.4。</li><li>我们建立 M 对射线 $\left\{(r,p,r^1)\cdots(r,p,r^m)\cdots(r,p,r^M)\right\}$，然后将它们输入到训练良好的可见性分类器 $h_{\Phi}$ 中，推断出它们的可见性得分 $\{\nu^1\cdots\nu^m\cdots\nu^M\}.$。</li><li>我们 feed 主光线采样 M 多视点射线 $\{r,r^1\cdots r^m\cdots r^M\}$ 到 ray-surface distance network $f_{\mathbf{\Theta}}$,估算其表面距离 $\{\hat{d},\hat{d^1}\cdots\hat{d^m}\cdots\hat{d^M}\}.$。由于网络 $f_{\mathbf{\Theta}}$ 是随机初始化的，因此一开始估计的距离是不准确的。</li><li>我们设计了以下多视图一致性损失函数来优化ray-surface distance network直至收敛:$\ell_{m\nu}=\frac1{\sum_{m=1}^M\nu^m+1}\Big(|\hat{d}-d|+\sum_{m=1}^M\left(|\hat{d}^m-\tilde{d}^m|*\nu^m\right)\Big)$</li></ul><p>基本上，这种简单的损失驱动网络不仅要拟合主要的射线-表面距离(训练集中的可见射线)，而且要满足可见多视图射线(训练集中的无限未见射线)也具有准确的距离估计。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231114111158.png" alt="image.png|333"></p><h2 id="Surface-Normal-Derivation-and-Outlier-Points-Removal"><a href="#Surface-Normal-Derivation-and-Outlier-Points-Removal" class="headerlink" title="Surface Normal Derivation and Outlier Points Removal"></a>Surface Normal Derivation and Outlier Points Removal</h2><p>在上述3.1&amp;3.2&amp;3.3章节中，我们有两个网络设计和一个优化模块分别对它们进行训练。然而，我们的经验发现，主要的射线-表面距离网络可能预测不准确的距离值，特别是对于锐利边缘附近的射线。从本质上讲，这是因为在极端视角变化的情况下，实际的射线表面距离在尖锐边缘处可能是不连续的。<strong>这种形状不连续实际上是几乎所有现有的隐式神经表示的共同挑战</strong>，因为现代神经网络在理论上只能对连续函数建模。</p><p>幸运的是，我们的射线-表面距离场的一个很好的性质是，每个估计的三维表面点的法向量可以很容易地用网络的自微分推导出一个封闭形式的表达式。特别地，给定一条输入射线$r=(\theta^{in},\phi^{in},\theta^{out},\phi^{out}),$以及它到网络$f_{\mathbf{\Theta}}$的估计射线-曲面距离d，则该估计曲面点对应的法向量n可以推导为如下所示的具体函数: $n=Q\left(\frac{\partial\hat{d}}{\partial r},r,D\right)$</p><p>有了这个法向量，我们可以选择添加一个额外的损失来正则化估计的表面点，使其尽可能光滑。<strong>然而，我们从经验上发现，整个3D场景的整体性能提升是相当有限的，因为这些极端不连续的情况实际上是稀疏的。</strong></p><p>在这方面，我们转向简单地去除预测的表面点，即离群点，其法向量的欧几里得距离大于网络推理阶段的阈值。实际上，PRIF[23]也采用了类似的策略来过滤掉异常值。<strong>请注意，先进的平滑或插值技术可以集成来改进我们的框架，这将留给未来的探索。</strong></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>我们的方法在三种类型的公共数据集上进行了评估:<br>1)来自原始NeRF论文[47]的对象级合成Blender数据集，<br>2)来自最近DM-NeRF论文[73]的场景级合成DM-SR数据集，<br>3)场景级真实世界的ScanNet数据集[16]。</p><p>基线:我们精心选择了以下六个成功且具有代表性的隐式神经形状表示作为我们的基线:1)OF [46]， 2) DeepSDF [54]， 3) NDF [14]， 4) news [77]， 5) DSNeRF [19]， 6) LFN [64]， 7) PRIF[23]。<br>OF/DeepSDF/NDF/NeuS方法是基于坐标的水平集方法，在三维结构建模中表现出优异的性能。<br>DS-NeRF是一种深度监督的NeRF[47]，继承了2D视图渲染的优秀能力。<br>LFN和PRIF是两种基于光线的方法，在生成二维视图方面效率较高。<br>我们注意到，这些基线有许多复杂的变体，可以在各种数据集上实现SOTA性能。然而，我们并不打算与它们进行全面的比较，主要是因为它们的许多技术，如更高级的实现、添加额外的条件、替换更强大的骨干等，也可以很容易地集成到我们的框架中。我们将这些潜在的改进留给未来的探索，但在本文中只关注我们的香草射线表面距离场。为了公平的比较，<strong>所有基线都与我们的深度扫描量相同</strong>，以相同的场景特定方式从头开始仔细训练。关于所有基线的实施和可能的小调整的更多细节见附录A.3.1。</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>在本文中，我们已经证明，通过使用多视图一致的基于光线的框架，有效和准确地学习3D形状表示是真正可能的。与现有的基于坐标的方法相比，我们使用简单的射线-表面距离场来表示三维几何形状，并进一步由一种新的双射线可见性分类器驱动，以实现多视图形状一致。在多个数据集上的大量实验证明了我们的方法具有极高的渲染效率和出色的性能。用更高级的技术(如更快的实现和额外的正则化)扩展我们的框架会很有趣。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>创建虚拟环境<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n raydf python=3.8 -y</span><br><span class="line">conda activate raydf</span><br></pre></td></tr></table></figure></p><p>问题：</p><ul><li>在wsl2中配置好<code>pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113</code> 和 <code>pip install -r requirements.txt</code>后发现cuda无法使用<ul><li><a href="https://blog.csdn.net/Papageno_Xue/article/details/125754893">JAX: 库安装和GPU使用，解决不能识别gpu问题_jax安装_Papageno2018的博客-CSDN博客</a></li></ul></li><li>转到win10中配置，发现jax和jaxlib的0.4.10版本无法安装到python3.8中<ul><li><a href="https://blog.csdn.net/zhangyi0626/article/details/120417503">jaxlib暂不支持windows_jaxlib-0.3.20+cuda11.cudnn82-cp38-cp38-win_amd64.w_GoldMinnie的博客-CSDN博客</a></li></ul></li></ul><p>==环境配置失败2023.11.14~11.16==</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Consistency </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DiffuStereo</title>
      <link href="/3DReconstruction/Multi-view/Generative%20Models/DiffuStereo/"/>
      <url>/3DReconstruction/Multi-view/Generative%20Models/DiffuStereo/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras</th></tr></thead><tbody><tr><td>Author</td><td>Ruizhi Shao, Zerong Zheng, Hongwen Zhang, Jingxiang Sun, Yebin Liu</td></tr><tr><td>Conf/Jour</td><td>ECCV 2022 Oral</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://liuyebin.com/diffustereo/diffustereo.html">DiffuStereo Project Page (liuyebin.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4648031844416110593&amp;noteId=2044584813117342208">DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231111120652.png" alt="image.png|666"></p><p>THUman2.0数据集demo结果好(除手、脸和脚)，训练代码还未开源<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231113161728.png" alt="image.png|666"></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们提出了 DiffuStereo，这是一种仅使用稀疏相机(本工作中有 8 个)进行高质量 3D 人体重建的新系统。其核心是一种新型的基于扩散的立体模型，将扩散模型这一功能强大的生成模型引入到迭代立体匹配网络中。为此，我们设计了一个新的扩散核和附加的立体约束来促进网络中的立体匹配和深度估计。我们进一步提出了一个多级立体网络架构来处理高分辨率(高达 4k)的输入，而不需要负担得起的内存占用。给定一组稀疏视图彩色人体图像，基于多层次扩散的立体网络可以生成高精度的深度图，然后通过高效的多视图融合策略将深度图转换为高质量的三维人体模型。总的来说，我们的方法可以自动重建人体模型，其质量与高端密集视角相机平台相当，这是使用更轻的硬件设置实现的。实验表明，我们的方法在定性和定量上都大大优于最先进的方法。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>贡献:<br>1)我们提出了 DiffuStereo，这是一个轻量级和高质量的系统，用于稀疏多视图相机下的人体体积重建。<br>2)据我们所知，我们提出了第一个将扩散模型引入立体和人体重建的方法。我们通过精心设计一个新的扩散核并在扩散条件中引入额外的立体约束来扩展香草扩散模型。<br>3)我们提出了一种新的多层次扩散立体网络，以实现准确和高质量的人体深度估计。我们的网络可以优雅地处理高分辨率(高达 4k)图像，而不会遭受内存过载的困扰。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231111120652.png" alt="image.png|666"></p><p>DiffuStereo 可以从稀疏的(少至 8 个)摄像机中重建高质量的人体模型（所有摄像机均匀分布在目标人周围的一个环上）。这种稀疏的设置给高质量的重建带来了巨大的挑战，因为两个相邻视图之间的角度可以大到 45 度。DiffuStereo 通过一种现成的重建方法 DoubleField[52]、一种基于扩散的立体网络和一种轻量级多视图融合模块的共同努力，解决了这些挑战。</p><p>DiffuStereo 系统由三个关键步骤组成:</p><ul><li>通过 DoubleField[52]预测初始人体网格，并渲染为粗视差流(第 3.1 节); 作者先前提出的 DoubleField，最新的最先进的人体重建方法之一，表面和辐射场被桥接以利用人体几何先验和多视图外观，在给定稀疏视图输入的情况下提供良好的网格初始化。</li><li>对于每两个相邻视图，在基于扩散的立体中对粗视差图进行细化，得到高质量的深度图(第 3.2 节);基于扩散的立体网络对每个输入视图的视差图有很强的改进能力，其中使用扩散过程进行连续的视差细化。</li><li>初始的人体网格和高质量的深度图被融合成最终的高质量人体网格(第 3.3 节)，其中一个轻量级的多视图融合模块以初始网格作为锚点位置，有效地组装了部分精细的深度图。</li></ul><h2 id="Mesh-Depth-Disparity-Initialization"><a href="#Mesh-Depth-Disparity-Initialization" class="headerlink" title="Mesh, Depth, Disparity Initialization"></a>Mesh, Depth, Disparity Initialization</h2><p>DoubleField 得到初始人体 mesh，然后渲染为深度图</p><p>m 和 n 是两个相邻视图的索引，为了得到视图 m 到相邻视图 n 的粗视差图 $x_{c}$，取视图 m 的深度图 $D^m_c$，计算像素位置 o = (i, j)处的视差: $\mathbf{x}_c(\boldsymbol{o})=\pi^n\left((\pi^m)^{-1}\left([\boldsymbol{o},\mathbf{D}_c^m(\boldsymbol{o})]^\mathrm{T}\right)\right)-\boldsymbol{o}$ Eqn.1</p><p>其中 $(\pi^{m})^{-1}$ 将深度图 $D^m_c$ 中的点变换为世界坐标系，$π^n$ 将世界坐标系中的点投影为图像坐标系。</p><p>由于初始视差图是从粗糙的人体网格中计算的，因此可以在很大程度上缓解大位移和遮挡区域的问题。正如即将介绍的那样，<strong>这些视差图通过 Diffusion-based Stereo 进一步细化，以获得每个输入视点的高质量深度图</strong></p><h2 id="Diffusion-based-Stereo-for-Disparity-Refinement"><a href="#Diffusion-based-Stereo-for-Disparity-Refinement" class="headerlink" title="Diffusion-based Stereo for Disparity Refinement"></a>Diffusion-based Stereo for Disparity Refinement</h2><p>现有的(stereo methods)立体方法 <a href="https://readpaper.com/pdf-annotate/note?pdfId=4518062699161739265&amp;noteId=1986540055632613120">63MVSNet: Depth Inference for Unstructured Multi-view Stereo (readpaper.com)</a> [9、20、23、68、33、63、69、70]采用 3D/4D 成本体离散预测视差图，难以实现亚像素级的流量 flow 估计。为了克服这一限制，我们提出了一种基于扩散的立体网络，使立体网络可以在迭代过程中学习连续流。<br>具体来说，我们的基于扩散的立体图像包含一个正演过程和一个反演过程，以获得最终的高质量视差图。在正演过程中，将初始视差图扩散到有噪声分布的图中。<strong>在反向的过程中</strong>，在具有多个立体相关特征的条件下，<strong>从噪声图中恢复出高质量的视差图</strong>。<br>下面，我们简要介绍了一般的扩散模型，然后介绍了我们将扩散模型的连续性与基于学习的迭代立体相结合的解决方案。此外，我们还提出了一个多层次的网络结构，以解决高分辨率图像输入的内存问题。</p><h3 id="Generic-Diffusion-Model"><a href="#Generic-Diffusion-Model" class="headerlink" title="Generic Diffusion Model"></a>Generic Diffusion Model</h3><p>More formally, the diffusion model can be written as two Markov Chains:<br>Eqn.2,3,4</p><script type="math/tex; mode=display">\begin{aligned}&q(\mathbf{y}_{1:T}|\mathbf{y}_{0}) =\prod\limits_{t=1}^Tq(\mathbf{y}_t|\mathbf{y}_{t-1}),  \\&q(\mathbf{y}_t|\mathbf{y}_{t-1}) =\mathcal{N}(\sqrt{1-\beta_t}\mathbf{y}_{t-1},\beta_tI)  \\&p_{\theta}(\mathbf{y}_{0:T}|\mathbf{s}) =p(\mathbf{y}_T)\prod_{t=1}^Tp_\theta(\mathbf{y}_{t-1}|\mathbf{y}_t,\mathbf{s}), \end{aligned}</script><p>其中 $q(\mathbf{y}_{1:T}|\mathbf{y}_{0})$ 为正向函数，$q(\mathbf{y}_t|\mathbf{y}_{t-1})$ 为扩散核，表示加入噪声的方式，$\mathcal{N}$ 为正态分布，I 是 the identical matrix，pθ()为反向函数，采用去噪网络 $\mathcal{F}_θ$ 对 $y_t$ 进行去噪，为附加条件。当 T→∞时，正反过程可以看作是连续过程或随机微分方程[55]，这是连续 flows 估计的自然形式。正如之前的工作[55]所验证的那样，在参数更新中注入高斯噪声使迭代过程更加连续，并且可以避免陷入局部极小。在这项工作中，我们将展示，这样一个强大的生成工具也可以用来为以人为中心的立体任务产生连续的 flows。</p><p>与一般的扩散模型相比，我们的 diffusion-based stereo 采用了两种特定于任务的设计：<br>I)考虑到 stereo flow estimation 不是纯粹的生成任务，<strong>采用了一种新的扩散核</strong>；<br>Ii)<strong>反向过程中涉及与立体相关的特征和监督</strong>，以确保颜色的一致性和极线约束 epipolar constraints.。</p><h3 id="Disparity-Forward-Process"><a href="#Disparity-Forward-Process" class="headerlink" title="Disparity Forward Process"></a>Disparity Forward Process</h3><p>本文扩散模型的输入 $\mathbf{y}_{0}$ 是地面真实视差图 $\mathbf{\hat{x}}$ 和粗视差图 $x_c$ 之间的残差视差 $\mathbf{\hat{y}}_0$，即 $\mathbf{\hat{y}}_0=\hat{\mathbf{x}}-\mathbf{x}_c$。<br><strong>与现有的图像合成生成扩散模型</strong>利用 $\sqrt{1-\beta_t}$ 逐步减小 $\mathbf{y}_{t-1}$ 的尺度<strong>不同</strong>，我们设计了一个扩散核来保持 $\mathbf{y}_{t-1}$ 的尺度，线性漂移 $\mathbf{y}_{0}$ 到 $\mathbf{y}_{t}$，即 Eqn(3) 改写为：Eqn.5<br>$q(\mathbf{y}_{t}|\mathbf{y}_{t-1})=\mathcal{N}(\mathbf{y}_{t}|\mathbf{y}_{t-1}-\alpha_{t}\mathbf{y}_{0},\alpha_{t}\mathbf{I}),$</p><p>其中 $α_t$ 是缩放噪声的参数。基于这种新的扩散核，视差正向过程使用 Eqn(2) 和 (5)逐渐向地面真实残差视差 $\mathbf{\hat{y}}_0$ 添加噪声。在我们的实验中，我们发现我们的扩散内核使反向过程更加稳定，并且在推理时有效地减少了所需的迭代步数。我们新内核下的前向过程的推导与[24]相似，可以在 Supp 中找到</p><h3 id="Stereo-conditioned-Reverse-Process"><a href="#Stereo-conditioned-Reverse-Process" class="headerlink" title="Stereo-conditioned Reverse Process."></a>Stereo-conditioned Reverse Process.</h3><p>Eqn.4<br>$p_{\theta}(\mathbf{y}_{0:T}|\mathbf{s}) =p(\mathbf{y}_T)\prod_{t=1}^Tp_\theta(\mathbf{y}_{t-1}|\mathbf{y}_t,\mathbf{s})$</p><p>基于扩散的立体的反向过程旨在利用 Eqn(4)从噪声 $\mathbf{y}_T$ 中恢复残差视差 $\mathbf{\hat{y}}_0$。在这个过程中，扩散立体网络充当 Eqn(4)中的去噪网络 $\mathcal{F}_θ$ 。通过将 $\mathbf{y}_{t}$ 和 s 作为输入并预测 $\widetilde{\mathbf{y}}_0.$。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231111184852.png" alt="image.png|666"></p><p>由于扩散核也会影响反向过程，我们网络的去噪过程与我们的新核下的泛型去噪过程不同。给定方程式 5 中的内核，反向过程中每一步的公式可以写成：$p_{\theta}(\mathbf{y}_{t-1}|\mathbf{y}_{t},\mathbf{s})=\mathcal{N}(\mathbf{y}_{t-1}|\mu_{\theta}(\mathbf{y}_{t},\gamma_{t},\mathbf{s}),\sigma_{t}^{2}\mathbf{I}),$ Eqn.6</p><p>其中 $\gamma_{t}=\sum_{i=1}^{t}\alpha_{i},\sigma_{t}^{2}=\frac{\alpha_{t}\gamma_{t-1}}{\gamma_{t}},\mathrm{and}\mu_{\theta}()$ 为去噪网络 $\mathcal{F}_θ$ 的预测过程。此外，我们将预测的 $\widetilde{\mathbf{y}}_0$ 代入 $q(\mathbf{y}_{t-1}|\mathbf{y}_t,\mathbf{y}_0)$ 的后验分布，表示 $p_\theta(\mathbf{y}_{t-1}|\mathbf{y}_t)$ 的均值: $\mu_{\theta}(\mathbf{y}_{t},\gamma_{t},\mathbf{s})=\frac{\alpha_{t}}{\gamma_{t}}\widetilde{\mathbf{y}}_{0}+\frac{\gamma_{t-1}}{\gamma_{t}}\mathbf{y}_{t}.$ Eqn.7</p><p><strong>the whole reverse process</strong>：$\mathbf{y}_{t-1}\leftarrow\frac{\alpha_{t}}{\gamma_{t}}\widetilde{\mathbf{y}}_{0}+\frac{\gamma_{t-1}}{\gamma_{t}}\mathbf{y}_{t}+\frac{\alpha_{t}\gamma_{t-1}}{\gamma_{t}}\epsilon_{t},\epsilon_{t}\sim\mathcal{N}(\mathbf{0},\mathbf{1}).$ Eqn.8</p><p>由于视差细化不是完全生成过程，扩散立体网络 $\mathcal{F}_θ$ 以附加条件作为输入来恢复高质量的视差流。在我们的解决方案中，四种类型的立体相关图（图 3 右）充当方程式(4)中的附加条件 s 保证反向过程每一步的颜色一致性和极线约束</p><ul><li>The original image $\mathbf{I}_m$ of the view m;</li><li>The warped image $\mathbf{I}_w^n$ of view n, which is obtained by transforming pixels of $\mathbf{I}^n$ using current flow $\mathbf{x}_t^m=\mathbf{x}^m+\mathbf{y}_t$: $\mathbf{I}_w^n(\boldsymbol{o})=\mathbf{I}^n(\mathbf{x}_t^m(\boldsymbol{o})+\boldsymbol{o}).$</li><li>The current flow map $\mathbf{x}_t^m$ （$\mathbf{x}_t^m=\mathbf{x}^m+\mathbf{y}_t$）</li><li>The direction map $\mathbf{e}^m$ of epipolar line 极线, which is computed as: $\mathbf{e}^m=(\dot{\mathbf{x}}_c^m-\mathbf{x}_c^m)/|\dot{\mathbf{x}}_c^m-\mathbf{x}_c^m|_2,$<ul><li>其中̇ $\dot{\mathbf{x}}_c^m$ 是基于粗深度图 $\mathbf{D}_c^m$ 和固定移位β变换的移位流图: $\dot{\mathbf{x}}_c^m(\boldsymbol{o})=\pi^n((\pi^m)^{-1}([\boldsymbol{o},\mathbf{D}_c^m(\boldsymbol{o})+\beta]^T))-\boldsymbol{o}.$</li></ul></li></ul><p>在上述四种条件下， $\mathbf{I}_m$ 和 $\mathbf{I}_w^n$ 鼓励网络意识到颜色一致性，而 $\mathbf{x}_t^m$ 和 $\mathbf{e}^m$ 提供了关于流向网络的提示以进行更好预测。我们通过将上述立体相关映射连接为 $\mathrm{s}_t^m=\bigoplus(\mathbf{I}^m,\mathbf{I}_w^n,\mathbf{x}_t^m,\mathbf{e}^m).$ 来约束网络。条件 $\mathrm{s}_t^m$ 与 $(\mathbf{y}_t^m,t)$ 进一步串联，并馈入扩散立体网络。此外，我们还将网络输出映射 R 约束为仅一个通道，以便预测的剩余 flow $\widetilde{\mathbf{y}}_t=\mathrm{e}^m\cdot R$ 被迫沿着极线移动。</p><p>当反向过程完成时，利用 Eqn(1)的逆公式，将最终流 $\mathbf{x}^m+\widetilde{\mathbf{y}}_0^m$ 转换回视图 m 的精细深度图 $\mathrm{D}_f^m$。</p><h3 id="Multi-level-Network-Structure-解决-memory-问题"><a href="#Multi-level-Network-Structure-解决-memory-问题" class="headerlink" title="Multi-level Network Structure 解决 memory 问题"></a>Multi-level Network Structure 解决 memory 问题</h3><p>对于高质量的人体重建，利用高分辨率输入图像至关重要。当应用上述扩散立体网络时，当采用高分辨率图像(在我们的实验中为4K)作为输入时，会出现内存问题。受 PIFuHD[51]的启发，我们采用多层网络结构来解决这一问题，其中全局网络 $\mathcal{F}_{g}$ 与扩散立体网络 $\mathcal{F}_{\theta}$ 相结合。这样，$\mathcal{F}_{g}$ 和 $\mathcal{F}_{\theta}$ 可以分别在全局和扩散层面产生视差流。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231111211733.png" alt="image.png|666"></p><p>如图4所示，在全局级别上，粗初始流 $\mathrm{x}_c^m$ 和条件立体特征 $s^{m}$ 都被下采样到 512×512 的分辨率，然后被馈送到全局网络 $\mathcal{F}_{g}$ 中。其直接预测低分辨率残差流 $\widetilde{\mathbf{y}}_g^m.$ 注意，全局级别的流量估计不是扩散过程，而是学习包含人类语义信息的全局特征。在扩散级，我们采用来自全局网络的最后图像特征 $\mathbf{I}_g^m$ 作为扩散立体网络的附加条件。因此，本地级网络中的立体相关特征被修改为 $\bigoplus(\mathbf{I}^m,\mathbf{I}_w^n,\mathbf{x}_t^m,\mathbf{e}^m,\mathbf{I}_g^m).$ 得益于多级结构，由于可以以基于补丁的方式训练扩散立体网络，因此可以在很大程度上克服内存问题。此外，在全局特征 $\mathbf{I}_g^m$ 的指导下，我们的扩散立体网络可以更加专注于精细细节的恢复。</p><h3 id="Training-of-Diffusion-based-Stereo"><a href="#Training-of-Diffusion-based-Stereo" class="headerlink" title="Training of Diffusion-based Stereo"></a>Training of Diffusion-based Stereo</h3><p>Global Level. 我们将 GT 残差流下采样到512的分辨率，并用全局损失 $\mathcal{L}_g$ 训练全局网络 $\mathcal{F}_{g}$：<br>$\mathcal{L}_g=\frac{1}{HW}\sum_{i=1}^{H}\sum_{j=1}^{W}|\widetilde{\mathbf{y}}_g(i,j)-\mathbf{y}_0(i,j)|^2,$</p><p>全局损失促使网络学习人类语义特征进行流量估计。</p><p>Diffusion Level. Follow <a href="https://readpaper.com/pdf-annotate/note?pdfId=4545125180618989569&amp;noteId=2045159659192404736">WaveGrad: Estimating Gradients for Waveform Generation (readpaper.com)</a>，我们随机选择一个时间步长 t，并将 GT 残余流 $\mathbf{y}_0$ 扩散到 $\mathbf{y}_t$，生成训练样本，使用: $q(\mathbf{y}_t|\mathbf{y}_0)=\mathcal{N}(\mathbf{y}_t|(1-\gamma_t)\mathbf{y}_0,\gamma_t\mathbf{I}).$</p><p>然后在第 t 步采用扩散损失 $\mathcal{L}_{d}=$ 来训练扩散立体网络 $\mathcal{F}_{\theta}$: $\mathcal{L}_{d}=\frac{1}{HW}\sum_{i=1}^{H}\sum_{j=1}^{W}|(\mathcal{F}_{\theta}(\mathbf{y}_{t},\mathbf{s}_{t},t))(i,j)-\mathbf{y}_{0}(i,j)|_{2}^{2}.$</p><h2 id="Light-weight-Multi-view-Fusion"><a href="#Light-weight-Multi-view-Fusion" class="headerlink" title="Light-weight Multi-view Fusion"></a>Light-weight Multi-view Fusion</h2><p>在本节中，我们提出了一种轻量级的多视图混合融合来融合精细的深度图 $\mathbf{D}_{f}^{1},…,\mathbf{D}_{f}^{n}$ 和粗网格 $\mathrm{m}_{c}$ 来重建最终模型。<br>在融合之前，我们首先使用侵蚀核去除深度边界，并将每个精细的深度图 $\mathbf{D}_f^i$ 转换为一个点云 $\mathbf{p}^{i}=(\pi^i)^{-1}(\mathbf{D}_f^i).$<br>由于标定误差在实际数据中是不可避免的，因此从多视角估计的精细化深度图可能无法准确对齐。为了解决这个问题，我们利用非刚性 ICP 对深度点云 $\mathrm{p}^{i}$ 和粗网格的点云 $\mathrm{p}^{c}$ 进行对齐，其中粗点云作为后续对齐的锚点模型。在我们的非刚性 ICP 中，优化目标 $\mathcal{L}_{icp}=\mathcal{L}_d+\mathcal{L}_s$ 由数据项 $\mathcal{L}_d$ 和光滑项 $\mathcal{L}_s$ 组成，<br>$\mathcal{L}_d=\sum_{i=1}^n\sum_{j=i+1}^n\sum_{(a,b)\in\widetilde{\mathbf{N}}^{i,j}}|\widetilde{\mathbf{p}}_a^i-\widetilde{\mathbf{p}}_b^j|^2+\lambda_d\sum_{i=1}^n\sum_{(a,b)\in\widetilde{\mathbf{N}}_c^i}|\widetilde{\mathbf{p}}_a^i-\mathbf{p}_b^c|^2$<br>$\mathcal{L}_s=\lambda_s\sum_{i=1}^n\sum_{(a,b)\in\mathbf{N_i}}|\widetilde{\mathbf{d}}_a^i-\widetilde{\mathbf{d}}_b^i|^2/|\mathbf{p}_a^i-\mathbf{p}_b^i|^2,$</p><p>式中 $\mathbf{d}^{i}$ 为深度点云 $\mathrm{p}^{i}$ 的位移，N 为搜索到的邻域对应关系的集合。我们采用最近邻算法搜索对应，搜索半径的阈值为 2mm。<br>优化后的最终点云 $p^{f}$ 是优化后的深度点云 ${\tilde{\mathrm{p}}}^{i}$ 与粗点云 $p ^{c}$ 的结合，而最终的网格可由最终点云 $p^{f}$ 使用泊松重建[32]重建。</p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>在我们的实现中，我们采用类似于 Diffusion Models Beat GANs on Image Synthesis[12]的 U-Net[47]模型作为全局网络 $\mathcal{F}_{g}$ 和扩散网络 $\mathcal{F}_{\theta}$ 的结构。我们的扩散模型 T 是30，在图像生成任务中比原始模型小得多。关于其他扩散参数，包括$\alpha_t,\gamma_t$ 和更多的实现细节，请参阅补充材料。</p><p>Training Data Preparation.<br>我们从Twindom[57]收集了300个模型，并渲染图像对进行训练。我们首先从360°角度密集渲染4K分辨率的图像和深度图。然后，我们在8个均匀分布视图的图像上运行DoubleField[52]，预测分辨率为$512^3$的SDF体积，并使用marching cube进一步检索粗糙的人体网格。在我们的基于扩散的立体网络训练过程中，我们从同一模型的渲染图像中随机选择两个视图，并在[20,50]的间隔内约束它们的角度。我们还计算了两个视图之间的遮挡区域，并过滤掉不好的部分，以避免不稳定的训练。</p><p>Evaluation Data Preparation.<br>我们从THUman2.0[65]数据集中随机选择300个模型进行评估。4K分辨率的人物图像和深度图从360角度渲染。</p><h1 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h1><p>Conclusion.<br>我们介绍了DiffuStereo，一个从稀疏视图RGB图像重建高质量3D人体模型的新系统。通过对人类模型的初步估计，我们的系统利用基于扩散模型的新型迭代立体网络，<strong>从每两个相邻视图生成高精度的深度图</strong>。这种基于扩散的立体网络经过精心设计，可以处理稀疏视图、高分辨率输入。<strong>高质量的深度图可以组装来生成最终的3D模型</strong>。与现有的方法相比，我们的方法可以重建更清晰的几何细节，达到更高的精度。</p><p>Limitation.<br>我们的方法的主要限制是依赖于双场来估计一个初始的人类模型。此外，由于在稀疏视图设置中缺乏观测，我们的方法无法重建不可见区域的几何细节。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Generative Models </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> 3DReconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DoubleField</title>
      <link href="/3DReconstruction/Single-view/Implicit%20Function/DoubleField/"/>
      <url>/3DReconstruction/Single-view/Implicit%20Function/DoubleField/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Rendering</th></tr></thead><tbody><tr><td>Author</td><td>Ruizhi Shao1, Hongwen Zhang1, He Zhang2, Yanpei Cao3, Tao Yu1, and Yebin Liu1</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="http://www.liuyebin.com/dbfield/dbfield.html">DoubleField Project Page (liuyebin.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4546361709874012161&amp;noteId=2043418109076863744">DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Rendering. (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231110163602.png" alt="image.png|666"></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们介绍了一种结合表面场和亮度场优点的高保真人体渲染的新表示方法——DoubleField。在 DoubleField 中，通过共享特征嵌入和表面引导采样策略将表面场和辐射场关联在一起。这样，DoubleField 就为几何和外观建模提供了一个连续但不纠缠的学习空间，支持快速训练、推理和微调。为了实现高保真的自由视点渲染，DoubleField 进一步增强以利用超高分辨率输入，其中引入了视图到视图转换器和迁移学习方案，以便在原始分辨率下从稀疏视图输入进行更有效的学习和微调。通过对多个数据集的定量评估和对真实稀疏多视点系统的定性结果验证了双场算法的有效性，显示了其在真实自由视点人体渲染方面的优越能力。</p><h2 id="Introduce"><a href="#Introduce" class="headerlink" title="Introduce"></a>Introduce</h2><p>最近，表面场[28,25,2]和辐射场[26,48]作为一种有前途的解决方案，以独立于分辨率和连续的方式建模 3D 几何形状和外观。尽管在详细的几何形状恢复[32,33,50,10]和逼真的纹理恢复和渲染[45,29]方面取得了重大进展，但在考虑几何形状和外观的同时重建时，它们的局限性就变得明显了。<br>现有表面场和辐射场的局限性源于连续性和解纠缠性之间的权衡。具体而言：</p><ul><li>基于表面场的重建方法[32,27,20]通常在表面上学习纹理，导致预测纹理的分布高度集中在表面上。这种狭窄的纹理场通常是不连续的，阻碍了可微渲染的优化过程。</li><li>亮度场[26,48]可以学习连续纹理场，但其几何形状与纹理纠缠，缺乏足够的约束。这种几何外观纠缠不仅会导致几何恢复中的不一致和伪影，特别是在稀疏的多视图设置下，而且会使辐射场的训练和推断非常耗时[1]</li></ul><p>为了克服现有神经场表示的局限性，我们提出了一种新的 DoubleField 框架，以桥接表面和辐射场，并为几何和外观学习提供连续但不纠缠的空间。具体来说，我们从网络结构和采样策略方面建立了表面场和辐射场之间的关联。<br>1)在我们的网络架构中，中间 MLP 学习两个字段的共享双嵌入。共享的学习空间促进了这两个领域的反向传播梯度的更新，从而可以以连续的方式学习几何和外观。<br>2)提出了一种曲面引导的采样策略，首先对稀疏点进行采样以确定相交表面，然后对表面周围的密集点进行采样，在亮度场中进行体绘制。该策略对辐射场施加几何约束，将几何分量从外观建模中解放出来，不仅加快了学习过程，而且提高了自由视点渲染结果的质量和一致性。</p><p>采用所提出的架构和采样策略，DoubleField 结合了两个领域的优点，自然支持基于可微渲染的自监督信号对新数据进行有效微调。</p><p>为了充分利用双场的潜在力量，我们又向前迈进了一步，利用超高分辨率输入。与只学习粗糙的图像特征不同，DoubleField 进一步增强了 view-to-view 转换器，直接将原始分辨率下图像的原始 RGB 值作为输入。这是由于观察到自由视点渲染可以被视为一个视图到视图的问题，即在给定稀疏视图图像的情况下生成新视图图像，这让人想起 NLP[31]中典型任务的文本到文本问题。为了更有效地对高保真外观进行建模，我们对网络<strong>采用了转导迁移学习方案</strong>。具体来说，我们的网络首先在低分辨率的预训练任务上进行训练，以学习一般的多视图先验，然后在处理超高分辨率输入时通过快速微调转移并采用到高保真域。然而，这不是微不足道的，因为在稀疏视图图像上进行微调容易出现过拟合。为了克服这一问题，我们进行了全面的实验来衡量不同模块的影响，并经验地引入了一种自下而上的微调策略，可以避免过度拟合和快速收敛。从稀疏视图输入进行人体重建的实验结果表明，我们的方法具有最先进的性能和高保真的渲染质量。表 1 总结了提出的 DoubleField 表示法与现有表示法的比较。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231110164218.png" alt="image.png|666"></p><p>贡献：</p><ul><li>我们提出了一种新的双场表示，<strong>结合了表面场和辐射场的优点</strong>。我们通过共享双嵌入和表面引导采样策略将这两个领域连接起来，以便 <strong>DoubleField 具有连续但不纠缠的几何和外观建模学习空间</strong>。</li><li>我们进一步增强了 DoubleField，通过<strong>引入一个视图到视图的转换器</strong>，将超高分辨率图像的原始 RGB 值作为输入，从而支持高保真渲染。View-to-view 转换器学习超高分辨率域上从已知视点到查询视点的纹理映射。</li><li>我们<strong>利用迁移学习方案和自下而上的微调策略</strong>来更有效地训练我们的网络，使其具有快速的收敛速度，同时避免了过拟合问题。通过这种方式，DoubleField 可以在只给出稀疏视图输入的情况下产生高保真的自由视图渲染结果，这在之前的工作中证明了显著的性能改进。</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>Neural implicit field<br>与传统的网格、体、点云等显式表示相比，神经隐式场通过神经网络对三维模型进行编码，直接将三维位置或视点映射为相应的</p><ul><li>占用值——Occupancy Networks、Learning Implicit Fields for Generative Shape Modeling[25,2]</li><li>SDF——DeepSDF[28]</li><li>体积——Neural Volumes[21]</li><li>辐射——NeRF[26]等属性<ul><li>神经隐式场以空间坐标为条件，而不是以离散体素或顶点为条件，具有连续、分辨率无关、更灵活的特点，可实现更高质量的表面恢复和逼真的渲染。</li></ul></li></ul><p>对于几何重建，基于表面场的方法[32,33,41]可以从一幅或几幅图像中生成详细的模型，并使用局部隐式场[14,1]实现高保真几何。对于图形渲染，基于隐式域的方法适合于可微渲染[20,44,15,35,26]。其中，最近提出的 NeRF[26]在新颖视图合成和真实感渲染方面取得了重大进展，激发了许多衍生方法[45,24,34,38,19,30]和应用。</p><p>Multi-view human reconstruction<br>基于 template 的人体的多视角相机在不同的层次上进行了大量的研究，包括形状和姿势[12,18]，以及布料表面[4,37,8,7,42]。<strong>受表示能力的限制，这些方法通常在几何和外观恢复方面的结果质量较低</strong>。此外，<strong>这些基于模板的算法也难以处理拓扑变化</strong>。<br><strong>其他</strong>高质量人体重建的方法<strong>需要极其昂贵的依赖关系</strong>，如密集视点[16,40]，甚至控制照明[3,9]。</p><p>最近，隐式域[13,49,32]使稀疏视图的详细几何重建成为可能。基于稀疏 RGB-D 相机，还可以实现高保真的实时几何重建。最近，Peng 等人 Neural Body[29]提出在预定义模板(即 SMPL[22])的指导下学习神经辐射场，并在动态序列的新视图合成方面取得了令人鼓舞的结果。<strong>然而，他们的方法假设了身体模板的准确估计的可用性</strong>。此外，从稀疏视图输入同时重建高保真几何和外观对于现有的解决方案来说仍然是非常具有挑战性的。<strong>我们的工作开辟了高保真人体渲染的新途径，而不需要身体模板</strong>。</p><p>Transformer<br>Transformer 的有效性最近在广泛的 NLP 和 CV 问题中得到了证明[5,6,47]。<strong>注意机制是 Transformer 的核心</strong>，已被大量文献证明可以捕获远程依赖[36,39]。它获得相关性的能力已经应用于许多应用，如视觉问答[17]、纹理传输[43]、多视图立体[23]和手部姿势估计[11]。此外，基于 Transformer[5]的迁移学习在自然语言处理方面取得了重大进展，显示出巨大的泛化潜力。<strong>在我们的工作中，我们将自由视点呈现问题视为一个视图到视图的问题</strong>，并应用转换器来捕获跨多视图输入的对应关系。受前人工作的启发，<strong>我们采用迁移学习方案来解决超高分辨率图像的学习问题</strong>。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231110165440.png" alt="image.png|666"></p><p>(a)虽然 <strong>PIFu</strong> 提供了一种直观的几何和外观联合建模的解决方案，但它将几何和纹理隔离开来，使得纹理的学习空间不连续，高度集中在表面周围，这种不连续的纹理空间阻碍了在纹理监督下使用可微渲染技术的优化过程</p><p>(b)NeRF，为了从稀疏的多视图输入中实现新颖的视图合成，PixelNeRF[45]扩展了 NeRF，以类似于 PIFu 的方式利用像素对齐的图像特征。由于密度和颜色的纠缠建模为 NeRF 的训练带来了很高的灵活性，在 PixelNeRF 中学习到的表面在只有稀疏视图输入的情况下是不一致的，这会导致在新视图渲染中出现幽灵或模糊结果等伪影。此外，vanilla NeRF 的高度灵活性使得其导数解的训练、推理和微调[45,29]非常耗时。</p><p>(c)The proposed DoubleField.<br>(d) DoubleField with the raw ultra-high-resolution inputs.</p><h2 id="DoubleField-Representation"><a href="#DoubleField-Representation" class="headerlink" title="DoubleField Representation"></a>DoubleField Representation</h2><p>在我们的方法中，提出了一种新的神经场表示 DoubleField 来桥接表面场和辐射场。如图2(c)所示，我们的 DoubleField 可以被表述为一个相互隐式函数 $f_{db}$，由多层感知器(MLP)表示，以拟合表面场和亮度场: $f_{db}(\boldsymbol{x},\boldsymbol{d})=(s,\sigma,\boldsymbol{c}).$。两个场之间共享的 MLP 以隐式方式对亮度场施加几何约束，并鼓励更一致的密度分布以进行神经渲染。</p><p><strong>网络架构</strong><br>DoubleField 由一个用于双嵌入的共享 MLP $E_{db}$ 和两个用于几何和纹理建模的独立 MLP $E_g$ 和 $E_c$ 组成。</p><p>Eq. 4</p><script type="math/tex; mode=display">\begin{gathered}\mathbf{e}_{db}=E_{db}(\gamma(x),\phi(\boldsymbol{x},\boldsymbol{I}),\boldsymbol{d}), \\\begin{aligned}(s,\sigma)=E_{g}(\mathbf{e}_{db}),\boldsymbol{c}=E_{c}(\mathbf{e}_{db}),\end{aligned} \\f_{db}(\boldsymbol{x},\boldsymbol{d}|\phi(\boldsymbol{x},\boldsymbol{I}))=(s,\sigma,\boldsymbol{c}), \end{gathered}</script><p>其中γ(x)是 x 的位置编码</p><p><strong>Sampling Strategy</strong><br>表面引导采样策略首先确定表面场中的交点，然后在交点周围进行细粒度采样</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231110170758.png" alt="image.png|333"></p><h3 id="DoubleField-with-Multi-view-Inputs"><a href="#DoubleField-with-Multi-view-Inputs" class="headerlink" title="DoubleField with Multi-view Inputs"></a>DoubleField with Multi-view Inputs</h3><p>对于查询的3D 点 x，先得到 x 在每个图像上的投影特征，然后将多视图图像中提取出来的 pixel-aligned features，融合为 $\boldsymbol{\Phi}(\boldsymbol{x})$</p><p>Eq. 5<br>$\begin{gathered}\Phi^i=\oplus(\phi^i(\boldsymbol{x},\boldsymbol{I}^i),\boldsymbol{d}^i)\\\boldsymbol{\Phi}(\boldsymbol{x})=\psi(\Phi^1,…,\Phi^n),\end{gathered}$</p><p>其中⊕(…) is a concatenation operator，$\psi(…)$ 是一种特征融合运算，如平均池化 PIFu[32]或自关注 <a href="https://readpaper.com/pdf-annotate/note?pdfId=4666395825761435649">DeepMultiCap</a>。将融合后的 features $\boldsymbol{\Phi}(\boldsymbol{x})$ 作为 DoubleField 的条件特征来预测相应的几何形状和外观: $f_{db}(\boldsymbol{x},\boldsymbol{d}|\boldsymbol{\Phi}(\boldsymbol{x}))=(s,\sigma,\boldsymbol{c}).$</p><h3 id="DoubleField-on-Ultra-high-resolution-Domain"><a href="#DoubleField-on-Ultra-high-resolution-Domain" class="headerlink" title="DoubleField on Ultra-high-resolution Domain"></a>DoubleField on Ultra-high-resolution Domain</h3><p>由于对粗糙图像特征的学习限制了最终渲染结果的质量。为了克服这个问题，我们进一步增强 DoubleField，将原始分辨率下的图像作为附加条件输入 $f_{db}(\boldsymbol{x},\boldsymbol{d}|\boldsymbol{\Phi}(\boldsymbol{x}),\boldsymbol{p}(\boldsymbol{x}))=(s,\sigma,\boldsymbol{c}),$ Eq. 6 式中 p(x)表示 x 投影处的像素 RGB 值。</p><p>视图到视图转换器在其编码器中融合原始 RGB 值和多视图特征，并通过其解码器在新视图空间产生特征。此外，将原始 RGB 值映射到高维空间作为高频外观变化学习的彩色编码。<strong>视图到视图转换器的关键组件</strong>如下所示：</p><ul><li>Colored Encoding：类似于位置编码[36,26]，使用不同频率[26]的正弦和余弦函数，将超高分辨率图像 I 的每个像素上的原始 RGB 值 p 嵌入为彩色编码γ(p)。这样，每个单独的 RGB 值被映射到更高维度的空间，显著提高了性能，加快了收敛速度。这与之前关于神经场表征[26]和 NLP[36]的工作是一致的。</li><li>Encoder：在我们的视图到视图转换器中，在编码器中使用了一个“完全可见”的注意掩码，这鼓励模型通过自注意机制关注与新视图相关的每个视图。编码器作为 Eq. 5中的特征融合操作ψ来获得融合后的 featuresΦ，该融合后的 featuresΦ将被馈入双 MLP $E_{db}$ 中，用于生成双嵌入 $E_{db}(\gamma(\boldsymbol{x}),\boldsymbol{\Phi}).$。</li><li>Decoder：我们的视图到视图转换器的解码器将在稀疏视图输入上学习到的特征映射到新视点的特征。具体来说，解码器以双嵌入 $\mathbf{e}_{db}$、查询观看方向 d、查询点 x 的位置编码、RGB 值 p(x)的彩色编码作为输入，得到纹理嵌入 $\mathbf{e}_{c}:$$\boldsymbol{e}_c=D_{v2v}(\boldsymbol{e}_{db},\gamma(\boldsymbol{p}(\boldsymbol{x}))|\boldsymbol{d}).$</li></ul><p>最后，通过纹理 MLP $E_{c}$ 预测点 x 处的高分辨率颜色。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231110163602.png" alt="image.png|666"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231110172930.png" alt="image.png|666"></p><h2 id="Learning-High-fidelity-Human-Rendering"><a href="#Learning-High-fidelity-Human-Rendering" class="headerlink" title="Learning High-fidelity Human Rendering"></a>Learning High-fidelity Human Rendering</h2><p>在稀疏视图人体图像的几何和外观重建上验证了双场算法的有效性。如图4所示，给定稀疏的多视图图像和相应的射线方向，我们的视点到视点转换器的编码器作为操作ψ来融合来自不同视点的低分辨率图像特征，并使用Eq. 5输出融合后的特征。该方法以融合后的特征为输入，生成双嵌入特征，并利用几何MLP预测表面场s和密度值σ。对于高保真纹理的预测，解码器以超高分辨率图像的双嵌入$e_{db}$、查询观看方向d和彩色编码p(x)为输入，生成用于预测颜色值c的纹理嵌入$e_{c}$。</p><p>虽然我们的网络可以直接在超高分辨率图像上进行训练，但是在如此高保真度的域上，<strong>昂贵的训练时间开销</strong>仍然是一个问题。为了更可行的解决方案，<strong>我们采用了一种传导迁移学习方案</strong>，<strong>将问题分为两个阶段: 低分辨率预训练和高保真微调</strong>。<br>在预训练阶段，网络学习下采样图像的两个粗先验:<br>1)人体的一般几何和外观先验<br>2)多视图特征与原始RGB值的融合先验</p><p>具体来说，为了训练我们的模型，我们从3D扫描数据集(如Twindom1)中收集人体模型，并渲染512 × 512大小的低分辨率图像。在微调阶段，网络以特定人类稀疏多视图的超高分辨率图像作为输入，利用多视图自监督进行微调。这样，在低分辨率图像上预训练的模型就可以适应超高分辨率领域。关于迁移学习方案的更多细节可以在手册中找到。</p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>我们在几个数据集上评估了我们的DoubleField表示和视图到视图转换器:</p><ol><li>Twindom dataset我们将1700个人体模型分成1500个用于训练的模型和200个用于评估的模型。</li><li>THuman2.0 dataset[46]，这是一个由500个高质量人体模型组成的公开数据集。</li></ol><p>我们首先从训练、推理和微调方面验证了所提出的DoubleField。在此基础上，给出了不同调整策略的实验结果。最后，我们将我们的解决方案与先前最先进的方法进行比较。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231110173540.png" alt="image.png|666"></p><h1 id="Discussion-amp-Future-Works"><a href="#Discussion-amp-Future-Works" class="headerlink" title="Discussion &amp; Future Works"></a>Discussion &amp; Future Works</h1><p>我们提出了双场表示，以结合几何和外观场的优点，以实现高保真的人体渲染。虽然我们的方法在稀疏视图超高分辨率输入的重建上取得了优异的性能，<strong>但高质量的3D人体模型仍然是学习几何之前必不可少的</strong>。此外，对几何优化微调的努力是有限的，这阻碍了我们的方法来处理极具挑战性的姿势。<br>在我们的工作中，两个领域之间的关联是以隐式的方式建立的。<strong>一个更加统一和明确的表述仍然值得探索</strong>。此外，提出的视图到视图转换器和迁移学习方案为高保真渲染提供了新的解决方案。我们希望我们的方法能够对自由视点人体渲染领域的后续工作有所启发。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Implicit Function </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ClothedHumans </tag>
            
            <tag> 3DReconstruction </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HaP</title>
      <link href="/3DReconstruction/Single-view/Hybrid%20Methods/HaP/"/>
      <url>/3DReconstruction/Single-view/Hybrid%20Methods/HaP/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Human as Points: Explicit Point-based 3D Human Reconstruction from Single-view RGB Images</th></tr></thead><tbody><tr><td>Author</td><td>Yingzhi Tang, Qijian Zhang, Junhui Hou, and Yebin Liu</td></tr><tr><td>Conf/Jour</td><td>arXiv</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://github.com/yztang4/HaP">yztang4/HaP (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=2039003253707810304&amp;noteId=2039256760006808832">Human as Points—— Explicit Point-based 3D Human Reconstruction from Single-view RGB Images.pdf (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231107193647.png" alt="image.png|666"></p><p>深度估计+SMPL 估计+Diffusion Model 精细化(PointNet++)</p><p>缺陷：依赖于深度估计方法和 SMPL 估计方法的精度<br>为了保护隐私不对人脸进行重建</p><span id="more"></span><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>单视图人体重构研究的最新趋势是学习受外显形体先验约束的深层隐式函数。尽管与传统的处理管道相比，现有的学习方法有了显著的性能改进，但在灵活性、泛化性、鲁棒性和/或表示能力方面，现有的学习方法仍然表现出不同方面的局限性。为了全面解决上述问题，本文研究了一种<strong>基于点的显式人体重建框架 HaP</strong>，该框架采用点云作为目标几何结构的中间表示。从技术上讲，我们的方法的特点是<strong>在3D 几何空间中进行完全显式的点云估计，操作，生成和细化</strong>，而不是隐式的学习过程，可能是模糊的和不太可控的。整个工作流程是精心组织的，并专门设计了相应的专业学习组件和处理程序。广泛的实验表明，我们的框架比当前最先进的方法实现了20%到40%的定量性能改进，并获得了更好的定性结果。我们有希望的结果可能表明范式回滚到完全显式和以几何为中心的算法设计，这使得能够利用各种强大的点云建模架构和处理技术。我们将在 <a href="https://github.com/yztang4/HaP">https://github.com/yztang4/HaP</a> 上公开我们的代码和数据。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>基于图像的人体重建：</p><ul><li>单视图：PIFu、ICON</li><li>多视图：<strong>DoubleField</strong>、<strong>DiffuStereo</strong></li></ul><p>已有大量基于优化的和基于学习的方法被提出用于<strong>单视图人体估计</strong></p><ul><li>基于优化：<ul><li>Detailed Human Shape and Pose from Images</li><li>Keep it SMPL</li><li>Estimating human shape and pose from a single image</li><li>Expressive Body Capture：i.e. <strong>SMPL-X</strong></li><li>Parametric reshaping of human bodies in images</li></ul></li><li>基于学习：<ul><li>Hierarchical Kinematic Human Mesh Recovery</li><li>Coherent Reconstruction of Multiple Humans From a Single Image</li><li>End-to-end Recovery of Human Shape and Pose</li><li>Learning to Reconstruct 3D Human Pose and Shape via Model-Fitting in the Loop</li></ul></li></ul><p>近年来，由于深层隐式函数(<em>Learning Implicit Fields for Generative Shape Modeling，Occupancy Network，DeepSDF</em>)在表示具有精细几何细节的无约束拓扑三维形状方面的效率和灵活性，像素对齐的隐式重建管道 PIFu, PIFuHD 因其能够恢复穿著人体的高保真几何形状而成为主导的处理范式。<strong>然而，由于缺乏人体先验，这些方法通常会过度拟合有限的训练数据，在面对看不见的姿势/服装时，会产生退化的身体结构和断肢</strong>。</p><p>为了克服这些限制，最近的趋势 ICON，PaMIR 是通过引入参数化人体模型(如 SMPL[30]，SMPL-X[41])作为显式形状先验来约束全隐式学习过程。<strong>然而，这种隐式重构和显式正则化的结合也会带来新的问题</strong></p><p>通常，从输入图像像素推断的特征或中间结果与估计的参数模型的<strong>融合会导致几何过平滑</strong>，<strong>从而削弱表面细节</strong>。更重要的是，所选择的参数化模板模型与目标人体形状(特别是宽松的服装或复杂的配饰)之间的<strong>拓扑不一致通常会导致过度约束效果</strong>。尽管最近 ECON[58]努力探索更好的方法来共同利用隐式域的表示能力和显式体模型的鲁棒性，但参数体模型 SMPL-X[41]和法线映射的不满意估计仍然会严重降低重建质量。</p><p><strong>在实践中，在评价人体全再造管道的潜力和优越性时，有几个方面的关键考虑因素</strong>:<br>1)灵活性，可以对任意穿着的人体形状进行无约束拓扑建模。<br>2)对看不见的数据分布(例如，新姿势和衣服)的泛化性。<br>3)鲁棒性，避免非人类重建的结果(例如，不自然的姿势，退化的身体结构，断肢)。<br>4)在三维空间中表现和捕捉表面几何细节的能力。</p><p>在本文中，我们试图开发一种基于学习的单视图人体重建框架，该框架同时满足上述关键要求，与以前的方法相比，它是一种更有前途的处理范式，具有更大的潜力。在架构上，我们构建了 HaP，这是一个完全显式的基于点的建模管道，其特点是在显式 3D 几何空间中直接进行点云估计(来自 2D 图像平面)，操作，生成和细化。<br>更具体地说，给定输入的 RGB 图像，我们的过程从深度估计开始，这使我们能够推断出代表高保真可视几何形状的部分 3D 点云。同时，我们还从二维图像空间估计 SMPL 模型，以提供缺失的人体信息。在估计深度图基本准确的前提下，通过专门的 SMPL 校正程序，进一步利用之前深度推断的部分 3D 点进行位姿校正，<strong>使估计的 SMPL 模型与 z 轴方向的点配准良好</strong>。将深度推断的部分点的三维信息与估计的 SMPL 模型直接合并，可以粗略地形成具有丰富纹理和皱纹细节的完整的三维人体点云;<strong>然而，部分点与估计的 SMPL 模型之间仍然存在明显的差距</strong>，这使得它们不自然并且可能存在问题。<br>为此，我们定制了一个扩散式的点云生成框架，在粗合并三维人体点的基础上学习真实人体的潜在空间分布，目的是生成与深度推导的部分三维点和 SMPL 模型相一致的服装和姿态信息的三维人体。此外，我们提出了一个细化阶段，包括基于学习的流量估计和简单有效的深度替换策略，以进一步提高几何质量。最后，使用典型的曲面重建方法，如筛选泊松曲面重建[23]，可以方便地从得到的点云中获得高质量的网格模型。</p><ul><li>我们采用点云作为中间表示，它对于建模任意拓扑几何结构是<strong>灵活的</strong></li><li>从二维图像平面推断粗糙三维几何信息的过程包括<strong>二维深度估计</strong>和<strong>SMPL 估计</strong>。前者作为一项研究丰富、较为成熟的任务，具有较好的<strong>通用性</strong>。后者通过注入人体先验而不破坏从特定输入图像中挖掘的服装线索来提高重建<strong>鲁棒性</strong></li><li>在原三维几何空间中直接实现基于点的学习过程，姿态校正和表面细节捕捉更为<strong>直观有效</strong></li></ul><p>贡献：</p><ul><li>我们提出了一个新的管道，HaP，通过在三维空间中的深度图和 SMPL 模型条件下的<strong>扩散过程</strong>来明确地在点云中生成人体;</li><li>我们提出了一个有效的 SMPL 校正 <em>rectification</em> 模块来优化 SMPL 模型以获得准确的姿态;</li><li>我们提供了一个包含高保真度 3D 人体扫描的新数据集，以促进未来的研究工作，如图 2 所示。</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231107194805.png" alt="image.png|666"></p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>Monocular Depth Estimation</p><ul><li><strong>Predicting depth maps</strong> from single-view RGB images is a challenging task,<ul><li>Make3D[46]将图像分割成均匀的小块，并利用马尔科夫随机场推断平面参数，从而捕获每个小块在三维空间中的位置和方向。</li><li>Eigen 等人提出了一种开创性的基于深度学习的方法来学习 RGB 图像到深度图的端到端映射。(<em>Depth Map Prediction from a Single Image using a Multi-Scale Deep Network</em>)</li><li>BTS[26]设计了新颖的局部平面制导层，以便在解码的多个阶段更有效地利用密集编码的特征(<em>From big to small: Multi-scale local planar guidance for monocular depth estimation</em>)</li><li>P3Depth[39]迭代和选择性地利用共面像素的信息来提高预测深度的质量</li><li>Xie 等人发现，蒙面图像建模(MIM)在深度估计方面也可以达到最先进的性能(Revealing the Dark Secrets of Masked Image Modeling)</li></ul></li><li>Particularly, various methods have been proposed for <strong>estimating human depth maps</strong> from single-view images.<ul><li>Tang 等人[51]采用分割网和骨架网生成人体关键点和身体部位热图。这些热图被用来通过深度网计算基本形状和细节形状。Tan 等人提出了一种利用视频中预测的 SMPL 模型的自监督方法。(<em>Self-Supervised Human Depth Estimation from Monocular Videos</em>)</li><li>HDNet[20]预测每张图像对应的密集姿态，而不是 SMPL 模型(<em>Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos</em>)</li><li>Tan et al.[50]和 Jafarian et al.[20]都将身体部位扭曲到视频的不同帧中，并对深度图进行监督，这导致了照片一致性的损失，因为深度图在不同帧之间不会有太大的变化</li></ul></li></ul><p>Point Cloud Generation 点云生成任务可以大致分为两类:无条件和条件</p><ul><li>Unconditioned<ul><li>RGAN[1]是一种经典的无条件点云生成方法，它使用具有多个 mlp 的 GAN 结构来对抗性地生成点云</li><li>WarpingGAN[53]是一个轻量级且高效的网络，它将多个均匀的 3D 网格扭曲成各种分辨率的点云</li><li>ShapeGF[4]首先引入基于分数的网络无条件生成点云，它以噪声为输入，旨在学习三维形状的隐式表面</li></ul></li><li>Conditioned 条件生成任务通常从 RGB 图像[11]、文本信息[36]或部分点云[31]、[37]，[63]中生成点云<ul><li>Fan[11]等人提出了一种 PointOutNet，在 RGB 图像条件下预测多个可信的点云</li><li>Point-E[36]首先使用文本到图像的扩散模型合成单视图 RGB 图像，然后使用另一种扩散模型基于生成的图像生成点云。</li><li>VRCNet[37]生成以局部点云为条件的点云，它将补全任务视为一个关系推理问题，学习对点之间的空间关系进行推理，从而补全缺失的部分</li><li>PVD[63]和 PDR[31]是基于扩散的网络，它们也在条件情景下运行，以部分点云为条件补全缺失部分</li></ul></li></ul><p>在本文中，HaP 将重建任务作为一个条件点云生成任务，<strong>依赖于 RGB 图像估计的深度图和 SMPL 模型作为条件</strong>。</p><p>Human Pose and Shape Estimation</p><ul><li>估计 2D 关键点 OpenPose[6]、AlphaPose[12]或人类 parsing：Beyond Appearance[8]、Deep Human Parsing with Active Template Regression[27]可以提供关于人类结构的有价值的信息，然而，它们缺乏精确三维重建所需的空间信息。</li><li>从 RGB 图像中预测参数模型，如 SMPL(-X)[30]，可以更好地理解人体姿势和形状，作为人体重建的先验信息</li><li>Zhang 等人 PyMAF[61]提出了一种基于回归的方法，该方法使用特征金字塔，通过对齐 SMPL 网格和图像来校正预测参数</li><li>PIXIE[13]引入了一个主持人，它根据专家的置信度权重合并了他们的身体、面部和手的特征</li><li>Kocabas 等人提出了一种对抗性学习框架 VIBE，用于从视频中学习运动学 SMPL 运动序列</li><li>METRO[28]尝试对顶点-顶点和顶点-关节的相互作用进行建模，最终输出人体关节坐标和 SMPL 顶点。<strong>然而，预测的 SMPL(-X)[30]，[41]模型在三维空间观测时有时会出现不对准问题</strong></li></ul><p>为了解决这个问题，<strong>我们提出了一种新的 SMPL 校正模块</strong>，该模块可以最大限度地减少 SMPL 模型的可见分区与深度推导的部分点云(被认为是准确的)在 3D 空间中的距离，我们生成的 SMPL 模型更准确，更符合给定人体。</p><p>Single-View Human Reconstruction</p><ul><li>SiCloPe[35]以 RGB 图像为条件训练生成对抗网络，并合成一致的轮廓来生成纹理三维网格</li><li>Ma et al.[32]提出了一种 POP 模型，用于生成三维点云的衣层参数模型</li><li>PIFu[44]是一项开创性的工作，将隐式函数引入人类重建任务。</li><li>PIFuHD[45]通过采用前后法线贴图训练的多层次架构来增强高分辨率图像的性能</li><li>Xiu et al.ICON[59]，PaMIR[62]，也使用 SMPL 模型来提供关于人体的先验信息</li><li>Xiu 等人提出了 ECON 来隐式和显式地解决重建问题。他们从学习到的正面和背面法线贴图中重建 2.5D 网格，最后使用隐式函数和筛选的泊松方法实现全网格。<strong>然而，ECON 仍然依赖于基于渲染的 SMPL 细化方法，限制了其性能</strong></li><li>Han 等人(<strong>2K2K</strong>)引入了一种基于不同身体部位学习深度图的两阶段深度估计网络。在获得正面和背面深度图后，将其转换为点云，并使用筛选后的泊松方法重建人体。<strong>尽管该方法遵循基于显式的方法，但其主要重点是学习二维平面上的精确深度图</strong>。</li><li>Tang et al.[52]设计了一个两阶段的三维卷积网络来明确学习人体的 TSDF 体积。(<em>High-Resolution Volumetric Reconstruction for Clothed Humans</em>)</li><li>已经提出了[5]，[20]，[56]，[57]等多种方法来探索深度在三维重建中的潜力。<ul><li>Wang[54] 等人训练了一个对抗网络<strong>NormalGAN</strong>来对 RGB-D 相机采集的深度进行降噪，并生成后视图的几何细节。</li><li>Xiong et al.[57]提出了一种深度引导自监督学习策略<strong>PIFu for the Real World</strong>，该策略使用有符号距离函数(signed distance function, SDF)值代替占用值来学习隐式曲面。</li><li>Chan[7] 等人<strong>IntegratedPIFu</strong>设计了一种面向深度的采样方案来学习高保真的几何细节。他们还提出了一种具有少量参数的高分辨率积分器来学习 RGB 图像的全局信息。</li><li>[7]，[57]这些方法通常利用网络来预测给定 RGB 图像的法线图和深度图。Xiong et al.[57]和 Chan et al.[7]都利用深度图作为隐式函数二维平面上的额外特征。</li></ul></li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>提出的单视图三维人体重建框架的整体处理流程包括两个子任务:<br>(1)从输入的二维 RGB 图像中推断和校正三维几何信息;<br>(2)根据生成的三维几何信息生成高质量的三维人体点云，然后直接进行网格重建。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231107193647.png" alt="image.png|666"></p><p>如图 3 所示，流程：</p><ul><li>HaP 从<strong>深度和 SMPL 估计</strong>的两个平行分支开始，这两个分支在推断人体几何的三维信息方面显示出互补的特征。具体来说，深度估计能够捕获前视图的详细几何图案，但需要额外的努力来恢复看不见的身体部位;不同的是，估计的 SMPL 提供了强大的人体先验，避免了非人类结构(例如，不自然的姿势，退化的身体，断裂的肢体)，但存在固定拓扑和过度光滑的表面细节。因此，我们有动力共同利用这两个三维信息来源。特别地，我们提出了一个基于优化的 SMPL 校正 Rectification 模块，将初始估计的 SMPL 模型与深度推导的部分三维点云对齐。</li><li>在将深度和 SMPL 的几何信息统一为三维点云之后，我们提出了一种<strong>条件点扩散模型</strong>，该模型将深度和 SMPL 的几何信息同时作为输入，以产生更精确的完整人体的三维点云。</li><li>此外，在推理过程中，探索另一个细化阶段，以进一步提高得到的三维点云的质量。</li><li>最后，我们使用经典的表面重建算法[23]直接提取网格表示。<em>Screened poisson surface reconstruction</em></li></ul><h2 id="Estimating-3D-Information-from-Single-2D-Images"><a href="#Estimating-3D-Information-from-Single-2D-Images" class="headerlink" title="Estimating 3D Information from Single 2D Images"></a>Estimating 3D Information from Single 2D Images</h2><p>从单个 RGB 图像 $\mathcal{I}$ 中：深度估计+SMPL 估计，为了解决 P 和 S 估计对不齐，假设深度估计通常具有较高的精度，提出了一种新的 SMPL rectification 模块</p><ol><li><p>Depth estimation: 使用最新的最先进的单目深度估计器 MIM <a href="https://readpaper.com/paper/4628144371611484161">56</a> 来预测图片 $\mathcal{I}$ 的相应深度图，然后我们根据相机的内在参数将每个 2D 像素坐标(u, v)的深度值投影到 3D 点(x, y, z)。此外，我们还附加了每个像素的原始 RGB 值，推导出一个彩色的三维点，记为 $\mathbf{p} = (x, y, z, r, g, b)$。这样，通过屏蔽掉无效的背景像素，我们可以得到一个三维部分人体点云，记为 $\mathcal{P}=\{\mathbf{p}\}.$。</p></li><li><p>SMPL Estimation and Rectification: 为了提供明确的人体形状先验，并补充部分深度推断的点云 P 中缺失的身体部位，与[13]，PyMAF[61]类似，我们从 $\mathcal{I}$ SMPL 模型中估计 S。然而，正如之前的研究 PaMIR[62]所分析的那样，由于 z 轴深度模糊，不可避免地会导致 S 和 P 之间的不对齐，特别是在单图像设置下，这会直接降低整个人体重建管道的质量。为了解决这一问题，<strong>假设深度估计通常具有较高的精度</strong>，因此可以以令人满意的质量推断 P，我们提出了一种新的 SMPL rectification 模块，<strong>通过更新 SMPL 参数进一步促进 S 与 P 的对齐</strong>。</p></li></ol><h3 id="SMPL-rectification"><a href="#SMPL-rectification" class="headerlink" title="SMPL rectification"></a>SMPL rectification</h3><p>SMPL 估计出来的点云：$\mathcal{S}=\mathcal{S}_{v}\cup\overline{\mathcal{S}}_{v}$</p><ul><li>$\mathcal{S}_v=(\mathcal{V}_v,\mathcal{F}_v)$ 可见部分</li><li>$\begin{aligned}\overline{\mathcal{S}}_v=(\overline{\mathcal{V}}_v,\overline{\mathcal{F}}_v)\end{aligned}$ 不可见部分</li></ul><p>修正后的 SMPL 点云 $\widehat{\mathcal{S}}$<br>一般情况下，以最小化 $\mathcal{S}_{v}$ 与 P 之间的距离为目标，<strong>通过对 S 的形状参数β和位姿参数θ进行迭代校正即可实现该问题</strong>。</p><ul><li>为了避免不可见部分不自然地靠近 P 而导致位姿不自然的情况，我们增加了正则化项来提高 P 与 $\mathcal{S}_{v}$ 之间的距离。</li><li>我们还正则化形状参数β，由于其固有的零均值性质，如 Keep it SMPL[3]中提出。</li><li>优化还受到两个约束:<ul><li>(1) $\widehat{\mathcal{S}}$ 的 2D 关键点与 S 的不一致，避免了 $\widehat{\mathcal{S}}$ 在二维平面上姿态的过度变化;</li><li>(2) $\widehat{\mathcal{S}}$ 的掩模(即 $\mathcal{M}_{\widehat{\mathcal{S}}}$)投影到二维平面上时与 ${\mathcal{I}}$ (即 $\mathcal{M}_{h}$)的人体掩模之间的轮廓差异，保持了 $\widehat{\mathcal{S}}$  的整体人体形状。</li></ul></li></ul><p>总之，我们明确地将问题表述为:</p><script type="math/tex; mode=display">\begin{aligned}\operatorname*{min}_{\boldsymbol{\beta},\boldsymbol{\theta}}\mathcal{L}_{\mathrm{r}1}=& \lambda_{1} \text{Р2F}(\mathcal{P},~\mathcal{S}_v)+\lambda_2\text{СD}(\mathcal{V}_v,\mathcal{P})  \\&-\lambda_3\text{P2F}(\mathcal{P},\overline{\mathcal{S}}_v)+\lambda_4\beta, \\&s.t.\mathcal{K}=\mathcal{K}_{0},\mathcal{M}_{\widehat{\mathcal{S}}}=\mathcal{M}_{h},\end{aligned}</script><p>式中，P2F(·，·)和 CD(·，·)分别表示点到面距离(P2F)和倒角距离(CD) $\mathcal{K}\mathrm{~and~}\mathcal{K}_0$ 分别为每次迭代和 S 的 SMPL 2D 关键点;非负超参数λ1， λ2， λ3 和λ4 平衡了不同的正则化项。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231108165047.png" alt="image.png|666"><br>SMPL rectification process.<br>(a) Initially estimated SMPL model.<br>(b) Registration before rectification.<br>(c) Rectification process.<br>(d) Registration after rectification.<br>直接估计的 SMPL 模型与部分点云的配准不太好，经过整改后情况得到明显缓解。</p><h2 id="Diffusion-based-Explicit-Generation-of-Human-Body"><a href="#Diffusion-based-Explicit-Generation-of-Human-Body" class="headerlink" title="Diffusion-based Explicit Generation of Human Body"></a>Diffusion-based Explicit Generation of Human Body</h2><p>Depth-deduced 的部分人体点云 P 提供了自由而精细的几何结构，如衣服正面的褶皱，而完全忽略了背面和遮挡部分。而修正后的 SMPL 参数化人体形状面 $\widehat{S}$ 则保证了完整的体表，姿态和形状合理，但其固定的拓扑结构在处理宽松复杂的服装时造成了本质上的困难。这些互补的特性促使我们共同利用 P 和 $\widehat{S}$ 来重建高保真的三维人体几何。然而，直接将深度 deduced 和 SMPL-deduced 的三维几何信息合并在一起可能会有问题，因为 P 和 $\widehat{S}$ 之间通常存在不可忽略的间隙。</p><p>为此，我们进一步提出训练条件去噪扩散概率模型(conditional denoising diffusion probabilistic model, DDPM)来生成高质量的3D 人体点云，目的是将 P 和 $\widehat{S}$ 自然融合并消除间隙。扩散模型的主干是一个双分支的 PointNet++体系结构 <a href="https://readpaper.com/pdf-annotate/note?pdfId=4665141791788384257">31</a>，<a href="https://readpaper.com/pdf-annotate/note?pdfId=4545028882578432001">42</a> 参数化为 $\Theta_{1}$，表示为 $\operatorname{PNet}_{\Theta_1}(\cdot)$。<br>在扩散阶段之后，我们进一步提出了细化阶段来提高生成点云的质量。具体来说，我们使用与 $\Theta_{2}$ (即 $\operatorname{PNet}_{\Theta_2}(\cdot)$)参数化的相同架构，<strong>通过学习每个点的位移来细化生成的人体点云</strong>。<br>然后采用深度替换策略<em>depth replacement strategy</em>对生成的点云进行密度化处理。</p><ol><li>Diffusion Stage: 在这个阶段，我们训练一个<strong>条件扩散模型</strong>来生成一个人体点云。</li></ol><p>由于有限的 GPU 内存和计算时间的限制，我们配置 $\operatorname{PNet}_{\Theta_1}(\cdot)$ 来生成一个具有 10,000 个点的相对稀疏的 $\mathcal{H}_{\mathrm{coarse}}$。在训练过程中，我们使用最远点采样操作分别从 $\mathcal{P}\mathrm{~and~}\widehat{\mathcal{S}}$ 中采样点，并将其作为扩散模型的条件。一个条件 DDPM 由两个相反方向的马尔可夫链组成，即正向过程和反向过程，这两个过程的采样步长 T 相同。在前向过程中，在每个采样步长 t 处加入高斯噪声 $\mathcal{H}_{\mathrm{gt}}$，当 t 足够大时，前向过程的输出应与高斯分布 $\mathcal{N}(\mathbf{0},\boldsymbol{I})$ 接近，前向过程从初始步长 $\mathcal{H}_{\mathbf{gt}}^{\boldsymbol{0}}$ 到最后一步步长 $\mathcal{H}_{\mathrm{gt}}^T.$。注意，条件 $(\mathcal{P},\widehat{\mathcal{S}})$ 不包括在正向过程中。<br>前向的采样过程是迭代的，但是我们不能在每个时间步都训练 DDPM，因为这需要花费大量的时间。根据 DDPM[18]，我们可以通过定义 $\begin{aligned}1-\gamma_t=\alpha_t,\bar{\alpha_t}=\prod_{i=1}^t\alpha_i,\end{aligned}$ 来实现任意步长 $\mathcal{H}_{\mathrm{gt}}^t$，其中γ是一个预定义的超参数。因此，公式 $q(\mathcal{H}_{\mathrm{gt}}^t|\mathcal{H}_{\mathrm{gt}}^0)=\mathcal{N}(\mathcal{H}_{\mathrm{gt}}^t;\sqrt{\bar{\alpha}_{t}}\mathcal{H}_{\mathrm{gt}}^0,(1-\bar{\alpha}_{t})\boldsymbol{I}),$，和 $\mathcal{H}_{\mathrm{gt}}^t$ 可以通过: $\mathcal{H}_{\mathrm{gt}}^{t}=\sqrt{\bar{\alpha_{t}}}\mathcal{H}_{\mathrm{gt}}^{0}+\sqrt{1-\bar{\alpha_{t}}}\boldsymbol{\epsilon},$<br>其中ε为高斯噪声。利用 DDPM[18]中提出的参数化技术，我们可以通过最小化来训练 $\Theta_{1}$：<br>$\mathcal{L}_{\mathrm{ddpm}}=\mathbb{E}_{t,\mathcal{H}_{\mathrm{gt}}^{t},\epsilon}|\epsilon-\mathrm{PNet}_{\Theta_{1}}(\mathcal{H}_{\mathrm{gt}}^{t},\mathcal{P},\widehat{\mathcal{S}},t)|^{2}$，PointNet++以 $\mathcal{P}\mathrm{~and~}\widehat{\mathcal{S}}$ 作为条件来预测 $\mathcal{H}_{\mathrm{gt}}^t$ 中的噪声</p><p>在训练过程中，每个时间步骤都将 $\mathcal{P}\mathrm{~and~}\widehat{\mathcal{S}}$ 作为条件，对服装信息和人体信息进行监督。在推理过程中，我们按照相反的过程生成 $\mathcal{H}_{\mathrm{coarse}}$，即从 T 开始，从 $\mathcal{H}_{\mathrm{gt}}^t$ 逐步采样 $\mathcal{H}_{\mathrm{gt}}^{t-1}$。生成的 $\mathcal{H}_{\mathrm{coarse}}$ 与提供的 P 具有相同的服装信息，其姿态与 $\widehat{\mathcal{S}}$ 提供的姿态保持一致。</p><ol><li>Refinement Stage:</li></ol><p>在推理过程中，我们观察到 $\operatorname{PNet}_{\Theta_1}(\cdot)$ 通常不可能消除 $\mathcal{H}_{\mathrm{coarse}}$ 中的所有噪声。因此，我们只能从 $\mathcal{H}_{\mathrm{coarse}}$ 中恢复一个粗糙的表面。为了克服这个问题，我们用 $\Theta_{2}$ 参数化 $\operatorname{PNet}_{\Theta_2}(\cdot)$ 来训练学习 $\mathcal{H}_{\mathrm{coarse}}$ 中每个点的位移，即 $\Delta=\mathrm{PNet}_{\Theta_2}(\mathcal{H}_{\mathrm{coarse}},\mathcal{P},\widehat{\mathcal{S}}),$，以光滑 $\mathcal{H}_{\mathrm{coarse}}$ 的表面。我们用 $\mathcal{H}=\mathcal{H}_{\mathrm{coarse}}+\Delta.$ 到人体点云 $\mathcal{H}$。我们把这个问题表述为：<br>$\min_{\Theta_2}\text{D}(\mathcal{H},\mathcal{H}_{\mathrm{gt}})+\alpha\text{R}_{\mathrm{smooth}}(\Delta),$</p><ul><li>D(·,·)表示 <a href="https://readpaper.com/paper/4762319849401614337">43</a> 实现的距离度量，</li><li>$R_{smooth}(·)$ 是空间平滑正则化项，表示为: $\mathrm{R}_{\mathrm{smooth}}(\Delta)=\frac1{3N_{\mathrm{src}}K_s}\sum_{x\in\mathcal{H}_{\mathrm{coarse}}}\sum_{x^{\prime}\in\mathcal{N}(x)}|\delta_x-\delta_{x^{\prime}}|_2^2,$<ul><li>式中 N(·)返回 the K-NN(k-nearest neighbors) points，$\delta_{x}\in\Delta$ 为一个典型点的位移。</li></ul></li></ul><p>此外，在前面的扩散阶段，我们对一组相对稀疏的 3D 点进行采样，作为 DDPM 的输入，以节省内存和计算成本，这决定了 $\dot{\mathcal{H}_{\mathrm{coarse}}}$ 的稀疏性，与 $\mathcal{H}$ 类似，为了产生具有增强几何细节的密集点云，我们专门采用了一种简单而有效的深度替换策略，该策略充分利用了前面深度推断的部分点云 P 的密度，如图 5 所示。我们的策略首先使用球查询操作 PointNet++[42]来定位 P 在 $\mathcal{H}$ 内的最近点;这些最近的点随后被移除并记为 $s_{1}$。在此基础上，我们确定了 P 内 $\mathcal{H}−s_{1}$ 的最近点，并将其命名为 $s_{2}$。最后，我们确定 P 内离 $s_2$ 最近的点，并将它们标记为 $s_{3 }$。最终点云记为 $\mathcal{H}_{\mathrm{final}}$，由 $\mathcal{H}-s_1$ 和 $s_{3}$ 组合而成。<br>TLDR. 从 H 中选取离 P 最近的点 $s_{1}$，然后从 P 中选取离 $s_{1}$ 最近的点 $s_{2}$ ，再从 P 中选取离 $s_{2}$ 最近的点 $s_{3}$ ，最后将 $s_{3}$ 与 $\mathcal{H}-s_1$ 组合起来</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231108220043.png" alt="image.png|666"></p><p>在得到替换后的点云 $\mathcal{H}_{\mathrm{final}}$ 后，我们直接采用筛选过的泊松[23]进行网格提取。</p><h1 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h1><p>我们为 CityUHuman 数据集新收集的人体模型显示出相对更平滑和更精确的表面结构</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231108221650.png" alt="image.png|666"></p><p>(a) 本文 CityUHuman<br>(b) Thuman</p><p>具体来说，我们使用名为 Artec Eva2 的高精度便携式 3D 扫描仪获得了高质量的 3D 人体扫描。每次扫描的完整集合包括超过 1000 万个点。我们一共邀请了 10 个志愿者，每个志愿者展示了大约 10 个不同的姿势。秉承道德原则和尊重个人信息的所有权，我们在收集数据之前获得了每位志愿者的同意。志愿者被告知，他们的 3d 扫描数据将专门用于非商业研究应用。<strong>为了保护志愿者的隐私，我们恳请用户在任何准备发表的材料中对面部进行模糊处理</strong>，比如论文、视频、海报等等。我们从图 2 的数据集中提供了大约 60 个不同衣服和姿势的样本扫描，以展示其在研究中的质量和潜力</p><p>1)训练数据:在我们的实验中，我们统一采用 Thuman2.0[60]对我们提出的 HaP 学习架构进行训练。一方面，我们从每个模型中提取 10000 个三维点作为地面真点云。另一方面，我们使用了 Blender 软件来渲染真实的深度图和 photo-realistic 的图像，间隔为 10 度。因此，对于每个作为输入的 RGB 图像，我们有三个方面的监督信号，即深度图，smpl 推导的点云和人的点云。得到的训练样本总数为 18000。特别是，为了与 IntegratedPIFu[7]进行公平的比较，我们进一步从我们的训练样本中创建了一个子集，通过保留 IntegratedPIFu 训练过程中使用的相同扫描索引来训练我们的 HaP，同时从每次扫描中均匀地选择 10 个视图。<br>2)测试数据:我们从 Thuman3.0[49]和我们收集的 CityUHuman 数据集中生成测试数据，以评估不同方法的性能。我们使用 8 个视图在偏航轴上每 45 度渲染一次扫描，然后从 Thuman3.0 中随机选择 459 张 RGB 图像，从 CityUHuman 中随机选择 40 张 RGB 图像。此外，我们还从互联网上收集了几张真实的图像，并推断出相应的人体。</p><h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><p>所采用的深度估计方法 MIM 以分辨率为 512 × 512 的 RGB 图像为输入。在 MIM 的训练过程中，我们采用了 swin-v2-base 架构[29]作为骨干。学习率设置为 0.3 × 10e−4，批大小设置为 8,epoch 数设置为 30。其余参数与原 MIM 中的参数保持一致。训练在 4 块 RTX 3090 Ti gpu 上进行。</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了一种新的基于点的学习管道，用于单视图三维人体重建。在从 2D 图像域粗略推断 3D 信息之后，<strong>我们的方法的特点是在原始 3D 几何空间中进行显式的点云操作，生成和细化</strong>。与以往主流的隐式基于场的方法相比，该方法具有高度的灵活性、通用性、鲁棒性和精细几何细节建模能力。大量的实验证明了我们的 HaP 优于目前最先进的方法。<strong>然而，必须承认，在处理 RGB 图像时，HaP 的性能仍然取决于深度估计模块和 SMPL 估计模块的有效性</strong>。因此，我们未来的努力将集中在提高这些关键模块的性能上</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Hybrid Methods </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CCD-3DR</title>
      <link href="/3DReconstruction/Single-view/Generative%20Models/CCD-3DR/"/>
      <url>/3DReconstruction/Single-view/Generative%20Models/CCD-3DR/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>CCD-3DR: Consistent Conditioning in Diffusion for Single-Image 3D Reconstruction</th></tr></thead><tbody><tr><td>Author</td><td>Yan Di1, Chenyangguang Zhang2, Pengyuan Wang1, Guangyao Zhai1, Ruida Zhang2, Fabian Manhardt3, Benjamin Busam1, Xiangyang Ji2, and Federico Tombari1,3</td></tr><tr><td>Conf/Jour</td><td>arXiv</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4789424767274844161&amp;noteId=2014146864821066240">CCD-3DR: Consistent Conditioning in Diffusion for Single-Image 3D Reconstruction (readpaper.com)</a></td></tr></tbody></table></div><p>No Code<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021115606.png" alt="image.png|666"></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文提出了一种利用扩散模型对<strong>单幅RGB图像</strong>中捕获的物体生成三维稀疏点云的形状重建方法。最近的方法通常利用全局嵌入或基于局部投影的特征作为指导扩散模型的条件。然而，这种策略不能将去噪点云与给定图像一致对齐，导致条件不稳定，性能较差。在本文中，我们提出了CCD-3DR，它利用了一种新的中心扩散概率模型来进行一致的局部特征条件反射。我们将扩散模型中的噪声和采样点云约束到一个子空间中，在这个子空间中，点云中心在正向扩散过程和反向扩散过程中保持不变。稳定的点云中心进一步充当锚，将每个点与其相应的基于投影的局部特征对齐。在合成基准ShapeNet-R2N2上进行的大量实验表明，CCD-3DR的性能大大优于所有竞争对手，改进幅度超过40%。我们还提供了实际数据集Pix3D的结果，以彻底展示CCD3DR在实际应用中的潜力。代码将很快发布</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Generative Models </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learn-Algo</title>
      <link href="/Learn/Learn-Algo/"/>
      <url>/Learn/Learn-Algo/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.hello-algo.com/">Hello 算法 (hello-algo.com)</a></p></blockquote><span id="more"></span><h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><p>按逻辑结构分类</p><ul><li><strong>线性结构</strong>：数组、链表、队列、栈、哈希表，<strong>元素之间是一对一的顺序关系</strong>。</li><li><strong>树形结构</strong>：树、堆、哈希表，<strong>元素之间是一对多的关系</strong>。</li><li><strong>网状结构</strong>：图，<strong>元素之间是多对多的关系</strong>。</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231107163456.png" alt="image.png|666"></p><p>按物理结构分类<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231107163328.png" alt="image.png|666"></p><p><strong>所有数据结构都是基于数组、链表或二者的组合实现的</strong>。例如，栈和队列既可以使用数组实现，也可以使用链表实现；而哈希表的实现可能同时包含数组和链表。</p><ul><li><strong>基于数组可实现</strong>：栈、队列、哈希表、树、堆、图、矩阵、张量（维度 ≥3 的数组）等。<strong>静态数据结构</strong></li><li><strong>基于链表可实现</strong>：栈、队列、哈希表、树、堆、图等。<strong>动态数据结构</strong></li></ul><p>计算机编码</p><ul><li>整数在计算机中是以补码的形式存储的。在补码表示下，计算机可以对正数和负数的加法一视同仁，不需要为减法操作单独设计特殊的硬件电路，并且不存在正负零歧义的问题。</li></ul><p>补码的补码是原码 </p><blockquote><p><a href="https://www.zhihu.com/question/23164162">负数的补码怎么变回原码？ - 知乎 (zhihu.com)</a></p></blockquote><h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><p><a href="https://www.hello-algo.com/chapter_array_and_linkedlist/array/">4.1   数组 - Hello 算法 (hello-algo.com)</a></p><p>数组存储在连续的内存空间内，且元素类型相同。这种做法包含丰富的先验信息，系统可以利用这些信息来优化数据结构的操作效率。</p><p>数组典型应用</p><ul><li>随机访问：如果我们想要随机抽取一些样本，那么可以用数组存储，并生成一个随机序列，根据索引实现样本的随机抽取。</li><li>排序和搜索：数组是排序和搜索算法最常用的数据结构。快速排序、归并排序、二分查找等都主要在数组上进行。</li><li>查找表：当我们需要快速查找一个元素或者需要查找一个元素的对应关系时，可以使用数组作为查找表。假如我们想要实现字符到 ASCII 码的映射，则可以将字符的 ASCII 码值作为索引，对应的元素存放在数组中的对应位置。</li><li>机器学习：神经网络中大量使用了向量、矩阵、张量之间的线性代数运算，这些数据都是以数组的形式构建的。数组是神经网络编程中最常使用的数据结构。</li><li>数据结构实现：数组可以用于实现栈、队列、哈希表、堆、图等数据结构。例如，图的邻接矩阵表示实际上是一个二维数组。</li></ul><p>当数组非常大时，内存可能无法提供如此大的连续空间 —&gt; 链表的灵活性</p><h2 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h2><p><a href="https://www.hello-algo.com/chapter_array_and_linkedlist/linked_list/">4.2   链表 - Hello 算法 (hello-algo.com)</a></p><p>链表的组成单位是「节点 node」对象。每个节点都包含两项数据：节点的“值”和指向下一节点的“引用”。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化链表 1 -&gt; 3 -&gt; 2 -&gt; 5 -&gt; 4</span></span><br><span class="line"><span class="comment"># 初始化各个节点</span></span><br><span class="line">n0 = ListNode(<span class="number">1</span>)</span><br><span class="line">n1 = ListNode(<span class="number">3</span>)</span><br><span class="line">n2 = ListNode(<span class="number">2</span>)</span><br><span class="line">n3 = ListNode(<span class="number">5</span>)</span><br><span class="line">n4 = ListNode(<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 构建引用指向</span></span><br><span class="line">n0.<span class="built_in">next</span> = n1</span><br><span class="line">n1.<span class="built_in">next</span> = n2</span><br><span class="line">n2.<span class="built_in">next</span> = n3</span><br><span class="line">n3.<span class="built_in">next</span> = n4</span><br></pre></td></tr></table></figure><p><strong>通常将头节点当作链表的代称</strong>，比如以上代码中的链表可被记做链表 <code>n0</code> 。</p><p>数组与链表对比：</p><div class="table-container"><table><thead><tr><th></th><th>数组</th><th>链表</th></tr></thead><tbody><tr><td>存储方式</td><td>连续内存空间</td><td>分散内存空间</td></tr><tr><td>缓存局部性</td><td>友好</td><td>不友好</td></tr><tr><td>容量扩展</td><td>长度不可变</td><td>可灵活扩展</td></tr><tr><td>内存效率</td><td>占用内存少、浪费部分空间</td><td>占用内存多</td></tr><tr><td>访问元素</td><td>O(1)</td><td>O(n)</td></tr><tr><td>添加元素</td><td>O(n)</td><td>O(1)</td></tr><tr><td>删除元素</td><td>O(n)</td><td>O(1)</td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231122152228.png" alt="image.png|666"></p><p> 链表典型应用</p><ul><li>单向链表通常用于实现栈、队列、哈希表和图等数据结构。</li><li>双向链表常被用于需要快速查找前一个和下一个元素的场景。<ul><li>高级数据结构：比如在红黑树、B 树中，我们需要访问节点的父节点，这可以通过在节点中保存一个指向父节点的引用来实现，类似于双向链表。</li><li>浏览器历史：在网页浏览器中，当用户点击前进或后退按钮时，浏览器需要知道用户访问过的前一个和后一个网页。双向链表的特性使得这种操作变得简单。</li><li>LRU 算法：在缓存淘汰算法（LRU）中，我们需要快速找到最近最少使用的数据，以及支持快速地添加和删除节点。这时候使用双向链表就非常合适。</li><li>循环链表常被用于需要周期性操作的场景，比如操作系统的资源调度。</li><li>时间片轮转调度算法：在操作系统中，时间片轮转调度算法是一种常见的 CPU 调度算法，它需要对一组进程进行循环。每个进程被赋予一个时间片，当时间片用完时，CPU 将切换到下一个进程。这种循环的操作就可以通过循环链表来实现。</li><li>数据缓冲区：在某些数据缓冲区的实现中，也可能会使用到循环链表。比如在音频、视频播放器中，数据流可能会被分成多个缓冲块并放入一个循环链表，以便实现无缝播放。</li></ul></li></ul><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><p>「列表 list」是一个抽象的数据结构概念，它表示元素的有序集合，支持元素访问、修改、添加、删除和遍历等操作，无需使用者考虑容量限制的问题。列表可以基于链表或数组实现。</p><p>实际上，<strong>许多编程语言中的标准库提供的列表都是基于动态数组实现的</strong>，例如 Python 中的 <code>list</code> 、Java 中的 <code>ArrayList</code> 、C++ 中的 <code>vector</code> 和 C# 中的 <code>List</code> 等。在接下来的讨论中，我们将把“列表”和“动态数组”视为等同的概念。</p><h2 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h2><h1 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h1><h2 id="数据结构-1"><a href="#数据结构-1" class="headerlink" title="数据结构"></a>数据结构</h2><p>数组：Python 的 list 是动态数组，可以直接扩展</p>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DMTet</title>
      <link href="/3DReconstruction/Single-view/Generative%20Models/DMTet/"/>
      <url>/3DReconstruction/Single-view/Generative%20Models/DMTet/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis</th></tr></thead><tbody><tr><td>Author</td><td>Tianchang Shen and Jun Gao and Kangxue Yin and Ming-Yu Liu and Sanja Fidler</td></tr><tr><td>Conf/Jour</td><td>NeurIPS</td></tr><tr><td>Year</td><td>2021</td></tr><tr><td>Project</td><td><a href="https://research.nvidia.com/labs/toronto-ai/DMTet/">Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis (nvidia.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4556109173671075841&amp;noteId=2037631312002498560">Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231106171053.png" alt="image.png|666"></p><p>输入点云或低分辨率体素，提取特征后利用GAN网络，生成每个顶点的位置和SDF偏移值，得到优化后顶点的位置和SDF<br>结合显式与隐式表达的表示方法，利用MT，从隐式SDF中重建出显式mesh </p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们介绍了 DMTET，一个深度3D 条件生成模型，可以使用简单的用户指南(如粗体素)合成高分辨率3D 形状。它通过利用一种新的混合3D 表示结合了隐式和显式3D 表示的优点。与目前的隐式方法相比，<strong>DMTET 直接针对重建表面进行优化</strong>，使我们能够以更少的人工合成更精细的几何细节。与直接生成显式表示(如网格)的深度3D 生成模型不同，我们的模型可以合成具有任意拓扑结构的形状。DMTET 的核心包括一个可变形的四面体网格，它编码一个离散的符号距离函数和一个可微的移动四面体层，它将隐式的符号距离表示转换为显式的表面网格表示。这种组合允许表面几何和拓扑结构的联合优化，以及使用重建和在表面网格上明确定义的对抗损失来生成细分层次。我们的方法明显优于现有的粗糙体素输入条件形状合成的工作，这些工作是在复杂的3D 动物形状数据集上训练的。项目页面: <a href="https://nv-tlabs.github.io/DMTet/">https://nv-tlabs.github.io/DMTet/</a></p><p>贡献：</p><ul><li>我们表明，与之前的研究[31,45]的分析相比，使用行进四面体(MT)作为可微的等面层允许隐式场表示的底层形状的拓扑变化。</li><li>我们将 MT 合并到深度学习框架中，并引入 DMTET，这是一种结合隐式和显式表面表示的混合表示。我们证明了直接在从隐场提取的表面上定义的额外监督(例如倒角距离，对抗损失)提高了形状合成质量。</li><li>我们引入了一种从粗到精的优化策略，在训练期间将 DMTET 扩展到高分辨率。因此，在具有挑战性的三维形状合成任务中，我们比最先进的方法获得了更好的重建质量，同时需要更低的计算成本。</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="3D-Representation"><a href="#3D-Representation" class="headerlink" title="3D Representation"></a>3D Representation</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231106171053.png" alt="image.png|666"></p><ul><li>采用 DefTet：不同于 DefTet 用占用值表示, 本文用 SDF 表示 shape, SDF 由可变形的四面体网格编码</li><li>与八叉树思想类似，围绕预测表面细分四面体</li><li>使用 Marching Tetrahedra layer 将从 SDF 的隐式表示中提取三角形 mesh</li></ul><h3 id="Deformable-Tetrahedral-Mesh-as-an-Approximation-of-an-Implicit-Function"><a href="#Deformable-Tetrahedral-Mesh-as-an-Approximation-of-an-Implicit-Function" class="headerlink" title="Deformable Tetrahedral Mesh as an Approximation of an Implicit Function"></a>Deformable Tetrahedral Mesh as an Approximation of an Implicit Function</h3><p>将四面体表示为 $(V_T,T)$，$V_{T}$ 表示 4 个顶点 $\{v_{ak} , v_{bk} , v_{ck} , v_{dk}\}$，共有 T 个四面体<br>通过插值定义在网格顶点上的 SDF 值来表示符号距离域</p><h3 id="Volume-Subdivision"><a href="#Volume-Subdivision" class="headerlink" title="Volume Subdivision"></a>Volume Subdivision</h3><p>为了提高效率，我们用从粗到细的方式来表示形状。我们通过检查四面体是否具有不同 SDF 符号的顶点来确定表面四面体 $T_{surf}$ -表明它与 SDF 编码的表面相交。我们将 $T_{surf}$ 及其近邻进行细分，并通过向每个边缘添加中点来提高分辨率。我们通过平均边缘上的 SDF 值来计算新顶点的 SDF 值(图 2)。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231107094332.png" alt="image.png|333"></p><h3 id="Marching-Tetrahedra-for-converting-between-an-Implicit-and-Explicit-Representation"><a href="#Marching-Tetrahedra-for-converting-between-an-Implicit-and-Explicit-Representation" class="headerlink" title="Marching Tetrahedra for converting between an Implicit and Explicit Representation"></a>Marching Tetrahedra for converting between an Implicit and Explicit Representation</h3><p>我们使用 Marching Tetrahedra[15]算法将编码的 SDF 转换为显式三角形网格。<br>给定四面体顶点的 SDF 值 $\{s(v_a),s(v_b),s(v_c),s(v_d)\}$， MT 根据 s(v)的符号确定四面体内部的表面类型，如图3所示。构型的总数为 $2^4 = 16$，在考虑旋转对称性后可分为 3 种独特的情况。一旦确定了四面体内部的曲面类型，在沿四面体边缘的线性插值的零交点处计算等距曲面的顶点位置，如图 3 所示。</p><h3 id="Surface-Subdivision"><a href="#Surface-Subdivision" class="headerlink" title="Surface Subdivision"></a>Surface Subdivision</h3><p>有一个表面网格作为输出允许我们进一步增加表示能力和形状的视觉质量与一个可微分的表面细分模块。我们遵循循环细分方法[35]的方案，但不是使用一组固定的参数进行细分，而是使这些参数在 DMTET 中可学习。具体来说，可学习的参数包括每个网格顶点 $v_{i}^{\prime}$ 的位置，以及 $\alpha_{i}$， $\alpha_{i}$ 通过加权相邻顶点的平滑度来控制生成的表面。注意，与 Liu et al.[33]不同，我们只在开始时预测每个顶点参数，并将其带入后续的细分迭代，以获得更低的计算成本。</p><h2 id="DMTET-3D-Deep-Conditional-Generative-Model"><a href="#DMTET-3D-Deep-Conditional-Generative-Model" class="headerlink" title="DMTET: 3D Deep Conditional Generative Model"></a>DMTET: 3D Deep Conditional Generative Model</h2><p>目的：从输入 x(点云或粗体素化形状)输出高分辨率 3D 网格 M</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231107095201.png" alt="image.png|666"></p><h3 id="3D-Generator"><a href="#3D-Generator" class="headerlink" title="3D Generator"></a>3D Generator</h3><p><strong>Input Encoder</strong><br>点云：使用<strong>PVCNN</strong>[34]作为输入编码器，从点云中提取3D 特征体 $F_{vol}(x)$。<br>粗体素：我们在其表面采样点。我们通过三线性插值计算网格顶点 $v\in\mathbb{R}^3$ 的特征向量 $F_{vol}(v,x)$。</p><p><strong>Initial Prediction of SDF</strong><br>通过 MLP 预测每个顶点的 SDF：$s(v)=\boldsymbol{MLP}(F_{\boldsymbol{vol}}(v,x),v).$ 全连接网络还输出一个特征向量 f (v)，用于体细分阶段的表面细化</p><p><strong>Surface Refinement with Volume Subdivision</strong></p><p>Surface Refinement<br>在获得初始 SDF 后，迭代细化曲面并细分四面体网格。我们首先根据当前 s(v)值识别表面四面体 $T_{surf}$。然后我们建立一个图 $G=(V_{surf},E_{surf}),$，其中 $V_{surf},E_{surf}$ 对应于 $T_{surf}$ 中的顶点和边。然后，我们使用图形卷积网络 GCN[32]预测 $V_{surf}$ 中每个顶点 i 的位置偏移量 $\Delta v_{i}$ 和 SDF 残差值 $\Delta s(v_i)$</p><p>$\begin{array}{rcl}f_{v_i}^{\prime}&amp;=&amp;\operatorname{concat}(v_i,s(v_i),F_{vol}(v_i,x),f(v_i)),\end{array}$<br>$(\Delta v_i,\Delta s(v_i),\overline{f(v_i)})_{i=1,\cdots N_{surf}}\quad=\quad\mathrm{GCN}\big((f_{v_i}^{\prime})_{i=1,\cdots N_{surf}},G\big),$</p><p>通过 GCN，更新:</p><ul><li>$v’_i=v_i+\Delta v_i$</li><li>$s(v’_i)=s(v_i)+\Delta s(v_i).$</li><li>$f_{v_i}^{\prime} \to \overline{f(v_i)}$</li></ul><p>Volume Subdivision<br>在表面细化之后，我们执行体积细分步骤，然后执行附加的表面细化步骤。特别是，我们重新识别了 $T_{surf}$，并细分了 $T_{surf}$ 及其近邻。在这两个步骤中，我们都从完整的四面体网格中删除了未细分的四面体，这节省了内存和计算，因为 $T_{surf}$ 的大小与对象的表面积成正比，并且随着网格分辨率的增加呈二次而不是三次缩放。</p><p>注意，SDF 值和顶点的位置是从细分前的水平继承的，因此，在最终表面计算的损失可以反向传播到所有水平的所有顶点。因此，我们的 DMTET 自动学习细分四面体，并且不需要在中间步骤中添加额外的损失项来监督八叉树层次结构的学习，就像之前的工作 <a href="https://readpaper.com/paper/2949394278">Octree Generating Networks</a> 一样</p><p><strong>Learnable Surface Subdivision</strong><br>在 MT 提取表面网格后，我们可以进一步进行可学习的表面细分。具体来说，我们在提取的网格上构建一个新的图，并使用 GCN 来预测每个顶点的更新位置 $v_{i}^{\prime}$， $\alpha_{i}$ 用于循环细分。该步骤消除了量化误差，并通过调整 $\alpha_{i}$ 减轻了经典环路细分方法中固定的近似误差。</p><h3 id="3D-Discriminator"><a href="#3D-Discriminator" class="headerlink" title="3D Discriminator"></a>3D Discriminator</h3><p>我们在生成器预测的最终表面上应用三维鉴别器 D。我们的经验发现，使用 DECOR-GAN[6]的3D CNN 作为从预测网格计算的带符号距离域的判别器可以有效地捕获局部细节。具体来说，我们首先从目标网格中随机选择一个高曲率顶点 v，在 v 周围的体素化区域计算地面真符号距离场 $S_{real}\in\mathbb{R}^{N\times N\times N}$。同样，我们在同一位置计算预测表面网格 M 的符号距离场，得到 $S_{pred}\in\mathbb{R}^{N\times N\times N}$。请注意，$S_{pred}$ 是网格 M 的解析函数，因此 $S_{pred}$ 的梯度可以反向传播到 M 中的顶点位置。我们将 $S_{real}$ 或 $S_{pred}$ 输入鉴别器，以及位置 v 中的特征向量 $F_{vol}(\bar{v},x)$，鉴别器然后预测指示输入是来自真实形状还是生成形状的概率。</p><h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><ul><li><strong>a surface alignment loss</strong> to encourage the alignment with ground truth surface, <ul><li>$L_{\mathrm{cd}}=\sum\limits_{p\in P_{pred}}\min\limits_{q\in P_{gt}}||p-q||_2+\sum\limits_{q\in P_{gt}}\min\limits_{p\in P_{pred}}||q-p||_2,L_{\mathrm{normal}}=\sum\limits_{p\in P_{pred}}(1-|\vec{\mathbf{n}}_p\cdot\vec{\mathbf{n}}_{\hat{q}}|),$</li><li>从 GT 和 Pred 中分别采样一系列点 $P_{gt}$ 和 $P_{pred}$，计算两者之间的 L2 Chamfer Distance 和 normal consistency loss</li></ul></li><li><strong>an adversarial loss</strong> to improve realism of the generated shape, <a href="https://readpaper.com/paper/2949496494">LSGAN</a> 中提出<ul><li>$L_{\mathbf{D}}=\frac{1}{2}[(D(M_{gt})-1)^{2}+D(M_{pred})^{2}],L_{\mathbf{G}}=\frac{1}{2}[(D(M_{pred})-1)^{2}].$</li></ul></li><li><strong>regularizations</strong> to regularize the behavior of SDF and vertex deformations.<ul><li>上述损失函数作用于提取的曲面上，因此，在四面体网格中，只有靠近等面的顶点接收梯度，而其他顶点不接收梯度。此外，表面损失不能提供内部/外部的信息，因为翻转四面体中所有顶点的 SDF 符号将导致 MT 提取相同的表面。这可能导致训练过程中分离的组件。为了缓解这个问题，我们增加了一个 SDF 损失来正则化 SDF 值</li><li>$L_{\mathrm{SDF}}=\sum_{v_i\in V_T}|s(v_i)-SDF(v_i,M_{gt})|^2,$</li><li>$SDF(v_i,M_{gt})$ 表示点 $v_i$ 到 GT mesh 的 SDF 值</li><li>此外，预测顶点变形的正则化损失，避免伪影 $L_{\mathsf{def}}=\sum_{v_i\in V_T}||\Delta v_i||_2.$</li></ul></li></ul><p>最终的总损失：$L=\lambda_\mathrm{cd}L_\mathrm{cd}+\lambda_\mathrm{normal}L_\mathrm{normal}+\lambda_\mathrm{G}L_\mathrm{G}+\lambda_\mathrm{SDF}L_\mathrm{SDF}+\lambda_\mathrm{def}L_\mathrm{def},$</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>在本文中，我们介绍了一种深度 3D 条件生成模型，该模型可以使用简单的用户 guides（例如粗体素）来合成高分辨率 3D 形状。我们的 DMTET 具有一种新颖的 3D 表示，通过利用两者的优势来汇集隐式和显式表示。我们通过实验表明，我们的方法合成的质量形状明显更高，几何细节比现有方法更好，由定量指标和广泛的用户研究证实。通过展示提升粗体素（如 Minecraft 形状）的能力，我们希望我们一步更接近民主化 3D 内容创建。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Generative Models </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RODIN</title>
      <link href="/3DReconstruction/Single-view/Generative%20Models/RODIN/"/>
      <url>/3DReconstruction/Single-view/Generative%20Models/RODIN/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion</th></tr></thead><tbody><tr><td>Author</td><td>Tengfei Wang1†<em> Bo Zhang2</em> Ting Zhang2 Shuyang Gu2 Jianmin Bao2</td></tr><tr><td>Conf/Jour</td><td>arXiv preprint</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://3d-avatar-diffusion.microsoft.com/">RODIN Diffusion (microsoft.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4700249091733454849&amp;noteId=2024885528733762560">Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231028214552.png" alt="image.png|666"></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文提出了一种利用扩散模型自动生成以神经辐射场表示的三维数字化身的三维生成模型。生成这种虚拟形象的一个重大挑战是，3D 中的内存和处理成本令人望而却步，无法生成高质量虚拟形象所需的丰富细节。为了解决这个问题，我们提出了推出扩散网络(Rodin)，它将神经辐射场表示为多个 2D 特征图，并将这些地图推出到单个 2D 特征平面中，我们在其中执行 3d 感知扩散。Rodin 模型在保证三维扩散完整性的同时，利用三维感知卷积，在二维特征平面中根据原始关系处理投影特征，从而提高了计算效率。我们还使用潜在条件反射来协调全局一致性的特征生成，从而产生高保真的虚拟形象，并使其基于文本提示进行语义编辑。最后，我们使用层次合成来进一步增强细节。与现有的生成技术相比，我们的模型生成的 3D 头像效果更好。我们可以生成具有逼真发型和胡须等面部毛发的高度详细的化身。我们还演示了从图像或文本生成 3D 头像以及文本引导的可编辑性。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231028214552.png" alt="image.png|666"></p><p>与之前从 2D 图像集合中学习 3D 感知生成的方法不同，我们的目标是使用来自 Blender 合成管道的多视图渲染来学习 3D 角色生成[69]。而不是将同一主题的多视图图像作为单独的训练样本，我们拟合每个头像的体积神经表示，用于解释从不同角度观察到的所有结果。然后，我们<strong>使用扩散模型来表征这些 3D 实例的分布</strong>。我们基于扩散的 3D 生成是一个分层过程-我们首先利用扩散模型生成粗糙的几何形状，然后是扩散上采样器进行细节合成。如图 2 所示，整个 3D 肖像生成包括多个训练阶段，我们将在下面的小节中详细介绍。</p><h2 id="Robust-3D-Representation-Fitting"><a href="#Robust-3D-Representation-Fitting" class="headerlink" title="Robust 3D Representation Fitting"></a>Robust 3D Representation Fitting</h2><p>鲁棒的 3D 表示：</p><ul><li>适合生成网络处理的<strong>显式表示</strong></li><li><strong>紧凑的表示</strong>，这是高效的的内存利用</li><li>表示可以<strong>快速拟合</strong>，原始 NeRF 那样耗时数小时的优化将无法生成生成建模所需的大量 3D 训练数据。</li></ul><p>三维体被分解成三个轴向的正交特征平面 $y_{uv},y_{wu},y_{vw}\in\mathbb{R}^{H\times W \times C}$<br>每个特征平面的空间分辨率为 H×W，通道数为 c。与体素网格相比，三平面表示在不牺牲表达性的情况下提供了相当小的内存占用。因此，丰富的三维信息被显式地记忆在三平面特征中，可以通过将三维点 $p\in\mathbb{R}^3$ 投影到各个平面上，并将检索到的特征聚合在一起来查询其特征，即 $y_{p}=y_{uv}(p_{uv})+y_{wu}(p_{wu})+y_{vw}(p_{vw}).$</p><p>轻量级 MLP 解码器 $\mathcal{G}_\theta^{\mathrm{MLP}}$ 推导出给定观察方向 $d\in S^{2}$ 的每个 3D 位置的密度 $\sigma\in{\mathbb{R}^{+}}$ 和与视图相关的颜色 $c\in \mathbb{R}^{3}$，即：$c(p,d),\sigma(p)=\mathcal{G}_\theta^{\mathrm{MLP}}\big(y_p,\xi(y_p),d\big).$ 我们将傅里叶嵌入算子ξ(·)应用于查询的特征上，而不是空间坐标上。</p><p>对三平面特征和 MLP 解码器进行了优化，使得神经辐射场的渲染与给定主题的多视图图像 $\left\{x\right\}_{N_v}$ 匹配，其中 $x\in\mathbb{R}^{H_0\times W_0\times3}$。我们强制体积渲染[38]给出的渲染图像，即: $\hat{x}={\cal R}\left(c,\sigma\right),$，以均方误差损失匹配相应的地面真值。此外，我们还引入了稀疏、平滑和紧凑的正则化器来减少自由空间中的“浮动”伪像(Mip-NeRF 360)</p><p>虽然之前的每个场景重建主要关注拟合质量，但我们的 3D 拟合程序也应该考虑生成目的的几个关键方面。</p><ul><li>首先，不同主体的三平面特征必须严格处于同一域。为了实现这一点，我们<strong>在拟合不同的肖像时采用了一个共享的 MLP 解码器</strong>，从而隐式地将三平面特征推到解码器可识别的共享潜在空间。</li><li>其次，MLP 解码器必须具有一定程度的鲁棒性。也就是说，<strong>解码器应该能够容忍三面特征的轻微扰动</strong>，因此即使三面特征不完全生成，仍然可以获得可信的结果。</li><li>更重要的是，<strong>解码器应该对不同的三平面尺寸具有鲁棒性</strong>，因为分层 3D 生成是在多分辨率三平面特征上训练的。如图 3 所示，当单独拟合 256 × 256 三平面时，其 64 × 64 分辨率变体无法有效呈现。为了解决这个问题，我们在拟合过程中随机缩放三平面，这有助于通过共享解码器同时导出多分辨率三平面特征。</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231029104814.png" alt="image.png|666"></p><h2 id="Latent-Conditioned-3D-Diffusion-Model"><a href="#Latent-Conditioned-3D-Diffusion-Model" class="headerlink" title="Latent Conditioned 3D Diffusion Model"></a>Latent Conditioned 3D Diffusion Model</h2><p>现在3D 角色生成被简化为学习三平面特征的分布，即 $p\left(y\right)$，其中 $y = (y_{uv},y_{wu},y_{vw})$。这种生成式建模是 non-trivial 的，因为 y 是高维的。我们利用扩散模型来完成这项任务，它在复杂的图像建模中显示出令人信服的质量</p><p>在较高的层次上，扩散模型通过逐步反转马尔科夫正演过程产生 y。从 $y_0\sim p(y)$ 开始，正演过程 q 根据 $y_t:=\alpha_t y_0+\sigma_t\epsilon,$ 得到一个递增的噪声潜码序列 $\{y_t\mid t\in[0,T]\}$，其中 $\epsilon\in\mathcal{N}(\mathbf{0},I)$ 为加入的高斯噪声;αt 和σt 定义了一个噪声调度，其对数信噪比 $\lambda_t=\log[\alpha_t^2/\sigma_t^2]$ 随时间步长 t 线性减小。有了足够的噪声步长，我们得到一个纯高斯噪声，即 $y_T \sim \mathcal{N}(0,I)$。生成过程对应于上述噪声过程的反转，其中扩散模型被训练成使用均方误差损失将 $y_{T}$ 降噪为 $y_{0}$。Following <a href="https://readpaper.com/pdf-annotate/note?pdfId=4557071478495911937&amp;noteId=2025686505055348480">Denoising Diffusion Probabilistic Models.</a>，通过参数化扩散模型 $\hat{\epsilon}_{\theta}$ 来预测增加的噪声，可以获得更好的生成质量: $\mathcal{L}_{\mathrm{simple}}=\mathbb{E}_{t,\boldsymbol{x}_0,\boldsymbol{\epsilon}}\bigg[\big|\hat{\epsilon}_{\boldsymbol{\theta}}(\alpha_t y_0+\sigma_t\boldsymbol{\epsilon},t)-\boldsymbol{\epsilon}\big|_2^2\bigg].$<br>在实践中，我们的扩散模型训练也共同优化了 <a href="https://readpaper.com/pdf-annotate/note?pdfId=4557460151058046977&amp;noteId=2025686157179426304">41</a> 中提出的变分下界损失 ${\mathcal{L}}_{\mathrm{VLB}}:$，从而可以用更少的时间步长实现高质量的生成。在推理过程中，使用随机祖先采样器 <a href="https://readpaper.com/pdf-annotate/note?pdfId=4557071478495911937&amp;noteId=2025686505055348480">24</a> 生成最终样本，该样本从高斯噪声 $y_T \sim \mathcal{N}(0,I)$ 开始，依次产生较少噪声的样本 $\{y_T,y_{T-1},\dots\}$，直到达到 y0。</p><p>我们首先训练一个基本扩散模型来生成粗糙的三平面，例如，在 64 × 64 分辨率下。<br><strong>一种直接的方法</strong>是在我们的三平面生成中采用最先进的基于图像的扩散模型中使用的 2D 网络结构。具体而言，我们可以将通道维度中的三平面特征如[9]中所示进行串联，形成 $y=\left(\boldsymbol{y}_{uv}\oplus\boldsymbol{y}_{wu}\oplus\boldsymbol{y}_{vw}\right)\in\mathbb{R}^{H\times W\times3C},$ 并采用精心设计的二维 U-Net，通过去噪扩散过程对数据分布进行建模。然而，<strong>这样的基线模型会产生带有严重伪影的 3D 化身</strong>。我们推测生成伪影来自于三平面表示与二维 U-Net 之间的不兼容。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231029110243.png" alt="image.png|666"></p><p>如图 4(a)所示，可以直观地将三平面特征视为神经体积在正面、底部和侧面的投影。因此，在 CNN 处理中，这些正交平面的<strong>通道级联</strong>是有问题的，因为这些平面在空间上没有对齐。为了更好地处理三平面表示，我们做了以下努力: </p><p><strong>3D-aware convolution</strong></p><p>使用 CNN 处理按通道拼接的三平面会导致在3D 方面混合理论上未校正的特征。解决这个问题的一个简单而有效的方法是在空间上展开三平面特征。如图4(b)所示，我们将三平面特征水平连接，得到 $\tilde{y}= \text{hstack}(y_{uv},y_{wu},y_{vw})\in\mathbb{R}^{H\times3W\times C}.$。这样的特征展开允许对特征平面进行独立处理。为简单起见，我们随后默认使用 y 来表示这样的输入形式。然而，三平面推出阻碍了跨平面通信，而 3D 生成需要三平面生成的协同作用。</p><p>为了更好地处理三平面特征，我们需要一个在三平面上执行的有效的 3D 算子，而不是将其视为普通的 2D 输入。为了实现这一点，我们提出了<strong>3D 感知卷积</strong>来有效地处理三平面特征，同时尊重它们的 3D 关系。<strong>在某一特征平面上的一个点实际上对应着体中的一条轴向三维直线，在其他平面上也有两条对应的直线投影</strong>，如图 4(A)所示。这些相应位置的特征本质上描述了相同的 3D 原语，应该同步学习。然而，当使用平面二维卷积进行三平面处理时，这种三维关系被忽略了。因此，我们的<strong>3D 感知卷积通过将每个平面的特征与其他平面的相应行/列相关联，明确地引入了这种 3D 感应偏置。通过这种方式，我们实现了 2D cnn 的 3D 处理能力</strong>。这种 3D 感知卷积应用于三平面表示，实际上是一种简化 3D 卷积的通用方法，以前在建模高分辨率 3D 体积时计算成本太高。</p><p>3d 感知卷积如图4(b)所示。理想情况下，$y_{uv}$ 的计算将关注来自其他平面的相应行/列的完整元素，即 $y_{wu}$ 和 $y_{vw}$。对于并行计算，我们将其简化并聚合行/列元素。具体来说，我们对 $y_{wu}$ 和 $y_{vw}$ 应用了轴向池化，分别得到一个行向量 $y_{wu\to u}\in\mathbb{R}^{1\times W\times C}$ 和一个列向量 $y_{vw\to v}\in\mathbb{R}^{H\times 1\times C}$。对于 $y_{uv}$ 的每个点，我们可以很容易地访问聚合向量中相应的元素。我们将聚合向量扩展到原来的二维维度(即沿行维度复制列向量，反之亦然)，从而得到 $y_{(\cdot)u},y_{v(\cdot)}\in\mathbb{R}^{H\times W\times C}$。到目前为止，我们可以对特征映射的通道级连接执行2D 卷积，即 $\text{Conv2D}(\boldsymbol{y}_{uv}\oplus\boldsymbol{y}_{(\cdot)u}\oplus\boldsymbol{y}_{v(\cdot)}).$。因为 $y_{uv}$ 现在在空间上与来自其他平面的相应元素的聚合对齐。对 $y_{wu}$ 和 $y_{vw}$ 进行了同样的计算。3d 感知卷积极大地增强了平面间的通信，我们通过经验观察到减少了伪影，并改善了头发等细结构的生成。</p><p><strong>Latent conditioning</strong>.</p><p>我们进一步提出学习一个潜在向量来协调三平面生成。如图2所示，我们另外训练了一个图像编码器 $\mathcal{E}$ 来提取一个语义潜在向量作为基本扩散模型的条件输入，所以本质上整个框架就是一个自编码器。具体来说，我们从每个训练对象的正面视图中提取潜在向量，即 $z=\mathcal{E}_\theta(x_{\mathrm{front}})\in\mathbb{R}^{512},$ 训练以 z 为条件的扩散模型重构同一主体的三平面。我们使用自适应群归一化(AdaGN)来调节扩散模型的激活，其中 z 注入到每个残差块中，这样，根据共享潜函数同步生成正交平面的特征。<br>潜在条件反射不仅可以提高生成质量，还可以提供一个解纠缠的潜在空间，从而可以对生成的结果进行语义编辑。为了实现更好的可编辑性，我们采用了与文本提示共享潜在空间的冻结 CLIP 图像编码器[48]。我们将展示学习模型如何产生可控的文本引导生成结果。<br>潜在条件反射的另一个显著优点是它<strong>允许无分类器的引导</strong>，这是一种通常用于提高条件生成中的采样质量的技术。在训练扩散模型时，我们以20%的概率随机将潜在嵌入归零，从而使扩散解码器适应无条件生成。在推理过程中，我们可以根据 $\hat{\epsilon}_\theta(y,z)=\lambda\epsilon_\theta(y,z)+(1-\lambda)\epsilon_\theta(y),$  引导模型朝着更好的 generation sampling 方向发展</p><ul><li>其中 $\epsilon_\theta(y,z) \text{and}\epsilon_\theta(y)$ 分别为条件预测和无条件预测，λ &gt; 0 为引导强度。</li></ul><p>因此，我们的潜在条件基础模型既支持无条件生成，也支持用于肖像反演的条件生成。为了考虑无条件采样期间的完全多样性，我们额外训练了一个扩散模型来模拟潜在 z 的分布，而潜在 $y_{T}$ 描述残差变化。我们在图 2 中包含了这个潜在扩散模型。</p><h2 id="Diffusion-Tri-plane-Upsampler"><a href="#Diffusion-Tri-plane-Upsampler" class="headerlink" title="Diffusion Tri-plane Upsampler"></a>Diffusion Tri-plane Upsampler</h2><p>为了生成高保真的3D 结构，我们进一步训练扩散超分辨率(SR)模型，将三平面分辨率从64×64提高到256×256。在这个阶段，扩散上采样器的条件是低分辨率(LR)三平面 $y^{\mathrm{LR}}$。与基本模型训练不同的是，我们将扩散上采样器 $y_{\theta}^{\mathrm{HR}}(y_{t}^{\mathrm{HR}},y^{\mathrm{LR}},t)$ 参数化来预测高分辨率(HR)地面真值 $y_0^{\mathrm{HR}}$，而不是 added 噪声 $\epsilon.$。在每个残差块中利用三维感知卷积来增强细节合成。</p><p>在之前的级联图像生成工作之后，我们应用条件增强来减少基本模型输出和 LR 条件输入之间的域间隙，用于 SR 训练。我们<strong>通过随机下采样、高斯模糊和高斯噪声</strong>的组合对三平面增强进行了仔细的调整，使渲染增强的 LR 三平面尽可能地与基本渲染输出相似。<br>尽管如此，我们发现与地面真实较低 L2距离的三平面恢复不一定对应于令人满意的图像渲染。因此，我们需要直接约束渲染的图像。具体来说，我们从预测的三平面 $\hat y_{0}^{HR}$ 中获得了渲染图像 $x^{\hat{\mathrm{HR}}}\in\mathbb{R}^{256\times256\times3}$，其中 $\hat{\boldsymbol{x}}=\mathcal{R}(\mathcal{G}_{\theta}^{\mathbf{MLP}}(\hat{\boldsymbol{y}}_{0}^{\mathbf{HR}}))$，我们进一步惩罚了这个渲染结果与地面真实之间的感知损失[27]，根据 $\mathcal{L}_{\mathtt{perc}}=\mathbb{E}_{t,\hat{\boldsymbol{x}}}\sum\lVert\Psi^l(\hat{\boldsymbol{x}})-\Psi^l(\boldsymbol{x})\rVert_2^2,$ 其中 $\Psi^l$ 表示使用预训练的 VGG 进行多级特征提取。<br>通常，体绘制需要沿每条射线分层采样，这对于高分辨率渲染来说在计算上是禁止的。因此，我们在人脸区域具有高采样重要性的随机112 × 112图像块上计算 $\mathcal{L}_{perc}$。与之前需要渲染完整图像的 3d 感知 gan 相比，我们的 3d 感知 SR 可以很容易地扩展到高分辨率，因为允许在直接监督下进行 patchwise 训练</p><p>$\mathcal{L}_{perc}$ 是 patchwise 的</p><p>建模高频细节和薄结构是特别具有挑战性的体绘制。因此，在这个阶段，我们在我们的数据上共同训练一个<a href="https://readpaper.com/pdf-annotate/note?pdfId=4557204956864585729&amp;noteId=2027670377728176384">卷积细化器</a>，<strong>它补充了NeRF渲染中缺失的细节</strong>，最终产生引人注目的1024 × 1024图像输出。</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>从实验中，我们观察到 Rodin 模型是一个强大的 3D 头像生成模型。这种模式还允许用户从肖像或文字中定制头像，从而大大降低了个性化头像创建的障碍。虽然本文只关注虚拟人物，但罗丹模型背后的主要思想适用于一般 3D 场景的扩散模型。事实上，高昂的计算成本一直是 3D 内容创作的一大挑战。在 3D 中执行连贯和 3D 感知扩散的高效 2D 架构是解决这一挑战的重要一步。<strong>在今后的工作中，提高三维扩散模型的采样速度，共同研究利用充足的二维数据来缓解三维数据瓶颈将是富有成效的</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Generative Models </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TransHuman</title>
      <link href="/3DReconstruction/Single-view/Implicit%20Function/TransHuman/"/>
      <url>/3DReconstruction/Single-view/Implicit%20Function/TransHuman/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering</th></tr></thead><tbody><tr><td>Author</td><td>Xiao Pan1,2,∗, Zongxin Yang1, Jianxin Ma2, Chang Zhou2, Yi Yang1,†</td></tr><tr><td>Conf/Jour</td><td>ICCV</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://pansanity666.github.io/TransHuman/">TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering (pansanity666.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4781388859208368129&amp;noteId=2020479109538002688">TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022103225.png" alt="image.png|666"></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>在本文中，我们重点研究了广义神经人体渲染的任务，即从不同角色的多视点视频中训练条件神经辐射场(NeRF)。为了处理动态的人体运动，以前的方法主要是使用基于 SparseConvNet (SPC)的人类表征来处理绘制的 SMPL。然而，这种基于 spc 的表示，<br>I)在不稳定的观察空间下进行优化，导致训练和推理阶段之间的正对齐，<br>Ii)缺乏人体各部分之间的全局关系，这对于处理不完整的绘制 SMPL 至关重要。<br>针对这些问题，我们提出了一个全新的框架 TransHuman，该框架在规范空间下学习绘制的 SMPL，并通过变压器捕获人体部位之间的全局关系。具体来说，TransHuman 主要由基于变压器的人类编码(TransHE)、可变形局部辐射场(DPaRF)和细粒度细节集成(FDI)组成。TransHE 首先通过变压器在规范空间下处理绘制的 SMPL，以捕获人体部位之间的全局关系。然后，DPaRF 将每个输出标记与一个可变形的亮度字段绑定，用于对观测空间下的查询点进行编码。最后，利用 FDI 进一步整合参考图像中的细粒度信息。在 ZJUMoCap 和 H36M 上进行的大量实验表明，我们的 TransHuman 以高效率实现了最先进的新性能。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>TransHE 首先建立了一个管道，通过规范空间下的变压器捕获人体各部分之间的全局关系。然后，DPaRF 将坐标系统从规范空间变形回观测空间，并将查询点编码为坐标和条件特征的集合。最后，FDI 在人类表征的指导下，从像素对齐的外观特征中进一步收集观察空间的细粒度信息</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022103225.png" alt="image.png|666"></p><h2 id="Transformer-based-Human-Encoding"><a href="#Transformer-based-Human-Encoding" class="headerlink" title="Transformer-based Human Encoding"></a>Transformer-based Human Encoding</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231025215706.png" alt="image.png|666"></p><p>给定某一时间步长的参考图像 I 及其在观测姿态†下对应的预拟 SMPL 模型 $V^o∈\mathbb{R}^{6890×3}$，我们首先根据摄像机信息将 CNN 提取的 I 的一维深度特征投影到 V 的顶点上，得到绘制好的 SMPL : $F∈\mathbb{R}^{6890×d_1}$</p><p>以前的方法[18,5]主要是利用 Sparse CNN(SPC)[21]将painted的 SMPL 扩散到附近的空间(图1)。然而，它们在不同的观察空间下进行优化，导致训练和推理阶段之间的位姿不一致，并且三维卷积块的有限接受域使其对由于人体严重的自我遮挡而导致的未完全绘制的 SMPL 输入敏感。为了解决这些问题，我们提出了一个名为基于转换器的人体编码(TransHE)的管道，它在规范空间下捕获人体各部分之间的全局关系。TranHE 的核心包括避免语义模糊的规范化主体分组策略和简化优化和提高泛化能力的规范化学习方案。</p><h2 id="Deformable-Partial-Radiance-Fields"><a href="#Deformable-Partial-Radiance-Fields" class="headerlink" title="Deformable Partial Radiance Fields"></a>Deformable Partial Radiance Fields</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231025215724.png" alt="image.png|666"></p><h2 id="Fine-grained-Detail-Integration"><a href="#Fine-grained-Detail-Integration" class="headerlink" title="Fine-grained Detail Integration"></a>Fine-grained Detail Integration</h2><h2 id="Training-amp-Inference"><a href="#Training-amp-Inference" class="headerlink" title="Training &amp; Inference"></a>Training &amp; Inference</h2>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Implicit Function </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ClothedHumans </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pose Estimation Review</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/CameraPose/Pose%20Estimation%20Review/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/CameraPose/Pose%20Estimation%20Review/</url>
      
        <content type="html"><![CDATA[<p>相机位姿估计，可以用于类似NeRF方法，深度估计…</p><span id="more"></span><h1 id="Pose-Estimation"><a href="#Pose-Estimation" class="headerlink" title="Pose Estimation"></a>Pose Estimation</h1><p>传统方法：SFM (from COLMAP)</p><h2 id="NOPE"><a href="#NOPE" class="headerlink" title="NOPE"></a>NOPE</h2><p><a href="https://nv-nguyen.github.io/nope/">NOPE (nv-nguyen.github.io)</a><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023172331.png" alt="image.png|666"></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/CameraPose </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PointCloud Review</title>
      <link href="/3DReconstruction/Multi-view/PointCloud/PointCloud%20Review/"/>
      <url>/3DReconstruction/Multi-view/PointCloud/PointCloud%20Review/</url>
      
        <content type="html"><![CDATA[<p>PointCloud</p><ul><li><strong>Registration</strong></li><li><strong>Surface Reconstruction</strong></li></ul><p>Follow</p><ul><li><a href="https://github.com/Yochengliu/awesome-point-cloud-analysis">Yochengliu/awesome-point-cloud-analysis: A list of papers and datasets about point cloud analysis (processing) (github.com)</a></li><li><a href="https://github.com/zhulf0804/3D-PointCloud">zhulf0804/3D-PointCloud: Papers and Datasets about Point Cloud. (github.com)</a></li><li><a href="https://github.com/XuyangBai/awesome-point-cloud-registration?tab=readme-ov-file#traditional">XuyangBai/awesome-point-cloud-registration: A curated list of point cloud registration. (github.com)</a></li></ul><span id="more"></span><h1 id="Point-Cloud-Registration"><a href="#Point-Cloud-Registration" class="headerlink" title="Point Cloud Registration"></a>Point Cloud Registration</h1><h2 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h2><ul><li><a href="https://readpaper.com/pdf-annotate/note?pdfId=4545266789335588865&amp;noteId=2010010466642902272">A comprehensive survey on point cloud registration. (readpaper.com)</a></li><li><a href="http://www.cjig.cn/html/2022/2/20220203.htm">深度学习刚性点云配准前沿进展 (cjig.cn)</a></li><li><a href="http://geometryhub.net/notes/registration">点云拼接注册 (geometryhub.net)</a></li></ul><h3 id="Old-Method-Library："><a href="#Old-Method-Library：" class="headerlink" title="Old Method Library："></a>Old Method Library：</h3><ul><li>Probreg is a library that implements point cloud <strong>reg</strong>istration algorithms with <strong>prob</strong>ablistic model.</li><li>PCL: Point Cloud Library</li></ul><blockquote><p><a href="https://github.com/neka-nat/probreg">neka-nat/probreg: Python package for point cloud registration using probabilistic model (Coherent Point Drift, GMMReg, SVR, GMMTree, FilterReg, Bayesian CPD) (github.com)</a> &gt; <a href="https://pointclouds.org/">Point Cloud Library | The Point Cloud Library (PCL) is a standalone, large scale, open project for 2D/3D image and point cloud processing. (pointclouds.org)</a></p></blockquote><h2 id="Greedy-Grid-Search"><a href="#Greedy-Grid-Search" class="headerlink" title="Greedy Grid Search"></a>Greedy Grid Search</h2><p><a href="https://github.com/davidboja/greedy-grid-search">DavidBoja/greedy-grid-search: [BMVC 2022 workshop] Greedy Grid Search: A 3D Registration Baseline (github.com)</a><br><a href="https://github.com/DavidBoja/FAUST-partial">DavidBoja/FAUST-partial: [BMVC 2022 workshop] 3D registration benchmark dataset FAUST-partial (github.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231019094915.png" alt="image.png|666"></p><h2 id="SGHR"><a href="#SGHR" class="headerlink" title="SGHR"></a>SGHR</h2><p><a href="https://github.com/WHU-USI3DV/SGHR">WHU-USI3DV/SGHR: [CVPR 2023] Robust Multiview Point Cloud Registration with Reliable Pose Graph Initialization and History Reweighting (github.com)</a><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231017210035.png" alt="image.png|666"></p><h2 id="GeoTransformer"><a href="#GeoTransformer" class="headerlink" title="GeoTransformer"></a>GeoTransformer</h2><p><a href="https://github.com/qinzheng93/GeoTransformer?tab=readme-ov-file">qinzheng93/GeoTransformer: [CVPR2022] Geometric Transformer for Fast and Robust Point Cloud Registration (github.com)</a><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231018113631.png" alt="image.png|666"></p><h2 id="Fusion-of-cross-view-images"><a href="#Fusion-of-cross-view-images" class="headerlink" title="Fusion of cross-view images"></a>Fusion of cross-view images</h2><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=4546340788354310145&amp;noteId=2015534437671973888">3D Reconstruction through Fusion of Cross-View Images (readpaper.com)</a></p><p>多张卫星图片配准<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022110219.png" alt="image.png|666"></p><h1 id="PointCloud-Surface-Reconstruction"><a href="#PointCloud-Surface-Reconstruction" class="headerlink" title="PointCloud Surface Reconstruction"></a>PointCloud Surface Reconstruction</h1><p><a href="https://pymeshlab.readthedocs.io/en/2022.2/classes/meshset.html">MeshSet — PyMeshLab documentation</a><br>Use pymeshlab to <strong>screened Poisson surface construction</strong></p><h2 id="NKSR"><a href="#NKSR" class="headerlink" title="NKSR"></a>NKSR</h2><p><a href="https://github.com/nv-tlabs/NKSR">nv-tlabs/NKSR: [CVPR 2023 Highlight] Neural Kernel Surface Reconstruction (github.com)</a><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231017200157.png" alt="image.png|666"></p><h2 id="FlexiCubes"><a href="#FlexiCubes" class="headerlink" title="FlexiCubes"></a>FlexiCubes</h2><p><a href="https://research.nvidia.com/labs/toronto-ai/flexicubes/">Flexible Isosurface Extraction for Gradient-Based Mesh Optimization (FlexiCubes) (nvidia.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023164911.png" alt="image.png|666"></p><h2 id="DualMesh-UDF"><a href="#DualMesh-UDF" class="headerlink" title="DualMesh-UDF"></a>DualMesh-UDF</h2><p><a href="https://cong-yi.github.io/projects/dualmeshudf/">Surface Extraction from Neural Unsigned Distance Fields (cong-yi.github.io)</a></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/PointCloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PointCloud </tag>
            
            <tag> Registration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Generative Models Review about 3D Reconstruction</title>
      <link href="/3DReconstruction/Basic%20Knowledge/Generative%20Models%20Review/"/>
      <url>/3DReconstruction/Basic%20Knowledge/Generative%20Models%20Review/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Paper</th><th>Model</th><th>Input</th><th>Parameter/Pnum</th><th>GPU</th></tr></thead><tbody><tr><td>DiT-3D</td><td>Diffusion Transformers</td><td>Voxelized PC</td><td></td><td></td></tr><tr><td>PointFlow</td><td>AE flow-based</td><td>PointCloud</td><td>1.61M</td><td></td></tr><tr><td>FlowGAN</td><td>GAN flow-based</td><td>Single Image</td><td>N = 2500</td><td>A40 45GB</td></tr><tr><td>BuilDiff</td><td>Diffusion models</td><td>Single Image</td><td>1024 to 4096</td><td>A40 45GB</td></tr><tr><td>CCD-3DR</td><td>CDPM</td><td>Single Image</td><td>8192</td><td>3090Ti 24GB</td></tr><tr><td>SG-GAN</td><td>SG-GAN</td><td>Single Image</td><td></td><td></td></tr><tr><td><strong>HaP</strong></td><td>Diffusion+SMPL+DepthEstimation</td><td>Single Image</td><td>10000</td><td>4x3090Ti</td></tr></tbody></table></div><span id="more"></span><h1 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h1><ul><li><a href="https://arxiv.org/abs/2311.06786">Explainability of Vision Transformers: A Comprehensive Review and New Perspectives</a> 关于视觉中使用 Transformer 的 Review</li></ul><h1 id="Generative-approach-Img2PC"><a href="#Generative-approach-Img2PC" class="headerlink" title="Generative approach(Img2PC)"></a>Generative approach(Img2PC)</h1><h2 id="Network-Framework"><a href="#Network-Framework" class="headerlink" title="Network Framework"></a>Network Framework</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021163349.png" alt="Image.png|555"></p><ul><li>GAN(generative adversarial networks)</li><li>VAE(variational auto-encoders)</li><li>Auto-regressive models</li><li>Normalized flows(flow-based models), PointFlow<ul><li>相当于多个生成器，并且可逆</li><li><a href="https://www.youtube.com/watch?v=uXY18nzdSsM&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=60">Flow-based Generative Model - YouTube</a></li><li><a href="https://zhuanlan.zhihu.com/p/267305869">Flow-based Generative Model 笔记整理 - 知乎 (zhihu.com)</a></li><li><a href="https://zhuanlan.zhihu.com/p/59615785">Normalization Flow (标准化流) 总结 - 知乎 (zhihu.com)</a></li></ul></li><li>DiT(Diffusion Transformers), DiT-3D</li></ul><p>Flow-based：<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021100952.png" alt="image.png|333"><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021101055.png" alt="image.png|666"></p><h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p>点云倒角距离 CD ↓<br>$\begin{aligned}\mathcal{L}_{CD}&amp;=\sum_{y’\in Y’}min_{y\in Y}||y’-y||_2^2+\sum_{y\in Y}min_{y’\in Y’}||y-y’||_2^2,\end{aligned}$</p><p>推土距离 EMD (Earth Mover’s distance)↓<br>$\mathcal{L}_{EMD}=min_{\phi:Y\rightarrow Y^{\prime}}\sum_{x\in Y}||x-\phi(x)||_{2}$ , φ indicates a parameter of bijection.</p><h2 id="Diffusion-Models"><a href="#Diffusion-Models" class="headerlink" title="Diffusion Models"></a>Diffusion Models</h2><h3 id="DMV3D"><a href="#DMV3D" class="headerlink" title="DMV3D"></a>DMV3D</h3><p><a href="https://justimyhxu.github.io/projects/dmv3d/">DMV3D: Denoising Multi-View Diffusion Using 3D Large Reconstruction Mode (justimyhxu.github.io)</a></p><p>Diffusion Model + Triplane NeRF + Multi-view Image input<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231116161909.png" alt="image.png|666"></p><h3 id="DiffuStereo"><a href="#DiffuStereo" class="headerlink" title="DiffuStereo"></a>DiffuStereo</h3><p><a href="https://liuyebin.com/diffustereo/diffustereo.html">DiffuStereo Project Page (liuyebin.com)</a></p><p>多视图<br>DoubleField 粗网格估计 + Diffusion 生成高质量 Disparity Flow 和 Depth + ICP 配准（点云融合）</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231111120652.png" alt="image.png|666"></p><h3 id="Human-as-Points-HaP"><a href="#Human-as-Points-HaP" class="headerlink" title="Human as Points(HaP)"></a>Human as Points(HaP)</h3><p><a href="https://github.com/yztang4/HaP">yztang4/HaP (github.com)</a><br><a href="https://arxiv.org/pdf/2311.02892.pdf">Human as Points: Explicit Point-based 3D Human Reconstruction from Single-view RGB Images(arxiv.org)</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=2039003253707810304&amp;noteId=2039256760006808832">Human as Points—— Explicit Point-based 3D Human Reconstruction from Single-view RGB Images.pdf (readpaper.com)</a></p><p>深度估计+SMPL 估计得到两个稀疏点云，输入进 Diffusion Model 进行精细化生成</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231107193647.png" alt="image.png|666"></p><h3 id="HumanNorm"><a href="#HumanNorm" class="headerlink" title="HumanNorm"></a>HumanNorm</h3><p><a href="https://humannorm.github.io/">HumanNorm</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231111214245.png" alt="image.png|666"></p><h3 id="BuilDiff"><a href="#BuilDiff" class="headerlink" title="BuilDiff"></a>BuilDiff</h3><p>预计 2023.11 release<br><a href="BuilDiff.md">BuilDiff论文阅读笔记</a><br><a href="https://github.com/weiyao1996/BuilDiff">weiyao1996/BuilDiff: BuilDiff: 3D Building Shape Generation using Single-Image Conditional Point Cloud Diffusion Models (github.com)</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4796303379219349505&amp;noteId=2014132586369911808">BuilDiff: 3D Building Shape Generation using Single-Image Conditional Point Cloud Diffusion Models (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021114740.png" alt="image.png|666"></p><h3 id="RODIN"><a href="#RODIN" class="headerlink" title="RODIN"></a>RODIN</h3><p><a href="https://3d-avatar-diffusion.microsoft.com/">RODIN Diffusion (microsoft.com)</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4700249091733454849&amp;noteId=2024885528733762560">Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion (readpaper.com)</a></p><p>微软大数据集 + Diffusion + NeRF Tri-plane<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231028214552.png" alt="image.png|666"></p><h3 id="Single-Image-3D-Human-Digitization-with-Shape-Guided-Diffusion"><a href="#Single-Image-3D-Human-Digitization-with-Shape-Guided-Diffusion" class="headerlink" title="Single-Image 3D Human Digitization with Shape-Guided Diffusion"></a>Single-Image 3D Human Digitization with Shape-Guided Diffusion</h3><p><a href="https://arxiv.org/abs/2311.09221">Single-Image 3D Human Digitization with Shape-Guided Diffusion</a></p><p>利用针对一般图像合成任务<strong>预先训练的高容量二维扩散模型</strong>作为穿着人类的外观先验<br>通过以轮廓和表面法线为条件的形状引导扩散来修复缺失区域<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231116160820.png" alt="image.png|666"></p><h3 id="CCD-3DR"><a href="#CCD-3DR" class="headerlink" title="CCD-3DR"></a>CCD-3DR</h3><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=4789424767274844161&amp;noteId=2014146864821066240">CCD-3DR: Consistent Conditioning in Diffusion for Single-Image 3D Reconstruction (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021115606.png" alt="image.png|666"></p><h3 id="PC-2"><a href="#PC-2" class="headerlink" title="PC^2"></a>PC^2</h3><p><a href="https://lukemelas.github.io/projection-conditioned-point-cloud-diffusion/">PC2: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction (lukemelas.github.io)</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4726675454702911489&amp;noteId=2014671879272477696">$PC^2$: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction (readpaper.com)</a></p><p>相机位姿???<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021203845.png" alt="image.png|666"></p><h3 id="DiT-3D"><a href="#DiT-3D" class="headerlink" title="DiT-3D"></a>DiT-3D</h3><p><a href="DiT-3D.md">DiT-3D论文阅读笔记</a><br><a href="https://dit-3d.github.io/">DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4776143720479195137&amp;noteId=2011558450133224704">DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231019170328.png" alt="image.png|666"></p><h3 id="Make-It-3D"><a href="#Make-It-3D" class="headerlink" title="Make-It-3D"></a>Make-It-3D</h3><p><a href="https://make-it-3d.github.io/">Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023154817.png" alt="image.png|666"></p><h3 id="DreamGaussian"><a href="#DreamGaussian" class="headerlink" title="DreamGaussian"></a>DreamGaussian</h3><p><a href="https://dreamgaussian.github.io/">DreamGaussian</a></p><p>Gaussian Splatting + Diffusion<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231024164334.png" alt="image.png|666"></p><h3 id="Wonder3D"><a href="#Wonder3D" class="headerlink" title="Wonder3D"></a>Wonder3D</h3><p><a href="https://www.xxlong.site/Wonder3D/">Wonder3D: Single Image to 3D using Cross-Domain Diffusion (xxlong.site)</a></p><p>Diffusion 一致性出图 + Geometry Fusion (novel geometric-aware optimization scheme)</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231024205005.png" alt="image.png|666"></p><h4 id="CLIP"><a href="#CLIP" class="headerlink" title="CLIP"></a>CLIP</h4><p><a href="https://github.com/openai/CLIP">openai/CLIP: CLIP (Contrastive Language-Image Pretraining), Predict the most relevant text snippet given an image (github.com)</a></p><p>对比语言-图片预训练模型<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231025093839.png" alt="image.png|666"></p><h3 id="GenNeRF"><a href="#GenNeRF" class="headerlink" title="GenNeRF"></a>GenNeRF</h3><p><a href="https://arxiv.org/abs/2310.19464">Generative Neural Fields by Mixtures of Neural Implicit Functions (arxiv.org)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231031184253.png" alt="image.png|666"></p><h3 id="LDM3D-VR"><a href="#LDM3D-VR" class="headerlink" title="LDM3D-VR"></a>LDM3D-VR</h3><p><a href="https://arxiv.org/abs/2311.03226">LDM3D-VR: Latent Diffusion Model for 3D VR (arxiv.org)</a><br><a href="https://t.ly/tdi2">视频演示T.LY URL Shortener</a></p><p>从给定的文本提示生成图像和深度图数据，此外开发了一个 DepthFusion 的应用程序，它使用生成的 RGB 图像和深度图来使用 TouchDesigner 创建身临其境的交互式 360°视图体验<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231108223206.png" alt="image.png|666"></p><h3 id="Control3D"><a href="#Control3D" class="headerlink" title="Control3D"></a>Control3D</h3><p><a href="https://arxiv.org/pdf/2311.05461.pdf">Control3D: Towards Controllable Text-to-3D Generation</a></p><p>草图+文本条件生成 3D</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231110160231.png" alt="image.png|666"></p><h3 id="Etc"><a href="#Etc" class="headerlink" title="Etc"></a>Etc</h3><p>Colored PC <a href="https://readpaper.com/pdf-annotate/note?pdfId=4723055356805120001&amp;noteId=2014681937165010176">3D Colored Shape Reconstruction from a Single RGB Image through Diffusion (readpaper.com)</a></p><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><h3 id="3DHumanGAN"><a href="#3DHumanGAN" class="headerlink" title="3DHumanGAN"></a>3DHumanGAN</h3><p><a href="https://3dhumangan.github.io/">3DHumanGAN: 3D-Aware Human Image Generation with 3D Pose Mapping</a></p><p>多视图一致的人体照片生成</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231122161513.png" alt="image.png|666"></p><h3 id="SE-MD"><a href="#SE-MD" class="headerlink" title="SE-MD"></a>SE-MD</h3><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=4546363961368010753&amp;noteId=2015501024673795072">SE-MD: A Single-encoder multiple-decoder deep network for point cloud generation from 2D images. (readpaper.com)</a><br>单编码器—&gt;多解码器<br>每个解码器生成某些固定视点，然后融合所有视点来生成密集的点云</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022105308.png" alt="image.png|666"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231031152551.png" alt="image.png|666"></p><h3 id="FlowGAN"><a href="#FlowGAN" class="headerlink" title="FlowGAN"></a>FlowGAN</h3><p><a href="FlowGAN.md">FlowGAN论文阅读笔记</a><br><a href="https://bmvc2022.mpi-inf.mpg.de/569/">Flow-based GAN for 3D Point Cloud Generation from a Single Image (mpg.de)</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4677468038203719681&amp;noteId=2011515461854903552">Flow-based GAN for 3D Point Cloud Generation from a Single Image (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231019162233.png" alt="image.png|555"></p><h3 id="SG-GAN"><a href="#SG-GAN" class="headerlink" title="SG-GAN"></a>SG-GAN</h3><p><a href="SG-GAN.md">SG-GAN论文阅读笔记</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4762223320204574721&amp;noteId=2014610279544785920">SG-GAN: Fine Stereoscopic-Aware Generation for 3D Brain Point Cloud Up-sampling from a Single Image (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021193539.png" alt="image.png|666"></p><h3 id="3D-Brain-Reconstruction-and-Complete"><a href="#3D-Brain-Reconstruction-and-Complete" class="headerlink" title="3D Brain Reconstruction and Complete"></a>3D Brain Reconstruction and Complete</h3><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=4552080679232348161&amp;noteId=2015496104888593408">3D Brain Reconstruction by Hierarchical Shape-Perception Network from a Single Incomplete Image. (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022101838.png" alt="image.png|666"></p><h3 id="Deep3DSketch"><a href="#Deep3DSketch" class="headerlink" title="Deep3DSketch+"></a>Deep3DSketch+</h3><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=4803935841323843585&amp;noteId=2028490687877420544">Deep3DSketch+: Rapid 3D Modeling from Single Free-Hand Sketches (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231031092339.png" alt="image.png|666"></p><h3 id="Reality3DSketch"><a href="#Reality3DSketch" class="headerlink" title="Reality3DSketch"></a>Reality3DSketch</h3><p><a href="https://arxiv.org/abs/2310.18148">[2310.18148] Reality3DSketch: Rapid 3D Modeling of Objects from Single Freehand Sketches (arxiv.Org)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231031092703.png" alt="image.png|666"></p><h3 id="Deep3DSketch-1"><a href="#Deep3DSketch-1" class="headerlink" title="Deep3DSketch+\+"></a>Deep3DSketch+\+</h3><p><a href="https://arxiv.org/abs/2310.18178">[2310.18178] Deep3DSketch++: High-Fidelity 3D Modeling from Single Free-hand Sketches (arxiv.Org)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231031093421.png" alt="image.png|666"></p><h3 id="GET3D"><a href="#GET3D" class="headerlink" title="GET3D"></a>GET3D</h3><p><a href="https://github.com/nv-tlabs/GET3D">nv-tlabs/GET3D (github.com)</a><br><a href="https://nv-tlabs.github.io/GET3D/">GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images (nv-tlabs.github.io)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023154445.png" alt="image.png|666"></p><h3 id="AG3D"><a href="#AG3D" class="headerlink" title="AG3D"></a>AG3D</h3><p><a href="https://zj-dong.github.io/AG3D/">AG3D: Learning to Generate 3D Avatars from 2D Image Collections (zj-dong.github.io)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023160511.png" alt="image.png|666"></p><h2 id="NFs-Normalizing-Flows"><a href="#NFs-Normalizing-Flows" class="headerlink" title="NFs (Normalizing Flows)"></a>NFs (Normalizing Flows)</h2><h3 id="PointFlow"><a href="#PointFlow" class="headerlink" title="PointFlow"></a>PointFlow</h3><p><a href="https://github.com/stevenygd/PointFlow?tab=readme-ov-file">stevenygd/PointFlow: PointFlow : 3D Point Cloud Generation with Continuous Normalizing Flows (github.com)</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4500221890681004033&amp;noteId=2011462053836873984">PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231019154043.png" alt="image.png|666"></p><h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><h3 id="Automatic-Reverse-Engineering"><a href="#Automatic-Reverse-Engineering" class="headerlink" title="Automatic Reverse Engineering"></a>Automatic Reverse Engineering</h3><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=4804297472033685505&amp;noteId=2014396749893884416">Automatic Reverse Engineering: Creating computer-aided design (CAD) models from multi-view images (readpaper.com)</a></p><p>多视图图像生成 CAD 命令序列<br>局限性：</p><ul><li>CAD 序列的长度仍然局限于 60 个命令，因此只支持相对简单的对象</li><li>表示仅限于平面和圆柱表面，而许多现实世界的对象可能包括更灵活的三角形网格或样条表示</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021160431.png" alt="image.png|666"></p><h3 id="SimIPU"><a href="#SimIPU" class="headerlink" title="SimIPU"></a>SimIPU</h3><p><a href="https://github.com/zhyever/SimIPU">zhyever/SimIPU: [AAAI 2021] Official Implementation of “SimIPU: Simple 2D Image and 3D Point Cloud Unsupervised Pre-Training for Spatial-Aware Visual Representations” (github. Com)</a></p><p>雷达点云+图片<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022103332.png" alt="image.png|666"></p><h3 id="3DRIMR"><a href="#3DRIMR" class="headerlink" title="3DRIMR"></a>3DRIMR</h3><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=4542232848617857025&amp;noteId=2015491146701999104">3DRIMR: 3D Reconstruction and Imaging via mmWave Radar based on Deep Learning. (readpaper.com)</a><br>MmWave Radar + GAN<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022102437.png" alt="image.png|666"></p><h3 id="TransHuman"><a href="#TransHuman" class="headerlink" title="TransHuman"></a>TransHuman</h3><p><a href="TransHuman.md">TransHuman论文阅读笔记</a><br><a href="https://pansanity666.github.io/TransHuman/">TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering (pansanity666.github.io)</a></p><p>ImplicitFunction(NeRF)<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022103225.png" alt="image.png|666"></p><h3 id="Nvdiffrec"><a href="#Nvdiffrec" class="headerlink" title="Nvdiffrec"></a>Nvdiffrec</h3><p>网格优化比 mlp 优化难，速度慢 from NeRF wechat</p><ul><li>还有一个 nvdiffrcmc，效果可能好一些 Shape, Light, and Material Decomposition from Images using Monte Carlo Rendering and Denoising</li><li>后续还有个 NeuManifold: Neural Watertight Manifold Reconstruction with Efficient and High-Quality Rendering Support，应该比 nvdiffrec 要好</li></ul><p><a href="https://github.com/NVlabs/nvdiffrec">NVlabs/nvdiffrec: Official code for the CVPR 2022 (oral) paper “Extracting Triangular 3D Models, Materials, and Lighting From Images”. (github.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023154513.png" alt="image.png|666"></p><h3 id="HyperHuman"><a href="#HyperHuman" class="headerlink" title="HyperHuman"></a>HyperHuman</h3><p>高质量人类图片<br><a href="https://snap-research.github.io/HyperHuman/">HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion (snap-research.github.io)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023155025.png" alt="image.png|666"></p><h3 id="SyncDreamer"><a href="#SyncDreamer" class="headerlink" title="SyncDreamer"></a>SyncDreamer</h3><p><a href="https://liuyuan-pal.github.io/SyncDreamer/">SyncDreamer: Generating Multiview-consistent Images from a Single-view Image (liuyuan-pal.github.io)</a></p><p>多视图一致的图片生成<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023165400.png" alt="image.png|666"></p><h3 id="One-2-3-45"><a href="#One-2-3-45" class="headerlink" title="One-2-3-45"></a>One-2-3-45</h3><p><a href="https://one-2-3-45.github.io/">One-2-3-45</a></p><p>MVS+NeRF<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023165804.png" alt="image.png|666"></p><h3 id="One-2-3-45-1"><a href="#One-2-3-45-1" class="headerlink" title="One-2-3-45++"></a>One-2-3-45++</h3><p><a href="https://sudo-ai-3d.github.io/One2345plus_page/">One-2-3-45++ (sudo-ai-3d.github.io)</a></p><p>==提供了一个可以图片生成3D 资产的 demo== <a href="https://www.sudo.ai/3dgen">sudoAI</a></p><p>一致性图像生成 + CLIP + 3D Diffusion Model</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231117094931.png" alt="image.png|666"></p><h3 id="Zero-1-to-3"><a href="#Zero-1-to-3" class="headerlink" title="Zero-1-to-3"></a>Zero-1-to-3</h3><p><a href="https://zero123.cs.columbia.edu/">Zero-1-to-3: Zero-shot One Image to 3D Object (columbia.edu)</a></p><p>多视图一致 Diffusion Model + NeRF</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023171814.png" alt="image.png|666"></p><h3 id="Pix2pix3D"><a href="#Pix2pix3D" class="headerlink" title="Pix2pix3D"></a>Pix2pix3D</h3><p><a href="http://www.cs.cmu.edu/~pix2pix3D/">pix2pix3D: 3D-aware Conditional Image Synthesis (cmu.edu)</a></p><p>一致性图像生成+NeRF</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231105173051.png" alt="image.png|666"></p><h3 id="Consistent4D"><a href="#Consistent4D" class="headerlink" title="Consistent4D"></a>Consistent4D</h3><p><a href="https://consistent4d.github.io/">Consistent4D (consistent4d.github.io)</a></p><p>单目视频生成 4D 动态物体，Diffusion Model 生成多视图(时空)一致性的图像<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231107192857.png" alt="image.png|666"></p><h3 id="ConRad"><a href="#ConRad" class="headerlink" title="ConRad"></a>ConRad</h3><p><a href="https://arxiv.org/pdf/2311.05230.pdf">ConRad: Image Constrained Radiance Fields for 3D Generation from a Single Image</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=2043334481936337152&amp;noteId=2043393675895077376">ConRad: Image Constrained Radiance Fields for 3D Generation from a Single Image (readpaper.com)</a></p><p>多视图一致<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231110160631.png" alt="image.png|666"></p><h3 id="LRM"><a href="#LRM" class="headerlink" title="LRM"></a>LRM</h3><p><a href="https://scalei3d.github.io/LRM/">LRM: Large Reconstruction Model for Single Image to 3D (scalei3d.github.io)</a></p><p>大模型 Transformer(5 亿个可学习参数) + 5s 单视图生成 3D</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231110162856.png" alt="image.png|666"></p><h3 id="Instant3D"><a href="#Instant3D" class="headerlink" title="Instant3D"></a>Instant3D</h3><p><a href="https://ming1993li.github.io/Instant3DProj/">Instant3D: Instant Text-to-3D Generation (ming1993li.github.io)</a></p><p>文本生成 3D<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231116154716.png" alt="image.png|666"></p><h3 id="LucidDreamer"><a href="#LucidDreamer" class="headerlink" title="LucidDreamer"></a>LucidDreamer</h3><p><a href="https://github.com/EnVision-Research/LucidDreamer">EnVision-Research/LucidDreamer: Official implementation of “LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching” (github.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231122201401.png" alt="image.png|666"></p><h3 id="HumanGaussian"><a href="#HumanGaussian" class="headerlink" title="HumanGaussian"></a>HumanGaussian</h3><p><a href="https://alvinliu0.github.io/projects/HumanGaussian">HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting (alvinliu0.github.io)</a></p><p>Gaussian Splatting 文本生成 3DHuman</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231129164200.png" alt="image.png|666"></p><h3 id="Surf-D"><a href="#Surf-D" class="headerlink" title="Surf-D"></a>Surf-D</h3><p><a href="https://arxiv.org/abs/2311.17050">[2311.17050] Surf-D: High-Quality Surface Generation for Arbitrary Topologies using Diffusion Models (arxiv.Org)</a></p><p>高质量拓扑 3D 衣服生成</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231129164359.png" alt="image.png|666"></p><h3 id="HumanRef"><a href="#HumanRef" class="headerlink" title="HumanRef"></a>HumanRef</h3><p><a href="https://arxiv.org/abs/2311.16961">[2311.16961] HumanRef: Single Image to 3D Human Generation via Reference-Guided Diffusion (arxiv.Org)</a></p><p>单视图 Diffusion 人体生成<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231129164832.png" alt="image.png|666"></p><h3 id="RichDreamer"><a href="#RichDreamer" class="headerlink" title="RichDreamer"></a>RichDreamer</h3><p><a href="https://lingtengqiu.github.io/RichDreamer/">RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D (lingtengqiu.github.io)</a></p><p>文本生成 3D<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231129165112.png" alt="image.png|666"></p><h3 id="TriplaneGaussian"><a href="#TriplaneGaussian" class="headerlink" title="TriplaneGaussian"></a>TriplaneGaussian</h3><p><a href="https://zouzx.github.io/TriplaneGaussian/">Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers (zouzx.github.io)</a></p><p>单视图生成3D</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231218202024.png" alt="image.png|666"></p><h3 id="Mosaic-SDF"><a href="#Mosaic-SDF" class="headerlink" title="Mosaic-SDF"></a>Mosaic-SDF</h3><p><a href="https://lioryariv.github.io/msdf/">Mosaic-SDF (lioryariv.github.io)</a></p><p>一种新的三维模型的表示方法：M-SDF，基于Flow的生成式方法<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231218203722.png" alt="image.png|666"></p><h3 id="PI3D"><a href="#PI3D" class="headerlink" title="PI3D"></a>PI3D</h3><p><a href="https://arxiv.org/abs/2312.09069">[2312.09069] PI3D: Efficient Text-to-3D Generation with Pseudo-Image Diffusion (arxiv.org)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231218203837.png" alt="image.png|666"></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> GAN </tag>
            
            <tag> Flow </tag>
            
            <tag> Review </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-view 3D Reconstruction</title>
      <link href="/3DReconstruction/Basic%20Knowledge/Multi-view%203D%20Reconstruction/"/>
      <url>/3DReconstruction/Basic%20Knowledge/Multi-view%203D%20Reconstruction/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th style="text-align:center">3D Reconstruction</th><th style="text-align:center">Single-view</th><th style="text-align:center">Multi-view</th></tr></thead><tbody><tr><td style="text-align:center">特点</td><td style="text-align:center"><strong>简单但信息不足，未见区域很难重建</strong></td><td style="text-align:center"><strong>多视图信息互补但一致性很难保证</strong></td></tr><tr><td style="text-align:center">深度估计 <strong><a href="/3DReconstruction/Basic%20Knowledge/Other%20Paper%20About%20Reconstruction">DE</a></strong></td><td style="text-align:center">2K2K,ECON</td><td style="text-align:center">MVS,MVSNet-based</td></tr><tr><td style="text-align:center">隐式函数 <strong><a href="/3DReconstruction/Basic%20Knowledge/Other%20Paper%20About%20Reconstruction">IF</a></strong></td><td style="text-align:center">PIFu,ICON</td><td style="text-align:center">NeuS,DoubleField,SuGaR</td></tr><tr><td style="text-align:center">生成模型 <strong><a href="/3DReconstruction/Basic%20Knowledge/Generative%20Models%20Review">GM</a></strong></td><td style="text-align:center">BuilDIff, SG-GAN</td><td style="text-align:center">DiffuStereo</td></tr><tr><td style="text-align:center">混合方法 <strong>HM</strong></td><td style="text-align:center">HaP</td><td style="text-align:center">DMV3D</td></tr></tbody></table></div><p>NeRF：<a href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF-review">NeRF-review</a> | <a href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF-Mine">NeRF-Mine</a></p><p>Follow: <a href="https://www.zhihu.com/column/c_1710703836652716032">NeRF and Beyond日报</a> | <a href="https://github.com/yangjiheng/nerf_and_beyond_docs">nerf and beyond docs</a> | <strong><a href="https://github.com/ventusff/neurecon">ventusff/neurecon</a></strong> | <a href="https://paperswithcode.com/task/surface-reconstruction">Surface Reconstruction</a> | <a href="https://github.com/openMVG/awesome_3DReconstruction_list">传统3D Reconstruction</a><br>Blog: <a href="https://longtimenohack.com/">Jianfei Guo</a> |<br>人体: <a href="/3DReconstruction/Basic%20Knowledge/Multi-view%20Human%20Body%20Reconstruction">Multi-view Human Body Reconstruction</a></p><span id="more"></span><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231219125844.png" alt="image.png|666"></p><p>被动式单目视觉的多视图三维重建，目前重建结果最好的是基于 <strong>NeRF</strong> 的多视图重建方法。NeRF基本流程为从相机位姿出发，得到多条从相机原点到图片像素的光线，在光线上进行采样得到一系列空间点，然后对采样点坐标进行编码，输入密度MLP网络进行计算，得到采样点位置的密度值，同时对该点的方向进行编码，输入颜色MLP网络计算得到该点的颜色值。然后根据体渲染函数沿着光线积分，得到像素预测的颜色值并与真实的颜色值作损失，优化MLP网络参数，最后得到一个用MLP参数隐式表达的三维模型。为了从隐式函数中提取显示模型，需要使用MarchingCube得到物体表面的点云和网格。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231219125859.png" alt="image.png|666"></p><p>应用：</p><ul><li><a href="https://mp.weixin.qq.com/s/-VU-OBpdmU0DLiEgtTFEeg">快手智能3D物体重建系统解析 (qq.com)</a></li><li><a href="https://www.zhihu.com/question/449185693">三维重建如今有什么很现实的应用吗？ - 知乎 (zhihu.com)</a></li></ul><hr><p><strong><em>基于NeRF的多视图三维重建</em></strong></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><h1 id="Introduction-RelatedWork"><a href="#Introduction-RelatedWork" class="headerlink" title="Introduction+RelatedWork"></a>Introduction+RelatedWork</h1><p>COLMAP：</p><ul><li>SFM(Structure from motion)，估计相机位姿，特征点的稀疏点云</li><li>MVS(Multi-View Stereo)，估计深度图，深度图融合稠密点云</li><li>泊松表面重建(Screened Poisson Surface Reconstruction)，稠密点云重建网格</li></ul><h2 id="传统的多视图三维重建方法"><a href="#传统的多视图三维重建方法" class="headerlink" title="传统的多视图三维重建方法"></a>传统的多视图三维重建方法</h2><ul><li>基于点云<ul><li>SFM</li></ul></li><li>基于网格</li><li>基于体素</li><li>基于深度图<ul><li><a href="https://readpaper.com/pdf-annotate/note?pdfId=4518062699161739265&amp;noteId=1986540055632613120">MVSNet: Depth Inference for Unstructured Multi-view Stereo (readpaper.com)</a></li><li><a href="https://github.com/doubleZ0108/MVS">MVS: Multi-View Stereo based on deep learning. | Learning notes, codes and more. (github.com)</a></li></ul></li></ul><h2 id="基于NeRF体渲染的重建方法："><a href="#基于NeRF体渲染的重建方法：" class="headerlink" title="基于NeRF体渲染的重建方法："></a>基于NeRF体渲染的重建方法：</h2><h3 id="基于隐式表示的重建方法："><a href="#基于隐式表示的重建方法：" class="headerlink" title="基于隐式表示的重建方法："></a>基于隐式表示的重建方法：</h3><ul><li><a href="https://github.com/autonomousvision/occupancy_networks">occupancy_networks: This repository contains the code for the paper “Occupancy Networks - Learning 3D Reconstruction in Function Space” (github.com)</a></li><li><a href="https://github.com/facebookresearch/DeepSDF?tab=readme-ov-file">facebookresearch/DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation (github.com)</a></li></ul><p><strong>NeRF被提出</strong><br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Network.png" alt="Network.png|666"></p><p>对NeRF改进：</p><ul><li>快：Plenoxels、<strong>InstantNGP</strong></li><li>好：<a href="https://github.com/autonomousvision/unisurf">UNISURF</a>、VolSDF、<strong>NeuS</strong></li><li>InstantNGP+NeuS：Neuralangelo、PermutoSDF、NeuS2、NeuDA、Instant-NSR、BakedSDF</li></ul><p><div class="mxgraph" style="max-width:100%;border:1px solid transparent;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;nav&quot;:true,&quot;resize&quot;:true,&quot;toolbar&quot;:&quot;zoom layers tags lightbox&quot;,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile host=\&quot;Electron\&quot; modified=\&quot;2023-12-19T05:21:51.620Z\&quot; agent=\&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) draw.io/22.1.2 Chrome/114.0.5735.289 Electron/25.9.4 Safari/537.36\&quot; etag=\&quot;e0nhU3MTx1s-iZX-p5GG\&quot; version=\&quot;22.1.2\&quot; type=\&quot;device\&quot;&gt;\n  &lt;diagram name=\&quot;Page-1\&quot; id=\&quot;7M3fPsfFLWG27eusrSni\&quot;&gt;\n    &lt;mxGraphModel dx=\&quot;1418\&quot; dy=\&quot;828\&quot; grid=\&quot;0\&quot; gridSize=\&quot;10\&quot; guides=\&quot;0\&quot; tooltips=\&quot;0\&quot; connect=\&quot;1\&quot; arrows=\&quot;1\&quot; fold=\&quot;1\&quot; page=\&quot;0\&quot; pageScale=\&quot;1\&quot; pageWidth=\&quot;827\&quot; pageHeight=\&quot;1169\&quot; math=\&quot;1\&quot; shadow=\&quot;0\&quot;&gt;\n      &lt;root&gt;\n        &lt;mxCell id=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;1\&quot; parent=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-1\&quot; value=\&quot;\&quot; style=\&quot;endArrow=classic;html=1;rounded=0;strokeWidth=3;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;20\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1320\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-2\&quot; value=\&quot;多视图三维重建\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;10\&quot; y=\&quot;290\&quot; width=\&quot;90\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-4\&quot; value=\&quot;传统方法\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;120\&quot; y=\&quot;200\&quot; width=\&quot;60\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-6\&quot; value=\&quot;\&quot; style=\&quot;shape=curlyBracket;whiteSpace=wrap;html=1;rounded=1;labelPosition=left;verticalLabelPosition=middle;align=right;verticalAlign=middle;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;180\&quot; y=\&quot;168.75\&quot; width=\&quot;20\&quot; height=\&quot;92.5\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-7\&quot; value=\&quot;基于点云\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;200\&quot; y=\&quot;150\&quot; width=\&quot;60\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-8\&quot; value=\&quot;基于网格\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;200\&quot; y=\&quot;180\&quot; width=\&quot;60\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-9\&quot; value=\&quot;基于深度图\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;200\&quot; y=\&quot;240\&quot; width=\&quot;70\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-10\&quot; value=\&quot;基于体素\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;200\&quot; y=\&quot;210\&quot; width=\&quot;60\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-11\&quot; value=\&quot;隐式表示\&quot; style=\&quot;text;html=1;strokeColor=#d6b656;fillColor=#fff2cc;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;360\&quot; y=\&quot;210\&quot; width=\&quot;60\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-12\&quot; value=\&quot;显式表示\&quot; style=\&quot;text;html=1;strokeColor=#d6b656;fillColor=#fff2cc;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;160\&quot; y=\&quot;120\&quot; width=\&quot;60\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-14\&quot; value=\&quot;Occupancy Networks\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;347.5\&quot; y=\&quot;260\&quot; width=\&quot;125\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-17\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;150.95999999999998\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;150.95999999999998\&quot; y=\&quot;230\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-19\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;strokeWidth=2;strokeColor=#FF3333;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;319.93\&quot; y=\&quot;340\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;319.93\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-21\&quot; value=\&quot;2018\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;292.5\&quot; y=\&quot;340\&quot; width=\&quot;55\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-22\&quot; value=\&quot;DeepSDF\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;488.5\&quot; y=\&quot;260\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-24\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;strokeWidth=2;strokeColor=#FF3333;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;480\&quot; y=\&quot;340\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;480.33\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-25\&quot; value=\&quot;2019\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;452.5\&quot; y=\&quot;340\&quot; width=\&quot;55\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-26\&quot; value=\&quot;2020\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;613.5\&quot; y=\&quot;340\&quot; width=\&quot;55\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-27\&quot; value=\&quot;NeRF\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;640\&quot; y=\&quot;208.75\&quot; width=\&quot;46.5\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-29\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;strokeWidth=3;entryX=0.5;entryY=1;entryDx=0;entryDy=0;\&quot; parent=\&quot;1\&quot; target=\&quot;uzRY4Giuk2QTnD9La2iV-27\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;663.08\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;663.08\&quot; y=\&quot;250\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-31\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;650\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;650\&quot; y=\&quot;280\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-32\&quot; value=\&quot;DVR\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;600\&quot; y=\&quot;360\&quot; width=\&quot;40\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-33\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;620\&quot; y=\&quot;360\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;620\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-34\&quot; value=\&quot;IDR\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;630\&quot; y=\&quot;260\&quot; width=\&quot;26\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-35\&quot; value=\&quot;2021\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;772\&quot; y=\&quot;340\&quot; width=\&quot;55\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-36\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;edgeStyle=orthogonalEdgeStyle;entryX=0.5;entryY=1;entryDx=0;entryDy=0;\&quot; parent=\&quot;1\&quot; target=\&quot;uzRY4Giuk2QTnD9La2iV-14\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;470\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;470\&quot; y=\&quot;280\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-37\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;edgeStyle=orthogonalEdgeStyle;\&quot; parent=\&quot;1\&quot; target=\&quot;uzRY4Giuk2QTnD9La2iV-22\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;490\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;520\&quot; y=\&quot;280\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-39\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;strokeWidth=2;strokeColor=#FF3333;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;640\&quot; y=\&quot;340\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;640.3299999999999\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-40\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;strokeWidth=2;strokeColor=#FF3333;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;800\&quot; y=\&quot;340\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;800.3299999999999\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-41\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;827\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;827\&quot; y=\&quot;280\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-42\&quot; value=\&quot;NLR\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;810\&quot; y=\&quot;260\&quot; width=\&quot;30\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-43\&quot; value=\&quot;&amp;lt;font style=&amp;quot;font-size: 8px;&amp;quot;&amp;gt;Neural Lumigraph Rendering&amp;lt;/font&amp;gt;\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;730\&quot; y=\&quot;248.75\&quot; width=\&quot;120\&quot; height=\&quot;11.25\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-45\&quot; value=\&quot;SDFDiff\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;580\&quot; y=\&quot;261.25\&quot; width=\&quot;40\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-46\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;edgeStyle=orthogonalEdgeStyle;\&quot; parent=\&quot;1\&quot; target=\&quot;uzRY4Giuk2QTnD9La2iV-45\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;610\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;619.67\&quot; y=\&quot;281.25\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-47\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;580\&quot; y=\&quot;360\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;580\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-48\&quot; value=\&quot;DIST\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;560\&quot; y=\&quot;360\&quot; width=\&quot;40\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-49\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;831.83\&quot; y=\&quot;360\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;831.83\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-50\&quot; value=\&quot;UNISURF\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;810\&quot; y=\&quot;360\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-52\&quot; value=\&quot;2004 Multiple View Geometry in Computer Vision\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;10\&quot; y=\&quot;360\&quot; width=\&quot;282.5\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-53\&quot; value=\&quot;\&quot; style=\&quot;endArrow=classic;html=1;rounded=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;exitX=0.25;exitY=0;exitDx=0;exitDy=0;edgeStyle=orthogonalEdgeStyle;\&quot; parent=\&quot;1\&quot; source=\&quot;uzRY4Giuk2QTnD9La2iV-52\&quot; target=\&quot;uzRY4Giuk2QTnD9La2iV-2\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;50\&quot; y=\&quot;420\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;100\&quot; y=\&quot;370\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-54\&quot; value=\&quot;提出问题\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;55\&quot; y=\&quot;320\&quot; width=\&quot;55\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-55\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;strokeWidth=2;strokeColor=#FF3333;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;960\&quot; y=\&quot;340\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;960.3299999999999\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-56\&quot; value=\&quot;2022\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;930\&quot; y=\&quot;340\&quot; width=\&quot;55\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-57\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;strokeWidth=2;strokeColor=#FF3333;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1120\&quot; y=\&quot;340\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1120.33\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-58\&quot; value=\&quot;2023\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1090\&quot; y=\&quot;340\&quot; width=\&quot;55\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-59\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;strokeWidth=2;strokeColor=#FF3333;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1280\&quot; y=\&quot;340\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1280.33\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-60\&quot; value=\&quot;2024\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1250\&quot; y=\&quot;340\&quot; width=\&quot;55\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-61\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;strokeWidth=3;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;860\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;860\&quot; y=\&quot;280\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-62\&quot; value=\&quot;NeuS\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;850\&quot; y=\&quot;260\&quot; width=\&quot;30\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-63\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1110\&quot; y=\&quot;321.25\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1110\&quot; y=\&quot;281.25\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-64\&quot; value=\&quot;NeuS2\&quot; style=\&quot;text;html=1;strokeColor=#82b366;fillColor=#d5e8d4;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1090\&quot; y=\&quot;261.25\&quot; width=\&quot;40\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-65\&quot; value=\&quot;VolSDF\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;940\&quot; y=\&quot;261.25\&quot; width=\&quot;30\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-66\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;950\&quot; y=\&quot;321.25\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;950\&quot; y=\&quot;281.25\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-67\&quot; value=\&quot;速度↑\&quot; style=\&quot;text;html=1;strokeColor=#82b366;fillColor=#d5e8d4;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;640\&quot; y=\&quot;180\&quot; width=\&quot;40\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-70\&quot; value=\&quot;结合显式\&quot; style=\&quot;text;html=1;strokeColor=#b85450;fillColor=#f8cecc;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;640\&quot; y=\&quot;160\&quot; width=\&quot;56.75\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-71\&quot; value=\&quot;Plenoxels\&quot; style=\&quot;text;html=1;strokeColor=#b85450;fillColor=#f8cecc;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;985\&quot; y=\&quot;261.25\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-72\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;edgeStyle=orthogonalEdgeStyle;\&quot; parent=\&quot;1\&quot; target=\&quot;uzRY4Giuk2QTnD9La2iV-71\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;970\&quot; y=\&quot;321.25\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;970\&quot; y=\&quot;281.25\&quot; as=\&quot;targetPoint\&quot; /&gt;\n            &lt;Array as=\&quot;points\&quot;&gt;\n              &lt;mxPoint x=\&quot;970\&quot; y=\&quot;300\&quot; /&gt;\n              &lt;mxPoint x=\&quot;1015\&quot; y=\&quot;300\&quot; /&gt;\n            &lt;/Array&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-74\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;910\&quot; y=\&quot;360\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;910\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-75\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;strokeWidth=3;entryX=0.5;entryY=1;entryDx=0;entryDy=0;\&quot; parent=\&quot;1\&quot; target=\&quot;uzRY4Giuk2QTnD9La2iV-77\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1060\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1060\&quot; y=\&quot;250\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-77\&quot; value=\&quot;InstantNGP\&quot; style=\&quot;text;html=1;strokeColor=#b85450;fillColor=#f8cecc;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1020\&quot; y=\&quot;205\&quot; width=\&quot;80\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-79\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1070\&quot; y=\&quot;360\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1070\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-80\&quot; value=\&quot;Voxurf\&quot; style=\&quot;text;html=1;strokeColor=#b85450;fillColor=#f8cecc;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1045\&quot; y=\&quot;360\&quot; width=\&quot;50\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-81\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;edgeStyle=orthogonalEdgeStyle;\&quot; parent=\&quot;1\&quot; target=\&quot;uzRY4Giuk2QTnD9La2iV-82\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1160\&quot; y=\&quot;321.25\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1160\&quot; y=\&quot;281.25\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-82\&quot; value=\&quot;Neuralangelo\&quot; style=\&quot;text;html=1;strokeColor=#82b366;fillColor=#d5e8d4;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1150\&quot; y=\&quot;261.25\&quot; width=\&quot;80\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-83\&quot; value=\&quot;NeuDA\&quot; style=\&quot;text;html=1;strokeColor=#82b366;fillColor=#d5e8d4;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1160.5\&quot; y=\&quot;360\&quot; width=\&quot;59\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-84\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1189.83\&quot; y=\&quot;360\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1189.83\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-85\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1249.86\&quot; y=\&quot;360\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1249.86\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-86\&quot; value=\&quot;NeUDF\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1230\&quot; y=\&quot;360\&quot; width=\&quot;40\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-87\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;exitX=0.5;exitY=0;exitDx=0;exitDy=0;edgeStyle=orthogonalEdgeStyle;\&quot; parent=\&quot;1\&quot; source=\&quot;uzRY4Giuk2QTnD9La2iV-88\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1040\&quot; y=\&quot;360\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1040\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-88\&quot; value=\&quot;HF-NeuS\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;985\&quot; y=\&quot;360\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-90\&quot; value=\&quot;Geo-NeuS\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;985\&quot; y=\&quot;380\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-91\&quot; value=\&quot;Color-NeuS\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1241\&quot; y=\&quot;261.25\&quot; width=\&quot;69\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-92\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;edgeStyle=orthogonalEdgeStyle;\&quot; parent=\&quot;1\&quot; target=\&quot;uzRY4Giuk2QTnD9La2iV-91\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1230\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1230\&quot; y=\&quot;280\&quot; as=\&quot;targetPoint\&quot; /&gt;\n            &lt;Array as=\&quot;points\&quot;&gt;\n              &lt;mxPoint x=\&quot;1230\&quot; y=\&quot;300\&quot; /&gt;\n              &lt;mxPoint x=\&quot;1276\&quot; y=\&quot;300\&quot; /&gt;\n            &lt;/Array&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-93\&quot; value=\&quot;D-NeuS\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;985\&quot; y=\&quot;400\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-94\&quot; value=\&quot;PermutoSDF\&quot; style=\&quot;text;html=1;strokeColor=#82b366;fillColor=#d5e8d4;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1150\&quot; y=\&quot;230\&quot; width=\&quot;80\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-95\&quot; value=\&quot;Instant-NSR\&quot; style=\&quot;text;html=1;strokeColor=#82b366;fillColor=#d5e8d4;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1075\&quot; y=\&quot;230\&quot; width=\&quot;70\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-96\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;strokeWidth=3;edgeStyle=orthogonalEdgeStyle;endFill=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;680\&quot; y=\&quot;319\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;680\&quot; y=\&quot;633\&quot; as=\&quot;targetPoint\&quot; /&gt;\n            &lt;Array as=\&quot;points\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-97\&quot; value=\&quot;\&quot; style=\&quot;endArrow=classic;html=1;rounded=0;strokeWidth=3;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;680\&quot; y=\&quot;489.64\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1320\&quot; y=\&quot;490\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-98\&quot; value=\&quot;采样方式\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;682\&quot; y=\&quot;465\&quot; width=\&quot;55\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-100\&quot; value=\&quot;编码方式\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;682\&quot; y=\&quot;494\&quot; width=\&quot;55\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-101\&quot; value=\&quot;NerfAcc\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1150\&quot; y=\&quot;430\&quot; width=\&quot;40\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-102\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1170.5\&quot; y=\&quot;490\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1170.5\&quot; y=\&quot;450\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-103\&quot; value=\&quot;Mip-NeRF\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;800\&quot; y=\&quot;450\&quot; width=\&quot;70\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-104\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;830.5\&quot; y=\&quot;510\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;830.5\&quot; y=\&quot;470\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-105\&quot; value=\&quot;InstantNGP\&quot; style=\&quot;text;html=1;strokeColor=#b85450;fillColor=#f8cecc;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1020\&quot; y=\&quot;450\&quot; width=\&quot;80\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-106\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;strokeWidth=3;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1059.8600000000001\&quot; y=\&quot;510\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1059.8600000000001\&quot; y=\&quot;470\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-107\&quot; value=\&quot;Mip-NeRF 360\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;910\&quot; y=\&quot;430\&quot; width=\&quot;90\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-108\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;950.5\&quot; y=\&quot;490\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;950.5\&quot; y=\&quot;450\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-109\&quot; value=\&quot;Zip-NeRF\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1100.5\&quot; y=\&quot;450\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-110\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1130.16\&quot; y=\&quot;510\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1130.16\&quot; y=\&quot;470\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-111\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1210.66\&quot; y=\&quot;510\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1210.66\&quot; y=\&quot;470\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-112\&quot; value=\&quot;Tri-MipRF\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1181\&quot; y=\&quot;450\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-115\&quot; value=\&quot;体渲染函数\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;680\&quot; y=\&quot;610\&quot; width=\&quot;80\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-116\&quot; value=\&quot;损失函数/辅助信息\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;673\&quot; y=\&quot;631\&quot; width=\&quot;130\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-123\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1189.43\&quot; y=\&quot;530\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1189.43\&quot; y=\&quot;490\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-125\&quot; value=\&quot;PET-NeuS\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1160.5\&quot; y=\&quot;530\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-126\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;740\&quot; y=\&quot;530\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;740\&quot; y=\&quot;490\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-127\&quot; value=\&quot;Fourier Features\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;696.75\&quot; y=\&quot;530\&quot; width=\&quot;100\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-129\&quot; value=\&quot;Vox-Surf\&quot; style=\&quot;text;html=1;strokeColor=#b85450;fillColor=#f8cecc;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1045\&quot; y=\&quot;380\&quot; width=\&quot;50\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-130\&quot; value=\&quot;\&quot; style=\&quot;endArrow=classic;html=1;rounded=0;strokeWidth=3;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;680\&quot; y=\&quot;630\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1320\&quot; y=\&quot;630.36\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-133\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1109.66\&quot; y=\&quot;670\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1109.66\&quot; y=\&quot;630\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-134\&quot; value=\&quot;NeuralWarp\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1080\&quot; y=\&quot;670\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-135\&quot; value=\&quot;Geo-NeuS\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1010\&quot; y=\&quot;710\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-136\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1039.66\&quot; y=\&quot;670\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1039.66\&quot; y=\&quot;630\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-137\&quot; value=\&quot;MonoSDF\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1010\&quot; y=\&quot;690\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-138\&quot; value=\&quot;RegSDF\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1010\&quot; y=\&quot;670\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-139\&quot; value=\&quot;neuralrecon-w.\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;940\&quot; y=\&quot;650\&quot; width=\&quot;90\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-141\&quot; value=\&quot;POCO\&quot; style=\&quot;text;html=1;strokeColor=#b85450;fillColor=#f8cecc;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;985\&quot; y=\&quot;240\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-142\&quot; value=\&quot;SPIDR\&quot; style=\&quot;text;html=1;strokeColor=#b85450;fillColor=#f8cecc;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1045\&quot; y=\&quot;400\&quot; width=\&quot;50\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-143\&quot; value=\&quot;Point-NeRF\&quot; style=\&quot;text;html=1;strokeColor=#b85450;fillColor=#f8cecc;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;940\&quot; y=\&quot;220\&quot; width=\&quot;70\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-144\&quot; value=\&quot;BakedSDF\&quot; style=\&quot;text;html=1;strokeColor=#82b366;fillColor=#d5e8d4;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1150.5\&quot; y=\&quot;200\&quot; width=\&quot;80\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-145\&quot; value=\&quot;快\&quot; style=\&quot;ellipse;whiteSpace=wrap;html=1;aspect=fixed;fillColor=#60a917;fontColor=#ffffff;strokeColor=#2D7600;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;525\&quot; y=\&quot;421\&quot; width=\&quot;40\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-146\&quot; value=\&quot;好\&quot; style=\&quot;ellipse;whiteSpace=wrap;html=1;aspect=fixed;fillColor=#e51400;fontColor=#ffffff;strokeColor=#B20000;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;605\&quot; y=\&quot;421\&quot; width=\&quot;40\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-147\&quot; value=\&quot;省\&quot; style=\&quot;ellipse;whiteSpace=wrap;html=1;aspect=fixed;fillColor=#1ba1e2;fontColor=#ffffff;strokeColor=#006EAF;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;565\&quot; y=\&quot;486\&quot; width=\&quot;40\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-148\&quot; value=\&quot;多\&quot; style=\&quot;ellipse;whiteSpace=wrap;html=1;aspect=fixed;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;20\&quot; y=\&quot;420\&quot; width=\&quot;40\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-149\&quot; value=\&quot;NeRF是在线训练的方法，可以不考虑泛化性\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;60\&quot; y=\&quot;425\&quot; width=\&quot;250\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-150\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;\&quot; parent=\&quot;1\&quot; source=\&quot;uzRY4Giuk2QTnD9La2iV-145\&quot; target=\&quot;uzRY4Giuk2QTnD9La2iV-146\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;555\&quot; y=\&quot;481\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;604\&quot; y=\&quot;441\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-152\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;entryX=0.771;entryY=0.904;entryDx=0;entryDy=0;entryPerimeter=0;exitX=0;exitY=0;exitDx=0;exitDy=0;\&quot; parent=\&quot;1\&quot; source=\&quot;uzRY4Giuk2QTnD9La2iV-147\&quot; target=\&quot;uzRY4Giuk2QTnD9La2iV-145\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;525\&quot; y=\&quot;531\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;575\&quot; y=\&quot;481\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-153\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;exitX=1;exitY=0;exitDx=0;exitDy=0;entryX=0;entryY=1;entryDx=0;entryDy=0;\&quot; parent=\&quot;1\&quot; source=\&quot;uzRY4Giuk2QTnD9La2iV-147\&quot; target=\&quot;uzRY4Giuk2QTnD9La2iV-146\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;635\&quot; y=\&quot;521\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;685\&quot; y=\&quot;471\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-154\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;730\&quot; y=\&quot;360\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;730\&quot; y=\&quot;320\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-155\&quot; value=\&quot;&amp;lt;div&amp;gt;Neural Sparse Voxel Fields&amp;lt;/div&amp;gt;\&quot; style=\&quot;text;html=1;strokeColor=#b85450;fillColor=#f8cecc;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;692\&quot; y=\&quot;360\&quot; width=\&quot;88\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-156\&quot; value=\&quot;&amp;lt;div&amp;gt;DVGO&amp;lt;/div&amp;gt;\&quot; style=\&quot;text;html=1;strokeColor=#b85450;fillColor=#f8cecc;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;880\&quot; y=\&quot;360\&quot; width=\&quot;60\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-157\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1229.31\&quot; y=\&quot;670\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1229.31\&quot; y=\&quot;630\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-158\&quot; value=\&quot;S3IM\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1210\&quot; y=\&quot;670\&quot; width=\&quot;40\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-163\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;strokeWidth=3;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;860\&quot; y=\&quot;630\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;860\&quot; y=\&quot;590\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-164\&quot; value=\&quot;NeuS\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;850\&quot; y=\&quot;570\&quot; width=\&quot;30\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-165\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1181\&quot; y=\&quot;670\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1181\&quot; y=\&quot;630\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-166\&quot; value=\&quot;StEik\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1160.5\&quot; y=\&quot;670\&quot; width=\&quot;40\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-168\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1260\&quot; y=\&quot;630\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1260\&quot; y=\&quot;590\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-169\&quot; value=\&quot;LiNeRF\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1241\&quot; y=\&quot;570\&quot; width=\&quot;40\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-170\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1210.8\&quot; y=\&quot;630\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1210.8\&quot; y=\&quot;590\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-171\&quot; value=\&quot;PL-NeRF\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1180\&quot; y=\&quot;570\&quot; width=\&quot;61\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-173\&quot; value=\&quot;\&quot; style=\&quot;endArrow=none;html=1;rounded=0;\&quot; parent=\&quot;1\&quot; edge=\&quot;1\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1150.5\&quot; y=\&quot;630\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1150.5\&quot; y=\&quot;590\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;uzRY4Giuk2QTnD9La2iV-174\&quot; value=\&quot;NeUDF\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1130\&quot; y=\&quot;570\&quot; width=\&quot;50\&quot; height=\&quot;20\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n      &lt;/root&gt;\n    &lt;/mxGraphModel&gt;\n  &lt;/diagram&gt;\n&lt;/mxfile&gt;\n&quot;}"></div></p><script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BuilDiff</title>
      <link href="/3DReconstruction/Single-view/Generative%20Models/BuilDiff/"/>
      <url>/3DReconstruction/Single-view/Generative%20Models/BuilDiff/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>BuilDiff: 3D Building Shape Generation using Single-Image Conditional Point Cloud Diffusion Models</th></tr></thead><tbody><tr><td>Author</td><td>Wei, Yao and Vosselman, George and Yang, Michael Ying</td></tr><tr><td>Conf/Jour</td><td>ICCV</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://github.com/weiyao1996/BuilDiff">weiyao1996/BuilDiff: BuilDiff: 3D Building Shape Generation using Single-Image Conditional Point Cloud Diffusion Models (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4796303379219349505&amp;noteId=2014132586369911808">BuilDiff: 3D Building Shape Generation using Single-Image Conditional Point Cloud Diffusion Models (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021114740.png" alt="image.png|666"></p><ul><li>关注建筑物的重建，为 3D Diffusion Models 中添加了图片的信息嵌入(预训练了一个图片编码器)</li><li>两阶段的点云降噪模型，第一阶段关注全局，第二阶段关注细节</li><li>提出了两个自定义的新数据集，并在数据集上验证了本方法的优点</li></ul><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>具有低数据采集成本的三维建筑生成，如单图像到三维，变得越来越重要。然而，现有的单图像到 3d 的建筑创作作品大多局限于具有特定视角的图像，因此难以缩放到实际情况中常见的通用视图图像。为了填补这一空白，我们提出了一种新的三维建筑形状生成方法，利用<strong>点云扩散模型</strong>和<strong>图像调节方案</strong>，该方法对输入图像具有灵活性。该方法通过两种条件扩散模型的配合，在去噪过程中引入正则化策略，实现了在保持整体结构的前提下对建筑物屋顶进行合成。我们在两个新建立的数据集上验证了我们的框架，大量的实验表明，我们的方法在构建生成质量方面优于以前的工作。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>我们的目标是<strong>从单一的通用视图图像中</strong>生成建筑物的 3D 点云，而不是仅从特定角度(例如最低点或街道视图)获取图像，旨在提高所提出方法的适用性。如图 1 所示，我们引入了一个分层框架 BuilDiff，它由三个部分组成:<br>(a)图像自编码器 image auto-encoder<br>(b)image-conditional point cloud base diffusion<br>(c) image-conditional point cloud upsampler diffusion</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021114740.png" alt="image.png|666"></p><h2 id="Image-Auto-encoder-Pre-training"><a href="#Image-Auto-encoder-Pre-training" class="headerlink" title="Image Auto-encoder Pre-training"></a>Image Auto-encoder Pre-training</h2><p>对扩散模型进行条件约束的一种常见而直接的方法是<strong>将 cue images 压缩到潜在空间中</strong>。我们没有使用在公共图像数据库(例如 ImageNet[4])上预训练的编码器直接将输入图像映射到作为建筑物模糊条件的潜在特征向量，<strong>而是对编码器进行微调</strong>，并使用训练集的建筑物图像训练额外的解码器。<br>如图 2 所示，整个网络可以看作是一个图像自编码器，它对输入的建筑图像进行学习重构，提取出作为代表性条件的建筑特征。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023200406.png" alt="image.png|666"></p><ul><li>输入 HxW 图像，图像自编码器：ResNet-34</li><li>编码后大小为 H/32 x W/32，然后馈送到 <a href="https://readpaper.com/pdf-annotate/note?pdfId=4500174562083364865&amp;noteId=2027677837097375232">stacked dilated convolution layers</a> ($D^{*}$)<ul><li>$D^{*}$ 有四个 dilated convolution layers，膨胀率分别为 1、2、4、8</li></ul></li><li>在解码阶段有两种处理方式<ul><li>通过转置卷积层，可以将特征映射上采样到与输入图像 i 大小相同的 i</li><li>通过 1×1 卷积层和线性层将特征映射线性投影到嵌入 zI 的一维图像中，该图像的维数为 d</li></ul></li></ul><p>图像自编码器损失： $\mathcal L_{AE}=\mathcal L_{rec}(I,\hat{I})+\mathcal L_{con}(z_{I},z_{I}^{a})$</p><ul><li>图像之间的重建损失 $\mathcal L_{rec}$</li><li>一致性损失 $\mathcal L_{con}$，它促使图像 I 的嵌入 $z_{I}$ 尽可能接近图像 I 的增广版本 $I^{a}$ 的嵌入 $z_{I}^{a}$。</li></ul><p>训练后，我们使用冻结的预训练图像自编码器将图像映射到嵌入 $z_{I}$，这作为以下扩散模型的图像依赖条件。</p><h2 id="Image-conditional-Point-Cloud-Diffusion"><a href="#Image-conditional-Point-Cloud-Diffusion" class="headerlink" title="Image-conditional Point Cloud Diffusion"></a>Image-conditional Point Cloud Diffusion</h2><p>Forward diffusion process 橙色：GT PointCloud —&gt; Noisy PointCloud<br>Denoising diffusion process 蓝色：Noisy PointCloud —&gt; GT PointCloud</p><p>逐渐加噪 $q(x_t|x_{t-1})$，噪声点云: $x_{t}=\sqrt{\bar{\alpha}_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon$</p><ul><li>包含 K 个点的 GT 点云 ： $\begin{aligned}x_0&amp;\sim q(x_0)\end{aligned}$</li><li>$\alpha_{t}:=1-\beta_{t}$ ， $\bar{\alpha}_t:=\prod_{s=1}^t\alpha_s$<ul><li>$\beta_{t}\in\{\beta_{1},…\beta_{T}\}$ 递增噪声调度序列，T：final time step</li></ul></li></ul><p>逐渐降噪 $q(x_{t-1}|x_t,z_I)$，可用降噪网络 $p_\theta(x_{t-1}|x_t,z_I)$ 近似表示</p><ul><li>从高斯先验中采样 $p(x_{T})\sim\mathcal{N}(0,\mathcal{I}).$</li><li>对于 $p_\theta(x_{t-1}|x_t,z_I)$ 每个降噪步骤：<ul><li>输入噪声点云 $x_t \in \mathbb{R}^{K \times 3}$，time step t 和图片 embedding $z_I \in \mathbb{R}^d$</li><li>输出噪声 $\epsilon_\theta(x_t,t,z_I)\in\mathbb{R}^{K\times3}$，并与目标的标准高斯 $\epsilon \sim \mathcal{N}(0,\mathcal{I})$ 做损失</li><li>$\mathcal{L}_{eps}=\left|\epsilon-\epsilon_\theta(x_t,t,z_I)\right|^2$</li></ul></li></ul><p>降噪网络 $\theta$ 建立在 PVCNNs 的基础上</p><p>PointNet<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231030155645.png" alt="image.png|666"></p><p>PointNet++<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231030154311.png" alt="image.png|666"></p><ul><li>PVCNNs <a href="https://readpaper.com/pdf-annotate/note?pdfId=4544669809538392065&amp;noteId=2018413897297176576">Point-Voxel CNN for Efficient 3D Deep Learning (readpaper.com)</a><ul><li>主要由集合抽象(set abstraction SA)模块和特征传播(feature propagation FP)模块两大部分组成<ul><li>参考 <a href="https://readpaper.com/pdf-annotate/note?pdfId=4545028882578432001">PointNet++</a> 和 <a href="https://readpaper.com/pdf-annotate/note?pdfId=4500216149471551490&amp;noteId=2011522008392682752">PointNet</a></li><li>SA 模块通常由点体素卷积(PVConvs)和多层感知器(mlp)组成</li><li>FP 模块通常由<a href="https://blog.csdn.net/qq_39478403/article/details/105796249">最近邻插值</a>、mlp 和 PVConvs 组成，通过使用基于点和体素的分支，PVConvs 可以捕获点云的全局和局部结构</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231024222944.png" alt="image.png|666"></p><p>在向 SA 或 FP 模块发送输入之前，将图像嵌入 zI 与由 t 导出的时间嵌入(记为 zt，正弦位置嵌入生成)连接起来，扩展和 concat 后形成 Kx2d，然后输入进两个带 LeakyReLU 激活函数的卷积层，得到一个大小为 K × d 的特征图，然后将其与 SA 和 FP 模块中的点特征连接起来(concatenated)</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231024104815.png" alt="image.png|666"></p><p>额外的损失函数：$\mathcal{L}_{reg}=\lambda(t)\Omega(proj(x_0),proj(\hat{x}_0))$<br>基于 $\epsilon_\theta,$ 期望的 $x_{0}$ 为：$\hat{x}_0=\frac{1}{\sqrt{\bar{\alpha}_t}}(x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_\theta(x_t,t,z_I))$，然后将 $x_{0}$ 和 $\hat{x}_0$ 投影到 GT，得到 $proj(x_0),proj(\hat{x}_0)$，采用基于点的度量 $\Omega$ 来测量两个 footprints 的相似性。</p><ul><li>其中 λ 是依赖于时间步长 t 的权值<ul><li>当 t 趋近 T 时，$x_{T}$ 包含更多的噪声，因此 $\Omega$ 采用更小的权重</li><li>当 t 趋近 0 时，$x_{0}$ 包含更少的噪声，因此 $\Omega$ 采用更大的权重</li></ul></li></ul><p>$\theta$ 降噪网络的总损失 $\mathcal{L}_\theta=\mathcal{L}_{eps}+\rho\mathcal{L}_{reg}$<br>整个训练过程在算法 1 中描述。与[23]类似，我们采用了一个无分类器的引导策略[13]，它联合学习一个条件模型和一个无条件模型。将嵌入 zI 的条件图像随机丢弃，将条件输出 $\epsilon_\theta(x_t,t,z_I)$ 随机替换为无条件输出 $\epsilon_\theta(x_t,t,\varnothing).$<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231024111221.png" alt="image.png|444"></p><p>在采样过程中，去噪扩散从高斯噪声(即 $x_T\sim\mathcal{N}(0,\mathcal{I}))$)开始，逐步用网络 θ 的输出去噪。$x_{T-1}$ 可以预测为: $x_{t-1}=\frac{1}{\sqrt{\alpha_t}}\bigl(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{guided}(x_t,t,z_I)\bigr)+\sigma_t\mathbf{z}$<br>其中 t 从 T 到 1，z 从标准高斯分布中采样，当 t &gt;1 时。利用 guidance 标度 γ，guidance 噪声输出为: $\epsilon_{guided}:=(1+\gamma)\epsilon_\theta(x_t,t,z_I)-\gamma\epsilon_\theta(x_t,t,\varnothing)$ ，最终，当 t = 1时，可以对期望的 $x_0$ 进行采样。</p><h2 id="Point-Cloud-Upsampler-Diffusion"><a href="#Point-Cloud-Upsampler-Diffusion" class="headerlink" title="Point Cloud Upsampler Diffusion"></a>Point Cloud Upsampler Diffusion</h2><ul><li>对于点云基础扩散（Image-conditional Point Cloud Diffusion），关键目标是生成低分辨率点云(K 点)，可以粗略地捕捉建筑物的整体结构</li><li>. Point Cloud Upsampler Diffusion 由冷冻预训练的自编码器导出的图像嵌入 zI 和由基础扩散推断的低分辨率点云作为输入，目标在于生成具有细粒度结构的高分辨率点云</li></ul><p>假设期望点云 $x_0\in\mathbb{R}^{N\times3}$ 由 N 个点(N &gt; K)组成，我们从高斯先验分布 $p(x_T)\sim\mathcal{N}(0,\mathcal{I}).$ 中随机采样噪声张量 $x_T\in\mathbb{R}^{N\times3}$<br>在训练过程中，去噪网络θ取 K 个点(即低分辨率点云)和从噪声 xT 中采样的(N−K)个点、时间步长 t 和条件化图像嵌入 zI 作为输入，在每一步中，用低分辨率点云替换θ采样的 N 个点中的前 K 个点，更新后的 N 个点作为下一个时间步长的输入。<strong>简而言之，为了达到 N 个点，我们对 K 个点进行上采样，并对其余(N−K)个点进行去噪</strong></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><p>为了验证该方法的性能，我们创建了两个数据集，BuildingNet-SVI 和 BuildingNL3D，提供了数千对建筑物的图像- 3d 对。</p><p>BuildingNet-SVI：基于 BuildingNet[33]数据集，该数据集涵盖了多种合成三维建筑模型(如教堂、住宅、办公楼)，我们收集了相应的建筑单视图 RGB 合成图像，得到 406 对图像-3D 对。此外，我们在像素级手动标注前景对象(即单个建筑物)，并裁剪以建筑物为中心的每张图像。每个建筑点云有 10 万个均匀分布的 3D 点。我们遵循 BuildingNet[33]的官方分割规则，因此分别使用 321 和 85 对图像- 3d 对进行训练和测试<br>BuildingNL3D：我们收集了2,769对位于荷兰某城市市区的建筑物的航空 RGB 图像和机载激光扫描(机载激光扫描)点云。与具有单个建筑物和相对干净背景的合成图像不同，航空图像通常面临更多挑战，例如在一张图像中出现多个建筑物。因此，对建筑物进行手动标记，以便每张图像中只出现一个感兴趣的建筑物。原始 ALS 点云从其真实地理坐标归一化为 $[-1,1]^3$ 范围内的 xyz 坐标。在训练集和测试集中的建筑物不重复的情况下，根据基于 tile 的分割规则，将数据集划分为 2171 个 image3D 训练对和 598 个测试对。</p><h2 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h2><p>我们采用 Chamfer distance (CD)[8]、Earth mover’s distance (EMD)[32]和 F1-Score[16]来评价生成建筑与其参考建筑的两两相似性。其中，CD 和 EMD 乘以 $10^2$,F1的阈值τ设为0.001。在计算这些度量之前，将点云归一化为 $[-1,1]^3$ 。三维点云的可视化是通过使用三菱渲染器[22]实现的。</p><h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><p>这些模型是在内存为 45GB 的 NVIDIA A40 GPU 上使用 PyTorch[25]实现的。批量大小设置为 8。图像被调整为 1024×1024 像素，即 H = W = 1024。<strong>每个 3D 点云都标准化了 100,000 个点来代表建筑物的形状</strong>。</p><ul><li>Image auto-encoder details。我们采用增强技术，包括以 90°角度旋转图像和在色调-饱和度-值(HSV)空间范围从-255 到 255 的颜色移动。这些都是基于建筑物可能在图像中出现颠倒和建筑物图像可能表现出很大的颜色变化的考虑。图像嵌入 zI 的维数 d 为 128。我们训练了 30 个 epoch 的自编码器，并使用学习率为 0.0002 的 Adam 优化器。利用均方误差(MSE)实现图像重构损失 Lrec 和嵌入一致性 lossLcon。在训练以下图像条件扩散模型时，我们冻结预训练的自编码器以导出 zI。</li><li>Base diffusion details. 给定每个建筑100,000个点，我们随机采样 K = 1024个点(表示低分辨率点云)用于训练和测试。我们设β0 = 0.0001， βT = 0.02，并对其他βs 进行线性插值。与已有作品[48,46]相似，对基础扩散设置总时间步数为 T = 1000。时间嵌入的维数 d = 128。<strong>倒角距离[8]作为基于点的距离度量Ω来度量两个投影之间的相似性</strong>。正则化权值ρ设为0.001。对于无分类器的制导策略，我们在训练阶段使用下降概率0.1，在采样阶段使用制导尺度γ = 4。扩散模型经过700次训练，由 Adam 进行优化，学习率为0.0002。具体来说，λ(t)定义为: $\lambda(t)=\begin{cases}1,&amp;t=1\\ 0.75,&amp;1&lt;t\leq\frac{1}{4}T\\ 0.50,&amp;\frac{1}{4}T&lt;t\leq\frac{1}{2}T\\ 0.25,&amp;\frac{1}{2}T&lt;t\leq\frac{3}{4}T\\ 0,&amp;\frac{3}{4}T&lt;t\leq T\end{cases}$</li><li>Upsampler diffusion details. 对于训练和测试阶段，我们随机抽取 N = 4096 个点作为每个建筑的样本。上采样器的去噪网络训练 200 次，总时间步长 T 设为 500。</li></ul><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>本文提出了一种基于扩散的 buildiff 方法，用于从单幅通用视图图像中生成建筑物的三维点云。<br>为了控制扩散模型生成与输入图像一致的三维形状，<strong>通过预训练基于 cnn 的图像自编码器，提取建筑物的多尺度特征，并使用增强约束潜在一致性，得到图像嵌入</strong>。<br>然后，以图像嵌入为输入，在加权建筑足迹正则化的辅助下，学习从高斯噪声分布中逐步去除噪声的条件去噪扩散网络;<br>最后利用点云上采样器扩散产生高分辨率点云，条件是从基础扩散采样的低分辨率点云。<br>实验结果证明了该方法的有效性。我们相信我们的工作可以弥合快速发展的生成建模技术和 3D 建筑生成的紧迫问题。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Generative Models </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is Diffusion Models ?</title>
      <link href="/3DReconstruction/Basic%20Knowledge/Diffusion%20Models/"/>
      <url>/3DReconstruction/Basic%20Knowledge/Diffusion%20Models/</url>
      
        <content type="html"><![CDATA[<p>Diffusion Models 原理</p><span id="more"></span><p><a href="https://www.youtube.com/watch?v=73qwu77ZsTM">【生成式AI】Diffusion Model 原理剖析 (2/4) (optional) - YouTube</a></p><p>生成模型共同目标：<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022203426.png" alt="image.png|666"><br>如何优化：<br><strong>Maximum Likelihood == Minimize KL Divergence</strong><br>最大化 $P_{\theta}(x)$ 分布中从 $P_{data}(x)$ 采样出来的 $x_{i},…, x_{m}$ 的概率，相当于最小化 $P_{\theta}(x)$ 与 $P_{data}(x)$ 之间的差异 </p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022203514.png" alt="image.png|666"></p><h1 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h1><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022204157.png" alt="image.png|666"></p><p>$P_\theta(x)=\int\limits_zP(z)P_\theta(x|z)dz$<br>$\begin{aligned}&amp;P_\theta(x|\mathrm{z})\propto\exp(-|G(\mathrm{z})-x|_2)\end{aligned}$</p><p>Maximize ： Lower bound of $logP(x)$<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022204951.png" alt="image.png|666"></p><h1 id="Diffusion-Model"><a href="#Diffusion-Model" class="headerlink" title="Diffusion Model"></a>Diffusion Model</h1><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022210535.png" alt="image.png|666"><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022210617.png" alt="image.png|666"></p><h2 id="DDPM-Denoising-Diffusion-Probabilistic-Models"><a href="#DDPM-Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="DDPM(Denoising Diffusion Probabilistic Models)"></a>DDPM(Denoising Diffusion Probabilistic Models)</h2><p>$\text{Maximize E}_{q(x_1:x_T|x_0)}[log\left(\frac{P(x_0;x_T)}{q(x_1:x_T|x_0)}\right)]$</p><p>$q(x_t|x_0)$ 可以只做一次 sample(给定一系列 $\beta$)</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023100329.png" alt="image.png|666"></p><p>DDPM 的 Lower bound of $logP(x)$<br>复杂公式推导得到：<br>$logP(x) \geq \operatorname{E}_{q(x_1|x_0)}[logP(x_0|x_1)]-KL\big(q(x_T|x_0)||P(x_T)\big)-\sum_{t=2}^{T}\mathrm{E}_{q(x_{t}|x_{0})}\bigl[KL\bigl(q(x_{t-1}|x_{t},x_{0})||P(x_{t-1}|x_{t})\bigr)\bigr]$</p><ul><li>$q(x_{t-1}|x_{t},x_{0}) =\frac{q(x_{t}|x_{t-1})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})}$ 为一个 Gaussian distribution<ul><li>$mean = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_{t}x_{0}+\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t}}{1-\bar{\alpha}_{t}}$ ，$variance = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta$</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023102747.png" alt="image.png|666"></p><h2 id="DDPM-Code"><a href="#DDPM-Code" class="headerlink" title="DDPM Code"></a>DDPM Code</h2><p><a href="https://github.com/mikonvergence/DiffusionFastForward">mikonvergence/DiffusionFastForward: DiffusionFastForward: a free course and experimental framework for diffusion-based generative models (github.com)</a></p><h3 id="Schedule"><a href="#Schedule" class="headerlink" title="Schedule"></a>Schedule</h3><ul><li><code>betas</code>: $\beta_t$ , <code>betas=torch.linspace(1e-4,2e-2,num_timesteps)</code></li><li><code>alphas</code>: $\alpha_t=1-\beta_t$ </li><li><code>alphas_sqrt</code>:  $\sqrt{\alpha_t}$</li><li><code>alphas_prod</code>: $\bar{\alpha}_t=\prod_{i=0}^{t}\alpha_i$</li><li><code>alphas_prod_sqrt</code>: $\sqrt{\bar{\alpha}_t}$</li></ul><h3 id="Forward-Process"><a href="#Forward-Process" class="headerlink" title="Forward Process"></a>Forward Process</h3><p>Forward step:</p><script type="math/tex; mode=display">q(x_t|x_{t−1}) := \mathcal{N}(x_t; \sqrt{1 − \beta_t}x_{t−1}, \beta_tI) \tag{1}</script><p>Forward jump:</p><script type="math/tex; mode=display">q(x_t|x_0) = \mathcal{N}(x_t;\sqrt{\bar{\alpha_t}}x_0, (1 − \bar{\alpha_t})I) \tag{2}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_step</span>(<span class="params">t, condition_img, return_noise=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        forward step: t-1 -&gt; t</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> t &gt;= <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    mean=alphas_sqrt[t]*condition_img</span><br><span class="line">    std=betas[t].sqrt()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sampling from N</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> return_noise:</span><br><span class="line">        <span class="keyword">return</span> mean+std*torch.randn_like(img)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        noise=torch.randn_like(img)</span><br><span class="line">        <span class="keyword">return</span> mean+std*noise, noise</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward_jump</span>(<span class="params">t, condition_img, condition_idx=<span class="number">0</span>, return_noise=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        forward jump: 0 -&gt; t</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> t &gt;= <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    mean=alphas_cumprod_sqrt[t]*condition_img</span><br><span class="line">    std=(<span class="number">1</span>-alphas_cumprod[t]).sqrt()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sampling from N</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> return_noise:</span><br><span class="line">        <span class="keyword">return</span> mean+std*torch.randn_like(img)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        noise=torch.randn_like(img)</span><br><span class="line">        <span class="keyword">return</span> mean+std*noise, noise</span><br></pre></td></tr></table></figure><h3 id="Reverse-Process"><a href="#Reverse-Process" class="headerlink" title="Reverse Process"></a>Reverse Process</h3><p>至少三种逆向过程的求法，从 $x_{t}$ 到 $x_{0}$<br>There are <strong>at least 3 ways of parameterizing the mean</strong> of the reverse step distribution $p_\theta(x_{t-1}|x_t)$:</p><ul><li>Directly (a neural network will estimate $\mu_\theta$)直接用网络预测 $\mu_\theta$</li><li>Via $x_0$ (a neural network will estimate $x_0$)用网络预测 $x_0$<script type="math/tex; mode=display">\tilde{\mu}_\theta = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t\tag{4}</script></li><li>Via noise $\epsilon$ subtraction from $x_0$ (a neural network will estimate $\epsilon$)用网络预测噪声 $\epsilon$<script type="math/tex; mode=display">x_0=\frac{1}{\sqrt{\bar{\alpha}_t}}(x_t-\sqrt{1-\bar{\alpha}_t}\epsilon)\tag{5}</script></li></ul><h3 id="Test-to"><a href="#Test-to" class="headerlink" title="Test to"></a>Test to</h3><h1 id="其他概念"><a href="#其他概念" class="headerlink" title="其他概念"></a>其他概念</h1><h2 id="KL-散度"><a href="#KL-散度" class="headerlink" title="KL 散度"></a>KL 散度</h2><p><a href="https://wuli.wiki/online/KLD.html">KL 散度（相对熵） - 小时百科 (wuli.wiki)</a></p><p><strong>KL 散度</strong>（Kullback–Leibler divergence，缩写 KLD）是一种统计学度量，<strong>表示的是一个概率分布相对于另一个概率分布的差异程度</strong>，在信息论中又称为<strong>相对熵</strong>（Relative entropy）。</p><script type="math/tex; mode=display">\begin{equation}D_{KL}(P||Q)=\sum_{x\in X}P(x)ln(\frac{P(x)}{Q(x)})=\sum_{x\in X}P(x)(ln(P(x))-ln(Q(x)))~.\end{equation}</script><p>对于连续型随机变量，设概率空间 X 上有两个概率分布 P 和 Q，其概率密度分别为 p 和 q，那么，P 相对于 Q 的 KL 散度定义如下：</p><script type="math/tex; mode=display">\begin{equation}D_{KL}(P||Q)=\int_{-\infty}^{+\infty}p(x)ln(\frac{p(x)}{q(x)})dx~.\end{equation}</script><p> 显然，当 P=Q 时，$D_{KL}=0$</p><p>两个一维高斯分布的 KL 散度公式：</p><blockquote><p><a href="https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/">KL散度(Kullback-Leibler Divergence)介绍及详细公式推导 | HsinJhao’s Blogs</a></p></blockquote><script type="math/tex; mode=display">\begin{aligned}KL(p,q)& =\int[\left.p(x)\log(p(x))-p(x)\log(q(x))\right]dx  \\&=-\frac12\left[1+\log(2\pi\sigma_1^2)\right]-\left[-\frac12\log(2\pi\sigma_2^2)-\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}\right] \\&=\log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac12\end{aligned}</script><h2 id="参数重整化"><a href="#参数重整化" class="headerlink" title="参数重整化"></a>参数重整化</h2><p><a href="https://www.bilibili.com/video/BV1b541197HX/">DDPMb站视频</a>公式推导</p><p>从高斯分布中直接采样一个值出来是不可导的，无法进行梯度传递，需要进行参数重整化：<br>从 $\mathcal{N}(0,1)$ 中随机采样出来 z，然后对 z 做 $\mu + z * \sigma$ 相当于从高斯分布 $\mathcal{N}(\mu,\sigma)$ 中采样</p><h1 id="Stable-Diffusion"><a href="#Stable-Diffusion" class="headerlink" title="Stable Diffusion"></a>Stable Diffusion</h1><p>目前常见的 UI 有 WebUI 和 ComfyUI</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>模型格式：</p><ul><li>主模型 checkpoints：<em>ckpt, safetensors</em></li><li>微调模型<ul><li>LoRA 和 LyCORIS 控制画风和角色：<em>safetensors</em></li><li>文本编码器模型：<em>pt,safetensors</em><ul><li>Embedding 输入文本 prompt 进行编码 <em>pt</em></li></ul></li><li>Hypernetworks 低配版的 lora <em>pt</em></li><li>ControlNet</li><li>VAE 图片与潜在空间 <em>pt</em></li></ul></li></ul><h2 id="采样器"><a href="#采样器" class="headerlink" title="采样器"></a>采样器</h2><p><a href="https://www.bilibili.com/video/BV1FN411i7sB/">Stable diffusion采样器全解析，30种采样算法教程</a><br>DPM++2M Karras，收敛+速度快+质量 OK</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231022195103.png" alt="image.png|666"></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SG-GAN</title>
      <link href="/3DReconstruction/Single-view/Generative%20Models/SG-GAN/"/>
      <url>/3DReconstruction/Single-view/Generative%20Models/SG-GAN/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>SG-GAN: Fine Stereoscopic-Aware Generation for 3D Brain Point Cloud Up-sampling from a Single Image</th></tr></thead><tbody><tr><td>Author</td><td>Bowen Hu, Baiying Lei, Shuqiang Wang, Senior Member, IEEE</td></tr><tr><td>Conf/Jour</td><td>arXiv</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4762223320204574721&amp;noteId=2014610279544785920">SG-GAN: Fine Stereoscopic-Aware Generation for 3D Brain Point Cloud Up-sampling from a Single Image (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021193539.png" alt="image.png|666"></p><p>Stereoscopic-aware graph generative adversarial network (SG-GAN)</p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>在间接狭窄手术环境下的微创颅脑手术中，三维脑重建是至关重要的。然而，随着一些新型微创手术(如脑机接口手术)对精度的要求越来越高，点云(PC)等传统三维重建的输出面临着样本点过于稀疏、精度不足的挑战。另一方面，高密度点云数据集的稀缺，给直接重建高密度脑点云训练模型带来了挑战。在这项工作中，提出了一种新的模型，称为立体感知图形生成对抗网络(SG-GAN)，该模型具有两个阶段，可以在单个图像上生成精细的高密度 PC。Stage-I GAN 根据给定的图像绘制器官的原始形状和基本结构，产生 Stage-I 点云。第二阶段 GAN 采用第一阶段的结果，生成具有详细特征的高密度点云。ii 阶段 GAN 能够通过上采样过程纠正缺陷并恢复感兴趣区域(ROI)的详细特征。此外，开发了一种基于无参数注意的自由变换模块来学习输入的有效特征，同时保持良好的性能。与现有方法相比，SG-GAN 模型在视觉质量、客观测量和分类性能方面表现出优异的性能，pc - pc 误差和倒角距离等多个评价指标的综合结果表明。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>SG-GAN 架构包括<strong>一个基于 ResNet 和 FTM 的编码器</strong>，以及<strong>两个 GAN 结构</strong>。该方法避免了直接生成的重构误差，仅通过 PC 上采样过程修正缺陷和恢复细节。制定了统一的信息流，使相邻模块之间能够通信，并解决其输入和输出的差异。</p><h2 id="Fast-feature-aggregating-encoder-based-on-free-transforming-module-FTM"><a href="#Fast-feature-aggregating-encoder-based-on-free-transforming-module-FTM" class="headerlink" title="Fast feature aggregating encoder based on free transforming module (FTM)"></a>Fast feature aggregating encoder based on free transforming module (FTM)</h2><p>Develop a free transforming module (FTM) by introducing a <strong>parameter-free self-attention mechanism</strong> into the conventional <strong>ResNet network</strong></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021200340.png" alt="image.png|555"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021200651.png" alt="image.png|666"></p><h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p>$\begin{aligned}\mathcal{L}_{CD}&amp;=\sum_{y’\in Y’}min_{y\in Y}||y’-y||_2^2+\sum_{y\in Y}min_{y’\in Y’}||y-y’||_2^2,\end{aligned}$</p><p>$\mathcal{L}_{EMD}=min_{\phi:Y\rightarrow Y^{\prime}}\sum_{x\in Y}||x-\phi(x)||_{2}$ , φ indicates a parameter of bijection.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>本文针对脑机接口微创手术中现有三维重建方法清晰度低、密度低的问题，提出了一种新的图像到 pc 的重建网络 SG-GAN。提出的 SG-GAN 实现了3D 脑重建技术，手术导航可以从中受益。考虑到手术导航中对实时信息反馈的需求以及相关的计算时间成本，我们选择利用点云作为我们提出的模型的表示，并将单个图像作为输入。<br>一种简单的替代方法是训练一个基本模型，并在一个步骤中从输入重建整个高密度点云。然而，由于以下两个方面的原因，这种替代方案可能不切实际。(1)不能充分利用原始图像中的先验知识和已知信息;(2)生成的高密度点云的一些微观结构在几何角度上没有得到充分的调整，从而导致较大的细节误差。<br>通过集成几个互补模块，所提出的 SG-GAN 能够完成复杂脑形状的预定重建。共同完成预定的复杂重构和上采样任务。设计了一种基于数值计算的无参数注意机制来构成 GAN 的编码器。该编码器利用空间域自关注来调整提取特征的权重，同时保证手术场景的时间敏感性，从而使输出的特征向量更具形状合理性。然后，设计两个不同阶段的 gan 来形成生成网络。<strong>第一阶段 GAN 的重点是勾勒出目标大脑的特定形状，并利用这些特征生成低密度点云。第二阶段 GAN 的重点是描绘目标大脑的具体细节，并修复第一阶段的一些生成错误，以重建最终的高密度 PC</strong>。<br>目前，术中MRI技术对手术导航有了很大的改进。在本研究中，我们利用单片脑MR图像作为输入，重建相应的三维脑形状，得到的结果是有希望的。我们的目标是在脑外科手术导航中，为医生提供高密度的三维脑形态的即时访问。对于未来的工作，我们将与临床医生合作，从现实世界中收集脑外科导航产生的数据，以便我们可以尽可能地消除所提出模型的输入约束。最后，我们的模型可以适应现实世界数据集的输入，并获得相同的竞争三维形状结构。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Generative Models </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FlowGAN</title>
      <link href="/3DReconstruction/Single-view/Generative%20Models/FlowGAN/"/>
      <url>/3DReconstruction/Single-view/Generative%20Models/FlowGAN/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Flow-based GAN for 3D Point Cloud Generation from a Single Image</th></tr></thead><tbody><tr><td>Author</td><td>Yao Wei (University of Twente), George Vosselman (“University of Twente, the Netherlands”), Michael Ying Yang (University of Twente)*</td></tr><tr><td>Conf/Jour</td><td>BMVA</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://bmvc2022.mpi-inf.mpg.de/569/">Flow-based GAN for 3D Point Cloud Generation from a Single Image (mpg.de)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4677468038203719681&amp;noteId=2011515461854903552">Flow-based GAN for 3D Point Cloud Generation from a Single Image (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231019162233.png" alt="image.png|555"></p><ul><li><strong>flow-based explicit generative models</strong> for sampling point clouds with arbitrary resolutions</li><li>Improving the detailed 3D structures of point clouds by leveraging the <strong>implicit generative adversarial networks (GANs)</strong>.</li></ul><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>从单幅二维图像生成三维点云对于三维场景理解应用具有重要意义。为了重建图像中显示的物体的整个 3D 形状，现有的基于深度学习的方法使用显式或隐式的点云生成 modeling，然而，这些方法的质量有限。在这项工作中，我们的目标是通过引入一种<strong>混合显式-隐式生成建模</strong>方案来缓解这一问题，该方案<strong>继承了基于流的显式生成模型</strong>，用于任意分辨率的采样点云，同时通过利用隐式生成对抗网络(gan)改善点云的详细 3D 结构。在大规模合成数据集 ShapeNet 上进行了测试，实验结果证明了该方法的优越性能。此外，通过对 PASCAL3D+数据集的跨类别合成图像和真实图像进行测试，证明了该方法的泛化能力</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>Framework</p><ul><li>Generator built on normalizing flows<ul><li>VAEs with a flow-based decoder</li></ul></li><li>Discriminator from cross-modal perspective</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231019162233.png" alt="image.png|333"><br>_蓝色是训练过程也是推理过程_</p><h2 id="Flow-based-Generator"><a href="#Flow-based-Generator" class="headerlink" title="Flow-based Generator"></a>Flow-based Generator</h2><ul><li>Encoder<ul><li>$\psi$ ResNet18 —&gt; 图像 I 映射到潜在空间,图像条件分布用于在推理过程中对潜在 z 进行采样<ul><li><a href="https://readpaper.com/pdf-annotate/note?pdfId=4544567041242849281&amp;noteId=2011522466393902336">Deep Residual Learning for Image Recognition (readpaper.com)</a></li></ul></li><li>$\phi$ PointNet —&gt; d-dimensional latent vector Z (d=512)<ul><li><a href="https://readpaper.com/pdf-annotate/note?pdfId=4500216149471551490&amp;noteId=2011522008392682752">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (readpaper.com)</a></li><li><a href="https://github.com/fxia22/pointnet.pytorch">fxia22/pointnet.pytorch: pytorch implementation for “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation” https://arxiv.org/abs/1612.00593 (github.com)</a></li></ul></li></ul></li><li>Decoder (built on NFs)<ul><li>在形状潜 z (编码点云 X 到 Z) 的条件下，应用包含 F 个 (F = 63)仿射耦合层的流动模型来学习简单先验分布，从 p(X)到高斯 p ~ N(0,1)的变换是反向模式 θ−1，从 p ~ N(0,1)到 p(X)的变换是正向模式 θ</li></ul></li></ul><h2 id="Cross-modal-Discriminator"><a href="#Cross-modal-Discriminator" class="headerlink" title="Cross -modal Discriminator"></a>Cross -modal Discriminator</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021110741.png" alt="image.png|666"></p><ul><li>编码器 EX 用于分别提取预测点云和真实点云的 3D 特征 (PointNet)</li><li>编码器 EI 对输入图像的 2D 特征进行编码 (ResNet18)</li></ul><p>将融合的跨模态特征输入 MLP 层以输出值, 以 I 和 X 作为输入，该值预计为 1（真实样本）；当以 I 和 $\hat{X}$ 作为输入时，该值预计会为 0（假样本）</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>在本文中，我们提出了一种混合的显式-隐式生成建模方案，用于从单幅图像重建三维点云。为了解决生成固定分辨率的点云所带来的限制，我们引入了一个基于单流的生成器来近似3D点的分布，这使得我们可以对任意数量的点进行采样。此外，开发了一个跨模态鉴别器来引导生成器生成高质量的点云，这些点云既符合输入图像的合理条件，又具有与地面真实情况相似的三维结构。在ShapeNet和PASCAL3D+数据集上的实验结果证明了该方法的有效性和泛化能力。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Generative Models </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> Flow </tag>
            
            <tag> VAE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DiT-3D</title>
      <link href="/3DReconstruction/Single-view/Generative%20Models/DiT-3D/"/>
      <url>/3DReconstruction/Single-view/Generative%20Models/DiT-3D/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://scholar.google.com/citations?user=6aYncPAAAAAJ&amp;hl=en/">Shentong Mo</a> 1, <a href="https://xieenze.github.io/">Enze Xie</a> 2, <a href="http://ruihangchu.com/">Ruihang Chu</a> 3, <a href="https://scholar.google.com/citations?user=hqDyTg8AAAAJ&amp;hl=en/">Lewei Yao</a> 2,<a href="https://scholar.google.com.sg/citations?user=2p7x6OUAAAAJ&amp;hl=en/">Lanqing Hong</a>2, <a href="https://scholar.google.com/citations?user=eUtEs6YAAAAJ&amp;hl=en/">Matthias Nießner</a>4, <a href="https://scholar.google.com/citations?user=XboZC1AAAAAJ&amp;hl=en/">Zhenguo Li</a>2</td></tr><tr><td>Conf/Jour</td><td>arXiv</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://dit-3d.github.io/">DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4776143720479195137&amp;noteId=2011558450133224704">DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231019170328.png" alt="image.png|666"></p><p>New 3D Diffusion Transformer Model, 在体素化的点云上运行 DDPM(Denoising diffusion probabilistic models) 的去噪过程</p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>最近的扩散 Transformers(例如 DiT[1])已经证明了它们在生成高质量 2D 图像方面的强大有效性。然而，Transformer 架构是否在 3D 形状生成中表现得同样好还有待确定，因为之前的 3D 扩散方法大多采用 U-Net 架构。为了弥补这一差距，我们提出了一种新的用于 3D 形状生成的扩散 Transformer，即 DiT-3D，它可以直接使用普通 Transformer 对体素化点云进行去噪处理。与现有的 U-Net 方法相比，我们的 DiT-3D 在模型大小上更具可扩展性，并产生更高质量的 generations。具体来说，DiT-3D 采用了 DiT[1]的设计理念，但对其进行了修改，<strong>加入了 3D 位置嵌入和补丁嵌入(3D positional and patch embedding)</strong>，以自适应地聚合来自体素化点云的输入。为了减少 3D 形状生成中自注意的计算成本，我们将 3D 窗口注意 (3D window attention)合并到 Transformer 块中，因为由于体素的额外维度导致的 3D 令牌长度增加会导致高计算量。最后，利用<strong>线性层和去噪层</strong>对去噪后的点云进行预测。此外，我们的变压器架构支持从 2D 到 3D 的有效微调，其中 ImageNet 上预训练的 DiT-2D 检查点可以显着改善 ShapeNet 上的 DiT-3D。在 ShapeNet 数据集上的实验结果表明，所提出的 DiT-3D 在高保真度和多样化的三维点云生成方面达到了最先进的性能。特别是，当对倒角距离进行评估时，我们的 DiT-3D 将最先进方法的 1 近邻精度降低了 4.59，并将覆盖度量提高了 3.51。</p><ul><li>我们提出了 DiT-3D，这是第一个用于点云形状生成的普通扩散 Transformer 架构，可以有效地对体素化点云进行去噪操作。(设计一个基于普通 Transformer 的架构主干来取代 U-Net 主干,以逆转从观测点云到高斯噪声的扩散过程)</li><li>我们对 DiT-3D 进行了一些简单而有效的修改，包括 3D 位置和补丁嵌入，3D 窗口关注和 ImageNet 上的 2D 预训练。这些改进在保持效率的同时显著提高了 DiT-3D 的性能。</li><li>在 ShapeNet 数据集上进行的大量实验表明，DiT-3D 在生成高保真形状方面优于以前的非 DDPM 和 DDPM 基线。</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Diffusion-Transformer-for-3D-Point-Cloud-Generation"><a href="#Diffusion-Transformer-for-3D-Point-Cloud-Generation" class="headerlink" title="Diffusion Transformer for 3D Point Cloud Generation"></a>Diffusion Transformer for 3D Point Cloud Generation</h2><h2 id="Efficient-Modality-Domain-Transfer-with-Parameter-efficient-Fine-tuning"><a href="#Efficient-Modality-Domain-Transfer-with-Parameter-efficient-Fine-tuning" class="headerlink" title="Efficient Modality/Domain Transfer with Parameter-efficient Fine-tuning"></a>Efficient Modality/Domain Transfer with Parameter-efficient Fine-tuning</h2><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>在这项工作中，我们提出了 DiT-3D，一种用于三维形状生成的新型平面扩散变压器，它可以直接对体素化点云进行去噪处理。与现有的 U-Net 方法相比，我们的 DiT-3D 在模型大小上更具可扩展性，并产生更高质量的 generations。具体来说，我们结合了3D 位置和补丁嵌入来聚合来自体素化点云的输入。然后，我们将3D 窗口关注合并到 Transformer 块中，以减少3D Transformer 的计算成本，由于3D 中额外维度导致令牌长度增加，计算成本可能会非常高。最后，我们利用线性层和去噪层来预测去噪后的点云。由于 Transformer 的可扩展性，<strong>DiT-3D 可以很容易地支持具有模态和域可转移性的参数高效微调</strong>。实验结果证明了所提出的 DiT-3D 在高保真度和多样化的3D 点云生成方面的最先进性能。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Generative Models </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Greedy Grid Search</title>
      <link href="/3DReconstruction/Multi-view/PointCloud/Greedy%20Grid%20Search/"/>
      <url>/3DReconstruction/Multi-view/PointCloud/Greedy%20Grid%20Search/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Challenging universal representation of deep models for 3D point cloud registration</th></tr></thead><tbody><tr><td>Author</td><td>Bojani\’{c}, David and Bartol, Kristijan and Forest, Josep and Gumhold, Stefan and Petkovi\’{c}, Tomislav and Pribani\’{c}, Tomislav</td></tr><tr><td>Conf/Jour</td><td>BMVC</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://github.com/davidboja/greedy-grid-search">DavidBoja/greedy-grid-search: [BMVC 2022 workshop] Greedy Grid Search: A 3D Registration Baseline (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4695512524410322945&amp;noteId=2011119761938643456">Challenging the Universal Representation of Deep Models for 3D Point Cloud Registration (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231019094915.png" alt="image.png|666"></p><p>按步长穷举法，<strong>粗配准</strong>，需要根据ICP进行精配准</p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>学习跨不同应用领域的通用表示是一个开放的研究问题。事实上，在相同的应用程序中，在不同类型的数据集中<strong>找到通用的架构</strong>仍然是一个未解决的问题，特别是在涉及处理 3D 点云的应用程序中。在这项工作中，我们通过实验测试了几种最先进的基于学习的 3D 点云配准方法，以对抗所提出的非学习基线配准方法。所提出的方法优于或达到了基于学习方法的可比结果。此外，我们提出了一个基于学习的方法很难泛化的数据集。我们提出的方法和数据集，以及提供的实验，可以用于进一步研究通用表示的有效解决方案</p><ul><li>比较好的泛化</li><li>提出了新基准 FAUST-partial，基于 3D 人体扫描，这进一步对基于学习的方法的泛化提出了挑战</li><li>提出了一种新的三维配准基线，该基线根据体素化点云之间的最大相互关系选择变换候选点</li><li>在公共基准测试中展示与最先进的 3D 配准方法相当的性能，并在 FAUST-partial 上优于它们</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>分为三步：</p><ul><li>Pre-Processing<ul><li><strong>源点云X</strong>居中并按一定的 step 旋转(预计算N个旋转矩阵)，得到 N 个旋转后的源点云，然后将源点云移动到坐标全为正值的象限(方便体素化)，然后将 N 个源点云和<strong>目标点云Y</strong>体素化(voxel resolution of VR cm)，没有使用 0 1 值的 3D 网格，而是为体素填充正值 PV(包含点云点)和负值 NV( 不包含点云点)</li><li>得到 N 个 source volumes 和 1 个 target volume </li></ul></li><li>Processing<ul><li>计算每个 source volume 与 target volume 的 3D cross-correlation(两个 volume 的体素值相乘并相加)，结果产生 N 个 cross-correlation volumes 与 source volumes 的三个维度相同，可以使用 heatmaps 表示匹配度的高低。</li><li>cross-correlation 之前，每个 source volume 应该被 pad 以便 target volume 在 source volume 上 slide，用 6 维的 P 表示 Pad，每个维度分别表示在 source volume 的左右上下前后 pad 的数量</li><li>使用 Fourier 加速 cross-correlation 的计算，首先将两个 volume 使用 FFT 转换到 Fourier space，将 cross-correlation 简化为矩阵乘法，然后使用逆 FFT 将输出转换回来</li></ul></li><li>Post-Processing<ul><li>使用预先计算的旋转矩阵中的一个来估计将X旋转到Y的旋转矩阵(CC最大的source volume)</li><li>同时将target volume 的中心移动到最大CC volume voxel (xyz)，由于最大CC volume相对应某个source volume，本质上是将target volume 体素中心移动到 source volume 体素中心(使得CC最大) </li></ul></li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231019094915.png" alt="image.png|666"></p><p>$\left(\hat{R}\left(\mathcal{X}-t_{\mathcal{X}}^{\text{CENTER}}\right)\right)+t_{\mathcal{X}}^{\text{POSIT}}\sim\left(\mathcal{Y}+t_{\mathcal{Y}}^{\text{POSIT}}\right)-t_{\text{est}}$<br>其中~表示左右部分是对齐的，X源点云和Y目标点云，$t_{\mathcal{X}}^{\text{CENTER}}$将源点云的质心移动到原点，$t^{\text{POSIT}}$将最小边界框点移动到原点</p><p>公式变形：<br>$\left(\hat{R}\left({\mathcal X}-t_{\mathcal X}^{\mathrm{CENTER}}\right)\right)+t_{\mathcal X}^{\mathrm{POSIT}}+t_{\mathrm{est}}-t_{\mathcal Y}^{\mathrm{POSIT}}\sim{\mathcal Y}$<br>$\hat{R}=R_{i^{*}},\quad\hat{t}=-\hat{R}t_{\mathcal{X}}^{\mathrm{CENTER}}+t_{\mathcal{X}}^{\mathrm{POSIT}}+t_{\mathrm{est}}-t_{\mathcal{Y}}^{\mathrm{POSIT}}$</p><p>Refinement:<br>由于旋转和平移空间是离散的，所以初始对齐只是一个粗略的估计。如果地面真值解位于估计的离散位置，则旋转和平移误差的上界为$\frac12\max_{i,ji\neq j}\arccos\left(\frac{\operatorname{trace}(R_i^TR_j)-1}2\right)\frac{180}\pi$ degree 和$\frac{\mathrm{VR}\sqrt{3}}{2}$ cm。对于S = 15◦和V R = 6cm的角度步长，上限误差为7.5◦和5cm。因此，粗略的初始对齐应该为精细的配准算法提供非常好的初始化。我们使用广义ICP[53]来refine初始解决方案，因为它提供了最好的结果</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>提出的经典方法提供了良好的三维配准基线。该方法简单有效，并在公共基准测试中得到了验证。与最先进的方法的泛化性能相比，基线是相同的。在新提出的FAUST-partial基准测试中，即使生成的云对之间的重叠相当高，竞争方法也难以保留结果，或者执行得更差。与深度学习方法相反，基线简单且可解释，可用于详细分析。不同策略的效果是清晰和直观的，并提供了对注册过程的见解。因此，在寻找<strong>通用表示</strong>的过程中，设计一个模仿所提出的基线方法的深度模型是一个有趣的未来发展方向。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/PointCloud </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>GeoTransformer</title>
      <link href="/3DReconstruction/Multi-view/PointCloud/GeoTransformer/"/>
      <url>/3DReconstruction/Multi-view/PointCloud/GeoTransformer/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Geometric Transformer for Fast and Robust Point Cloud Registration</th></tr></thead><tbody><tr><td>Author</td><td>Zheng Qin1 Hao Yu2 Changjian Wang1 Yulan Guo1,3 Yuxing Peng1 Kai Xu1*</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://github.com/qinzheng93/GeoTransformer">qinzheng93/GeoTransformer: [CVPR2022] Geometric Transformer for Fast and Robust Point Cloud Registration (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4667181912255643649&amp;noteId=2010116153877133824">Geometric Transformer for Fast and Robust Point Cloud Registration (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231018113631.png" alt="image.png|666"></p><p>pairwise registration models, only <strong>Ubuntu</strong></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>研究了点云配准中精确对应点的提取问题。最近的无关键点方法绕过了在低重叠情况下难以检测可重复关键点的方法，在配准方面显示出巨大的潜力。他们在下采样的超点上寻找对应点，然后将其传播到密集点。根据相邻补丁是否重叠来匹配重叠点。这种稀疏和松散的匹配需要上下文特征捕捉点云的几何结构。我们提出几何变压器来学习几何特征，以实现鲁棒的重叠点匹配。它对成对距离和三重角度进行编码，使其在低重叠情况下具有鲁棒性，并且对刚性变换不变性。简单的设计获得了惊人的匹配精度，在估计对准变换时不需要RANSAC，从而获得了100倍的加速度。在具有挑战性的3DLoMatch基准测试中，我们的方法将初始比率提高了17 ~ 30个百分点，将注册召回率提高了7个百分点以上。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/PointCloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PointCloud </tag>
            
            <tag> Registration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SGHR</title>
      <link href="/3DReconstruction/Multi-view/PointCloud/SGHR/"/>
      <url>/3DReconstruction/Multi-view/PointCloud/SGHR/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Robust Multiview Point Cloud Registration with Reliable Pose Graph Initialization and History Reweighting</th></tr></thead><tbody><tr><td>Author</td><td>Haiping Wang and Yuan Liu and Zhen Dong and Yulan Guo and Yu-Shen Liu and Wenping Wang and Bisheng Yang</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://github.com/WHU-USI3DV/SGHR?tab=readme-ov-file">WHU-USI3DV/SGHR: [CVPR 2023] Robust Multiview Point Cloud Registration with Reliable Pose Graph Initialization and History Reweighting (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4740850412790218753&amp;noteId=2008923607452724224">Robust Multiview Point Cloud Registration with Reliable Pose Graph Initialization and History Reweighting (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231017210035.png" alt="image.png|666"></p><p>Issue:<br><a href="https://github.com/WHU-USI3DV/SGHR/issues/4">How should I train my dataset? · Issue #4 · WHU-USI3DV/SGHR (github.com)</a><br>I think several point clouds of a single statue is not enough for training deep descriptors. I suggest to directly use pairwise registration models such as <a href="https://github.com/qinzheng93/GeoTransformer">Geotrainsformer</a> pre-trained on object-level datasets such as ModelNet40 <strong>to solve the pairwise registrations</strong>.<br>And adopt SGHR’s <strong>transformation synchronization</strong> section to solve the global consistent scan poses.</p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文提出了一种点云多视图配准的新方法。以前的多视图配准方法依赖于穷举成对配准构造密集连接的位姿图，并在位姿图上应用迭代重加权最小二乘(IRLS)计算扫描位姿。然而，构造一个密集连接的图是耗时的，并且包含大量的离群边，这使得后续的IRLS很难找到正确的姿势。为了解决上述问题，我们首先提出使用神经网络来估计扫描对之间的重叠，这使我们能够构建一个稀疏但可靠的姿态图。然后，我们在IRLS方案中设计了一种新的历史重加权函数，该函数对图上的离群边具有较强的鲁棒性。与现有的多视图配准方法相比，我们的方法在3DMatch数据集上的配准召回率提高了11%，在ScanNet数据集上的配准误差降低了13%，同时所需的成对配准减少了70%。进行了全面的ablation研究，以证明我们设计的有效性</p><p>传统点云配准：</p><ul><li><strong>首先</strong>，采用成对配准算法[28,46,49]，穷尽估计所有N2扫描对的相对姿态，形成一个完全连通的姿态图。图的边表示扫描对的相对位置，节点表示扫描。</li><li>由于密集姿态图可能包含两次不相关扫描之间不准确甚至不正确的相对姿态(异常值)，因此在<strong>第二阶段</strong>，通过加强周期一致性[30]来联合优化这些成对姿态，以拒绝异常边并提高精度。对于第二阶段，最新的方法，包括手工方法[5,13,29]或基于学习的方法[21,30,55]，采用迭代加权最小二乘(IRLS)方案。Iteratively Reweighted Least Square (IRLS)</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>(1)给定N个未对齐的部分扫描，我们的目标是将所有这些扫描注册到(4)一个完整的点云中。我们的方法有两个贡献</p><ul><li>(2)学习全局特征向量初始化稀疏姿态图，使离群点更少，减少了两两配准所需的次数。<ul><li><strong>Global feature extraction</strong>：<strong>YOHO</strong> for extracting local descriptors , <strong>NetVLAD</strong> to extract a global feature F (train with a L1 loss between the predicted overlap score and the ground-truth overlap ratio.)</li><li><strong>Sparse graph construction</strong>：overlap score ==&gt;For each scan, select other k scan pairs with the largest overlap scores to connect with the scan</li><li>estimate a relative pose on the scan pair from their extracted local descriptors(follow YOHO to apply nearest neighborhood matcher )</li></ul></li><li>(3)提出了一种新的IRLS方案。在我们的IRLS方案中，我们从全局特征和两两配准中初始化权重。然后，我们设计了一个历史加权函数来迭代地改进姿态，提高了对异常值的鲁棒性。<em>(IRLS的关键思想是在每条边上关联一个权值来表示每个扫描对的可靠性。这些权重被迭代地细化，使得离群边缘的权重较小，这样这些离群相对姿态就不会影响最终的全局姿态)</em><ul><li>Weight initialization</li><li>Pose synchronization, 给定edge weights 和 input relative poses求解global scan poses<ul><li>Rotation synchronization, ref:  Global Motion Estimation from Point Matches</li><li>Translation synchronization, ref:  Learning Transformation Synchronization 最小二乘法求解</li></ul></li><li>History reweighting function</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231017215645.png" alt="image.png|444"></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>我们在三个广泛使用的基准上评估我们的方法:3DMatch/3DLoMatch数据集[28,59]，ScanNet数据集[16]和ETH数据集[44]</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231018170845.png" alt="image.png|666"><br>我们的全局特征提取网络架构如图A.1所示。我们采用与[49]架构相同的YOHO进行32点局部特征提取。更多的局部特征提取细节可以在[49]中找到。提取的局部特征通过NetVLAD层聚合为全局特征[3]。我们将netvlad中的集群数量设置为64，因此全局特征的维度为2048。请参考[3]了解更多的全局特征聚合细节。</p><p>我们使用预训练好的YOHO[49]进行局部特征提取，并使用3DMatch[59]训练分割中的46个场景训练N etVLAD层。我们采用以下数据扩充。对于3DMatch训练集中的每个场景，我们首先随机抽取α∈[8,60]扫描作为图节点。然后，在每次扫描中，我们随机抽取β∈[1024,5000]个关键点来提取YOHO特征。将α扫描的局部特征输入到NetVLAD中提取α扫描的全局特征。然后，我们通过穷列关联每两个全局特征来计算α2重叠分数，并计算真实重叠比率与预测重叠分数之间的L1距离作为训练损失。我们将批大小设置为1，并使用学习率为1e-3的Adam优化器。学习率每50个历元呈指数衰减0.7倍。总的来说，我们训练了netv LAD 300个epoch。</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>本文提出了一种新的多视点云配准方法。该方法的关键是基于学习的稀疏姿态图构建，该方法可以估计两次扫描之间的重叠比，使我们能够选择高重叠的扫描对来<strong>构建稀疏但可靠的图</strong>。在此基础上，提出了一种新的<strong>历史加权函数</strong>，提高了IRLS方案对异常值的鲁棒性，并对姿态校正有较好的收敛性。所提出的方法在室内和室外数据集上都具有最先进的性能，而且配对配准的次数要少得多。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/PointCloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PointCloud </tag>
            
            <tag> Registration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Math about 3D Reconstruction</title>
      <link href="/3DReconstruction/Basic%20Knowledge/Math/"/>
      <url>/3DReconstruction/Basic%20Knowledge/Math/</url>
      
        <content type="html"><![CDATA[<p>3D Reconstruction 相关数学方法</p><span id="more"></span><h1 id="SDF计算与求导"><a href="#SDF计算与求导" class="headerlink" title="SDF计算与求导"></a>SDF计算与求导</h1><p>空间中的子集$\partial\Omega$，SDF值定义为：$\left.f(x)=\left\{\begin{array}{ll}d(x,\partial\Omega)&amp;\mathrm{~if~}x\in\Omega\-d(x,\partial\Omega)&amp;\mathrm{~if~}x\in\Omega^c\end{array}\right.\right.$<br>其中$d(x,\partial\Omega):=\inf_{y\in\partial\Omega}d(x,y)$表示x到表面子集上一点的距离，inf表示infimum最大下界</p><h1 id="权重计算"><a href="#权重计算" class="headerlink" title="权重计算"></a>权重计算</h1><h2 id="2K2K"><a href="#2K2K" class="headerlink" title="2K2K"></a>2K2K</h2><p>来源：<a href="https://github.com/SangHunHan92/2K2K/blob/main/models/deep_human_models.py">2K2K Code</a><br>目的：part 法向量图 —&gt; 原图大小对应法向量图</p><p>根据 part 法向量图逆仿射变换回原图空间 $\mathbf{n}_{i}=\mathbf{M}_{i}^{-1}\mathbf{\bar{n}}_{i}$<br>要将 part 法向量图融合为原图空间法向量图，每个法向量图有不同的权重$\mathbf{N}^h\quad=\sum\limits_{i=1}^K\left(\mathbf{W}_i\odot\mathbf{n}_i\right)$</p><p>权重的<strong>计算方法</strong>：</p><script type="math/tex; mode=display">\mathbf{W}_i(x,y)=\frac{G(x,y)*\phi_i(x,y)}{\sum_iG(x,y)*\phi_i(x,y)}</script><ul><li>同时与 part 法向量图逆仿射变换的还有一个 Occupancy Grid Map O，表示在原图空间中每个 part 的占用值 0 或者 1，i.e. $\left.\phi_i(x,y)=\left\{\begin{array}{cc}1&amp;\text{if}&amp;\sum\mathbf{n}_i(x,y)\neq\mathbf{0}^\top\\0&amp;\text{otherwise}\end{array}\right.\right.$</li><li>对 O 做高斯模糊 GaussianBlur，<strong>使得 O map 的值到边缘逐渐减小</strong></li><li>如下图，face part 脖子上方中心处 O 值做完高斯模糊后依然近似 1(假设 1)，而 body part 上部分脖子中心处做完高斯模糊后 O 值<1(假设 1/2)，这会导致对于脖子这部分多 part 融合时，face part normal 的权重相对于 body part normal 的权重会更大一点(2/3 > 1/3)</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921163941.png" alt="image.png"></p><h1 id="卷积-Pytorch"><a href="#卷积-Pytorch" class="headerlink" title="卷积 Pytorch"></a>卷积 Pytorch</h1><p>图像卷积后的大小计算公式： $N=\left\lfloor\frac{W-F+2P}{Step}\right\rfloor+1$</p><ul><li>输入图片大小 <code>W * W</code></li><li>Filter（卷积核）大小 <code>F * F</code></li><li>步长 Step</li><li>padding（填充）的像素数 P</li><li>输出图片的大小为<code>N * N</code></li></ul><hr><h1 id="NeRF相关的数学知识"><a href="#NeRF相关的数学知识" class="headerlink" title="NeRF相关的数学知识"></a>NeRF相关的数学知识</h1><h2 id="坐标变换"><a href="#坐标变换" class="headerlink" title="坐标变换"></a>坐标变换</h2><p>内参矩阵 = c2p<br>外参矩阵 = w2c<br>像素坐标 = <code>c2p * w2c * world_position</code></p><h3 id="相机内参矩阵intrinsic-c2p"><a href="#相机内参矩阵intrinsic-c2p" class="headerlink" title="相机内参矩阵intrinsic_c2p"></a>相机内参矩阵intrinsic_c2p</h3><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230703144039.png" alt="image.png"></p><blockquote><p>理解与<a href="https://blog.csdn.net/OrdinaryMatthew/article/details/126670351">NeRF OpenCV OpenGL COLMAP DeepVoxels坐标系朝向_nerf坐标系_培之的博客-CSDN博客</a>一致</p></blockquote><p>NeRF = OpenGL = Blender<br>Neus = Colmap</p><div class="table-container"><table><thead><tr><th>Method</th><th>Pixel to Camera coordinate</th></tr></thead><tbody><tr><td>NeRF</td><td>$\vec d = \begin{pmatrix} \frac{i-\frac{W}{2}}{f} \\ -\frac{j-\frac{H}{2}}{f} \\ -1 \\ \end{pmatrix}$ , $intrinsics = K = \begin{bmatrix} f &amp; 0 &amp; \frac{W}{2}  \\ 0 &amp; f &amp; \frac{H}{2}  \\ 0 &amp; 0 &amp; 1 \\ \end{bmatrix}$</td></tr><tr><td>Neus</td><td>$\vec d = intrinsics^{-1} \times  pixel = \begin{bmatrix} \frac{1}{f} &amp; 0 &amp; -\frac{W}{2 \cdot f}  \\ 0 &amp; \frac{1}{f} &amp; -\frac{H}{2 \cdot f} \\ 0 &amp; 0 &amp; 1 \\ \end{bmatrix} \begin{pmatrix} i \\ j \\ 1 \\ \end{pmatrix} = \begin{pmatrix} \frac{i-\frac{W}{2}}{f} \\ \frac{j-\frac{H}{2}}{f} \\ 1 \\ \end{pmatrix}$</td></tr></tbody></table></div><p>$\mathbf{p_c}=\begin{bmatrix}\dfrac{1}{f}&amp;0&amp;-\dfrac{W}{2\cdot f}\\0&amp;\dfrac{1}{f}&amp;-\dfrac{H}{2\cdot f}\\0&amp;0&amp;1\end{bmatrix}\begin{pmatrix}i\\j\\1\end{pmatrix}=\begin{pmatrix}\dfrac{i-\dfrac{W}{2}}{f}\\\dfrac{j-\dfrac{H}{2}}{f}\\1\end{pmatrix}$</p><p>$\mathbf{d_{w}}= \begin{bmatrix}r_{11}&amp;r_{12}&amp;r_{13}&amp;t_x\\ r_{21}&amp;r_{22}&amp;r_{23}&amp;t_y\\ r_{31}&amp;r_{32}&amp;r_{33}&amp;t_z\\ 0&amp;0&amp;0&amp;1\end{bmatrix} \begin{bmatrix}X_w\\Y_w\\Z_w\\1\end{bmatrix} = \begin{bmatrix}X_c\\Y_c\\Z_c\\1\end{bmatrix}$</p><p>$\mathbf{o_w}=\begin{bmatrix}r_{11}&amp;r_{12}&amp;r_{13}&amp;t_x\\ r_{21}&amp;r_{22}&amp;r_{23}&amp;t_y\\ r_{31}&amp;r_{32}&amp;r_{33}&amp;t_z\\ 0&amp;0&amp;0&amp;1\end{bmatrix} \begin{pmatrix}0\\0\\0\\1\end{pmatrix} = \begin{pmatrix}t_x\\t_y\\t_z\\1\end{pmatrix}$</p><h3 id="相机外参矩阵w2c"><a href="#相机外参矩阵w2c" class="headerlink" title="相机外参矩阵w2c"></a>相机外参矩阵w2c</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/642715876">相机位姿(camera pose)与外参矩阵 - 知乎 (zhihu.com)</a></p></blockquote><p>colmap处理得到的<code>/colmap/sparse/0</code>中文件cameras, images, points3D.bin or .txt<br>其中images.bin中：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME</span><br><span class="line"># POINTS2D[] as (X, Y, POINT3D_ID)</span><br><span class="line"># Number of images: 2, mean observations per image: 2</span><br></pre></td></tr></table></figure></p><p>pose: QW, QX, QY, QZ, TX, TY, TZ</p><p>由QW、QX、QY、QZ得到旋转矩阵</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">R = | 1 - 2*(qy^2 + qz^2)   2*(qx*qy - qw*qz)   2*(qx*qz + qw*qy) |</span><br><span class="line">    | 2*(qx*qy + qw*qz)     1 - 2*(qx^2 + qz^2) 2*(qy*qz - qw*qx) |</span><br><span class="line">    | 2*(qx*qz - qw*qy)     2*(qy*qz + qw*qx)   1 - 2*(qx^2 + qy^2) |</span><br><span class="line"></span><br><span class="line">t = [[TX], </span><br><span class="line">    [TY], </span><br><span class="line">    [TZ]]</span><br></pre></td></tr></table></figure><p>横向catR和t得到：<code>w2c = [R, t]</code></p><h4 id="Neus"><a href="#Neus" class="headerlink" title="Neus"></a>Neus</h4><p>$c2w = \left[\begin{array}{c|c}\mathbf{R}_{c}&amp;\mathbf{C}\\\hline\mathbf{0}&amp;1\\\end{array}\right]$</p><p>c2w矩阵的值直接描述了相机坐标系的朝向和原点，因此称为相机位姿。具体的，旋转矩阵的第一列到第三列分别表示了相机坐标系的X, Y, Z轴在世界坐标系下对应的方向；平移向量表示的是相机原点在世界坐标系的对应位置。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230804191552.png" alt="c2w.png"></p><p>$w2c = \left[\begin{array}{c|c}\mathbf{R}&amp;t\\\hline\mathbf{0}&amp;1\end{array}\right] = \left[\begin{array}{c|c}\mathbf{R}_{c}^{\top}&amp;-\mathbf{R}_{c}^{\top}\mathbf{C}\\\hline\mathbf{0}&amp;1\\\end{array}\right]$</p><p>同理由于R转置为$R_c$，因此在w2c中，第一行的X为相机坐标系的X轴在世界坐标系下对应方向</p><p>$w2c = [R ,t] = \begin{bmatrix} X &amp; t_{x}  \\ Y &amp; t_{y} \\ Z &amp; t_{z} \\ \end{bmatrix}$</p><p>i.e.: </p><ul><li>$\mathbf{R} = \mathbf{R}_{c}^{\top}$ &lt;==&gt; $\mathbf{R}_{c} = \mathbf{R}^{\top}$</li><li>$t = - \mathbf{R}_{c}^{\top}\mathbf{C}$ &lt;==&gt; $\mathbf{C} = - \mathbf{R}_{c} t$</li></ul><p>在colmap_read_model.py中读取colmap文件信息，得到w2c并转换为c2w<br>在gen_cameras.py中将c2w转换为w2c，然后world_mat = intrinsic @ w2c<br>在Dataset()中加载world_mat，并P = world_mat @ scale_mat，对P进行decomposeProjectionMatrix，得到intrinsics和pose = w2c @ scale_mat，使得c2w时对世界坐标进行缩放<code>/scale_mat</code>，<strong>使得训练时感兴趣物体在单位坐标系内</strong></p><h5 id="光线生成"><a href="#光线生成" class="headerlink" title="光线生成"></a>光线生成</h5><p>pose: c2w (3,4)</p><ul><li><code>rays_v = pose[:3,:3] @ directions_cams</code></li><li><code>rays_o = pose[:3, 3].expand(rays_v.shape)</code></li></ul><h4 id="NeRO"><a href="#NeRO" class="headerlink" title="NeRO"></a>NeRO</h4><p>在database.py的_normalize中，对self.poses(w2c)作变换，使得变换后的self.poses为w’2c，即新的世界坐标系到相机坐标系的变换<br>其中w2c也乘以了scale，这也是为了使得c2w时对世界坐标进行缩放<code>/scale_mat</code>，<strong>使得训练时感兴趣物体在单位坐标系内</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pose w2c --&gt; w&#x27;2c</span></span><br><span class="line"><span class="comment"># x3 = R_rec @ (scale * (x0 + offset))</span></span><br><span class="line"><span class="comment"># R_rec.T @ x3 / scale - offset = x0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pose [R,t] x_c = R @ x0 + t</span></span><br><span class="line"><span class="comment"># pose [R,t] x_c = R @ (R_rec.T @ x3 / scale - offset) + t</span></span><br><span class="line"><span class="comment"># x_c = R @ R_rec.T @ x3 + (t - R @ offset) * scale</span></span><br><span class="line"><span class="comment"># R_new = R @ R_rec.T    t_new = (t - R @ offset) * scale</span></span><br><span class="line"><span class="keyword">for</span> img_id, pose <span class="keyword">in</span> self.poses.items():</span><br><span class="line">    R, t = pose[:,:<span class="number">3</span>], pose[:,<span class="number">3</span>]</span><br><span class="line">    R_new = R @ R_rec.T</span><br><span class="line">    t_new = (t - R @ offset) * scale</span><br><span class="line">    self.poses[img_id] = np.concatenate([R_new, t_new[:,<span class="literal">None</span>]], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h5 id="光线生成-1"><a href="#光线生成-1" class="headerlink" title="光线生成"></a>光线生成</h5><p>pose: w2c (3,4)</p><p>$w2c = \left[\begin{array}{c|c}\mathbf{R}&amp;t\\\hline\mathbf{0}&amp;1\end{array}\right] = \left[\begin{array}{c|c}\mathbf{R}_{c}^{\top}&amp;-\mathbf{R}_{c}^{\top}\mathbf{C}\\\hline\mathbf{0}&amp;1\\\end{array}\right]$<br>i.e.: </p><ul><li>$\mathbf{R} = \mathbf{R}_{c}^{\top}$ &lt;==&gt; $\mathbf{R}_{c} = \mathbf{R}^{\top}$</li><li>$t = - \mathbf{R}_{c}^{\top}\mathbf{C}$ &lt;==&gt; $\mathbf{C} = - \mathbf{R}_{c} t$</li></ul><p>$\mathbf{C} = - \mathbf{R}^{\top} t$<br>世界坐标系下相机原点C: <code>rays_o = poses[:, :, :3].permute(0, 2, 1) @ -poses[:, :, 3:]</code></p><p>世界坐标系下光线方向：<code>rays_d = poses[idxs, :, :3].permute(0, 2, 1) @ rays_d.unsqueeze(-1)</code> # (rays_d = ray_batch[‘dirs’])</p><h2 id="反射Reflection"><a href="#反射Reflection" class="headerlink" title="反射Reflection"></a>反射Reflection</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/BRDF.png" alt="BRDF.png|666"></p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/21376124">基于物理着色：BRDF - 知乎 (zhihu.com)</a></p></blockquote><p>Phong Reflection Model<br>$I_{Phong}=k_{a}I_{a}+k_{d}(n\cdot l)I_{d}+k_{s}(r\cdot v)^{\alpha}I_{s}$</p><ul><li>其中下标$a$表示环境光（Ambient Light），下标$d$表示漫射光（Diffuse Light），下标$s$表示镜面光（Specular Light），$k$表示反射系数或者材质颜色，$I$表示光的颜色或者亮度，$\alpha$可以模拟表面粗糙程度，值越小越粗糙，越大越光滑</li><li>入射方向$\mathbf{l}$，反射方向$\mathbf{r} = 2(\mathbf{n} \cdot \mathbf{l})\mathbf{n} - \mathbf{l}$，法向量$\mathbf{n}$</li><li><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230803205658.png" alt="image.png"></p></li><li><p>漫射光和高光分别会根据入射方向、反射方向和观察方向的变化而变化，还可以通过$\alpha$参数来调节表面粗糙程度，从而控制高光区域大小和锐利程度，而且运算简单，适合当时的计算机处理能力。</p></li></ul><h3 id="Blinn-Phong-Reflection-Model"><a href="#Blinn-Phong-Reflection-Model" class="headerlink" title="Blinn-Phong Reflection Model"></a>Blinn-Phong Reflection Model</h3><p>$I_{Blinn-Phong}=k_aI_a+k_d(n\cdot l)I_d+k_s(n\cdot h)^\alpha I_s$</p><ul><li>半角向量$\mathbf{h}$为光线入射向量和观察向量的中间向量：$h=\frac{l+v}{||l+v||}$</li><li>Blinn-Phong相比Phong，在观察方向趋向平行于表面时，高光形状会拉长，更接近真实情况。</li></ul><h3 id="基于物理的分析模型"><a href="#基于物理的分析模型" class="headerlink" title="基于物理的分析模型"></a>基于物理的分析模型</h3><p>1967年Torrance-Sparrow在Theory for Off-Specular Reflection From Roughened Surfaces中使用辐射度学和微表面理论推导出粗糙表面的高光反射模型，1981年<strong>Cook-Torrance</strong>在A Reflectance Model for Computer Graphics中把这个模型引入到计算机图形学领域，现在无论是CG电影，还是3D游戏，基于物理着色都是使用的这个模型。<strong>Cook-Torrance</strong>：ROBERT L. COOK 和 KENNETH E. TORRANCE，提出这个BRDF的文章叫做《A Reflectance Model for Computer Graphics》，发表于1982年。</p><blockquote><p>PBR 中的 Cook-Torrance BRDF 中，Cook-Torrance 是谁？ - 房燕良的回答 - 知乎 <a href="https://www.zhihu.com/question/351339310/answer/865238779">https://www.zhihu.com/question/351339310/answer/865238779</a></p></blockquote><ul><li>辐射度学（Radiometry）是度量电磁辐射能量传输的学科，也是基于物理着色模型的基础。<ul><li>能量（Energy）$Q$，单位焦耳（$J$），每个光子都具有一定量的能量，和频率相关，频率越高，能量也越高，(波长越短)。</li><li>功率（Power），单位瓦特（Watts），或者焦耳／秒（J/s）。<ul><li>辐射度学中，辐射功率也被称为<strong>辐射通量</strong>（Radiant Flux）或者通量（Flux），指单位时间内通过表面或者空间区域的能量的总量，用符号$Φ$表示：$\Phi={\frac{dQ}{dt}}。$</li></ul></li><li>辐照度（Irradiance），单位时间内到达单位面积的辐射能量，或<strong>到达单位面积的辐射通量</strong>。单位$W/m^2$ ，$E={\frac{d\Phi}{dA}}。$辐照度衡量的是到达表面的通量密度</li><li>辐出度（Radiant Existance），辐出度衡量的是离开表面的通量密度<ul><li>辐照度和辐出度都可以称为辐射通量密度（Radiant Flux Density）离光源越远，通量密度越低</li></ul></li><li>辐射强度<ul><li>立体角（Solid Angle）立体角则是度量三维角度的量，用符号$\omega$表示，单位为立体弧度（也叫球面度，Steradian，简写为sr）等于立体角在单位球上对应的区域的面积（实际上也就是在任意半径的球上的面积除以半径的平方$\omega=\frac{s}{r^{2}}$），单位球的表面积是$4\pi$，所以整个球面的立体角也是$4\pi$。</li><li>我们可以用一个向量和一个立体角来表示一束光线，向量表示这束光线的指向，立体角表示这束光线投射在单位球上的面积，也就是光束的粗细。</li><li>辐射强度（Radiant Intensity），指<strong>通过单位立体角的辐射通量</strong>。用符号$I$表示，单位W/sr，定义为$I=\frac{d\Phi}{d\omega}$</li><li>辐射强度不会随距离变化而变化，不像点光源的辐照度会随距离增大而衰减，这是因为立体角不会随距离变化而变化。</li></ul></li><li>辐射率（Radiance），指每<strong>单位面积每单位立体角的辐射通量密度</strong>。用符号$L$表示，单位$W/m^{2}sr$，$L=\frac{d\Phi}{d\omega dA^{\perp}}$其中$dA^{\perp}$⊥是微分面积$dA$在垂直于光线方向的投影，如下图所示<ul><li><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230731202820.png" alt="image.png"></li><li><strong>辐射率实际上可以看成是我们眼睛看到（或相机拍到）的物体上一点的颜色</strong>。在基于物理着色时，计算表面一点的颜色就是计算它的辐射率。</li><li>辐射率不会随距离变化而衰减，这和我们日常感受一致，在没有雾霾的干扰时，我们看到的物体表面上一点的颜色并不会随距离变化而变化。<strong>为什么辐照度会随距离增大而衰减，但是我们看到的颜色却不会衰减呢？这是因为随着距离变大，我们看到的物体上的一块区域到达视网膜的通量密度会变小，同时这块区域在视网膜表面上的立体角也会变小，正好抵消了通量密度的变化。</strong></li></ul></li></ul></li></ul><h3 id="BRDF"><a href="#BRDF" class="headerlink" title="BRDF"></a>BRDF</h3><p>我们看到一个表面，实际上是周围环境的光照射到表面上，然后表面将一部分光反射到我们眼睛里，双向反射分布函数BRDF（Bidirectional Reflectance Distribution Function）是描述表面入射光和反射光关系的。</p><p>对于一个方向的入射光，表面会将光反射到表面上半球的各个方向，不同方向反射的比例是不同的，我们用BRDF来表示指定方向的反射光和入射光的比例关系</p><p>$f(l,v)=\frac{dL_{o}(v)}{dE(l)}$</p><ul><li>$l$是入射光方向，$v$是观察方向，也就是我们关心的反射光方向。</li><li>$dL_{o}(v)$是表面反射到$v$方向的反射光的微分辐射率。表面反射到$v$方向的反射光的辐射率为$L_{o}(v)$，来自于表面上半球所有方向的入射光线的贡献，而微分辐射率$dL_{o}(v)$特指来自方向$l$的入射光贡献的反射辐射率。$W/m^{2}sr$</li><li>$dE(l)$是表面上来自入射光方向$l$的微分辐照度。表面接收到的辐照度为$E(l)$，来自上半球所有方向的入射光线的贡献，而微分辐照度$dE(l)$特指来自于方向$l$的入射光。$W/m^2$ </li><li>BRDF：f单位为$\frac{1}{sr}$</li></ul><blockquote><p><a href="https://www.zhihu.com/question/28476602/answer/41003204">(32 封私信 / 44 条消息) brdf为什么要定义为一个单位是sr-1的量？ - 知乎 (zhihu.com)</a></p></blockquote><p>表面对不同频率的光反射率可能不一样，因此BRDF和光的频率有关。在图形学中，将BRDF表示为RGB向量，三个分量各有自己的$f$函数</p><hr><p>BRDF需要处理表面上半球的各个方向，如下图使用<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Spherical_coordinate_system">球坐标系</a>定义方向更加方便。球坐标系使用两个角度来确定一个方向：</p><ol><li>方向相对法线的角度$\theta$，称为极角（Polar Angle）或天顶角（Zenith Angle）</li><li>方向在平面上的投影相对于平面上一个坐标轴的角度$\phi$，称为方位角（Azimuthal Angle）</li></ol><p>因此BRDF也可以写成：$f(\theta_i,\phi_i,\theta_o,\phi_o)$</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230731204150.png" alt="image.png"></p><p><strong>怎么用BRDF来计算表面辐射率</strong></p><p>来自方向$l$的入射<strong>辐射率</strong>：$L_i(l)=\frac{d\Phi}{d\omega_idA^-}=\frac{d\Phi}{d\omega_idAcos\theta_i}=\frac{dE(l)}{d\omega_icos\theta_i}$<br>则照射到表面来自于方向$l$的入射光贡献的<strong>微分辐照度</strong>：$dE(l)=L_i(l)d\omega_icos\theta_i$</p><ul><li>表面反射到$v$方向的由来自于方向$l$的入射光贡献的微分辐射率：$dL_o(v)=f(l,v)\otimes dE(l)=f(l,v)\otimes L_i(l)d\omega_icos\theta_i$</li><li>要计算表面反射到$v$方向的来自上半球所有方向入射光线贡献的<strong>辐射率</strong>，可以将上式对半球所有方向的光线积分：$L_{o}(v)=\int_{\Omega}f(l,v)\otimes L_{i}(l)cos\theta_{i}d\omega_{i}$(表面反射辐射率即方向$v$观察到的颜色)</li></ul><p><strong>对于点光源、方向光等理想化的精准光源（Punctual Light）</strong>，计算过程可以大大简化。我们考察单个精准光源照射表面，此时表面上的一点只会被来自一个方向的一条光线照射到（而面积光源照射表面时，表面上一点会被来自多个方向的多条光线照射到），则辐射率：$L_o(v)=f(l,v)\otimes E_Lcos\theta_i$，or多条光线：$L_{o}(v)=\sum_{k=1}^{n}f(l_{k},v)\otimes E_{L_{k}}cos\theta_{i_{k}}$</p><ul><li>这里使用光源的辐照度，对于阳光等全局方向光，可以认为整个场景的辐照度是一个常数，对于点光源，辐照度随距离的平方衰减，用公式$E_{L}=\frac{\Phi}{4\pi r^{2}}$就可以求出到达表面的辐照度，Φ是光源的功率，比如100瓦的灯泡，r是表面离光源的距离</li></ul><h3 id="BRDF（Microfacet-Theory）"><a href="#BRDF（Microfacet-Theory）" class="headerlink" title="BRDF（Microfacet Theory）"></a>BRDF（Microfacet Theory）</h3><p>我们用法线分布函数（Normal Distribution Function，简写为NDF）D(h)来描述组成表面一点的所有微表面的法线分布概率，现在可以这样理解：向NDF输入一个朝向ℎ，NDF会返回朝向是ℎ的微表面数占微表面总数的比例（虽然实际并不是这样，这点我们在讲推导过程的时候再讲），比如有1%的微表面朝向是ℎ，那么就有1%的微表面可能将光线反射到v方向。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230731210724.png" alt="image.png"></p><p>实际上并不是所有微表面都能收到接受到光线，如下面左边的图有一部分入射光线被遮挡住，这种现象称为Shadowing。也不是所有反射光线都能到达眼睛，下面中间的图，一部分反射光线被遮挡住了，这种现象称为Masking。光线在微表面之间还会互相反射，如下面右边的图，这可能也是一部分漫射光的来源，在建模高光时忽略掉这部分光线。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230731210734.png" alt="image.png"></p><ul><li>Shadowing和Masking用几何衰减因子（Geometrical Attenuation Factor）$G(l,v)$来建模，输入入射和出射光线方向，输出值表示光线未被遮蔽而能从$l$反射到$v$方向的比例</li><li>光学平面并不会将所有光线都反射掉，而是一部分被反射，一部分被折射，反射比例符合菲涅尔方程（Fresnel Equations）$F(l,h)$</li></ul><p>则BRDF镜面反射部分：<br>$f(l,v)=\frac{F(l,h)G(l,v)D(h)}{4cos\theta_icos\theta_o}=\frac{F(l,h)G(l,v)D(h)}{4(n\cdot l)(n\cdot v)}$</p><ul><li>n为宏观表面法线</li><li>h为微表面法线</li></ul><blockquote><p><a href="https://zhuanlan.zhihu.com/p/342807202">为什么BRDF的漫反射项要除以π？ - 知乎 (zhihu.com)</a></p></blockquote><p>光照模型有很多种<a href="https://zhuanlan.zhihu.com/p/342807202#ref_1">1</a>，Cook-Torrance 光照模型是最常用的。光照一般划分为漫反射和高光。漫反射模型提出的有 <a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Lambertian_reflectance">Lambert</a>、 <a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Oren%25E2%2580%2593Nayar_reflectance_model">Oren-Nayar</a>、<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Minnaert_function">Minnaert</a> 三种，它们有着不同的计算公式。Cook-Torrance 采用 Lambert 漫反射后的计算如下：<br>$f=k_{d}f_{lambert}+k_{s}f_{cook-torrance},\quad f_{lambert}=\frac{c_{diffuse}}{\pi}$</p><p><strong>反照率</strong>（albedo）是行星物理学中用来表示天体反射本领的物理量，定义为物体的<strong>辐射度</strong>（radiosity）与<strong>辐照度</strong>（irradiance）之比。射是出，照是入，出射量除以入射量，得到<strong>无量纲量</strong>。绝对<strong>黑体</strong>（black body）的反照率是0。煤炭呈黑色，反照率 接近0，因为它吸收了投射到其表面上的几乎所有可见光。镜面将可见光几乎全部反射出去，其反照率接近1。albedo 翻译成<strong>反照率</strong>，与 reflectance（<strong>反射率</strong>）是有区别的。反射率用来表示某一种波长的反射能量与入射能量之比；而反照率用来表示全波段的反射能量与入射能量之比。BRDF 的 R 是 reflectance，方程仅关注一种波长。</p><h3 id="NeRO中的BRDF"><a href="#NeRO中的BRDF" class="headerlink" title="NeRO中的BRDF"></a>NeRO中的BRDF</h3><script type="math/tex; mode=display">f(\omega_{i},\omega_{0})=\underbrace{(1-m)\frac{a}{\pi}}_{\mathrm{diffuse}}+\underbrace{\frac{DFG}{4(\omega_{i}\cdot\mathbf{n})(\omega_{0}\cdot\mathbf{n})}}_{\mathrm{specular}},</script><h4 id="Specular"><a href="#Specular" class="headerlink" title="Specular"></a>Specular</h4><p>微面BRDF，参考上</p><h4 id="Diffuse"><a href="#Diffuse" class="headerlink" title="Diffuse"></a>Diffuse</h4><blockquote><p><a href="https://ciel1012.github.io/2019/05/30/pbr/">Physically Based Rendering - 就决定是你了 | Ciel’s Blog (ciel1012.github.io)</a></p></blockquote><p>specular用于描述光线击中物体表面后直接反射回去，使表面看起来像一面镜子。有些光会透入被照明物体的内部。这些光线要么被物体吸收（通常转换为热量），要么在物体内被散射。有一些散射光线有可能会返回表面，被眼球或相机捕捉到，这就是diffuse light。diffuse和subsurface scattering(次表面散射)描述的都是同一个现象。<br>根据材质，吸收和散射的光通常具有不同波长，因此<strong>仅有部分光被吸收，使得物体具有了颜色</strong>。散射通常是方向随机的，具有各向同性。<strong>使用这种近似的着色器只需要输入一个反照率(albedo)，用来描述从表面散射回来的各种颜色的光的分量</strong>。Diffuse color有时是一个同义词。</p><p><strong>镜面反射和漫反射是互相排斥的</strong>。这是因为，如果一个光线想要漫反射，它必须先透射进材质里，也就是说，没有被镜面反射。这在着色语言中被称为“Energy Conservation（能量守恒）”，意思是一束光线在射出表面以后绝对不会比射入表面时更亮。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230731212626.png" alt="image.png"></p><p>金属度相当于镜面反射，金属度越高，镜面反射越强</p><h4 id="蒙特卡罗采样"><a href="#蒙特卡罗采样" class="headerlink" title="蒙特卡罗采样"></a>蒙特卡罗采样</h4><blockquote><p><a href="https://zhuanlan.zhihu.com/p/338103692">一文看懂蒙特卡洛采样方法 - 知乎 (zhihu.com)</a><br><a href="https://zhuanlan.zhihu.com/p/39628670">简明例析蒙特卡洛（Monte Carlo）抽样方法 - 知乎 (zhihu.com)</a><br><a href="https://www.cnblogs.com/barwe/p/14140681.html">逆变换采样和拒绝采样 - barwe - 博客园 (cnblogs.com)</a><br><a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo method - Wikipedia</a></p></blockquote><p>如何在不知道目标概率密度函数的情况下，抽取所需数量的样本，使得这些样本符合目标概率密度函数。这个问题简称为抽样，是蒙特卡洛方法的基本操作步骤。<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/v2-eb0945aa2185df958f4568e58300e77a_1440w.gif" alt="v2-eb0945aa2185df958f4568e58300e77a_1440w.gif"></p><p>MC sampling：</p><ul><li>Naive Method<ul><li>根据概率分布进行采样。对一个已知概率密度函数与累积概率密度函数的概率分布，我们可以直接从累积分布函数（cdf）进行采样</li><li>类似逆变换采样</li></ul></li><li>Acceptance-Rejection Method<ul><li>逆变换采样虽然简单有效，但是当累积分布函数或者反函数难求时却难以实施，可使用MC的接受拒绝采样</li><li>对于累积分布函数未知的分布，我们可以采用接受-拒绝采样。如下图所示，p(z)是我们希望采样的分布，q(z)是我们提议的分布(proposal distribution)，令kq(z)&gt;p(z)，我们首先在kq(z)中按照直接采样的方法采样粒子，接下来判断这个粒子落在途中什么区域，对于落在灰色区域的粒子予以拒绝，落在红线下的粒子接受，最终得到符合p(z)的N个粒子</li><li><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230801135729.png" alt="image.png"></li></ul></li></ul><p>数学推导：<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230801135800.png" alt="image.png|500"></p><ol><li>从 $f_r(x)$ 进行一次采样 $x_i$</li><li>计算 $x_i$ 的 <strong>接受概率</strong> $\alpha$（Acceptance Probability）:$\alpha=\frac{f\left(x_i\right)}{f_r\left(x_i\right)}$</li><li>从 (0,1) 均匀分布中进行一次采样 u</li><li>如果 $\alpha$≥u，接受 $x_i$ 作为一个来自 f(x) 的采样；否则，重复第1步</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">N=<span class="number">1000</span> <span class="comment">#number of samples needed</span></span><br><span class="line">i = <span class="number">1</span></span><br><span class="line">X = np.array([])</span><br><span class="line"><span class="keyword">while</span> i &lt; N:</span><br><span class="line">    u = np.random.rand()</span><br><span class="line">    x = (np.random.rand()-<span class="number">0.5</span>)*<span class="number">8</span></span><br><span class="line">    res = u &lt; <span class="built_in">eval</span>(x)/ref(x)</span><br><span class="line">    <span class="keyword">if</span> res:</span><br><span class="line">        X = np.hstack((X,x[res])) <span class="comment">#accept</span></span><br><span class="line">        ++i</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230801140044.png" alt="image.png"></p><ul><li><strong>Importance Sampling</strong><ul><li>接受拒绝采样完美的解决了累积分布函数不可求时的采样问题。但是接受拒绝采样非常依赖于提议分布(proposal distribution)的选择，如果提议分布选择的不好，可能采样时间很长却获得很少满足分布的粒子。</li><li>$E_{p(x)}[f(x)]=\int_a^bf(x)\frac{p(x)}{q(x)}q(x)dx=E_{q(x)}[f(x)\frac{p(x)}{q(x)}]$</li><li>我们从提议分布q(x)中采样大量粒子$x_1,x_2,…,x_n$，每个粒子的权重是 $\frac{p(x_i)}{q(x_i)}$，通过加权平均的方式可以计算出期望:</li><li>$E_{p(x)}[f(x)]=\frac{1}{N}\sum f(x_i)\frac{p(x_i)}{q(x_i)}$<ul><li>q提议的分布，p希望的采样分布</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">N=<span class="number">100000</span></span><br><span class="line">M=<span class="number">5000</span></span><br><span class="line">x = (np.random.rand(N)-<span class="number">0.5</span>)*<span class="number">16</span></span><br><span class="line">w_x = <span class="built_in">eval</span>(x)/ref(x)</span><br><span class="line">w_x = w_x/<span class="built_in">sum</span>(w_x)</span><br><span class="line">w_xc = np.cumsum(w_x) <span class="comment">#accumulate</span></span><br><span class="line"></span><br><span class="line">X=np.array([])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">    u = np.random.rand()</span><br><span class="line">    X = np.hstack((X,x[w_xc&gt;u][<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><p>其中，w_xc是对归一化后的权重计算的累计分布概率。每次取最终样本时，都会先随机一个(0,1)之间的随机数，并使用这个累计分布概率做选择。样本的权重越大，被选中的概率就越高。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230801140645.png" alt="image.png"></p><h2 id="linearColor-2-sRGB"><a href="#linearColor-2-sRGB" class="headerlink" title="linearColor 2 sRGB"></a>linearColor 2 sRGB</h2><p>固定色调映射函数，将线性颜色转换为sRGB[1]，并将输出颜色剪辑为lie[0,1]。<br><a href="https://readpaper.com/paper/35410341">Proposal for a Standard Default Color Space for the Internet - sRGB.-论文阅读讨论-ReadPaper - 轻松读论文 | 专业翻译 | 一键引文 | 图表同屏</a></p><p>(Why)为什么要将线性RGB转换成sRGB</p><blockquote><p><a href="https://www.zhangxinxu.com/wordpress/2017/12/linear-rgb-srgb-js-convert/">小tip: 了解LinearRGB和sRGB以及使用JS相互转换 « 张鑫旭-鑫空间-鑫生活 (zhangxinxu.com)</a></p></blockquote><p>假设白板的光线反射率是100%，黑板的光线反射率是0%。则在线性RGB的世界中，50%灰色就是光线反射率为50%的灰色。</p><p><strong>人这种动物，对于真实世界的颜色感受，并不是线性的，而是曲线的</strong></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810162118.png" alt="image.png"></p><p>(How)线性RGB与sRGB相互转化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear_to_srgb</span>(<span class="params">linear</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(linear, torch.Tensor):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Assumes `linear` is in [0, 1], see https://en.wikipedia.org/wiki/SRGB.&quot;&quot;&quot;</span></span><br><span class="line">        eps = torch.finfo(torch.float32).eps</span><br><span class="line">        srgb0 = <span class="number">323</span> / <span class="number">25</span> * linear</span><br><span class="line">        srgb1 = (<span class="number">211</span> * torch.clamp(linear, <span class="built_in">min</span>=eps)**(<span class="number">5</span> / <span class="number">12</span>) - <span class="number">11</span>) / <span class="number">200</span></span><br><span class="line">        <span class="keyword">return</span> torch.where(linear &lt;= <span class="number">0.0031308</span>, srgb0, srgb1)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(linear, np.ndarray):</span><br><span class="line">        eps = np.finfo(np.float32).eps</span><br><span class="line">        srgb0 = <span class="number">323</span> / <span class="number">25</span> * linear</span><br><span class="line">        srgb1 = (<span class="number">211</span> * np.maximum(eps, linear) ** (<span class="number">5</span> / <span class="number">12</span>) - <span class="number">11</span>) / <span class="number">200</span></span><br><span class="line">        <span class="keyword">return</span> np.where(linear &lt;= <span class="number">0.0031308</span>, srgb0, srgb1)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">srgb_to_linear</span>(<span class="params">srgb</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(srgb, torch.Tensor):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Assumes `srgb` is in [0, 1], see https://en.wikipedia.org/wiki/SRGB.&quot;&quot;&quot;</span></span><br><span class="line">        eps = torch.finfo(torch.float32).eps</span><br><span class="line">        linear0 = <span class="number">25</span> / <span class="number">323</span> * srgb</span><br><span class="line">        linear1 = torch.clamp(((<span class="number">200</span> * srgb + <span class="number">11</span>) / (<span class="number">211</span>)), <span class="built_in">min</span>=eps)**(<span class="number">12</span> / <span class="number">5</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.where(srgb &lt;= <span class="number">0.04045</span>, linear0, linear1)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(srgb, np.ndarray):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Assumes `srgb` is in [0, 1], see https://en.wikipedia.org/wiki/SRGB.&quot;&quot;&quot;</span></span><br><span class="line">        eps = np.finfo(np.float32).eps</span><br><span class="line">        linear0 = <span class="number">25</span> / <span class="number">323</span> * srgb</span><br><span class="line">        linear1 = np.maximum(((<span class="number">200</span> * srgb + <span class="number">11</span>) / (<span class="number">211</span>)), eps)**(<span class="number">12</span> / <span class="number">5</span>)</span><br><span class="line">        <span class="keyword">return</span> np.where(srgb &lt;= <span class="number">0.04045</span>, linear0, linear1)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
            <tag> Math </tag>
            
            <tag> SurfaceReconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Metrics</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF/Metrics/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/NeRF/Metrics/</url>
      
        <content type="html"><![CDATA[<p>NeRF、Depth Estimation、Object Detection</p><ul><li>评价指标</li><li>Loss损失</li></ul><span id="more"></span><p>评价指标代码</p><ul><li><a href="https://juejin.cn/post/7232499180659458109">NeRF与三维重建专栏（一）领域背景、难点与数据集介绍 - 掘金 (juejin.cn)</a></li></ul><p>论文：(很少)</p><ul><li>Towards a Robust Framework for NeRF Evaluation</li></ul><h1 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h1><p>L1<em>loss : $loss(x,y)=\frac{1}{n}\sum</em>{i=1}^{n}|y<em>i-f(x_i)|$<br>L2_loss: $loss(x,y)=\frac{1}{n}\sum</em>{i=1}^{n}(y_i-f(x_i))^2$</p><p>在标准设置中通过 NeRF 进行的新颖视图合成使用了视觉质量评估指标作为基准。这些指标试图评估单个图像的质量，要么有(完全参考)，要么没有(无参考)地面真相图像。峰值信噪比(PSNR)，结构相似指数度量(SSIM)[32]，学习感知图像补丁相似性(LPIPS)[33]是目前为止在 NeRF 文献中最常用的。</p><p><a href="https://zhuanlan.zhihu.com/p/309892873">有真实参照的图像质量的客观评估指标:SSIM、PSNR和LPIPS - 知乎 (zhihu.com)</a></p><h2 id="PSNR↑"><a href="#PSNR↑" class="headerlink" title="PSNR↑"></a>PSNR↑</h2><p>峰值信噪比 Peak Signal to Noise Ratio<br>PSNR 是一个无参考的质量评估指标<br>$PSNR(I)=10\cdot\log_{10}(\dfrac{MAX(I)^2}{MSE(I)})$<br>$MSE=\frac1{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i,j)-K(i,j)]^2$<br>$MAX(I)^{2}$（动态范围可能的最大像素值，b 位：$2^{b}-1$），eg: 8 位图像则$MAX(I)^{2} = 255$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Neus</span></span><br><span class="line">psnr = <span class="number">20.0</span> * torch.log10(<span class="number">1.0</span> / (((color_fine - true_rgb)**<span class="number">2</span> * mask).<span class="built_in">sum</span>() / (mask_sum * <span class="number">3.0</span>)).sqrt())</span><br><span class="line"></span><br><span class="line"><span class="comment"># instant-nsr-pl</span></span><br><span class="line">psnr = -<span class="number">10.</span> * torch.log10(torch.mean((pred_rgb.to(gt_rgb)-gt_rgb)**<span class="number">2</span>))</span><br></pre></td></tr></table></figure><h2 id="SSIM↑"><a href="#SSIM↑" class="headerlink" title="SSIM↑"></a>SSIM↑</h2><p><a href="https://github.com/VainF/pytorch-msssim">VainF/pytorch-msssim: Fast and differentiable MS-SSIM and SSIM for pytorch. (github.com)</a></p><p>结构相似性 Structural Similarity Index Measure<br>SSIM 是一个完整的参考质量评估指标。<br>$SSIM(x,y)=\dfrac{(2\mu_x\mu_y+C_1)(2\sigma_{xy}+C_2)}{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2+\sigma_y^2+C_2)}$<br>衡量了两张图片之间相似度：($C_1,C_2$为常数防止除以 0)</p><p>$S(x,y)=l(x,y)^{\alpha}\cdot c(x,y)^{\beta}\cdot s(x,y)^{\gamma}$</p><p>$C_1=(K_1L)^2,C_2=(K_2L)^2,C_3=C_2/2$<br>$K_{1}= 0.01 , K_{2} = 0.03 , L = 2^{b}-1$</p><ul><li>亮度，图像 x 与图像 y 亮度 $l(x,y) =\frac{2\mu_x\mu_y+C_1}{\mu_x^2+\mu_y^2+C_1}$<ul><li>$\mu_{x} =\frac1N\sum_{i=1}^Nx_i$像素均值<ul><li>$x_i$像素值，N 总像素数</li></ul></li><li>当 x 与 y 相同时，$l(x,y) = 1$</li></ul></li><li>对比度，$c(x,y)=\frac{2\sigma_x\sigma_y+C_2}{\sigma_x^2+\sigma_y^2+C_2}$<ul><li>图像标准差$\sigma_x=(\frac1{N-1}\sum_{i=1}^N(x_i-\mu_x)^2)^{\frac12}$</li></ul></li><li>结构对比，$s(x,y)=\frac{\sigma_{xy}+C_3}{\sigma_x\sigma_y+C_3}$ - 图像的协方差$\sigma_{xy}=\frac1{N-1}\sum_{i=1}^N(x_i-\mu_x)(y_i-\mu_y)$<br>实际使用中(圆对称高斯加权公式)，使用一个高斯核对局部像素求 SSIM，最后对所有的局部 SSIM 求平均得到 MSSIM</li></ul><p>使用高斯核，均值、标准差和协方差变为：<br>$\mu_{x}=\sum_{i}w_{i}x_{i}$<br>$\sigma_{x}=(\sum_{i}w_{i}(x_{i}-\mu_{x})^{2})^{1/2}$<br>$\sigma_{xy}=\sum_{i}w_{i}(x_{i}-\mu_{x})(y_{i}-\mu_{y})$</p><h2 id="LPIPS↓"><a href="#LPIPS↓" class="headerlink" title="LPIPS↓"></a>LPIPS↓</h2><p>学习感知图像块相似度 Learned Perceptual Image Patch Similarity<br><strong>LPIPS 比传统方法（比如 L2/PSNR, SSIM, FSIM）更符合人类的感知情况</strong>。<strong>LPIPS 的值越低表示两张图像越相似，反之，则差异越大。</strong><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230801170138.png" alt="image.png"></p><p>LPIPS 是一个完整的参考质量评估指标，它使用了学习的卷积特征。分数是由多层特征映射的加权像素级 MSE 给出的。<br>$LPIPS(x,y)=\sum\limits_{l}^{L}\dfrac{1}{H_lW_l}\sum\limits_{h,w}^{H_l,W_l}||w_l\odot(x^l_{hw}-y^l_{hw})||^2_2$</p><h2 id="CD↓"><a href="#CD↓" class="headerlink" title="CD↓"></a>CD↓</h2><p>Chamfer Distance 倒角距离<br>点云或 mesh 重建模型评估指标，它度量两个点集之间的距离，其中一个点集是参考点集，另一个点集是待评估点集</p><p>$d_{\mathrm{CD}}(S_1,S_2)=\frac{1}{S_1}\sum_{x\in S_1}\min_{y\in S_2}\lVert x-y\rVert_2^2+\frac{1}{S_2}\sum_{y\in S_2}\min_{x\in S_1}\lVert y-x\rVert_2^2$</p><p>S1 和 S2 分别表示两组 3D 点云，第一项代表 S1 中任意一点 x 到 S2 的最小距离之和，第二项则表示 S2 中任意一点 y 到 S1 的最小距离之和。<br>如果该距离较大，则说明两组点云区别较大；如果距离较小，则说明重建效果较好。</p><p>$\begin{aligned}\mathcal{L}_{CD}&amp;=\sum_{y’\in Y’}min_{y\in Y}||y’-y||_2^2+\sum_{y\in Y}min_{y’\in Y’}||y-y’||_2^2,\end{aligned}$</p><h2 id="P2S↓"><a href="#P2S↓" class="headerlink" title="P2S↓"></a>P2S↓</h2><p>average point-to-surface(P2S) distance平均点到面距离</p><p><strong>P2S距离：</strong> CAPE数据集scan包含大的空洞，为了排除孔洞影响，我们记录scan点到最近重构表面点之间距离，为Chamfer距离的单向版本；measure the average point-to-surface Euclidean distance (P2S) in cm <strong>from the vertices on the reconstructed surface to the ground truth</strong></p><h2 id="Normal↓"><a href="#Normal↓" class="headerlink" title="Normal↓"></a>Normal↓</h2><p>average surface normal error平均表面法向损失</p><p><strong>Normal difference:</strong> 表示使用重构的及真值surface分别进行渲染normal图片，计算两者之间L2距离，用于捕获高频几何细节误差。<br>For both reconstructed and ground truth surfaces, we <strong>render their normal maps</strong> in the image space from the input viewpoint respectively. We then <strong>calculate the L2 error</strong> between these two normal maps.</p><h2 id="IoU↑"><a href="#IoU↑" class="headerlink" title="IoU↑"></a>IoU↑</h2><p>Intersection over Union(IoU)交并比<br>在目标检测中用到的指标$IOU = \frac{A \cap B}{A \cup B}$<br>一般来说，这个比值 ＞ 0.5 就可以认为是一个不错的结果了。</p><ul><li>A: GT bounding box</li><li>B: Predicted bounding box</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231010101549.png" alt="image.png|333"></p><h2 id="EMD↓"><a href="#EMD↓" class="headerlink" title="EMD↓"></a>EMD↓</h2><p>Earth Mover’s distance 推土距离,度量两个分布之间的距离<br><a href="https://zhuanlan.zhihu.com/p/145739750">EMD(earth mover’s distances)距离 - 知乎 (zhihu.com)</a> </p><p>$\mathcal{L}_{EMD}=min_{\phi:Y\rightarrow Y^{\prime}}\sum_{x\in Y}||x-\phi(x)||_{2}$ , φ indicates a parameter of bijection.</p><h1 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h1><h2 id="RGB-Loss"><a href="#RGB-Loss" class="headerlink" title="RGB Loss"></a>RGB Loss</h2><p>L2 损失：<code>F.mse_loss(pred_rgb, gt_rgb)</code> $L=\sum_{i=1}^n(y_i-f(x_i))^2$<br>L1 损失：<code>F.l1_loss(pred_rgb, gt_rgb)</code>更稳定？ $L=\sum_{i=1}^n|y_i-f(x_i)|$</p><h2 id="Eikonal-Loss"><a href="#Eikonal-Loss" class="headerlink" title="Eikonal Loss"></a>Eikonal Loss</h2><p>$\mathcal{L}_{r e g}=\frac{1}{n m}\sum_{k,i}(|\nabla f(\hat{\mathbf{p}}_{k,i})|_{2}-1)^{2}.$</p><h2 id="Mask-Loss"><a href="#Mask-Loss" class="headerlink" title="Mask Loss"></a>Mask Loss</h2><p>$\mathcal{L}_{mask}=\mathrm{BCE}(M_k,\hat{O}_k)$</p><ul><li>$\hat{O}_k=\sum_{i=1}^n T_{k,i}\alpha_{k,i}$</li><li>$M_{k} ∈ {0, 1}$</li></ul><p>BCE 二值交叉熵损失：让输出$\hat{O}_k$去逼近 label $M_{k}$</p><blockquote><p>一种新的 BCE loss<a href="https://zhuanlan.zhihu.com/p/593711934">ECCV’22 ｜ Spatial-BCE - 知乎 (zhihu.com)</a></p></blockquote><h2 id="Opacity-Loss"><a href="#Opacity-Loss" class="headerlink" title="Opacity Loss"></a>Opacity Loss</h2><p><code>loss_opaque = -(opacity * torch.log(opacity) + (1 - opacity) * torch.log(1 - opacity)).mean()</code><br>$opaque = BCE(opaque,opaque) = -[opaque <em> ln(opaque) + (1-opaque) </em>ln(1-opaque)]$</p><p>使得 opacity 更加接近 0 或者 1</p><h2 id="Sparsity-Loss"><a href="#Sparsity-Loss" class="headerlink" title="Sparsity Loss"></a>Sparsity Loss</h2><p><code>loss_sparsity = torch.exp(-self.conf.loss.sparsity_scale * out[&#39;sdf_samples&#39;].abs()).mean()</code><br>$sparsity = \frac{1}{N} \sum e^{-scale * sdf}$<br>让 sdf 的平均值更小，前景物体更加稀疏，物体内的点往外发散</p><h2 id="Geo-Neus"><a href="#Geo-Neus" class="headerlink" title="Geo-Neus"></a>Geo-Neus</h2><ul><li>sdf loss<ul><li><code>sdf_loss = F.l1_loss(pts2sdf, torch.zeros_like(pts2sdf), reduction=&#39;sum&#39;) / pts2sdf.shape[0]</code></li><li>$\mathcal{L}_{sdf} = \frac{1}{N} \sum |sdf(spoint) - 0|$</li></ul></li></ul><h2 id="other-loss"><a href="#other-loss" class="headerlink" title="other loss"></a>other loss</h2><h3 id="加强Eikonal对SDF的优化"><a href="#加强Eikonal对SDF的优化" class="headerlink" title="加强Eikonal对SDF的优化"></a>加强Eikonal对SDF的优化</h3><p><a href="https://github.com/sunyx523/StEik">sunyx523/StEik (github.com)</a><br><a href="https://zhuanlan.zhihu.com/p/649921965">NeurIPS 2023 | 三维重建中的Neural SDF(Neural Implicit Surface) - 知乎 (zhihu.com)</a><br>一个好的SDF其实只需要其法线方向上的二阶导数为0，如果在切线方向上的二阶导数为0的话，得到的SDF轮廓会非常平滑，不利于学习到一些细节。</p><p>$L_\text{L. n.}(u)=\int_{\Omega}|\nabla u(x)^TD^2u(x)\cdot\nabla u(x)|dx.$</p><h3 id="S3IM-Loss"><a href="#S3IM-Loss" class="headerlink" title="S3IM Loss"></a>S3IM Loss</h3><p><a href="https://madaoer.github.io/s3im_nerf/">S3IM (madaoer.github.io)</a></p><p>$\begin{aligned}L_{\mathrm{S3IM}}(\Theta,\mathcal{R})=&amp;1-\mathrm{S3IM}(\hat{\mathcal{R}},\mathcal{R})=1-\frac{1}{M}\sum_{m=1}^{M}\mathrm{SSIM}(\mathcal{P}^{(m)}(\hat{\mathcal{C}}),\mathcal{P}^{(m)}(\mathcal{C})).\end{aligned}$</p><h3 id="Smoothness-Loss"><a href="#Smoothness-Loss" class="headerlink" title="Smoothness Loss"></a>Smoothness Loss</h3><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230919194046.png" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/NeRF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Metrics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-view Human Body Reconstruction</title>
      <link href="/3DReconstruction/Basic%20Knowledge/Multi-view%20Human%20Body%20Reconstruction/"/>
      <url>/3DReconstruction/Basic%20Knowledge/Multi-view%20Human%20Body%20Reconstruction/</url>
      
        <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/Human.png" alt="Human.png|666"></p><p>Terminology/Jargon</p><ul><li>Human Radiance Fields</li><li>3D <strong>Clothed Human</strong> Reconstruction | <strong>Digitization</strong></li></ul><p>Application</p><ul><li>三维重建设备：手持扫描仪或 360 度相机矩阵（成本高）</li><li><a href="https://www.yangtse.com/content/1604507html">复刻一个迷你版的自己</a></li></ul><p>Method</p><ol><li><strong>Depth&amp;Normal Estimation</strong>(2K2K) <a href="Other%20Paper%20About%20Reconstruction.md">Other Paper About Reconstruction</a></li><li><del><strong>Implicit Function</strong>(PIFu or NeRF)</del> <a href="Other%20Paper%20About%20Reconstruction.md">Other Paper About Reconstruction</a></li><li><strong>Generative approach</strong>  <a href="Generative%20Models%20Review.md">Generative approach</a></li></ol><span id="more"></span><h1 id="人体三维重建方法综述"><a href="#人体三维重建方法综述" class="headerlink" title="人体三维重建方法综述"></a>人体三维重建方法综述</h1><h2 id="Implicit-Function"><a href="#Implicit-Function" class="headerlink" title="Implicit Function"></a>Implicit Function</h2><p><strong>方法 0</strong>：训练隐式函数表示<br>(eg: NeRF、PIFu、ICON)<br><strong>DoubleField</strong>(多视图)</p><p><strong><em>问题：需要估计相机位姿，估计方法有一定的误差，视图少时误差更大</em></strong></p><h2 id="Depth-amp-Normal-Estimation"><a href="#Depth-amp-Normal-Estimation" class="headerlink" title="Depth&amp;Normal Estimation"></a>Depth&amp;Normal Estimation</h2><p><strong>方法 1</strong>：深度估计+多视图深度图融合 or 多视图点云配准<br>(2K2K-based)</p><p>深度估计: 2K2K、MVSNet、ECON…</p><ul><li><p>多视图深度图融合：<a href="https://github.com/touristCheng/DepthFusion">DepthFusion: Fuse multiple depth frames into a point cloud</a></p><ul><li>需要相机位姿，位姿估计有误差</li><li>更准确的位姿: BA(Bundle Adjusted 光束法平差，优化相机 pose 和 landmark)</li></ul></li><li><p>多视图点云配准：<a href="PointCloud%20Review.md">Point Cloud Registration</a></p><ul><li><strong>点云配准</strong>(Point Cloud Registration) 2K 生成的多角度点云形状不统一</li></ul></li></ul><p><strong><em>问题：无法保证生成的多视角深度图具有多视图一致性</em></strong></p><h2 id="Generative-approach"><a href="#Generative-approach" class="headerlink" title="Generative approach"></a>Generative approach</h2><p><strong>方法 2</strong>：生成式方法由图片生成点云<br>Generative approach(Multi-view image、pose (keypoints)… —&gt; PointCloud)</p><ol><li>扩散模型<ol><li>直接生成点云 <em>BuilDiff</em></li><li>生成三平面特征+NeRF <em>RODIN</em></li><li>多视图 Diffusion <a href="https://liuyebin.com/diffustereo/diffustereo.html">DiffuStereo</a></li></ol></li><li>GAN 网络生成点云 <em>SG-GAN</em></li><li>生成一致性图片+NeRF</li></ol><ul><li>参考 <a href="https://github.com/weiyao1996/BuilDiff">BuilDiff</a>，构建网络(<a href="https://readpaper.com/pdf-annotate/note?pdfId=4544669809538392065&amp;noteId=2018413897297176576">PVCNNs</a> 单类训练)<ul><li>是否更换扩散网络 <a href="https://dit-3d.github.io/">DiT-3D</a>，可以学习显式的类条件嵌入(生成多样化的点云)</li><li>是否依靠 SMPL，根据 LBS(Linear Blending Skinning)将人体 mesh 变形到规范化空间<ul><li><a href="https://moygcc.github.io/vid2avatar/">Video2Avatar</a> (NeRF-based)将整个人体规范化后采样</li><li><a href="https://hongfz16.github.io/projects/EVA3D">EVA3D</a> 将 NeRF 融入 GAN 生成图片，并与真实图片一同训练判别器(人体规范化后分块 NeRF)</li></ul></li></ul></li></ul><p><strong><em>问题：直接生成点云或者对点云进行扩散优化，会花费大量的内存</em></strong></p><h2 id="混合方法"><a href="#混合方法" class="headerlink" title="混合方法"></a>混合方法</h2><p><strong>方法 3</strong>：组合深度估计 + 生成式方法（缝合多个方法）<br><a href="https://github.com/yztang4/HaP">HaP</a>：深度估计+SMPL 估计+Diffusion Model 精细化</p><p><strong><em>问题：依赖深度估计和 SMPL 估计得到的结果</em></strong></p><p><strong>方法 4</strong>：隐函数 + 生成式方法 + 非刚ICP配准<br><a href="https://liuyebin.com/diffustereo/diffustereo.html">DiffuStereo</a>：NeRF(DoubleField) + Diffusion Model + non-rigid ICP （<strong><em>不开源</em></strong>）</p><h1 id="三维重建方法流程对比"><a href="#三维重建方法流程对比" class="headerlink" title="三维重建方法流程对比"></a>三维重建方法流程对比</h1><h2 id="Implicit-Function-1"><a href="#Implicit-Function-1" class="headerlink" title="Implicit Function"></a>Implicit Function</h2><h3 id="NeRF"><a href="#NeRF" class="headerlink" title="NeRF"></a>NeRF</h3><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231024153406.png" alt="NeuS2.png|666"><br>预测每个采样点 sdf 和 feature 向量<br>$(sdf,\mathbf{feature})=f_\Theta(\mathbf{e}),\quad\mathbf{e}=(\mathbf{x},h_\Omega(\mathbf{x})).$</p><p>预测每个采样点颜色值<br>$\mathbf c=c_{\Upsilon}(\mathbf x,\mathbf n,\mathbf v,sdf,\mathbf{feature})$，$\mathbf n=\nabla_\mathbf x sdf.$</p><p>体渲染像素颜色<br>$\hat{C}=\sum_{i=1}^n T_i\alpha_i c_i$， $T_i=\prod_{j=1}^{i-1}(1-\alpha_j)$ ，$\alpha_i=\max\left(\frac{\Phi_s(f(\mathbf{p}(t_i))))-\Phi_s(f(\mathbf{p}(t_{i+1})))}{\Phi_s(f(\mathbf{p}(t_i)))},0\right)$</p><p>训练得到 MLP，根据 MarchingCube 得到点云</p><h3 id="PIFu"><a href="#PIFu" class="headerlink" title="PIFu"></a>PIFu</h3><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230928170950.png" alt="image.png|666"></p><p>将输入图像中每个像素的特征通过 MLP 映射为占用场</p><h2 id="Depth-amp-Normal-Estimation-1"><a href="#Depth-amp-Normal-Estimation-1" class="headerlink" title="Depth&amp;Normal Estimation"></a>Depth&amp;Normal Estimation</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921160120.png" alt="2K2K.png|666"></p><p>预测低分辨率法向量图和深度图，$\hat M$ 为预测出的 mask<br>$\mathbf{D}^l=\hat{\mathbf{D}}^l\odot\hat{\mathbf{M}}^l$， $\hat{\mathbf{D}}^l,\hat{\mathbf{M}}^l,\mathbf{N}^l=G^l_{\mathbf{D}}(I^l)$</p><p>预测高分辨率 part 法向量图，M 为变换矩阵<br>$\bar{\mathbf{n}}_i=G_{\mathbf{N},i}(\bar{\mathbf{p}}_i,\mathbf{M}_i^{-1}\mathbf{N}^l)$， $\bar{\mathbf{p}}_i=\mathbf{M}_i\mathbf{p}_i,$</p><p>拼接为高分辨率整体法向量图<br>$\mathbf{N}^h=\sum\limits_{i=1}^K\left(\mathbf{W}_i\odot\mathbf{n}_i\right)$ ，$\mathbf{n}_i=\mathbf{M}_i^{-1}\bar{\mathbf{n}}_i$</p><p>预测高分辨率深度图<br>$\mathbf{D}^h=\hat{\mathbf{D}}^h\odot\hat{\mathbf{M}}^h$，$\hat{\mathbf{D}}^h,\hat{\mathbf{M}}^h=G^h_{\mathbf{D}}(\mathbf{N}^h,\mathbf{D}^l)$</p><p>深度图转点云</p><h2 id="Generative-approach-1"><a href="#Generative-approach-1" class="headerlink" title="Generative approach"></a>Generative approach</h2><h3 id="Diffusion-Model-Network"><a href="#Diffusion-Model-Network" class="headerlink" title="Diffusion Model Network"></a>Diffusion Model Network</h3><p><a href="Diffusion%20Models.md">Diffusion Model Network学习笔记</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021114740.png" alt="image.png|666"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231024111221.png" alt="image.png|444"></p><p><strong>3D CNN</strong>: PVCNN、PointNet、PointNet++</p><p><strong>2D CNN:</strong> 3D-aware convolution(RODIN)</p><h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><p>姿势估计可能有多种解决方案，但不准确的姿势可能会导致低分辨率的几何形状</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ClothedHumans </tag>
            
            <tag> 3DReconstruction </tag>
            
            <tag> PointCloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GeoMVSNet</title>
      <link href="/3DReconstruction/Multi-view/Depth%20Estimation/GeoMVSNet/"/>
      <url>/3DReconstruction/Multi-view/Depth%20Estimation/GeoMVSNet/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>GeoMVSNet: Learning Multi-View Stereo With Geometry Perception</th></tr></thead><tbody><tr><td>Author</td><td>Zhang, Zhe and Peng, Rui and Hu, Yuxi and Wang, Ronggang</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://github.com/doubleZ0108/GeoMVSNet">doubleZ0108/GeoMVSNet: [CVPR 23’] GeoMVSNet: Learning Multi-View Stereo with Geometry Perception (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=1807465782777293568&amp;noteId=1990827633705815808">GeoMVSNet: Learning Multi-View Stereo with Geometry Perception (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231005100022.png" alt="image.png|666"></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>最近的级联多视图立体 (MVS) 方法可以通过缩小假设范围来有效地估计高分辨率深度图。然而，以往的方法忽略了粗阶段嵌入的重要几何信息，导致代价匹配脆弱，重构结果次优。在本文中，我们提出了一个几何感知模型，称为GeoMVSNet，以显式地整合粗阶段隐含的几何线索进行精细深度估计。特别是，我们设计了一个双分支几何融合网络，从粗估计中提取几何先验，以增强更精细阶段的结构特征提取。此外，我们将编码有价值的深度分布属性的粗概率体积嵌入到轻量级正则化网络中，以进一步加强深度几何直觉。同时，我们应用频域滤波来减轻高频区域的负面影响，并采用课程学习策略逐步提升模型的几何集成。为了增强我们模型的全场景几何感知，我们提出了基于高斯混合模型假设的深度分布相似性损失。在DTU和Tanks和Temples (T&amp;T)数据集上的大量实验表明，我们的GeoMVSNet实现了最先进的结果，并在T&amp;T-Advanced集上排名第一</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Depth Estimation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MVS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MVSNet</title>
      <link href="/3DReconstruction/Multi-view/Depth%20Estimation/MVSNet/"/>
      <url>/3DReconstruction/Multi-view/Depth%20Estimation/MVSNet/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>MVSNet: Depth Inference for Unstructured Multi-view Stereo</th></tr></thead><tbody><tr><td>Author</td><td>Yao, Yao and Luo, Zixin and Li, Shiwei and Fang, Tian and Quan, Long</td></tr><tr><td>Conf/Jour</td><td>ECCV</td></tr><tr><td>Year</td><td>2018</td></tr><tr><td>Project</td><td><a href="https://github.com/YoYo000/MVSNet">YoYo000/MVSNet: MVSNet (ECCV2018) &amp; R-MVSNet (CVPR2019) (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4518062699161739265&amp;noteId=1986540055632613120">MVSNet: Depth Inference for Unstructured Multi-view Stereo (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231002110228.png" alt="image.png|666"></p><p>深度估计方法</p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们提出了一个端到端的深度学习架构，<strong>用于从多视图图像中推断深度图</strong>。在该网络中，我们首先提取深度视觉图像特征，然后通过<strong>可微单应性变形</strong>在参考摄像机截锥体上构建三维代价体。接下来，我们应用3D卷积对初始深度图进行正则化和回归，然后使用参考图像对其进行细化以生成最终输出。我们的框架灵活地适应任意n视图输入，使用基于方差的成本度量，将多个特征映射到一个成本特征。在大型室内DTU数据集上验证了所提出的MVSNet。通过简单的后处理，我们的方法不仅明显优于以前的最先进的技术，而且在运行时也快了好几倍。我们还在复杂的室外坦克和寺庙数据集上对MVSNet进行了评估，在2018年4月18日之前，我们的方法在没有任何微调的情况下排名第一，显示了MVSNet强大的泛化能力</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><strong>1 Image Features</strong> 通过CNN提取图片特征，共N张图片</p><p><strong>2 Differentiable Homography</strong> 可微单应性deep features→feature volumes</p><p><strong>第i个特征map与参考特征map在深度d之间的单应性</strong>$\mathrm{x}^{\prime}\sim\mathrm{H}_{i}(d)\cdot\mathbf{x},$</p><p>$\mathbf{H}_{i}(d)=\mathbf{K}_{i}\cdot\mathbf{R}_{i}\cdot\left(\mathbf{I}-\frac{(\mathbf{t}_{1}-\mathbf{t}_{i})\cdot\mathbf{n}_{1}^{T}}{d}\right)\cdot\mathbf{R}_{1}^{T}\cdot\mathbf{K}_{1}^{T}.$</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/363830541">Multi-View Stereo中的平面扫描(plane sweep) - 知乎 (zhihu.com)</a></p></blockquote><p>General Homography正确的：$H=K_{i}(R_{i}R_{1}^{T}-\frac{(\mathbf{t}_{i}-R_{i}R_{1}^{T}\mathbf{t}_{1})\mathbf{n}_{1}^{T}}{d})K_{1}^{-1}.$</p><p>将其他图像的feature通过可微的单应变换，warp到参考图像相机前的这些平行平面上，构成一个3D的Feature Volume<br>将所有特征映射扭曲到参考相机的不同前平行平面，形成N个特征体$\{\mathbf{V}_i\}_{i=1}^N.$</p><p><strong>3 Cost Metric</strong> 将多个特征体聚合为一个代价体feature volumes→Cost Volumes<br>$\mathbf{C}=\mathcal{M}(\mathbf{V}_1,\cdots,\mathbf{V}_N)=\frac{\sum\limits_{i=1}^N{(\mathbf{V}_i-\overline{\mathbf{V}_i})^2}}{N}$</p><p><strong>4 Cost Volume Regularization</strong> cost volume经过一个四级的U-Net结构来生成一个probability volume</p><p>probability volume：每个深度下，每个像素的可能性大小</p><p><strong>5 Depth Map</strong></p><p>$\mathbf{D}=\sum_{d=d_{min}}^{d_{max}}d\times\mathbf{P}(d)$</p><p>Refinement：将深度图与原始图像串连成一个四通道的输入，经过神经网络得到深度残差，然后加到之前的深度图上从而得到最终的深度图</p><p>优化：$Loss=\sum_{p\in\mathbf{p}_{valid}}\underbrace{|d(p)-\hat{d}_i(p)|_1}_{Loss0}+\lambda\cdot\underbrace{|d(p)-\hat{d}_r(p)|_1}_{Loss1}$</p><h1 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h1><p>MVSNet的效率要高得多，重建一次扫描大约需要230秒(每个视图4.7秒)<br>MVSNet所需的GPU内存与输入图像大小和深度采样数有关。为了在原始图像分辨率和足够深度假设下对坦克和寺庙进行测试，我们选择Tesla P100显卡(16gb)来实现我们的方法。值得注意的是，DTU数据集上的训练和验证可以使用一个消费级GTX 1080ti显卡(11 GB)完成<br>局限：<br>1)提供的地面真值网格不是100%完整，因此前景后面的一些三角形会被错误地渲染到深度图中作为有效像素，这可能会影响训练过程。<br>2)如果一个像素在其他所有视图中都被遮挡，则不应用于训练。然而，如果没有完整的网格表面，我们就不能正确识别被遮挡的像素。我们希望未来的MVS数据集能够提供具有完整遮挡和背景信息的地真深度图。</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了一个用于MVS重建的深度学习架构。提出的MVSNet将非结构化图像作为输入，并以端到端方式推断参考图像的深度图。MVSNet的核心贡献是将摄像机参数编码为可微单应词，在摄像机视台上构建代价体，架起了二维特征提取和三维代价正则化网络的桥梁。onDTU数据集证明，MVSNet不仅显著优于以前的方法，而且在速度上也提高了几倍。此外，MVSNet在没有任何微调的情况下，在坦克和庙宇数据集上产生了最先进的结果，这表明了它强大的泛化能力。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Depth Estimation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MVS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ECON</title>
      <link href="/3DReconstruction/Single-view/Depth%20Estimation/ECON/"/>
      <url>/3DReconstruction/Single-view/Depth%20Estimation/ECON/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>ECON: Explicit Clothed humans Obtained from Normals</th></tr></thead><tbody><tr><td>Author</td><td>Yuliang Xiu1 Jinlong Yang1 Xu Cao2 Dimitrios Tzionas3 Michael J. Black1</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://xiuyuliang.cn/econ/">ECON: Explicit Clothed humans Optimized via Normal integration (xiuyuliang.cn)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4700954831381069826&amp;noteId=1983981033620573952">ECON: Explicit Clothed humans Obtained from Normals (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230930173026.png" alt="image.png"></p><p>姿态稳定(<strong>ICON在难的姿势下较好地重建</strong>)+灵活拓扑(<strong>ECON还可以较好地重建宽松的衣服</strong>)</p><p>缺陷：<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230930204752.png" alt="image.png"></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>艺术家策展的扫描与深层隐式函数（IF）的结合，使得可以从图像中创建详细的、穿着衣物的3D人物成为可能。然而，现有方法远非完美。基于IF的方法可以恢复自由形式的几何形状，但在看不见的姿势或服装下会产生脱离身体的肢体或退化的形状。为了增加这些情况的稳健性，现有的工作使用显式参数化身体模型来限制表面重建，但这限制了自由形式表面（如与身体不符的宽松服装）的恢复。我们想要的是一种结合了隐式和显式方法的最佳特性的方法。为此，我们提出了两个关键观察点：（1）当前的网络在推断详细的2D maps方面表现更好，而不是完整的3D表面，以及（2）参数化模型可以被看作是将详细的表面片段拼接在一起的“画布”。ECON即使在宽松的服装和具有挑战性的姿势下也可以推断出高保真度的3D人物，同时具有逼真的面部和手指。这超越了以前的方法。对CAPE和Renderpeople数据集的定量评估表明，ECON比现有技术更精确。感知研究还表明，ECON的感知逼真度明显更高。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Detailed-normal-map-prediction"><a href="#Detailed-normal-map-prediction" class="headerlink" title="Detailed normal map prediction"></a>Detailed normal map prediction</h2><p>$\mathcal{L}_{\mathrm{SMPL-X}}=\mathcal{L}_{\mathrm{N.diff}}+\mathcal{L}_{\mathrm{S.diff}}+\mathcal{L}_{\mathrm{J.diff}},$ </p><ul><li>在ICON基础上添加了(2D body landmarks)二维地标间的关节损失(L2): $\mathcal{L}_\mathrm{J_diff}=\lambda_\mathrm{J_diff}|\mathcal{J}^\mathrm{b}-\widehat{\mathcal{J}^\mathrm{c}}|,$</li></ul><h2 id="Front-and-back-surface-reconstruction"><a href="#Front-and-back-surface-reconstruction" class="headerlink" title="Front and back surface reconstruction"></a>Front and back surface reconstruction</h2><p>将覆盖的法线贴图提升到2.5D表面。我们期望这些2.5D表面满足三个条件:<br>(1)高频表面细节与预测的覆盖法线图一致;<br>(2)低频表面变化(包括不连续面)与SMPL-X的一致;<br>(3)前后轮廓的深度彼此接近。</p><blockquote><p><a href="https://github.com/xucao-42/bilateral_normal_integration">xucao-42/bilateral_normal_integration: Official implementation of “Bilateral Normal Integration” (BiNI), ECCV 2022. (github.com)</a></p></blockquote><p>利用bilateral normal integration (BiNI)方法，利用<strong>粗糙先验、深度图和轮廓一致性</strong>进行全身网格重建。<br>本文提出了一种深度感知轮廓一致的双边法向积分(d-BiNI)方法<br>$\mathrm{d-BiNI}(\widehat{\mathcal{N}}_{\mathrm{F}}^{\mathrm{c}},\widehat{\mathcal{N}}_{\mathrm{B}}^{\mathrm{c}},\mathcal{Z}_{\mathrm{F}}^{\mathrm{b}},\mathcal{Z}_{\mathrm{B}}^{\mathrm{b}})\to\widehat{\mathcal{Z}}_{\mathrm{F}}^{\mathrm{c}},\widehat{\mathcal{Z}}_{\mathrm{B}}^{\mathrm{c}}.$</p><p>优化方法：<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230930184708.png" alt="image.png"></p><ul><li>$\mathcal{L}_{\mathrm{n}}$ 由BiNI引入的前后BiNI项</li><li>$\mathcal{L}_{\mathrm{d}}$ 前后深度prior项 ，$\mathcal{L}_{\mathrm{d}}(\widehat{\cal Z}_{i}^{\mathrm{c}};\mathcal{Z}_{i}^{\mathrm{b}})=|\widehat{\cal Z}_{i}^{\mathrm{c}}-\mathcal{Z}_{i}^{\mathrm{b}}|\quad i\in\{F,B\}.$</li><li>$\mathcal{L}_{\mathrm{s}}$ 前后轮廓一致性项，$\mathcal{L}_{\mathrm{s}}(\widehat{\mathcal{Z}_{\mathrm{F}}^{\mathrm{c}}},\widehat{\mathcal{Z}_{\mathrm{B}}^{\mathrm{c}}})=|\widehat{\mathcal{Z}_{\mathrm{F}}^{\mathrm{c}}}-\widehat{\mathcal{Z}_{\mathrm{B}}^{\mathrm{c}}}|_{\mathrm{silhouette}}.$</li></ul><h2 id="Human-shape-completion"><a href="#Human-shape-completion" class="headerlink" title="Human shape completion"></a>Human shape completion</h2><p>sPSR(Screened poisson surface reconstruction) completion with SMPL-X (ECONEX).<br>在SMPL-X的mesh中将前后摄像头可以看到的三角形网格移除，留下的三角形soup包括侧面和遮挡区域，将sPSR应用到soup和d-BiNI曲面$\{\mathcal{M}_{\mathrm{F}},\mathcal{M}_{\mathrm{B}}\}$的并集，得到一个水密重建。<em>(这种方法称为ECONEX。虽然ECONEX避免了四肢或侧面的缺失，但由于SMPL-X与实际的衣服或头发之间的差异，它不能为原来缺失的衣服和头发表面产生连贯的表面;见图4中的ECONEX)</em></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230930203744.png" alt="image.png"></p><p>Inpainting with IF-Nets+ (RIF)<br>为了提高重建一致性，我们使用学习的隐式函数(IF)模型来“补绘”给定的前后d-BiNI表面缺失的几何形状<br>IF-Nets+以体素化的前、后地真深度图$\{\mathcal{Z}_{\mathrm{F}}^{\mathfrak{c}},\mathcal{Z}_{\mathrm{B}}^{\mathfrak{c}}\}$和体素化(估计)的身体网格$\mathcal{M}^{\mathrm{b}}$作为输入进行训练，并以地真3D形状进行监督</p><p>sPSR completion with SMPL-X and RIF (ECONIF).<br>为了获得最终的网格R，我们应用sPSR来缝合<br>(1)d-BiNI表面，<br>(2)来自Rif的侧面和闭塞的三角形汤纹，<br>(3)从估计的SMPL-X体裁剪的脸或手</p><ul><li>虽然RIF已经是一个完整的人体网格，但我们只使用它的侧面和遮挡部分，因为与d-BiNI表面相比，它的正面和背面区域缺乏清晰的细节</li><li>此外，我们使用从$\mathcal{M}^{\mathrm{b}}$裁剪的脸部或手，因为这些部分在RIF中通常重建得很差</li></ul><p>IF-Nets+ ：<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231001100118.png" alt="image.png"></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>Training on THuman2.0. and 我们使用 THuman2.0 来训练 ICON 变体，IF-Nets+、IF-Nets、PIFu 和 PaMIR。</p><h1 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h1><p>Limitations<br>ECON 将 RGB 图像和估计的 SMPL-X 身体作为输入。然而，从单个图像中恢复 SMPL-X 身体（或类似的模型）仍然是一个悬而未决的问题，不能完全解决。这中的任何故障都会导致 ECON 故障，如图 10-A 和图 10-B 所示。ECON的重建质量主要依赖于预测法线图的准确性。如图 10-C 和图 10-D 所示，糟糕的法线贴图可能会导致过于接近甚至相交的前表面和后表面。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230930204752.png" alt="image.png"></p><p>Future work.<br>除了解决上述限制之外，其他几个方向对于实际应用很有用。目前，ECON只重建3D几何图形。还可以恢复底层骨架和蒙皮权重，例如，使用SSDR[40]，以获得完全动画的化身。此外，推断反向视图纹理将导致完全纹理的化身。从恢复的几何图形中解开服装、头发或配件将使这些样式的合成、编辑和转移成为可能。最后，ECON 的重建可用作训练神经化身的伪地面实况 [16, 19, 30]。</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了 ECON，一种从彩色图像重建详细的穿着衣服 3D 人体的方法。ECON结合了显式参数模型和深度隐函数的优点;它估计人体和服装的详细3D表面，而不局限于特定的拓扑，同时对具有挑战性的看不见的姿势和服装具有鲁棒性。为此，它采用了<strong>变分正态积分</strong>和<strong>形状补全</strong>的最新进展，并有效地将这些扩展到从彩色图像重建人体的任务。我们相信这项工作可以导致 3D 视觉社区的实际应用和有用的数据增强，因此，我们发布了我们的模型和代码</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Depth Estimation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ClothedHumans </tag>
            
            <tag> DepthEstimation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ICON</title>
      <link href="/3DReconstruction/Single-view/Implicit%20Function/ICON/"/>
      <url>/3DReconstruction/Single-view/Implicit%20Function/ICON/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>ICON: Implicit Clothed humans Obtained from Normals</th></tr></thead><tbody><tr><td>Author</td><td>Yuliang Xiu1,  Jinlong Yang1,  Dimitrios Tzionas1,2,  Michael J. Black1</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://icon.is.tue.mpg.de/">ICON (mpg.de)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?noteId=1983977872542331392">ICON: Implicit Clothed humans Obtained from Normals (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230930162915.png" alt="image.png"></p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/477379718">CVPR 2022 | ICON: 提高三维数字人重建的姿势水平 - 知乎 (zhihu.com)</a></p></blockquote><p>输入：</p><ul><li>经过分割的着衣人类的RGB图像</li><li>从图像估计得到的SMPL身体<ul><li>SMPL身体用于指导ICON的两个模块：一个推断着衣人类的详细表面法线（前视图和后视图），另一个推断一个具有可见性感知的隐式表面（占用场的等值表面）</li><li><strong>迭代反馈循环使用推断出的详细法线来优化SMPL</strong></li></ul></li></ul><p>缺点：</p><ul><li>宽松的衣服无法重建</li><li>依赖HPS估计出的SMPL body</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231001114030.png" alt="image.png"></p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>目前用于学习逼真和可动画化的3D服装化身的方法需要摆姿势的3D扫描或带有精心控制的用户姿势的2D图像。相比之下，我们的目标是仅从不受约束姿势的2D图像中学习化身。给定一组图像，我们的方法从每个图像中估计出一个详细的3D表面，然后将这些图像组合成一个可动画的化身。隐式函数非常适合第一个任务，因为它们可以捕获头发和衣服等细节。然而，目前的方法对不同的人体姿势并不健壮，并且经常产生带有断裂或无实体肢体，缺失细节或非人类形状的3D表面。问题是这些方法使用对全局姿态敏感的全局特征编码器。为了解决这个问题，我们提出了ICON(“从normal获得的隐式穿衣服的人”)，它使用了局部特征。<br>ICON有两个主要模块，它们都利用SMPL(-X)体模型。首先，ICON根据SMPL(-X)法线推断出详细的穿衣服的人法线(前/后)。其次，能见度感知隐式表面回归器(a visibility-aware implicit surface regressor)产生人类占用场的等面。重要的是，在推断时，反馈循环在改进SMPL(-X)网格并精炼法线之间交替进行。在多种姿势下的主体的多个重建帧的情况下，我们使用SCANimate从中生成可动画的化身。在AGORA和CAPE数据集上的评估结果显示，即使在有限的训练数据情况下，ICON在重建方面也超越了现有技术。此外，它对于野外姿势/图像和超出帧范围的剪裁等不同分布的样本更加稳健。ICON迈出了一步，朝着从野外图像中稳健地重建3D着装人物的方向迈进。这使得可以直接从视频中创建具有个性化和自然姿势依赖的服装变形的化身。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Body-guided-normal-prediction"><a href="#Body-guided-normal-prediction" class="headerlink" title="Body-guided normal prediction"></a>Body-guided normal prediction</h2><p><strong>PyTorch3D differentiable renderer DR</strong> render M(SMPL body) from two opposite views <strong>obtaining</strong> “front” ( i.e., observable side) and “back” ( i.e. , occluded side)<br>前后body法向贴图 <strong>SMPL-body normal maps</strong>： $\mathcal{N}^{\mathrm{b}}=\{\mathcal{N}_{\mathrm{front}}^{\mathrm{b}},\mathcal{N}_{\mathrm{back}}^{\mathrm{b}}\}.$</p><p>$\mathcal{N}^{\mathrm{b}}$ 和原始图片$\mathcal{I}$ ，通过<strong>法向量网络</strong>$\mathcal{G}^{\mathbb{N}}=\{\mathcal{G}_{\mathrm{front}}^{\mathbb{N}},\mathcal{G}_{\mathrm{back}}^{\mathbb{N}}\}$预测带衣服人体法向贴图$\widehat{\mathcal{N}}^{\mathrm{c}}=\{\widehat{\mathcal{N}}_{\mathrm{front}}^{\mathrm{c}},\widehat{\mathcal{N}}_{\mathrm{back}}^{\mathrm{c}}\}$</p><ul><li>训练法向量网络：$\mathcal{L}_{\mathrm{N}}=\mathcal{L}_{\mathrm{pixel}}+\lambda_{\mathrm{VGG}}\mathcal{L}_{\mathrm{VGG}},$<ul><li>$\mathcal{L}_{\mathrm{pixel}}=|\mathcal{N}_{\mathrm{v}}^{\mathrm{c}}-\mathcal{N}_{\mathrm{v}}^{\mathrm{c}}|,\mathrm{v}=\{\mathrm{front},\mathrm{back}\}$是GT与预测法向量图的L1损失</li><li>$\mathcal{L}_{\mathrm{VGG}}$是perceptual loss感知损失，有助于恢复细节</li></ul></li></ul><p>由于HPS回归器不能给出像素对齐的SMPL拟合，需要在训练中<strong>优化SMPL body的生成</strong>$\mathcal{L}_{\mathrm{SMPL}}=\min_{\theta,\beta,t}(\lambda_{\mathrm{N_diff}}\mathcal{L}_{\mathrm{N_diff}}+\mathcal{L}_{\mathrm{S_iff}}),$</p><ul><li>优化形状β，姿态θ和平移t</li><li>$\mathcal{L}_{\mathrm{N_diff}}=|\mathcal{N}^{\flat}-\widehat{\mathcal{N}^{c}}|,\quad\mathcal{L}_{\mathrm{S_diff}}=|\mathcal{S}^{\flat}-\widehat{\mathcal{S}^{c}}|,$<ul><li>$\mathcal{L}_{\mathrm{N_diff}}$：a normal-map loss (L1)</li><li>$\mathcal{L}_\mathrm{S_diff}$ ：SMPL人体法线图与人体掩模轮廓</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230930171015.png" alt="image.png"></p><p>在推理过程中，ICON交替进行:(1)使用推断的$\hat{\mathcal{N}^{\mathrm{c}}}$法线来精炼SMPL网格，(2)使用精炼的SMPL来重新推理$\hat{\mathcal{N}^{\mathrm{c}}}$</p><h2 id="Local-feature-based-implicit-3D-reconstruction"><a href="#Local-feature-based-implicit-3D-reconstruction" class="headerlink" title="Local-feature based implicit 3D reconstruction"></a>Local-feature based implicit 3D reconstruction</h2><p>给定predicted clothed-body normal maps和the SMPL-body mesh，我们基于局部特征对穿衣服的人的隐式三维表面进行回归$\mathcal{F}_\mathrm{P}=[\mathcal{F}_\mathrm{s}(\mathrm{P}),\mathcal{F}_\mathrm{n}^\mathrm{b}(\mathrm{P}),\mathcal{F}_\mathrm{n}^\mathrm{c}(\mathrm{P})],$</p><ul><li>$\mathcal{F}_{\mathrm{s}}$是查询点P到最近身体点$\mathrm{P^{b}\in\mathcal{M}}$ 的符号距离</li><li>$\mathcal{F}_{\mathrm{n}}^{\mathrm{b}}$是$\mathrm{P^{b}}$的质心面法线barycentric surface normal，两者都提供了针对自身遮挡的强正则化。</li><li>$\mathcal{F}_{\mathrm{n}}^{\mathrm{c}}$是根据$\mathrm{P^{b}}$可见度，从$\widehat{\mathcal{N}}_\mathrm{front}^\mathrm{c}\operatorname{or}\widehat{\mathcal{N}}_\mathrm{back}^\mathrm{c}$提取的法向量<ul><li>$\mathcal{F}_{\mathrm{n}}^{\mathrm{c}}(\mathrm{P})=\begin{cases}\widehat{\mathcal{N}}_{\mathrm{front}}^{\mathrm{c}}(\pi(\mathrm{P}))&amp;\text{if P}^{\mathrm{b}}\text{is visible}\\\widehat{\mathcal{N}}_{\mathrm{back}}^{\mathrm{c}}(\pi(\mathrm{P}))&amp;\text{else},\end{cases}$</li></ul></li></ul><p>最后将$\mathcal{F}_\mathrm{P}$输入implicit function，通过MLP估计<strong>the occupancy at point P</strong>： $\widehat{o}(\mathbf{P}).$</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>将ICON与PIFu[54]和PaMIR[70]进行比较</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们展示了ICON，它可以从单个图像中健壮地恢复3D穿衣服的人，其准确性和真实感超过了现有技术。其中有两个关键:(1)利用三维体模型对解进行正则化，同时迭代优化该体模型。(2)利用局部特征消除与全局姿态的伪相关。彻底的消融研究证实了这些选择。结果的质量足以从单目图像序列中构建3D化身<br>Limitations and future work.<br>由于ICON先前利用了强壮的身体，远离身体的宽松衣服可能难以重建;见图7。尽管ICON对体拟合的小误差具有鲁棒性，但体拟合的重大失效将导致重构失败。因为它是在正视视图上训练的，ICON在强烈的透视效果上有问题，产生不对称的肢体或解剖学上不可能的形状。一个关键的未来应用是单独使用图像来创建一个穿着衣服的化身数据集。这样的数据集可以推进人体形状生成的研究[15]，对时尚行业有价值，并促进图形应用。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Implicit Function </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ClothedHumans </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PIFuHD</title>
      <link href="/3DReconstruction/Single-view/Implicit%20Function/PIFuHD/"/>
      <url>/3DReconstruction/Single-view/Implicit%20Function/PIFuHD/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization</th></tr></thead><tbody><tr><td>Author</td><td>Shunsuke Saito1,3 Tomas Simon2 Jason Saragih2 Hanbyul Joo3</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2020</td></tr><tr><td>Project</td><td><a href="https://shunsukesaito.github.io/PIFuHD/">PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization (shunsukesaito.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=542688756272230400&amp;noteId=1981107888522777856">PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230928175323.png" alt="image.png"></p><p>Encoder: stacked hourglass network<br>MLP</p><ul><li>Coarse L：(257, 1024, 512, 256, 128, 1)</li><li>Fine H：(272, 512, 256, 128, 1)，将Coarse MLP的第四层输出$\Omega \in \mathbb{R}^{256}$作为输入<br>表面法线网络：由9个残差块和4个下采样层组成</li><li>$\mathcal{L}_{N}=\mathcal{L}_{VGG}+\lambda_{l1}\mathcal{L}_{l1},$ 其中$L_{VGG}$为Johnson等人[17]提出的感知损失，$L_{l1}$为预测与真值法向之间的l1距离</li></ul><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>基于图像的三维人体形状估计的最新进展是由深度神经网络提供的表示能力的显着改进所推动的。尽管目前的方法已经在现实世界中展示了潜力，但它们仍然无法产生具有输入图像中通常存在的细节水平的重建。我们认为这种限制主要源于两个相互冲突的要求; 准确的预测需要大量的背景，但精确的预测需要高分辨率。由于当前硬件的内存限制，以前的方法往往采用低分辨率图像作为输入来覆盖大的空间环境，结果产生不太精确(或低分辨率)的3D估计。我们通过制定端到端可训练的<strong>多层次体系结构</strong>来解决这一限制。粗级以较低的分辨率观察整个图像，并专注于整体推理。这为通过观察更高分辨率的图像来估计高度详细的几何形状提供了一个精细的水平。我们证明，通过充分利用k分辨率输入图像，我们的方法在<strong>单图像人体形状重建</strong>方面明显优于现有的最先进技术。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><strong>Pixel-Aligned Implicit Function</strong><br>$f(\mathbf{X},\mathbf{I})=\begin{cases}1&amp;\text{if }\mathbf{X}\text{is inside mesh surface}\\0&amp;\text{otherwise},\end{cases}$</p><ul><li>I为单个RGB图像</li><li>$f(\mathbf{X},\mathbf{I})=g\left(\Phi\left(\mathbf{x},\mathbf{I}\right),Z\right),$<ul><li>$\Phi\left(\mathbf{x},\mathbf{I}\right).$表示X的正交投影到x</li><li>$Z = X_{z}$是由二维投影x定义的射线的深度。</li></ul></li></ul><p>请注意，沿着同一条射线的所有3D点具有完全相同的图像特征Φ (x, I)，来自相同的投影位置x，因此函数g应该关注不同的输入深度Z，以消除沿射线占用的3D点的歧义。</p><p>PIFu中使用的stacked hourglass可以接受整个图片，从而可以采用整体推理进行一致的深度推理，对于实现具有泛化能力的鲁棒3D重建起着重要作用，但是该表示的表达性受到特征分辨率的限制。<br>(在PIFu中，二维特征嵌入函数Φ使用卷积神经网络(CNN)架构，函数g使用多层感知器(MLP))</p><p><strong>Multi-Level Pixel-Aligned Implicit Function</strong><br>以1024×1024分辨率图像为输入的多级方法</p><ul><li>以下采样的512 × 512图像为输入，<strong>重点整合全局几何信息</strong>，生成128 × 128分辨率的骨干图像特征</li><li>1024×1024分辨率图像作为输入，并产生512×512分辨率的骨干图像特征，<strong>从而添加更多细微的细节</strong></li></ul><p>$f^L(\mathbf{X})=g^L\left(\Phi^L\left(\mathbf{x}_L,\mathbf{I}_L,\mathbf{F}_L,\mathbf{B}_L,\right),Z\right)$<br>$f^{H}\left(\mathbf{X}\right)=g^{H}\left(\Phi^{H}\left(\mathbf{x}_{H},\mathbf{I}_{H},\mathbf{F}_{H},\mathbf{B}_{H},\right),\Omega(\mathbf{X})\right),$</p><p>$\mathbf{x}_H=2\mathbf{x}_L.$<br>$Φ^{H}$的接受域不覆盖整个图像，但由于其全卷积架构，网络可以用随机滑动窗口进行训练，并在原始图像分辨率(即1024 × 1024)下进行推断</p><p><strong>Note</strong>: 精细层模块采用粗层提取的3D嵌入特征，而不是绝对深度值。我们的粗级模块的定义类似于PIFu，<strong>进行修改</strong>，它也采用预测的正面F和背面B法线映射</p><ul><li>修改：预测人体背部的精确几何形状是一个不适定问题，因为它不能直接从图像中观察到。因此，背面必须完全由MLP预测网络推断，由于该问题的模糊性和多模态性质，三维重建往往是平滑和无特征的。这部分是由于占用损失(第3.4节)有利于不确定性下的平均重建，但也因为最终的MLP层需要学习复杂的预测函数。我们发现，如果我们将部分推理问题转移到特征提取阶段，网络可以产生更清晰的重构几何。为了做到这一点，我们预测法线映射作为图像空间中3D几何的代理，并将这些法线映射作为特征提供给像素对齐的预测器。然后，3D重建由这些地图引导，以推断特定的3D几何形状，使mlp更容易产生细节。我们<strong>使用pix2pixHD[44]网络预测图像空间中的背面和正面法线</strong>，从RGB颜色映射到法线贴图</li></ul><p><strong>Loss Functions and Surface Sampling</strong></p><p>在<strong>一组采样点</strong>上使用扩展的二进制交叉熵(BCE)损失</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_{o}& =\sum_{\mathbf{X}\in\mathcal{S}}\lambda f^{*}(\mathbf{X})\log f^{\{L,H\}}(\mathbf{X})  \\&+(1-\lambda)\left(1-f^*(\mathbf{X})\right)\log\left(1-f^{\{L,H\}}(\mathbf{X})\right)\end{aligned}</script><ul><li>其中S表示评估损失的样本集，λ是S中表面外点的比率，$f^{∗}(·)$表示该位置的GT占用，$f^{\{L,H\}}(·)$是L、H两个像素对齐隐式函数</li></ul><p>如PIFu中所述，我们使用均匀体积样本和均匀采样表面周围的重要性采样的混合采样点，在均匀采样的表面周围使用高斯扰动。我们发现这种采样方案产生的结果比采样点与表面的距离成反比的结果更清晰</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>Datasets：RenderPeople、HDRI Haven1中的163个二阶球面谐波，使用预先计算的辐射传输来渲染网格、使用COCO增强随机背景图像，不需要分割作为预处理</p><h1 id="Discussion-amp-Future-Work"><a href="#Discussion-amp-Future-Work" class="headerlink" title="Discussion &amp; Future Work"></a>Discussion &amp; Future Work</h1><p>我们提出了一个多层次框架，该框架对整体信息和局部细节进行联合推理，从而在没有任何额外后处理或侧信息的情况下，<strong>从单幅图像中获得穿衣服的人的高分辨率3D重建</strong>。我们的多层次像素对齐隐式函数通过规模金字塔作为隐式3D嵌入增量传播全局上下文来实现这一点。这就避免了对具有有限先验方法的显式几何做出过早的决定。我们的实验表明，将这种3d感知环境纳入准确和精确的重建是很重要的。此外，我们表明，在图像域规避模糊大大提高了遮挡区域的三维重建细节的一致性。<br>由于多层方法依赖于提取3D嵌入的前几个阶段的成功，因此提高基线模型的鲁棒性有望直接提高我们的整体重建精度。未来的工作可能包括纳入人类特定的先验(例如，语义分割、姿态和参数化3D面部模型)，并增加对隐式表面的2D监督[37,25]，以进一步支持野外输入。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Implicit Function </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PIFu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PIFu</title>
      <link href="/3DReconstruction/Single-view/Implicit%20Function/PIFu/"/>
      <url>/3DReconstruction/Single-view/Implicit%20Function/PIFu/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization</th></tr></thead><tbody><tr><td>Author</td><td>Shunsuke Saito1,2 <em> Zeng Huang1,2 </em> Ryota Natsume3 * Shigeo Morishima3 Angjoo Kanazawa4Hao Li1,2,5</td></tr><tr><td>Conf/Jour</td><td>ICCV</td></tr><tr><td>Year</td><td>2019</td></tr><tr><td>Project</td><td><a href="https://shunsukesaito.github.io/PIFu/">PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization (shunsukesaito.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4518249702759227393&amp;noteId=1981090816765700608">PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230928170950.png" alt="image.png"><br>表面重建网络：stacked hourglass<br>纹理推断网络：由残差块组成的architecture of CycleGAN<br>隐函数网络：MLP</p><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们引入了像素对齐隐函数（PIFu），这是一种隐式表示，它将2D图像的像素与其对应的3D对象的全局上下文局部对齐。使用PIFu，我们提出了一种端到端的深度学习方法，用于数字化高度详细的穿着人类，该方法可以从单个图像和可选的多个输入图像中推断3D表面和纹理。高度复杂的形状，如发型、服装，以及它们的变化和变形，可以以统一的方式数字化。与用于3D深度学习的现有表示相比，PIFu产生了高分辨率的表面，包括基本上看不见的区域，如人的背部。特别地，与体素表示不同，它具有存储效率，可以处理任意拓扑，并且所得表面与输入图像在空间上对齐。此外，虽然以前的技术被设计为处理单个图像或多个视图，但PIFu自然地扩展到任意数量的视图。我们展示了DeepFashion数据集对真实世界图像的高分辨率和稳健重建，该数据集包含各种具有挑战性的服装类型。我们的方法在公共基准上实现了最先进的性能，并优于之前从单个图像进行人体数字化的工作。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>PIFu: Pixel-Aligned Implicit Function</p><ul><li>$f(F(x),z(X))=s:s\in\mathbb{R},$<ul><li>$x=\pi(X)$，2D点x是3D点X的投影</li><li>z(X)是相机坐标空间中的深度值</li><li>F(x)＝g(I(x))是x处的图像特征，双线性采样获得</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230928170950.png" alt="image.png"></p><p>对于GT数据集，使用0.5水平集表示表面<br>$f_v^*(X)=\begin{cases}1,&amp;\text{if}X\text{is inside mesh surface}\\0,&amp;\text{otherwise}\end{cases}.$</p><p>通过Spatial Sampling.在空间中采样n个点X</p><p><strong>Surface Reconstruction</strong><br>$\mathcal{L}_{V}=\frac{1}{n}\sum_{i=1}^{n}|f_{v}(F_{V}(x_{i}),z(X_{i}))-f_{v}^{*}(X_{i})|^{2},$</p><ul><li>X为3D点，F是X对应像素x处来自编码器的图片特征</li></ul><p><strong>Texture Inference</strong><br>一般$\mathcal{L}_{C}=\frac{1}{n}\sum_{i=1}^{n}|f_{c}(F_{C}(x_{i}),z(X_{i}))-C(X_{i})|,$</p><ul><li>$C(X_{i})$是表面点X的地面真实RGB值</li></ul><p>使用上述损失函数天真地训练fc严重存在过拟合的问题<br>本文使用$\mathcal{L}_{C}=\frac{1}{n}\sum_{i=1}^{n}\big|f_{c}(F_{C}(x_{i}’,F_{V}),X_{i,z}’)-C(X_{i})\big|,$</p><ul><li>添加几何特征输入</li><li>引入偏移量：$\epsilon\sim\mathcal{N}(0,d)$<ul><li>$X_{i}^{\prime}=X_{i}+\epsilon\cdot N_{i}.$</li><li>d = 1.0 cm</li></ul></li></ul><p><strong>MVS</strong><br>将隐式函数f分解为特征嵌入函数f1和多视图推理函数f2<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230928173658.png" alt="image.png"></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>Datasets：RenderPeople、BUFF、DeepFashion</p><h1 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h1><p>我们引入了一种新的像素对齐隐式函数，该函数将输入图像的像素级信息与3D物体的形状在空间上对齐，用于基于深度学习的3D形状和纹理推理，从单个输入图像中推断穿衣服的人。<br>我们的实验表明，可以推断出高度可信的几何形状，包括大部分看不见的区域，如人的背部，同时保留图像中存在的高频细节。与基于体素的表示不同，我们的方法可以产生高分辨率的输出，因为我们不受体积表示的高内存要求的限制。此外，我们还演示了如何将这种方法自然地扩展到在给定部分观察的情况下推断一个人的整个纹理。与现有的基于图像空间中的正面视图合成背面区域的方法不同，我们的方法可以直接在表面上预测未见区域、凹区域和侧面区域的颜色。<br>特别是，我们的方法是第一个可以为任意拓扑形状绘制纹理的方法。由于我们能够从单个RGB相机生成穿衣服的人的纹理3D表面，因此我们正在朝着无需模板模型即可从视频中单目重建动态场景的方向迈进一步。我们处理任意附加视图的能力也使我们的方法特别适合使用稀疏视图的实用和有效的3D建模设置，传统的多视图立体或运动结构将fail。</p><p><strong>Future Work</strong>.<br>虽然我们的纹理预测是合理的，并且不受推断的3D表面的拓扑或参数化的限制，但我们相信可以推断出更高分辨率的外观，可能使用生成对抗网络或增加输入图像分辨率。在这项工作中，重建在像素坐标空间中进行，对准被试的尺度作为预处理。与其他单视图方法一样，推断尺度因子仍然是一个开放的问题，未来的工作可以解决这个问题。最后，在我们所有的例子中，没有一个被分割的主题被任何其他物体或场景元素遮挡。在现实世界中，遮挡经常发生，也许只有身体的一部分在相机中被框住。能够在部分可见的环境中对完整的物体进行数字化和预测，对于在不受约束的环境中分析人类非常有价值。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Implicit Function </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PIFu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>D-NeuS</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/D-NeuS/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/D-NeuS/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Recovering Fine Details for Neural Implicit Surface Reconstruction</th></tr></thead><tbody><tr><td>Author</td><td>Decai Chen1, Peng Zhang1,2, Ingo Feldmann1, Oliver Schreer1, and Peter Eisert1,3</td></tr><tr><td>Conf/Jour</td><td>WACV</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://github.com/fraunhoferhhi/D-NeuS">fraunhoferhhi/D-NeuS: Recovering Fine Details for Neural Implicit Surface Reconstruction (WACV2023) (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4692614734365589505&amp;noteId=1979864647240445952">Recovering Fine Details for Neural Implicit Surface Reconstruction (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230927202731.png" alt="image.png"></p><p>Idea：两个额外的损失函数</p><ul><li>几何偏差损失：鼓励隐式SDF场和体渲染的亮度场之间的几何一致性</li><li>多视图特征一致性损失：多个观察视图在表面点处的特征一致</li></ul><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>最近关于内隐神经表征的研究取得了重大进展。利用体绘制学习隐式神经表面在无三维监督的多视图重建中得到了广泛的应用。然而，由于几何和外观表现的潜在模糊性，准确地恢复精细细节仍然具有挑战性。在本文中，我们提出了D-NeuS，一种基于体绘制的神经隐式表面重建方法，能够恢复精细的几何细节，它通过<strong>两个额外的损失函数来扩展NeuS</strong>，目标是提高重建质量。首先，我们鼓励从alpha合成中渲染的表面点具有零符号距离值，减轻了将SDF转换为体渲染密度所产生的几何偏差。其次，我们通过沿射线插值采样点的SDF归零来对表面点施加多视图特征一致性。大量的定量和定性结果表明，我们的方法重建高精度的表面与细节，并优于目前的状态。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230927202107.png" alt="image.png"></p><h2 id="neus局限"><a href="#neus局限" class="headerlink" title="neus局限"></a>neus局限</h2><p>neus的unregularized sdf导致体渲染积分和SDF隐式表面之间存在偏差，颜色亮度场和几何SDF场之间的不一致导致了不理想的表面重建</p><p>单一平面相交的简单情况下，密度和权函数在不同SDF分布下的表现，显示出了非线性SDF值导致几何表面(橙色虚线)和体积渲染表面点(蓝色虚线)之间的偏差<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/15fcd4e5b38213b428a4fe32a140bf88_.jpg" alt="15fcd4e5b38213b428a4fe32a140bf88_.jpg"></p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>添加了两个损失项：</p><p><strong>几何偏差损失</strong>：鼓励隐式SDF场和体渲染的亮度场之间的几何一致性<br>$\mathcal{L}_{bias}=\frac1{|\mathbb{S}|}\sum_{\mathbf{x}_{rendered}\in\mathbb{S}}|f(\mathbf{x}_{rendered})|,$</p><ul><li>$t_{rendered}=\sum_{i}^{n}\frac{\omega_{i}t_{i}}{\sum_{i}^{n}\omega_{i}},$</li><li>$\mathbf{x}_{rendered}=\mathbf{o}+t_{rendered}\mathbf{v}.$</li><li>S为光线与表面的交点的集合</li><li>几何偏差损失与Eikonal损失相互支持，提高了重建质量</li></ul><p><strong>Multi-view Feature Consistency多视图特征一致性损失</strong><br>首先找到光线与表面的交点：</p><ul><li>s点sdf值大于0，s的下个采样点sdf值小于0：$s=\arg\min_i\{t_i\mid f(\mathbf{x}(t_i))&gt;0andf(\mathbf{x}(t_{i+1}))&lt;0\}$</li><li>根据s点和s的下个采样点s+1，使用可微线性插值可以得到表面点：$\hat{\mathbf{x}}=\left\{\mathbf{x}(\hat{t})\mid\hat{t}=\frac{f(\mathbf{x}(t_s))t_{s+1}-f(\mathbf{x}(t_{s+1}))t_s}{f(\mathbf{x}(t_s))-f(\mathbf{x}(t_{s+1}))}\right\}.$</li></ul><p>$\mathcal{L}_{feat.}=\frac{1}{N_{c}N_{v}}\sum_{i=1}^{N_{v}}|\mathbf{F}_{0}(\mathbf{p}_{0})-\mathbf{F}_{i}(\mathbf{K}_{i}(\mathbf{R}_{i}\hat{\mathbf{x}}+\mathbf{t}_{i}))|,$</p><ul><li>Nv和Nc分别为相邻源视图和特征通道的个数</li><li>F为提取的特征映射</li><li>p0为光线投射的像素</li><li>{Ki, Ri, ti}为第i个源视图的相机参数</li></ul><p>总损失：$\mathcal{L}=\mathcal{L}_{color}+\alpha\mathcal{L}_{eik.}+\beta\mathcal{L}_{bias}+\gamma\mathcal{L}_{feat.}.$</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>数据集：BlendedMVS + DTU<br>Baseline：COLMAP, IDR [33], MVSDF [37], VolSDF [32], NeuS [26], NeuralWarp [7].</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NISR</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/NISR/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/NISR/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Improving Neural Indoor Surface Reconstruction with Mask-Guided Adaptive Consistency Constraints</th></tr></thead><tbody><tr><td>Author</td><td>Xinyi Yu1, Liqin Lu1, Jintao Rong1, Guangkai Xu2,∗ and Linlin Ou1</td></tr><tr><td>Conf/Jour</td><td></td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4801757208966594561&amp;noteId=1976543992973611776">Improving Neural Indoor Surface Reconstruction with Mask-Guided Adaptive Consistency Constraints (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230925133140.png" alt="image.png"></p><p>Idea:</p><ul><li>法向预测网络，法向量约束</li><li>一致性约束(几何一致性和颜色一致性)，通过虚拟视点实现</li><li>mask的计算方法，只计算<strong>有价值</strong>的光线<span id="more"></span></li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>从2D图像中重建3D场景一直是一个长期存在的任务。最近的研究不是估计每帧深度图并在3D中融合它们，而是利用神经隐式表面作为3D重建的统一表示。配备数据驱动的预训练几何线索，这些方法已经证明了良好的性能。然而，不准确的先验估计通常是不可避免的，这可能导致重建质量次优，特别是在一些几何复杂的区域。在本文中，我们提出了一个两阶段的训练过程，解耦视图相关和视图无关的颜色，并利用<strong>两个新的一致性约束</strong>来增强细节重建性能，而<strong>不需要额外的先验</strong>。此外，我们引入了一个基本掩码方案来自适应地影响监督约束的选择，从而提高自监督范式的性能。在合成数据集和真实数据集上的实验表明，该方法能够减少先验估计误差的干扰，实现具有丰富几何细节的高质量场景重建。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>穿过一张图片$I_{k}$采样光线，并随机生成对应的虚拟光线，然后NeRF MLP渲染颜色、视图独立颜色、深度和法向。通预训练的法向估计模型来估计光线对应像素的法向，然后最小化颜色损失来优化MLP，此外还添加了mask驱动的一致性约束和法向约束</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230925133716.png" alt="image.png"></p><p>使用两个颜色网络来训练：与视图有关的颜色$\hat c^{vd}$和与视图无关的颜色$\hat c^{vi}$<br>深度图和法向量图的计算：$\hat{D}(r)=\sum_{i=1}^nT_i\alpha_it_i,\quad\hat{\mathbf{N}}(\mathbf{r})=\sum_{i=1}^nT_i\alpha_i\hat{\mathbf{n}}_i$</p><p>监督约束：</p><ul><li>颜色：$\mathcal{L}_{rgb}=\sum_{\mathbf{r}\in\mathcal{R}}\left|\hat{\mathbf{C}}(\mathbf{r})-\mathbf{C}(\mathbf{r})\right|_1$</li><li>法向量约束$\begin{aligned}\mathcal{L}_{normal}&amp;=\frac1{\left|\mathcal{M}_r\right|}\sum_{\text{r}\in\mathcal{M}_r}\left|\hat{\mathbf{N}}(\mathbf{r})-\bar{\mathbf{N}}(\mathbf{r})\right|_1\\&amp;+\left|1-\hat{\mathbf{N}}(\mathbf{r})^T\bar{\mathbf{N}}(\mathbf{r})\right|_1\end{aligned}$ ，其中$M_{r}$为射线掩模</li><li>几何一致性约束<ul><li>通过采样像素的射线生成深度图，根据深度图计算出目标3D点的位置。然后随机生成一个虚拟视点 ，根据3D目标点位置和虚拟视点可以计算出虚拟射线的方向$\mathbf{x}_t=\mathbf{o}+\hat{D}(\mathbf{r})\mathbf{v},\quad\mathbf{v}^v=\frac{\mathbf{x}_t-\mathbf{o}^v}{\left|\mathbf{x}_t-\mathbf{o}^v\right|_2}$</li><li>根据虚拟射线的视点和方向，可以由渲染框架MLP得到虚拟射线的深度图$\hat{D}(\mathbf{r}_{v})$和法向量图$\hat{\mathrm{N}}(\mathbf{r}_v)$，由两光线深度的几何一致性$\mathcal{L}_{gc}=\frac{1}{2|\mathcal{M}_{v}|}\sum_{\mathbf{r}_{v}\in\mathcal{M}_{v}}|\hat{D}(\mathbf{r}_{v})-\bar{D}(\mathbf{r}_{v})|^{2}$，其中$\bar{D}(\mathbf{r}_v)=\left|\mathbf{x}_t-\mathbf{o}^v\right|_2$</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230925140953.png" alt="image.png"></p><ul><li>光度一致性约束<ul><li>两光线渲染得到的像素颜色：$\mathcal{L}_{pc}=\frac{1}{|\mathcal{M}_{r}|}\sum_{\mathbf{r}\in\mathcal{M}_{r}}\left|\hat{\mathbf{C}}_{vi}(\mathbf{r})-\hat{\mathbf{C}}_{vi}(\mathbf{r}_{v})\right|_{1}$</li></ul></li></ul><p>$\mathcal{M}_{r}$与$\mathcal{M}_{v}$选择方法：</p><ul><li>Sample Mask：保证生成虚拟视点在物体外部<ul><li>$\left.\mathcal{M}_s=\left\{\begin{array}{lr}1,&amp;\quad if\hat{s}(\mathbf{o}_v)&gt;0.\\0,&amp;\quad otherwise.\end{array}\right.\right.$</li></ul></li><li>Occlusion Mask：解决由于两条光线的遮挡导致的深度一致性误差问题（两条光线都只穿过物体一次）<ul><li>$\left.\mathcal{M}_o^s=\left\{\begin{array}{lr}1,&amp;if~|diff(sgn(\hat{\mathbf{s}}))|_1\leq2.\\0,&amp;otherwise.\end{array}\right.\right.$</li><li>$\left.\mathcal{M}_o^v=\left\{\begin{array}{ccc}1,&amp;if&amp;|diff(sgn(\hat{\mathbf{s}}^v))|_1\leq2.\\0,&amp;&amp;otherwise.\end{array}\right.\right.$</li><li>$\mathcal{M}_o=\mathcal{M}_o^s\&amp;\mathcal{M}_o^v$</li></ul></li><li>Adaptive Check Mask<ul><li>两个光线得到像素的法向量值夹角余弦，与某个阈值比较</li><li>$cos(\hat{\mathbf{N}}(\mathbf{r}),\hat{\mathbf{N}}(\mathbf{r}_v))=\frac{\hat{\mathbf{N}}(\mathbf{r})\cdot\hat{\mathbf{N}}(\mathbf{r}_v)}{\left|\hat{\mathbf{N}}(\mathbf{r})\right|_2\left|\hat{\mathbf{N}}(\mathbf{r}_v)\right|_2}$</li><li>$\left.\mathcal{M}_a=\left\{\begin{array}{lr}1,\quad&amp;ifcos(\hat{\mathbf{N}}(\mathbf{r}),\hat{\mathbf{N}}(\mathbf{r}_v))&lt;\epsilon.\\0,&amp;otherwise.\end{array}\right.\right.$</li></ul></li><li>Mask integration<ul><li>法向量约束中mask $\mathcal{M}_{r}$：$\mathcal{M}_r=\mathcal{M}_s\&amp;\mathcal{M}_o\&amp;(1-\mathcal{M}_a)$<ul><li><strong>两个光线得到像素的法向量值夹角余弦</strong>大于某个阈值，无虚拟视点</li></ul></li><li>几何一致性约束中mask $\mathcal{M}_{v}$：$\mathcal{M}_v=\mathcal{M}_s\&amp;\mathcal{M}_o\&amp;\mathcal{M}_a$<ul><li><strong>两个光线得到像素的法向量值夹角余弦</strong>小于某个阈值，有虚拟视点</li></ul></li></ul></li></ul><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>一个NVIDIA RTX 3090 GPU</p><ul><li>ScanNet数据集</li><li>对比COLMAP、NeuralRecon、MonoSDF（MLP Version）、NeuRIS</li><li>指标：Accuracy、Completeness、Chamfer Distance、Precision、Recall、F-score、Normal Consistency</li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HRN</title>
      <link href="/3DReconstruction/Single-view/Other/HumanBody/HRN/"/>
      <url>/3DReconstruction/Single-view/Other/HumanBody/HRN/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images</th></tr></thead><tbody><tr><td>Author</td><td>Biwen Lei Jianqiang Ren Mengyang Feng Miaomiao Cui Xuansong Xie</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://younglbw.github.io/HRN-homepage/">HRN (younglbw.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4738635079538982913&amp;noteId=1972221785736728832">A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921173632.png" alt="image.png"></p><p>缺点：</p><ul><li>需要3D 先验：每张图像的GT变形图和位移图<br>Idea：</li><li>Contour-aware Loss. 新的轮廓感知损失算法，目的是拉动边缘的顶点以对齐面部轮廓</li></ul><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>由于3DMM的低维表示容量的性质限制，大多数基于3DMM(3D Morphable models)的人脸重建（FR）方法无法恢复高频面部细节，如皱纹、酒窝等。一些尝试通过引入细节映射或非线性操作来解决这个问题，然而结果仍然不够生动。为此，本文提出了一种新的分层表示网络（HRN），以实现从单一图像精确和详细的面部重建。具体而言，我们实施了几何解耦，并引入了分层表示来实现详细的面部建模。同时，还融合了面部细节的3D先验，以提高重建结果的准确性和真实性。我们还提出了一个去除修饰模块，以实现更好地解耦几何和外观。值得注意的是，我们的框架可以通过考虑不同视图的细节一致性扩展为多视图模式。对两个单视图和两个多视图FR基准的大量实验表明，我们的方法在重建准确性和视觉效果方面优于现有方法。最后，我们introduce了一个高质量的3D人脸数据集FaceHD-100，以推动高保真度人脸重建的研究。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>分层建模：</p><ul><li>低频部分：通过Basel Face Model(BFM是一种用于描述人脸形状和纹理的数学模型)作为基模型，实现输入面粗重构</li><li>中频细节：然后我们引入一个三通道变形图，它位于UV空间中，表示每个顶点相对于粗结果的偏移量。变形图作为中频细节的表示，提供了一种灵活的几何操作方法</li><li>高频细节：我们采用了[19]类似的位移图，是一个单通道图，表示沿法线方向的几何变形。位移图以像素的方式转换为渲染过程中使用的详细法线，以展示所有微小的细节，打破了基础模型顶点密度的限制</li></ul><p>输入人脸肖像：</p><ul><li>Pretraining + Training Data Generation<ul><li>使用回归网络作为人脸分析器预测BFM系数</li><li>利用3DMM数据库中的相应基得到粗对齐的meshM0和反照率A0</li><li>结合I和M0，我们可以通过采用从粗到细的可微渲染策略，在UV空间中获得内嵌纹理T</li><li>De-Retouching Module 将纹理细节烘烤到粗反照率A0中</li></ul></li><li>Training Geometry 使用两个pix2pix网络依次合成变形图和位移图<ul><li>将P和T concat 起来输入进pix2pix网络得到变形图(中频)</li><li>考虑到变形图会改变人脸几何，导致T与变形网格不对齐，我们通过将三通道变形图投影到二维空间，并将其转换为反转流F来重新对齐T，从而生成重新对齐的纹理T ‘作为第二个pix2pix网络的输入，得到位移图(高频)</li></ul></li><li>Training Reconstruction<ul><li>结合光照L和去除触摸模块生成的精细反照率，完成了单幅图像的详细人脸重建</li></ul></li></ul><p>提出了一个新数据集：由来自100个受试者的2000个高清三维网格和相应的多视图图像组成，数据由9个单反相机和8个LED灯组成的多视图3D重建系统捕获。9个摄像头均匀分布在脸部前方和侧面，每个摄像头提供8K图像，用于几何和纹理重建</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>在本文中，我们提出了一种新的层次表示网络(HRN)，用于从野外图像中精确和详细地重建人脸。具体而言，我们通过分层表示学习实现了面部几何解纠缠和建模。进一步结合细节的三维先验，提高重建结果的精度和视觉效果。此外，我们还提出了一个去修饰网络，以减轻几何和外观之间的歧义。此外，我们将HRN扩展到多视图模式，并引入了高质量的3D人脸数据集FaceHD-100，以促进稀疏视图FR的研究。大量实验表明，我们的方法在精度和视觉效果方面都优于现有方法</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Other/HumanBody </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
            <tag> Face </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vid2Avatar</title>
      <link href="/3DReconstruction/Single-view/Implicit%20Function/Vid2Avatar/"/>
      <url>/3DReconstruction/Single-view/Implicit%20Function/Vid2Avatar/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition</th></tr></thead><tbody><tr><td>Author</td><td>Chen Guo1, Tianjian Jiang1, Xu Chen1,2, Jie Song1, Otmar Hilliges1</td></tr><tr><td>Conf/Jour</td><td>CVPR 2023</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://moygcc.github.io/vid2avatar/">Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition (moygcc.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4726322030375354369&amp;noteId=1970979448862074368">Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921171140.png" alt="image.png"></p><p>Idea：$\mathcal{L}_\mathrm{dec}=\lambda_\mathrm{BCE}\mathcal{L}_\mathrm{BCE}+\lambda_\mathrm{sparse}\mathcal{L}_\mathrm{sparse}.$</p><ul><li>不透明度稀疏正则化$\mathcal{L}_{\mathrm{sparse}}^i=\frac1{|\mathcal{R}_{\mathrm{off}}^i|}\sum_{\mathbf{r}\in\mathcal{R}_{\mathrm{off}}^i}|\alpha^H(\mathbf{r})|.$惩罚与subject不相交的光线的非零光线不透明度</li><li>自监督射线分类$\begin{aligned}\mathcal{L}_\mathrm{BCE}^i&amp;=-\frac{1}{|\mathcal{R}^i|}\sum_{\mathrm{r}\in\mathcal{R}^i}(\alpha^H(\mathbf{r})\log(\alpha^H(\mathbf{r}))\\&amp;+(1-\alpha^H(\mathbf{r}))\log(1-\alpha^H(\mathbf{r}))),\end{aligned}$鼓励包含完全透明或不透明光线的光线分布<span id="more"></span></li></ul><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们提出了Vid2Avatar，一种从野外单目视频中学习人类Avatar的方法。从野外单目视频中重建自然移动的人类是困难的。解决这个问题需要准确地将人类与任意背景分离开来。此外，它需要从短视频序列中重建详细的3D表面，使问题变得更加具有挑战性。尽管面临这些挑战，<strong>我们的方法不需要任何来自大规模服装人体扫描数据集的地面真值监督或先验知识</strong>，也不依赖于任何外部分割模块。相反，它通过在三维中直接对场景中的人类和背景建模，通过两个分开的神经场参数化来解决场景分解和表面重建任务。具体来说，我们在规范空间中定义了一个时间一致的人类表示，并制定了一个全局优化，涵盖了背景模型、规范人体形状和纹理，以及每帧人体姿势参数。引入了一种粗到细的体积渲染采样策略和新的目标，用于清晰分离动态人体和静态背景，从而产生详细且稳健的3D人体几何重建。我们在公开可用的数据集上评估了我们的方法，并展示了相对于先前技术的改进。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>与前背景分离思想相同，一个网络用来预测背景，另一个网络用来预测前景(人体)的SDF和Color，然后预测背景+前景与输入视频帧的Loss用来优化网络</p><p>使用了人体姿势参数Pose(类似SMPL的定义)，在前景球内将人体warp到规范空间，预测人体的Canonical Shape Representation SDF，在变形空间中计算采样点的空间梯度。对动态前景采用<strong>基于表面的体绘制(NeuS)</strong>，对背景采用<strong>标准体绘制(NeRF)</strong></p><blockquote><p><em>我们扩展了 NeRF++ [66] 的倒置球体参数化来表示场景：外部体积（即背景）覆盖球内体积（即假设被人类占据的空间）的补，两者都由单独的网络建模。然后通过合成得到最终的像素值。</em></p></blockquote><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>The training usually takes 24-48 hours.</p><h1 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h1><p>虽然很容易获得，但Vid2Avatar 依赖于合理的姿态估计作为输入。此外，<strong>裙子或自由流动的服装等宽松的衣服由于其快速的动态而带来了重大挑战</strong>。我们参考Supp。Mat 对限制和社会影响进行更详细的讨论。</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>在本文中，我们提出了Vid2Avatar，通过自监督场景分解从单目野外视频中重建详细的3DAvatar。我们的方法不需要任何从穿着衣服的人体扫描的大型数据集中提取的groundtruth监督或先验，我们也不依赖于任何外部分割模块。通过精心设计的背景建模和时间一致的规范人类表示，建立了具有新颖场景分解目标的全局优化，通过可微复合体绘制联合优化背景场、规范人体形状和外观的参数，以及整个序列的人体姿态估计。我们的方法从单目视频中实现了鲁棒和高保真的人体重建。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Implicit Function </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2K2K</title>
      <link href="/3DReconstruction/Single-view/Depth%20Estimation/2K2K/"/>
      <url>/3DReconstruction/Single-view/Depth%20Estimation/2K2K/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>High-fidelity 3D Human Digitization from Single 2K Resolution Images</th></tr></thead><tbody><tr><td>Author</td><td>Sang-Hun Han1, Min-Gyu Park2, Ju Hong Yoon2,Ju-Mi Kang2, Young-Jae Park1, and Hae-Gon Jeon1</td></tr><tr><td>Conf/Jour</td><td>CVPR 2023 Highlight</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://sanghunhan92.github.io/conference/2K2K/">High-fidelity 3D Human Digitization from Single 2K Resolution Images Project Page (sanghunhan92.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4738290373604950017&amp;noteId=1970916692663980032">High-fidelity 3D Human Digitization from Single 2K Resolution Images (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921160120.png" alt="image.png"></p><p><strong>可以看成一种估计深度图的方法</strong><br>缺点：需要好的数据集</p><ul><li>需要提供法线图、mask、深度图(低分辨率+高分辨率)</li><li>需要人体模型的关节点信息</li><li>无法预测自遮挡部位</li><li>对低分辨率重建效果不好</li></ul><span id="more"></span><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>高质量的3D 人体重建需要高保真度和大规模的训练数据，以及有效利用<strong>高分辨率输入图像</strong>的适当网络设计。为了解决这些问题，我们提出了一种简单而有效的3D 人体数字化方法，称为2K2K，它构建了一个大规模的2K 人体数据集，并从2K 分辨率图像推断3D 人体模型。所提出的方法分别恢复了人体的全局形状和细节。低分辨率深度网络从低分辨率图像中预测全局结构，而部分图像到法线网络则预测3D 人体结构的细节。高分辨率深度网络合并全局3D 形状和详细结构，以推断高分辨率的前后侧深度图。最后，一个现成的网格生成器重建完整的3D 人体模型，可在获得。此外，我们还提供了2,050个3D 人体模型，包括纹理地图、3D 关节和 SMPL 参数，供研究目的使用。在实验中，我们展示了在各种数据集上与最新工作相比的竞争性性能。<a href="https://github.com/SangHunHan92/2K2K">https://github.com/SangHunHan92/2K2K</a></p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>分为两个阶段：</p><ul><li>Stage1 Loss：<ul><li>使用 L1 损失和 LSSIMloss 来优化图像到法线网络<ul><li>GT：低分辨率法线图，高分辨率法线图</li></ul></li><li>通过最小化平滑 L1 损失 Ls1 和二元交叉熵损失 LBCE 的线性组合来训练深度网络<ul><li>GT：低分辨率深度图，低分辨率法线图，低分辨率 Mask</li></ul></li></ul></li><li>Stage2 Loss：冻结阶段 1 训练的网络，并训练高分辨率深度生成器<ul><li>使用 Lsl1 和 LBCE 来优化高分辨率深度网络<ul><li>GT：高分辨率深度图，高分辨率法线图，高分辨率 Mask</li></ul></li></ul></li></ul><p><strong><em>最终得到的是双面的高分辨率深度图，通过深度图获取点云，然后预测法线，最后通过 a screened Poisson surface construction 来得到 mesh</em></strong></p><p>具体流程：<br>预测低分辨率法向量图和深度图，$\hat M$ 为预测出的 mask<br>$\mathbf{D}^l=\hat{\mathbf{D}}^l\odot\hat{\mathbf{M}}^l$， $\hat{\mathbf{D}}^l,\hat{\mathbf{M}}^l,\mathbf{N}^l=G^l_{\mathbf{D}}(I^l)$</p><p>预测高分辨率 part 法向量图，M 为变换矩阵<br>$\bar{\mathbf{n}}_i=G_{\mathbf{N},i}(\bar{\mathbf{p}}_i,\mathbf{M}_i^{-1}\mathbf{N}^l)$， $\bar{\mathbf{p}}_i=\mathbf{M}_i\mathbf{p}_i,$</p><p>拼接为高分辨率整体法向量图<br>$\mathbf{N}^h=\sum\limits_{i=1}^K\left(\mathbf{W}_i\odot\mathbf{n}_i\right)$ ，$\mathbf{n}_i=\mathbf{M}_i^{-1}\bar{\mathbf{n}}_i$</p><p>预测高分辨率深度图<br>$\mathbf{D}^h=\hat{\mathbf{D}}^h\odot\hat{\mathbf{M}}^h$，$\hat{\mathbf{D}}^h,\hat{\mathbf{M}}^h=G^h_{\mathbf{D}}(\mathbf{N}^h,\mathbf{D}^l)$</p><p>深度图转点云</p><h2 id="低分辨率深度网络"><a href="#低分辨率深度网络" class="headerlink" title="低分辨率深度网络"></a>低分辨率深度网络</h2><p><strong>低分辨率深度网络</strong>分别预测低分辨率深度和法向图，使用 dual-encoder AU-Net (D-AU-Net)网络, ref: <a href="https://readpaper.com/pdf-annotate/note?pdfId=4624854467519463425&amp;noteId=1997087774731159296">Monocular Human Digitization via Implicit Re-projection Networks (readpaper.com)</a></p><h2 id="Body-part-extraction和Part-wise-normal-prediction"><a href="#Body-part-extraction和Part-wise-normal-prediction" class="headerlink" title="Body part extraction和Part-wise normal prediction"></a>Body part extraction和Part-wise normal prediction</h2><p><strong>Body part extraction 和 Part-wise normal prediction</strong>预测高分辨率双面法向 map<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921163941.png" alt="image.png"></p><ul><li>将人体按照 keypoint 关节分成 patches，每个 patches 和 low 分辨率法线图来预测 high 分辨率双面法线 map<ul><li>只需训练头、躯干、手臂、腿和脚这 5 个法线预测网络 AU-Net，然后即可预测每个 patch</li><li><a href="https://readpaper.com/pdf-annotate/note?pdfId=4500191380932026369&amp;noteId=1997072200139556608">Attention U-Net: Learning Where to Look for the Pancreas (readpaper.com)</a></li></ul></li></ul><h2 id="高分辨率深度预测网络"><a href="#高分辨率深度预测网络" class="headerlink" title="高分辨率深度预测网络"></a>高分辨率深度预测网络</h2><p><strong>高分辨率深度预测网络</strong><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921164612.png" alt="image.png"><br>    <strong>网格生成</strong>：从深度图生成 3D 模型有多种方法。在这项工作中。我们采用与[13]类似的方法。我们将深度图转换为 3D 点云，然后从相邻点计算每个点的表面法线。之后，我们运行一个筛选的泊松曲面构造[22]来获得平滑的人体网格 V。</p><h1 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h1><ul><li>新的数据集：我们的数据集提供了高保真的 3D 人体模型，由 80 个 DSLR 相机、纹理贴图、3D 姿势（openpifpafw 全身）和 SMPL 模型参数捕获。<ul><li>2,050 个 scan 3D 模型</li><li>Skinned Multi-Person Linear (SMPL)</li></ul></li></ul><p>Booth 中一共 80 个 DSLR：每个条上 5 个相机，从上到下依次对齐：人头、上身、中部、下身、膝盖。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921162402.png" alt="image.png"></p><p>使用商业化软件 RealityCapture 生成初始粗糙的人体模型，然后专家手动对模型进行后处理(填充孔洞、头发几何细节)。已发布模型的顶点数约为 1M，其示例如图 2 (b) 所示。扫描模型正确地保留了手指和皱纹等几何细节，主要是因为在受控环境中捕获的高质量图像。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921162858.png" alt="image.png"></p><p>合成的 Rendered Image(使用 Unity、Unreal 等软件)</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>在四个 RTX A6000 GPU 上训练3天</p><p>Ubuntu 20.04 with Python 3.8, PyTorch 1.9.1 and CUDA 11.1<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">apt-get install -y freeglut3-dev libglib2.0-0 libsm6 libxrender1 libxext6 openexr libopenexr-dev libjpeg-dev zlib1g-dev</span><br><span class="line">apt install -y libgl1-mesa-dri libegl1-mesa libgbm1 libgl1-mesa-glx libglib2.0-0</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure></p><h2 id="Fast-test"><a href="#Fast-test" class="headerlink" title="Fast test"></a>Fast test</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> checkpoints &amp;&amp; wget https://github.com/SangHunHan92/2K2K/releases/download/Checkpoint/ckpt_bg_mask.pth.tar &amp;&amp; <span class="built_in">cd</span> ..</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python test_02_model.py --load_ckpt ckpt_bg_mask.pth.tar --save_path ./exp</span><br><span class="line"></span><br><span class="line">terminal output：</span><br><span class="line">u2net.onnx model下载&#x27;https://github.com/danielgatis/rembg/releases/download/v0.0.0/u2net  </span><br><span class="line">.onnx&#x27;</span><br><span class="line"></span><br><span class="line">python test_03_poisson.py --save_path ./exp</span><br></pre></td></tr></table></figure><p>Custom test<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin\OpenPoseDemo.exe --image_dir test_folder --write_json test_folder --hand --write_images test_folder\test --write_images_format jpg</span><br><span class="line"></span><br><span class="line">python test_02_model.py --load_ckpt ckpt_bg_mask.pth.tar --save_path ./exp --data_path ./test_folder --save_name ???</span><br><span class="line">python test_03_poisson.py --save_path ./exp --save_name ???</span><br><span class="line"></span><br><span class="line">bin\OpenPoseDemo.exe --image_dir test_2 --write_json test_2 --hand --write_images test_2\test --write_images_format jpg</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><h2 id="Render"><a href="#Render" class="headerlink" title="Render"></a>Render</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">├ IndoorCVPR09</span><br><span class="line">│  ├ airport_inside</span><br><span class="line">│  └ ⋮</span><br><span class="line">├ Joint3D</span><br><span class="line">│  ├ RP</span><br><span class="line">│  └ THuman2</span><br><span class="line">├ list</span><br><span class="line">├ obj</span><br><span class="line">│  ├ RP</span><br><span class="line">│  │  ├ rp_aaron_posed_013_OBJ</span><br><span class="line">│  │  └ ⋮</span><br><span class="line">│  └ THuman2</span><br><span class="line">│     ├ data</span><br><span class="line">│     └ smplx</span><br><span class="line">│  ├ 2K2K</span><br><span class="line">│  │  ├ 00003</span><br><span class="line">│  │  └ ⋮</span><br><span class="line">├ PERS</span><br><span class="line">│  ├ COLOR</span><br><span class="line">│  ├ DEPTH</span><br><span class="line">│  └ keypoint</span><br><span class="line">└ (ORTH)</span><br></pre></td></tr></table></figure><h3 id="Data-folder"><a href="#Data-folder" class="headerlink" title="Data folder"></a>Data folder</h3><ul><li>Obj<ul><li>2K2K<ul><li>00003<ul><li>00003.Ply</li></ul></li></ul></li></ul></li><li>PERS<ul><li>COLOR<ul><li>NOSHADING<ul><li><code>2K2K_0_y_-30_x</code><ul><li>00003_front.Png</li><li>00003_back.Png</li></ul></li><li><code>2K2K*0_y*-20_x</code></li><li>…</li><li><code>2K2K_0_y_30_x</code></li></ul></li><li>SHADED<ul><li><code>2K2K_0_y_-30_x</code><ul><li>00003_front.Png</li></ul></li><li>…</li></ul></li></ul></li><li>DEPTH<ul><li><code>2K2K_0_y_-30_x</code><ul><li>00003_front.Png</li><li>00003_back.Png</li></ul></li><li>…</li></ul></li></ul></li><li>ORTH<ul><li>COLOR<ul><li>NOSHADING</li><li>SHADED</li></ul></li><li>DEPTH</li></ul></li></ul><p>如果不算 ORTH：共 21+14=35 张图片</p><h3 id="Render-image"><a href="#Render-image" class="headerlink" title="Render image"></a>Render image</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pers2pc(image_front, front_depth.astype(np.float64) / <span class="number">32.0</span>, <span class="number">2048</span>, <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pers2pc</span>(<span class="params">pers_color, pers_depth, res, fov</span>):</span><br><span class="line">    focal = res / (<span class="number">2</span> * np.tan(np.radians(fov) / <span class="number">2.0</span>))</span><br><span class="line"></span><br><span class="line">res = <span class="number">2048</span></span><br><span class="line">fov = <span class="number">50</span></span><br><span class="line">focal = <span class="number">2195.975086601788</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python render/render.py --data_path ./data --data_name 2K2K</span><br><span class="line"></span><br><span class="line">python render/render.py --data_path ./data --data_name RP</span><br><span class="line">python render/render.py --data_path ./data --data_name THuman2 --smpl_model_path &#123;smpl_model_path&#125;</span><br></pre></td></tr></table></figure><p>Get ： PERS/COLOR and PERS/DEPTH</p><h3 id="Render-keypoint"><a href="#Render-keypoint" class="headerlink" title="Render keypoint"></a>Render keypoint</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">unzip data/Joint3D.zip -d data/Joint3D/</span><br><span class="line">python render/render_keypoint.py --data_path ./data --data_name RP</span><br><span class="line">python render/render_keypoint.py --data_path ./data --data_name THuman2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># openpose</span></span><br><span class="line"><span class="comment">## input image</span></span><br><span class="line">bin\OpenPoseDemo.exe --image_dir G:/2K2K_Datasets/230831/train/data/PERS/COLOR/FOR_POSE/2K2K_0_y_0_x --write_json G:/2K2K_Datasets/230831/train/data/PERS/keypoint_json/2K2K_0_y_0_x --face --hand --write_images_format png</span><br><span class="line"></span><br><span class="line">G:/2K2K_Datasets/230831/train/data/PERS/COLOR/FOR_POSE/2K2K_0_y_0_x</span><br><span class="line">    2K2K_0_y_10_x</span><br><span class="line">    2K2K_0_y_20_x</span><br><span class="line">    2K2K_0_y_30_x</span><br><span class="line">    2K2K_0_y_-10_x</span><br><span class="line">    2K2K_0_y_-20_x</span><br><span class="line">    2K2K_0_y_-30_x</span><br><span class="line"></span><br><span class="line"><span class="comment">## output keypoint_json</span></span><br><span class="line">G:/2K2K_Datasets/230831/train/data/PERS/keypoint_json/2K2K_0_y_0_x ...</span><br><span class="line"></span><br><span class="line"><span class="comment">## output keypoint_npy</span></span><br><span class="line">python render_keypoint_2k.py <span class="comment"># use json2npy function</span></span><br><span class="line"></span><br><span class="line">G:/2K2K_Datasets/230831/train/data/PERS/keypoint/2K2K_0_y_0_x ...</span><br></pre></td></tr></table></figure><p>Get ： PERS/keypoint</p><h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><p>Phase1 大概需要 7 天, (单张 3090)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python train.py --data_path ./data --phase 1 --batch_size 1</span><br><span class="line">python train.py --data_path ./data --phase 2 --batch_size 1 --load_ckpt &#123;checkpoint_file_name&#125;</span><br></pre></td></tr></table></figure><h1 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h1><p>由于我们明确地预测每个身体部位的法线贴图，我们的方法没有考虑严重的自遮挡，例如，when a lower arm is behind the back。我们声称这种现象本质上是模棱两可的，可能的补救措施要么是预测遮挡像素的语义，要么是在[34]之前使用人体来指导深度预测。由于空间限制，我们在补充材料中提供了几个失败案例。 </p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了 2K2K，这是一个从高分辨率单幅图像中数字化人类的有效框架。为此，我们首先通过扫描 2,050 个人体模型构建了一个大规模的人体模型数据集，并使用它们来训练我们的网络，由部分正态预测、低分辨率和高分辨率深度预测网络组成。为了有效地处理高分辨率输入，我们裁剪和弱对齐每个身体部位不仅可以处理姿势变化，还可以更好地恢复人体的精细细节，如面部表情。我们证明了所提出的方法适用于各种数据集的高分辨率图像有效工作。</p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><h2 id="Human-Body-Part-2K2K"><a href="#Human-Body-Part-2K2K" class="headerlink" title="Human Body Part(2K2K)"></a>Human Body Part(2K2K)</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921163941.png" alt="image.png"></p><p>将人体分为 12 个部分，可以只用头、躯干、手臂(4 part)、腿(4 part)和脚(2 part) 五个网络来预测</p><p><a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">CMU-Perceptual-Computing-Lab/openpose: OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation (github.com)</a></p><ul><li><strong>openpose</strong> … To get keypoint(json) ,then convert json to npy (shape: 31,3)</li><li>输入原图 image，根据 pose(.Npy)得到仿射变换矩阵(init_affine_2048)，仿射变换矩阵将目标部位移动到相机中心，然后通过 centercrop 得到 part image<ul><li>原图 image 下采样后通过网络得到低分辨率法向量图，low normal 插值到 2k 后同样变换得到 part low normal</li><li>Part image 和 part low normal 通过 5 个 part network 得到每个部分的 part high normal</li><li>Part high normal 通过 occupy 方式得到的权重，求和拼接成 high normal</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231011103025.png" alt="image.png|222"></p><p>Note：pose 中脸部只有 2eye、2ear 和 1nose，手部为 4 个 finger</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>] = [nose, L eye, R eye, L ear, R ear]</span><br><span class="line">[<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>] = [L shoudler, R shoudler, L elbow, R elbow, L wrist, R wrist]</span><br><span class="line">[<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>] = [L hip, R hip, L knee, R knee, L ankle, R ankle]</span><br><span class="line">[<span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>] = [L big toe, L little toe, L sole, R big toe, R little toe, R sole]</span><br><span class="line">[<span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>] = [L finger <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, R finger <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br></pre></td></tr></table></figure><p>根据 pose 将人体分为 12 个部分用 5 个网络预测:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Face[<span class="number">4</span>, <span class="number">3</span>]</span><br><span class="line">Body[<span class="number">6</span>, <span class="number">5</span>, <span class="number">12</span>, <span class="number">11</span>]</span><br><span class="line">Arm[<span class="number">5</span>, <span class="number">7</span>], [<span class="number">7</span>, <span class="number">9</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>], [<span class="number">6</span>, <span class="number">8</span>], [<span class="number">8</span>, <span class="number">10</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>]</span><br><span class="line">Leg[<span class="number">11</span>, <span class="number">13</span>], [<span class="number">13</span>, <span class="number">15</span>], [<span class="number">12</span>, <span class="number">14</span>], [<span class="number">14</span>, <span class="number">16</span>]</span><br><span class="line">Foot[<span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">15</span>], [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">16</span>]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Single-view/Depth Estimation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ClothedHumans </tag>
            
            <tag> DepthEstimation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Datasets</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF/Datasets/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/NeRF/Datasets/</url>
      
        <content type="html"><![CDATA[<p>数据集</p><span id="more"></span><h1 id="Custom-Datasets"><a href="#Custom-Datasets" class="headerlink" title="Custom Datasets"></a>Custom Datasets</h1><p>COLMAP + Blender(neuralangelo)</p><blockquote><p><a href="/Learn/Learn-Colmap">Learn-Colmap</a></p></blockquote><h1 id="DTU"><a href="#DTU" class="headerlink" title="DTU"></a>DTU</h1><p><a href="http://roboimagedata.compute.dtu.dk/">DTU Robot Image Data Sets | Data for Evaluating Computer Vision Methods etc.</a><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230923112720.png" alt="image.png"><br>1600x1200</p><p>在黑盒空间中，使用 6 轴工业机器人手部的结构光相机，structured light scanner 可以捕获所观察场景/对象的参考 3D 表面几何形状</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230923112501.png" alt="image.png|555"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230923112539.png" alt="image.png"></p><p>我们的机器人的<strong>定位精度难以控制</strong>，但重复性非常高，随机性很小。这意味着多次运行相同的定位脚本，每次定位几乎都是相同的。为了解决这个定位问题，我们不直接使用（或报告）发送给机器人的相机位置，而是确定并报告我们<strong>获取的相对相机位置</strong>。这是通过 MatLab 的相机校准工具箱完成的。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230923112650.png" alt="image.png|555"></p><h1 id="BlenderMVS"><a href="#BlenderMVS" class="headerlink" title="BlenderMVS"></a>BlenderMVS</h1><p><a href="https://github.com/YoYo000/BlendedMVS">YoYo000/BlendedMVS: BlendedMVS: A Large-scale Dataset for Generalized Multi-view Stereo Networks (github.com)</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4545062112983670785&amp;noteId=1973517268736296192">BlendedMVS: A Large-scale Dataset for Generalized Multi-view Stereo Networks (readpaper.com)</a><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230923143752.png" alt="image.png"><br>H ×W = 1536 × 2048</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230923111934.png" alt="image.png"></p><p>应用 3D 重建管道从精心选择的场景图像中恢复高质量的纹理网格。然后，我们将这些网格模型渲染为彩色图像和深度图。为了在训练期间引入环境照明信息，将渲染的彩色图像与输入图像进一步混合以生成训练输入</p><ul><li>Altizure 平台进行纹理网格重建，该软件将执行完整的 3D 重建管道并返回纹理网格和相机姿势作为最终输出</li><li>然后<strong>将网格模型渲染到每个相机视图点</strong>以生成渲染图像和渲染的深度图，<strong>渲染的深度图</strong>将用作 GT 深度图</li><li>由于渲染图像不包含与视图相关的照明，使用高通滤波器用于从渲染图像中提取图像视觉线索，而低通滤波器用于从输入中提取环境照明。最后线性混合生成混合图，<strong>混合图</strong>用作 GT 监督</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230923112434.png" alt="image.png|333"></p><p>混合图像与输入图像具有相似的背景照明，同时继承了渲染图像的纹理细节。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230923112838.png" alt="image.png|555"><br>与 DTU 数据集[2]不同的是，所有场景都由固定的机械臂捕获，BlendedMVS 中的场景包含各种不同的摄像机轨迹。非结构化摄像机轨迹可以更好地模拟不同的图像捕获风格，并能够使网络更一般化到真实世界的重建</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230923113119.png" alt="image.png"></p><h1 id="Tanks-and-Temples"><a href="#Tanks-and-Temples" class="headerlink" title="Tanks and Temples"></a>Tanks and Temples</h1><p><a href="https://www.tanksandtemples.org/">Tanks and Temples Benchmark</a></p><ul><li>工业激光扫描仪(industrial laser scanner.)捕获</li></ul><h1 id="Human-Body"><a href="#Human-Body" class="headerlink" title="Human Body"></a>Human Body</h1><blockquote><p><a href="https://github.com/rlczddl/awesome-3d-human-reconstruction#body-1">rlczddl/awesome-3d-human-reconstruction (github.com)</a></p></blockquote><div class="table-container"><table><thead><tr><th>Name</th><th>Link</th></tr></thead><tbody><tr><td>2K2K</td><td><a href="https://github.com/ketiVision/2K2K">ketiVision/2K2K (github.com)</a></td></tr><tr><td>ZJU-Mocap</td><td><a href="https://github.com/zju3dv/neuralbody/blob/master/INSTALL.md#zju-mocap-dataset">neuralbody/INSTALL.md at master · zju3dv/neuralbody (github.com)</a></td></tr><tr><td>_People-Snapshot_</td><td><a href="https://github.com/zju3dv/neuralbody/blob/master/INSTALL.md#people-snapshot-dataset">People-Snapshot</a></td></tr><tr><td>THUman</td><td><a href="https://github.com/ZhengZerong/DeepHuman/tree/master/THUmanDataset">THUmanDataset</a></td></tr><tr><td>THuman2.0</td><td><a href="https://github.com/ytrock/THuman2.0-Dataset">ytrock/THuman2.0-Dataset (github.com)</a></td></tr><tr><td>THuman3.0</td><td><a href="https://github.com/fwbx529/THuman3.0-Dataset">fwbx529/THuman3.0-Dataset (github.com)</a></td></tr><tr><td><strong>THUman4.0</strong></td><td><a href="https://github.com/ZhengZerong/THUman4.0-Dataset">ZhengZerong/THUman4.0-Dataset (github.com)</a></td></tr><tr><td>THUman 来源</td><td><a href="http://liuyebin.com/dataset.html">Yebin Liu (刘烨斌) (liuyebin.com)</a></td></tr><tr><td>renderpeople</td><td><a href="https://renderpeople.com/">https://renderpeople.com/</a></td></tr><tr><td>MVPHuman</td><td><a href="https://github.com/TingtingLiao/MVPHuman">MVP-Human</a></td></tr><tr><td><strong>HuMMan</strong></td><td><a href="https://github.com/caizhongang/humman_toolbox">caizhongang/humman_toolbox: Toolbox for HuMMan Dataset (github.com)</a></td></tr><tr><td>CLOTH4D</td><td><a href="https://github.com/AemikaChow/CLOTH4D">AemikaChow/CLOTH4D (github.com)</a></td></tr><tr><td>DNA-Rendering</td><td><a href="https://dna-rendering.github.io/">DNA-Rendering : A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering</a></td></tr></tbody></table></div><ul><li><p>UltraStage：多视角和多光照条件下捕获的高质量的人体资源</p><ul><li><a href="https://miaoing.github.io/RNHA/">Relightable Neural Human Assets from Multi-view Gradient Illuminations (miaoing.github.io)</a></li><li><a href="https://github.com/IHe-KaiI/RNHA_Dataset">IHe-KaiI/RNHA_Dataset: The dataset of the paper “Relightable Neural Human Assets from Multi-view Gradient Illuminations”. (github.com)</a></li></ul></li><li><p>SMPL、<a href="https://smpl-x.is.tue.mpg.de/index.html">SMPL-X (mpg.de)</a></p></li><li><p><a href="https://idea-research.github.io/HumanArt/">Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes (idea-research.github.io)</a></p></li></ul><h1 id="MVImgNet"><a href="#MVImgNet" class="headerlink" title="MVImgNet"></a>MVImgNet</h1><p>A Large-scale Dataset of Multi-view Images</p><p><a href="https://gaplab.cuhk.edu.cn/projects/MVImgNet/">MVImgNet (cuhk.edu.cn)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231021203414.png" alt=""></p><h1 id="Objaverse-XL"><a href="#Objaverse-XL" class="headerlink" title="Objaverse-XL"></a>Objaverse-XL</h1><p>Objaverse-XL is an open dataset of over 10 million 3D objects!</p><p><a href="https://github.com/allenai/objaverse-xl">allenai/objaverse-xl: 🪐 Objaverse-XL is a Universe of 10M+ 3D Objects. Contains API Scripts for Downloading and Processing! (github.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023161713.png" alt="image.png|666"></p><h1 id="ABC-Dataset"><a href="#ABC-Dataset" class="headerlink" title="ABC Dataset"></a>ABC Dataset</h1><p>A Big CAD Model Dataset<br><a href="https://deep-geometry.github.io/abc-dataset/">News | ABC Dataset (deep-geometry.github.io)</a><br>一百万个计算机辅助设计 (CAD) 模型的集合，用于研究几何深度学习方法和应用。每个模型都是显式参数化曲线和曲面的集合，为微分量、面片分割、几何特征检测和形状重建提供基本事实。对曲面和曲线的参数化描述进行采样可以生成不同格式和分辨率的数据，从而能够对各种几何学习算法进行公平比较</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231109094128.png" alt="img.png|666"></p><h1 id="NeuralLabeling-工具箱"><a href="#NeuralLabeling-工具箱" class="headerlink" title="NeuralLabeling 工具箱"></a>NeuralLabeling 工具箱</h1><blockquote><p><a href="NeuralLabeling.md">NeuralLabeling: A versatile toolset for labeling vision datasets using Neural Radiance Fields</a> &gt; <a href="https://florise.github.io/neural_labeling_web/">NeuralLabeling: A versatile toolset for labeling vision datasets using Neural Radiance Fields (florise.github.io)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230925144627.png" alt="image.png"></p><h1 id="MuSHRoom"><a href="#MuSHRoom" class="headerlink" title="MuSHRoom"></a>MuSHRoom</h1><p><a href="https://arxiv.org/abs/2311.02778">MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction and Novel View Synthesis</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231108222956.png" alt="image.png|666"></p><h1 id="室外场景"><a href="#室外场景" class="headerlink" title="室外场景"></a>室外场景</h1><h2 id="Stable-SFM"><a href="#Stable-SFM" class="headerlink" title="Stable SFM"></a>Stable SFM</h2><p>室外场景<a href="https://www.maths.lth.se/matematiklth/personal/calle/dataset/dataset.html">Carl Olsson (lth.se)</a></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/NeRF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Datasets </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Other Paper About Reconstruction</title>
      <link href="/3DReconstruction/Basic%20Knowledge/Other%20Paper%20About%20Reconstruction/"/>
      <url>/3DReconstruction/Basic%20Knowledge/Other%20Paper%20About%20Reconstruction/</url>
      
        <content type="html"><![CDATA[<p>Awesome Human Body Reconstruction</p><ol><li><strong>Depth&amp;Normal Estimation</strong>(2K2K)</li><li><strong>Implicit Function</strong>(PIFu or NeRF)</li></ol><div class="table-container"><table><thead><tr><th>Method</th><th>泛化</th><th>数据集监督</th><th>提取 mesh 方式</th><th>获得纹理方式</th></tr></thead><tbody><tr><td>2k2k</td><td>比较好</td><td>(mesh+texture:)depth、normal、mask、rgb</td><td>高质量深度图 —&gt; 点云 —&gt; mesh</td><td>图片 rgb 贴图</td></tr><tr><td>PIFu</td><td>比较好</td><td>点云(obj)、rgb(uv)、mask、camera</td><td>占用场 —&gt; MC —&gt; 点云,mesh</td><td>表面颜色场</td></tr><tr><td>NeRF</td><td>差</td><td>rgb、camera</td><td>密度场 —&gt; MC —&gt; 点云,mesh</td><td>体积颜色场</td></tr><tr><td>NeuS</td><td>差</td><td>rgb、camera</td><td>SDF —&gt; MC —&gt; 点云,mesh</td><td>体积颜色场</td></tr><tr><td>ICON</td><td>非常好</td><td>rgb+mask、SMPL、法向量估计器 DR</td><td>占用场 —&gt; MC —&gt; 点云,mesh</td><td>图片 rgb 贴图</td></tr><tr><td>ECON</td><td>非常好</td><td>rgb+mask、SMPL、法向量估计器 DR</td><td>d-BiNI + SC(shape completion)</td><td>图片 rgb 贴图</td></tr></tbody></table></div><span id="more"></span><h1 id="Gaussian-Splatting-Method"><a href="#Gaussian-Splatting-Method" class="headerlink" title="Gaussian Splatting Method"></a>Gaussian Splatting Method</h1><h2 id="SuGaR"><a href="#SuGaR" class="headerlink" title="SuGaR"></a>SuGaR</h2><p><a href="https://imagine.enpc.fr/~guedona/sugar/">SuGaR (enpc.fr)</a></p><p>3D Gaussian Splatting 提取mesh<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231123142658.png" alt="image.png|666"></p><h1 id="NeRF-Other-Object-Reconstruction"><a href="#NeRF-Other-Object-Reconstruction" class="headerlink" title="NeRF Other Object Reconstruction"></a>NeRF Other Object Reconstruction</h1><h2 id="RNb-NeuS"><a href="#RNb-NeuS" class="headerlink" title="RNb-NeuS"></a>RNb-NeuS</h2><p><a href="https://github.com/bbrument/RNb-NeuS">bbrument/RNb-NeuS: Code release for RNb-NeuS. (github.com)</a></p><p>将<strong>反射率</strong>和<strong>法线贴图</strong>无缝集成为基于神经体积渲染的 3D 重建中的输入数据<br>考虑高光和阴影：显著改善了高曲率或低可见度区域的详细 3D 重建<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231205152551.png" alt="image.png|666"></p><h2 id="Voxurf"><a href="#Voxurf" class="headerlink" title="Voxurf"></a>Voxurf</h2><p><a href="https://github.com/wutong16/Voxurf">wutong16/Voxurf: [ ICLR 2023 Spotlight ] Pytorch implementation for “Voxurf: Voxel-based Efficient and Accurate Neural Surface Reconstruction” (github.Com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023163503.png" alt="image.png|666"></p><h2 id="ReTR"><a href="#ReTR" class="headerlink" title="ReTR"></a>ReTR</h2><p><a href="https://yixunliang.github.io/ReTR/">Rethinking Rendering in Generalizable Neural Surface Reconstruction: A Learning-based Solution (yixunliang.github.io)</a><br>修改论文 title：ReTR: Modeling Rendering via Transformer for Generalizable Neural Surface Reconstruction</p><p>CNN + 3D Decoder + Transformer + NeRF 用深度图监督</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231109094904.png" alt="image.png|666"></p><h2 id="NISR"><a href="#NISR" class="headerlink" title="NISR"></a>NISR</h2><blockquote><p><a href="NISR.md">Improving Neural Indoor Surface Reconstruction with Mask-Guided Adaptive Consistency Constraints</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230925133140.png" alt="image.png|666"></p><h2 id="D-NeuS"><a href="#D-NeuS" class="headerlink" title="D-NeuS"></a>D-NeuS</h2><blockquote><p><a href="D-NeuS.md">Recovering Fine Details for Neural Implicit Surface Reconstruction</a><br><a href="https://github.com/fraunhoferhhi/D-NeuS">fraunhoferhhi/D-NeuS: Recovering Fine Details for Neural Implicit Surface Reconstruction (WACV2023) (github.com)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230927202731.png" alt="image.png|666"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/15fcd4e5b38213b428a4fe32a140bf88_.jpg" alt="15fcd4e5b38213b428a4fe32a140bf88_.jpg|333"></p><h2 id="AutoRecon"><a href="#AutoRecon" class="headerlink" title="AutoRecon"></a>AutoRecon</h2><p><a href="https://zju3dv.github.io/autorecon/">AutoRecon: Automated 3D Object Discovery and Reconstruction (zju3dv.github.io)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023164638.png" alt="image.png|666"></p><h2 id="G-Shell"><a href="#G-Shell" class="headerlink" title="G-Shell"></a>G-Shell</h2><p>重建水密物体+衣服等非水密物体——通用<br><a href="https://gshell3d.github.io/">G-Shell (gshell3d.github.io)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231024094322.png" alt="image.png|666"></p><h2 id="Adaptive-Shells"><a href="#Adaptive-Shells" class="headerlink" title="Adaptive Shells"></a>Adaptive Shells</h2><p><a href="https://research.nvidia.com/labs/toronto-ai/adaptive-shells/">Adaptive Shells for Efficient Neural Radiance Field Rendering (nvidia.com)</a></p><p>自适应使用基于体积的渲染和基于表面的渲染<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231117150239.png" alt="image.png|666"></p><h2 id="DynamicSurf"><a href="#DynamicSurf" class="headerlink" title="DynamicSurf"></a>DynamicSurf</h2><p><a href="https://arxiv.org/abs/2311.08159">DynamicSurf: Dynamic Neural RGB-D Surface Reconstruction with an Optimizable Feature Grid</a></p><p>单目 RGBD 视频重建 3D</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231116154921.png" alt="image.png|666"></p><h2 id="RayDF"><a href="#RayDF" class="headerlink" title="RayDF"></a>RayDF</h2><p><a href="https://vlar-group.github.io/RayDF.html">RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency (vlar-group.github.io)</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=2037229054391691776&amp;noteId=2047746094923644416">RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231113155552.png" alt="image.png|666"></p><h2 id="PonderV2"><a href="#PonderV2" class="headerlink" title="PonderV2"></a>PonderV2</h2><p><a href="https://github.com/OpenGVLab/PonderV2">OpenGVLab/PonderV2: PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm (github.com)</a></p><p>PointCloud 提取特征(点云编码器) + NeRF 渲染图片 + 图片损失优化点云编码器</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231107153921.png" alt="image.png|666"></p><h2 id="Spiking-NeRF"><a href="#Spiking-NeRF" class="headerlink" title="Spiking NeRF"></a>Spiking NeRF</h2><p>Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation</p><p>MLP 是连续函数，对 NeRF 网络结构的改进来生成不连续的密度场</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231116155214.png" alt="image.png|666"></p><h2 id="Hyb-NeRF"><a href="#Hyb-NeRF" class="headerlink" title="Hyb-NeRF"></a>Hyb-NeRF</h2><p><a href="https://arxiv.org/abs/2311.12490">[2311.12490] Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields (arxiv.org)</a></p><p>多分辨率混合编码</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231123143210.png" alt="image.png|666"></p><h2 id="Dynamic"><a href="#Dynamic" class="headerlink" title="Dynamic"></a>Dynamic</h2><h3 id="MorpheuS"><a href="#MorpheuS" class="headerlink" title="MorpheuS"></a>MorpheuS</h3><p><a href="https://hengyiwang.github.io/projects/morpheus">MorpheuS (hengyiwang.github.io)</a><br>MorpheuS: Neural Dynamic 360° Surface Reconstruction from <strong>Monocular RGB-D Video</strong><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231204133746.png" alt="image.png|666"></p><h3 id="NPGs"><a href="#NPGs" class="headerlink" title="NPGs"></a>NPGs</h3><p><a href="https://arxiv.org/abs/2312.01196">[2312.01196] Neural Parametric Gaussians for Monocular Non-Rigid Object Reconstruction (arxiv.org)</a><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231205153011.png" alt="image.png|666"></p><h1 id="NeRF-Human-Body-Reconstruction"><a href="#NeRF-Human-Body-Reconstruction" class="headerlink" title="NeRF Human Body Reconstruction"></a>NeRF Human Body Reconstruction</h1><h2 id="DoubleField"><a href="#DoubleField" class="headerlink" title="DoubleField"></a>DoubleField</h2><p><a href="http://www.liuyebin.com/dbfield/dbfield.html">DoubleField Project Page (liuyebin.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231110163602.png" alt="image.png|666"></p><h2 id="Learning-Visibility-Field-for-Detailed-3D-Human-Reconstruction-and-Relighting"><a href="#Learning-Visibility-Field-for-Detailed-3D-Human-Reconstruction-and-Relighting" class="headerlink" title="Learning Visibility Field for Detailed 3D Human Reconstruction and Relighting"></a>Learning Visibility Field for Detailed 3D Human Reconstruction and Relighting</h2><p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Learning_Visibility_Field_for_Detailed_3D_Human_Reconstruction_and_Relighting_CVPR_2023_paper.pdf">Learning Visibility Field for Detailed 3D Human Reconstruction and Relighting (thecvf.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008104907.png" alt="image.png|666"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008105237.png" alt="image.png|666"></p><h2 id="HumanGen"><a href="#HumanGen" class="headerlink" title="HumanGen"></a>HumanGen</h2><blockquote><p><a href="https://suezjiang.github.io/humangen/">HumanGen: Generating Human Radiance Fields with Explicit Priors (suezjiang.github.io)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231002104131.png" alt="image.png|666"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231002104310.png" alt="image.png|333"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231002104340.png" alt="image.png|666"></p><h2 id="GNeuVox"><a href="#GNeuVox" class="headerlink" title="GNeuVox"></a>GNeuVox</h2><p><a href="https://taoranyi.com/gneuvox/">GNeuVox: Generalizable Neural Voxels for Fast Human Radiance Fields (taoranyi.com)</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4738288024060706817&amp;noteId=1996978666924478208">Generalizable Neural Voxels for Fast Human Radiance Fields (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008173458.png" alt="image.png|666"></p><h2 id="CAR"><a href="#CAR" class="headerlink" title="CAR"></a>CAR</h2><p><a href="https://tingtingliao.github.io/CAR/">CAR (tingtingliao.github.io)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008172759.png" alt="image.png|666"></p><h2 id="HDHumans"><a href="#HDHumans" class="headerlink" title="HDHumans"></a>HDHumans</h2><p><a href="https://dl.acm.org/doi/pdf/10.1145/3606927">HDHumans (acm.org)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008193531.png" alt="image.png|666"></p><h2 id="EVA3D-2022"><a href="#EVA3D-2022" class="headerlink" title="EVA3D 2022"></a>EVA3D 2022</h2><p>Compositional Human body<br>质量很低<br>Idea：</p><ul><li>将人体分为几个部分分别训练</li><li>将 NeRF 融合进 GAN 的生成器中，并与一个判别器进行联合训练</li></ul><p>Cost：</p><ul><li>8 NVIDIA V100 Gpus for 5 days</li></ul><blockquote><p><a href="https://hongfz16.github.io/projects/EVA3D.html">EVA3D - Project Page (hongfz16.github.io)</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4677480793493209089&amp;noteId=1985412009585125888">EVA3D: Compositional 3D Human Generation from 2D Image Collections (readpaper.com)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230930153949.png" alt="image|666"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230930154048.png" alt="image.png|666"></p><h2 id="Dynamic-1"><a href="#Dynamic-1" class="headerlink" title="Dynamic"></a>Dynamic</h2><h3 id="3DGS-Avatar"><a href="#3DGS-Avatar" class="headerlink" title="3DGS-Avatar"></a>3DGS-Avatar</h3><p><a href="https://neuralbodies.github.io/3DGS-Avatar/">3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting (neuralbodies.github.io)</a><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231218201652.png" alt="image.png|666"></p><h3 id="GaussianAvatar"><a href="#GaussianAvatar" class="headerlink" title="GaussianAvatar"></a>GaussianAvatar</h3><p><a href="https://huliangxiao.github.io/GaussianAvatar">Projectpage of GaussianAvatar (huliangxiao.github.io)</a><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231205153138.png" alt="image.png|666"></p><h3 id="Vid2Avatar"><a href="#Vid2Avatar" class="headerlink" title="Vid2Avatar"></a>Vid2Avatar</h3><blockquote><p><a href="Vid2Avatar.md">Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition</a><br><a href="https://moygcc.github.io/vid2avatar/">Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition (moygcc.github.io)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921171140.png" alt="image.png|666"></p><h3 id="Im4D"><a href="#Im4D" class="headerlink" title="Im4D"></a>Im4D</h3><p><a href="https://zju3dv.github.io/im4d/">Im4D (zju3dv.github.io)</a><br>Im4D: High-Fidelity and Real-Time Novel View Synthesis for Dynamic Scenes</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231013171508.png" alt="image.png|666"></p><h3 id="HumanRF"><a href="#HumanRF" class="headerlink" title="HumanRF"></a>HumanRF</h3><blockquote><p><a href="https://synthesiaresearch.github.io/humanrf/">HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion (synthesiaresearch.github.io)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231001165622.png" alt="image.png|666"></p><h3 id="Neural-Body"><a href="#Neural-Body" class="headerlink" title="Neural Body"></a>Neural Body</h3><blockquote><p><a href="https://zju3dv.github.io/neuralbody/">Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans (zju3dv.github.io)</a></p></blockquote><p>首先在SMPL6890个顶点上定义一组潜在代码，然后<br>使用<a href="https://readpaper.com/pdf-annotate/note?pdfId=4498402014756757505&amp;noteId=2065156297063368192">Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies (readpaper.com)</a><br>从多视图图片中获取SMPL参数$S_{t}$</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231001170255.png" alt="image.png|666"></p><h3 id="InstantNVR"><a href="#InstantNVR" class="headerlink" title="InstantNVR"></a>InstantNVR</h3><p><a href="https://zju3dv.github.io/instant_nvr/">Learning Neural Volumetric Representations of Dynamic Humans in Minutes (zju3dv.github.io)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231001172828.png" alt="image.png|666"></p><h3 id="4K4D"><a href="#4K4D" class="headerlink" title="4K4D"></a>4K4D</h3><p><a href="https://zju3dv.github.io/4k4d/">4K4D (zju3dv.github.io)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023154027.png" alt="image.png|666"></p><h3 id="D3GA"><a href="#D3GA" class="headerlink" title="D3GA"></a>D3GA</h3><p><a href="https://zielon.github.io/d3ga/">D3GA - Drivable 3D Gaussian Avatars - Wojciech Zielonka</a></p><p>多视图视频作为输入 + 3DGS + 笼形变形</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231117103528.png" alt="image.png|666"></p><h2 id="Human-Object-Interactions"><a href="#Human-Object-Interactions" class="headerlink" title="Human-Object Interactions"></a>Human-Object Interactions</h2><h3 id="Instant-NVR"><a href="#Instant-NVR" class="headerlink" title="Instant-NVR"></a>Instant-NVR</h3><p><a href="https://nowheretrix.github.io/Instant-NVR/">Instant-NVR: Instant Neural Volumetric Rendering for Human-object Interactions from Monocular RGBD Stream</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008115305.png" alt="image.png|666"></p><h3 id="NeuralDome"><a href="#NeuralDome" class="headerlink" title="NeuralDome"></a>NeuralDome</h3><p><a href="https://juzezhang.github.io/NeuralDome/">NeuralDome (juzezhang.github.io)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008120011.png" alt="image.png|666"></p><h1 id="PIFu-Occupancy-Field"><a href="#PIFu-Occupancy-Field" class="headerlink" title="PIFu Occupancy Field"></a>PIFu Occupancy Field</h1><blockquote><p><a href="PIFu.md">PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization</a><br><a href="https://shunsukesaito.github.io/PIFu/">PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization (shunsukesaito.github.io)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230928170950.png" alt="image.png|666"></p><h2 id="PIFuHD"><a href="#PIFuHD" class="headerlink" title="PIFuHD"></a>PIFuHD</h2><blockquote><p><a href="PIFuHD.md">PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization</a><br><a href="https://shunsukesaito.github.io/PIFuHD/">PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization (shunsukesaito.github.io)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230928175323.png" alt="image.png|666"></p><h2 id="PIFu-for-the-Real-World"><a href="#PIFu-for-the-Real-World" class="headerlink" title="PIFu for the Real World"></a>PIFu for the Real World</h2><p><a href="https://github.com/X-zhangyang/SelfPIFu--PIFu-for-the-Real-World">X-zhangyang/SelfPIFu—PIFu-for-the-Real-World: Dressed Human Reconstrcution from Single-view Real World Image (github.com)</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4660017586591776769&amp;noteId=1996688855483354880">PIFu for the Real World: A Self-supervised Framework to Reconstruct Dressed Human from Single-view Images (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008194000.png" alt="image.png|666"></p><h2 id="DIFu"><a href="#DIFu" class="headerlink" title="DIFu"></a>DIFu</h2><p><a href="https://eadcat.github.io/DIFu/">DIFu: Depth-Guided Implicit Function for Clothed Human Reconstruction (eadcat.github.io)</a><br><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_DIFu_Depth-Guided_Implicit_Function_for_Clothed_Human_Reconstruction_CVPR_2023_paper.pdf">DIFu: Depth-Guided Implicit Function for Clothed Human Reconstruction (thecvf.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008114221.png" alt="image.png|666"></p><h2 id="SeSDF"><a href="#SeSDF" class="headerlink" title="SeSDF"></a>SeSDF</h2><p><a href="https://yukangcao.github.io/SeSDF/">SeSDF: Self-evolved Signed Distance Field for Implicit 3D Clothed Human Reconstruction (yukangcao.github.io)</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4740902287992438785&amp;noteId=1996730143273232896">SeSDF: Self-evolved Signed Distance Field for Implicit 3D Clothed Human Reconstruction (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008113348.png" alt="image.png|666"></p><h2 id="UNIF"><a href="#UNIF" class="headerlink" title="UNIF"></a>UNIF</h2><p><a href="https://shenhanqian.github.io/unif">UNIF: United Neural Implicit Functions for Clothed Human Reconstruction and Animation | Shenhan Qian</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4648065386802069505&amp;noteId=1996740483288731392">UNIF: United Neural Implicit Functions for Clothed Human Reconstruction and Animation (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008163003.png" alt="image.png|666"></p><h2 id="Structured-3D-Features"><a href="#Structured-3D-Features" class="headerlink" title="Structured 3D Features"></a>Structured 3D Features</h2><p>Reconstructing <strong>Relightable</strong> and <strong>Animatable</strong> Avatars<br><a href="https://enriccorona.github.io/s3f/">Enric Corona</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4700589883291336705&amp;noteId=1996756493166029056">Structured 3D Features for Reconstructing Relightable and Animatable Avatars (readpaper.com)</a></p><p>X,3d fea,2d fea —&gt; transformer —&gt; sdf, albedo<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008174219.png" alt="image.png|666"></p><h2 id="GTA"><a href="#GTA" class="headerlink" title="GTA"></a>GTA</h2><p><a href="https://river-zhang.github.io/GTA-projectpage/">Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction (river-zhang.github.io)</a><br><a href="https://readpaper.com/pdf-annotate/note?pdfId=4804636732393783297&amp;noteId=2021327250504312576">Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction (readpaper.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231016094412.png" alt="image.png|666"></p><h2 id="Get3DHuman"><a href="#Get3DHuman" class="headerlink" title="Get3DHuman"></a>Get3DHuman</h2><p><a href="https://x-zhangyang.github.io/2023_Get3DHuman/">Get3DHuman: Lifting StyleGAN-Human into a 3D Generative Model using Pixel-aligned Reconstruction Priors. (x-zhangyang.github.io)</a></p><p>GAN + PIFus<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231023160121.png" alt="image.png|666"></p><h2 id="DRIFu"><a href="#DRIFu" class="headerlink" title="DRIFu"></a>DRIFu</h2><p><a href="https://github.com/kuangzijian/drifu-for-animals">kuangzijian/drifu-for-animals: meta-learning based pifu model for animals (github.com)</a></p><p>鸟类PIFu<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231123151648.png" alt="image.png|666"><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231123151707.png" alt="image.png|666"></p><h3 id="SIFU"><a href="#SIFU" class="headerlink" title="SIFU"></a>SIFU</h3><p><a href="https://river-zhang.github.io/SIFU-projectpage/">SIFU Project Page (river-zhang.github.io)</a><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231218204004.png" alt="image.png|666"></p><h1 id="Depth-amp-Normal-Estimation"><a href="#Depth-amp-Normal-Estimation" class="headerlink" title="Depth&amp;Normal Estimation"></a>Depth&amp;Normal Estimation</h1><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230930182135.png" alt="image.png|444"></p><h2 id="ICON"><a href="#ICON" class="headerlink" title="ICON"></a>ICON</h2><blockquote><p><a href="ICON.md">ICON: Implicit Clothed humans Obtained from Normals</a><br><a href="https://icon.is.tue.mpg.de/">ICON (mpg.de)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230930162915.png" alt="image.png|666"></p><h2 id="ECON"><a href="#ECON" class="headerlink" title="ECON"></a>ECON</h2><blockquote><p><a href="ECON.md">ECON: Explicit Clothed humans Obtained from Normals</a><br><a href="https://xiuyuliang.cn/econ/">ECON: Explicit Clothed humans Optimized via Normal integration (xiuyuliang.cn)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230930173026.png" alt="image.png|666"></p><h2 id="2K2K"><a href="#2K2K" class="headerlink" title="2K2K"></a>2K2K</h2><p>DepthEstimation</p><blockquote><p><a href="2K2K.md">2K2K：High-fidelity 3D Human Digitization from Single 2K Resolution Images</a><br><a href="https://sanghunhan92.github.io/conference/2K2K/">High-fidelity 3D Human Digitization from Single 2K Resolution Images Project Page (sanghunhan92.github.io)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921160120.png" alt="image.png|666"></p><h2 id="MVSNet"><a href="#MVSNet" class="headerlink" title="MVSNet"></a>MVSNet</h2><p>DepthEstimation</p><blockquote><p><a href="MVSNet.md">MVSNet: Depth Inference for Unstructured Multi-view Stereo</a><br><a href="https://github.com/YoYo000/MVSNet">YoYo000/MVSNet: MVSNet (ECCV2018) &amp; R-MVSNet (CVPR2019) (github.com)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231002110228.png" alt="image.png|666"></p><h2 id="GC-MVSNet"><a href="#GC-MVSNet" class="headerlink" title="GC-MVSNet"></a>GC-MVSNet</h2><p>多尺度+多视图几何一致性<br><a href="https://arxiv.org/abs/2310.19583">GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo (arxiv.org)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231031172920.png" alt="image.png|666"></p><h2 id="MonoDiffusion"><a href="#MonoDiffusion" class="headerlink" title="MonoDiffusion"></a>MonoDiffusion</h2><p><a href="https://arxiv.org/abs/2311.07198">MonoDiffusion: Self-Supervised Monocular Depth Estimation Using Diffusion Model</a></p><p>用 Diffusion Model 进行深度估计(自动驾驶)</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231116153515.png" alt="image.png|666"></p><h2 id="NDDepth"><a href="#NDDepth" class="headerlink" title="NDDepth"></a>NDDepth</h2><p><a href="https://arxiv.org/abs/2311.07166">NDDepth: Normal-Distance Assisted Monocular Depth Estimation and Completion</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231116153659.png" alt="image.png|666"></p><h2 id="OccNeRF"><a href="#OccNeRF" class="headerlink" title="OccNeRF"></a>OccNeRF</h2><p><a href="https://github.com/LinShan-Bin/OccNeRF">LinShan-Bin/OccNeRF: Code of “OccNeRF: Self-Supervised Multi-Camera Occupancy Prediction with Neural Radiance Fields”. (github.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231218200234.png" alt="image.png|666"></p><h1 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h1><h2 id="Explicit-Template-Decomposition"><a href="#Explicit-Template-Decomposition" class="headerlink" title="Explicit Template Decomposition"></a>Explicit Template Decomposition</h2><h3 id="TeCH"><a href="#TeCH" class="headerlink" title="TeCH"></a>TeCH</h3><p><a href="https://huangyangyi.github.io/TeCH/">TeCH: Text-guided Reconstruction of Lifelike Clothed Humans (huangyangyi.github.io)</a></p><p>DMTet 表示：consists of an explicit body shape grid and an implicit distance field<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231102112309.png" alt="image.png|666"></p><h3 id="CloSET"><a href="#CloSET" class="headerlink" title="CloSET"></a>CloSET</h3><p><a href="https://www.liuyebin.com/closet/">CloSET CVPR 2023 (liuyebin.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008110803.png" alt="image.png|666"></p><h3 id="Chupa"><a href="#Chupa" class="headerlink" title="Chupa"></a>Chupa</h3><p><a href="https://snuvclab.github.io/chupa/">Chupa (snuvclab.github.io)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008164813.png" alt="image.png|666"></p><h2 id="Human-Face"><a href="#Human-Face" class="headerlink" title="Human Face"></a>Human Face</h2><h3 id="HeadRecon"><a href="#HeadRecon" class="headerlink" title="HeadRecon"></a>HeadRecon</h3><p><a href="https://arxiv.org/abs/2312.08863">[2312.08863] HeadRecon: High-Fidelity 3D Head Reconstruction from Monocular Video (arxiv.org)</a><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231218201501.png" alt="image.png|666"></p><h3 id="GaussianHead"><a href="#GaussianHead" class="headerlink" title="GaussianHead"></a>GaussianHead</h3><p><a href="https://arxiv.org/abs/2312.01632">[2312.01632] GaussianHead: Impressive 3D Gaussian-based Head Avatars with Dynamic Hybrid Neural Field (arxiv.org)</a><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231205153845.png" alt="image.png|666"></p><h3 id="GaussianAvatars"><a href="#GaussianAvatars" class="headerlink" title="GaussianAvatars"></a>GaussianAvatars</h3><p><a href="https://shenhanqian.github.io/gaussian-avatars">GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians | Shenhan Qian</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/picturesmethod.jpg" alt="method.jpg|666"></p><h3 id="TRAvatar"><a href="#TRAvatar" class="headerlink" title="TRAvatar"></a>TRAvatar</h3><p><a href="https://travatar-paper.github.io/">Towards Practical Capture of High-Fidelity Relightable Avatars (travatar-paper.github.io)</a></p><p>动态人脸<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231121121931.png" alt="image.png|666"></p><h3 id="FLARE"><a href="#FLARE" class="headerlink" title="FLARE"></a>FLARE</h3><p><a href="https://flare.is.tue.mpg.de/">FLARE (mpg.de)</a></p><p>FLARE: Fast Learning of Animatable and Relightable Mesh Avatars</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231114093649.png" alt="image.png|666"></p><h3 id="HRN"><a href="#HRN" class="headerlink" title="HRN"></a>HRN</h3><blockquote><p><a href="HRN.md">A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images</a><br><a href="https://younglbw.github.io/HRN-homepage/">HRN (younglbw.github.io)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921173632.png" alt="image.png|666"></p><h3 id="单目-3D-人脸重建"><a href="#单目-3D-人脸重建" class="headerlink" title="单目 3D 人脸重建"></a>单目 3D 人脸重建</h3><p><a href="https://arxiv.org/abs/2310.19580">A Perceptual Shape Loss for Monocular 3D Face Reconstruction</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231031181210.png" alt="image.png|666"></p><h3 id="BakedAvatar"><a href="#BakedAvatar" class="headerlink" title="BakedAvatar"></a>BakedAvatar</h3><p><a href="https://arxiv.org/pdf/2311.05521.pdf">BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis (arxiv.org)</a></p><p>头部实时新视图生成<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231110155612.png" alt="image.png|666"></p><h3 id="Video"><a href="#Video" class="headerlink" title="Video"></a>Video</h3><ul><li><strong>3D-Aware Talking-Head Video Motion Transfer</strong> <a href="https://arxiv.org/abs/2311.02549">https://arxiv.org/abs/2311.02549</a></li><li><a href="https://yudeng.github.io/Portrait4D/">Portrait4D: Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data (yudeng.github.io)</a></li><li><a href="https://tobias-kirschstein.github.io/diffusion-avatars/">DiffusionAvatars: Deferred Diffusion for High-fidelity 3D Head Avatars (tobias-kirschstein.github.io)</a></li><li><a href="https://ustc3dv.github.io/CosAvatar/">CosAvatar (ustc3dv.github.io)</a></li></ul><h2 id="Segmented-Instance-Object"><a href="#Segmented-Instance-Object" class="headerlink" title="Segmented Instance/Object"></a>Segmented Instance/Object</h2><h3 id="Registered-and-Segmented-Deformable-Object-Reconstruction-from-a-Single-View-Point-Cloud"><a href="#Registered-and-Segmented-Deformable-Object-Reconstruction-from-a-Single-View-Point-Cloud" class="headerlink" title="Registered and Segmented Deformable Object Reconstruction from a Single View Point Cloud"></a>Registered and Segmented Deformable Object Reconstruction from a Single View Point Cloud</h3><p><a href="https://arxiv.org/abs/2311.07357">Registered and Segmented Deformable Object Reconstruction from a Single View Point Cloud</a></p><p>配准 + 分割物体重建<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231116153203.png" alt="image.png|666"></p><h3 id="3DFusion-A-real-time-3D-object-reconstruction-pipeline-based-on-streamed-instance-segmented-data"><a href="#3DFusion-A-real-time-3D-object-reconstruction-pipeline-based-on-streamed-instance-segmented-data" class="headerlink" title="3DFusion, A real-time 3D object reconstruction pipeline based on streamed instance segmented data"></a>3DFusion, A real-time 3D object reconstruction pipeline based on streamed instance segmented data</h3><p><a href="https://arxiv.org/abs/2311.06659">3DFusion, A real-time 3D object reconstruction pipeline based on streamed instance segmented data</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231116153353.png" alt="image.png|555"></p><h2 id="Human-Body-Shape-Completion"><a href="#Human-Body-Shape-Completion" class="headerlink" title="Human Body Shape Completion"></a>Human Body Shape Completion</h2><p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Human_Body_Shape_Completion_With_Implicit_Shape_and_Flow_Learning_CVPR_2023_paper.pdf">Human Body Shape Completion With Implicit Shape and Flow Learning (thecvf.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008160354.png" alt="image.png|666"></p><h2 id="Incomplete-Image"><a href="#Incomplete-Image" class="headerlink" title="Incomplete Image"></a>Incomplete Image</h2><p>Complete 3D Human Reconstruction from a Single Incomplete Image</p><p><a href="https://junyingw.github.io/paper/3d_inpainting/">Complete 3D Human Reconstruction from a Single Incomplete Image (junyingw.github.io)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008114841.png" alt="image.png|666"></p><h2 id="New-NetWork-FeatER"><a href="#New-NetWork-FeatER" class="headerlink" title="New NetWork FeatER"></a>New NetWork FeatER</h2><p><a href="https://zczcwh.github.io/feater_page/">FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER (zczcwh.github.io)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231008160659.png" alt="image.png|666"></p><h2 id="HF-Avatar"><a href="#HF-Avatar" class="headerlink" title="HF-Avatar"></a>HF-Avatar</h2><p><a href="https://github.com/hzhao1997/HF-Avatar?tab=readme-ov-file">hzhao1997/HF-Avatar (github.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231017182026.png" alt="image.png|666"></p><h2 id="多模态数字人生成-数字人视频"><a href="#多模态数字人生成-数字人视频" class="headerlink" title="多模态数字人生成(数字人视频)"></a>多模态数字人生成(数字人视频)</h2><p><a href="https://arxiv.org/pdf/2310.20251.pdf">An Implementation of Multimodal Fusion System for Intelligent Digital Human Generation</a></p><p>输入：文本、音频、图片<br>输出：自定义人物视频(图片/+修改/+风格化)+音频(文本合成+音频音色参考)</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231101153147.png" alt="image.png|666"></p><h3 id="IPVNet"><a href="#IPVNet" class="headerlink" title="IPVNet"></a>IPVNet</h3><p><a href="https://github.com/robotic-vision-lab/Implicit-Point-Voxel-Features-Network">robotic-vision-lab/Implicit-Point-Voxel-Features-Network: Implicit deep neural network for 3D surface reconstruction. (github.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231108222654.png" alt="image.png|666"></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> NeRF </tag>
            
            <tag> MVS </tag>
            
            <tag> PIFu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Review of Deep Learning-Powered Mesh Reconstruction Methods</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Review/A%20Review%20of%20Deep%20Learning-Powered%20Mesh%20Reconstruction%20Methods/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Review/A%20Review%20of%20Deep%20Learning-Powered%20Mesh%20Reconstruction%20Methods/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>A Review of Deep Learning-Powered Mesh Reconstruction Methods</th></tr></thead><tbody><tr><td>Author</td><td>Zhiqin Chen</td></tr><tr><td>Conf/Jour</td><td></td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4730673785804439553&amp;noteId=1969284111421948928">A Review of Deep Learning-Powered Mesh Reconstruction Methods (readpaper.com)</a></td></tr></tbody></table></div><p>介绍了几种3D模型表示方法+回顾了将DL应用到Mesh重建中的方法</p><span id="more"></span><h1 id="Conclusion-and-Future-work"><a href="#Conclusion-and-Future-work" class="headerlink" title="Conclusion and Future work"></a>Conclusion and Future work</h1><p>在这项调查中，我们回顾了不同的形状表示，以及各种深度学习3D重建方法，这些方法可以从体素、点云、单图像和多视图图像中重建表面。<br>虽然这项调查旨在涵盖可以产生显式网格的方法，但被引用的作品中有一半使用隐式表示，除了与形状解析相关的任务外，<strong>最先进的总是那些采用隐式表示的作品。因此，研究学习显式表征的基本问题，以及/或弥合隐式表征和显式表征之间的差距是值得的</strong>。<br>大型图像和语言模型表明，只要有更多的训练数据，就可以实现泛化。但在三维领域，训练数据确实非常有限。例如，大多数3d监督方法使用ShapeNet [17]， ShapeNet中总共有6778把椅子。怎么能指望一个仅仅在6778把椅子上训练过的方法推广到由数百万种不同结构、风格和纹理组成的现实生活中的椅子呢?  因此，ahead 有多个新的方向，例如，有效的数据增强，使用真实的2D图像来促进3D学习，合成数据集的预训练，合成到真实的域适应，半自动3D数据收集和标记，当然，需要有人来做创建大规模3D数据集的脏活。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>随着最近硬件和渲染技术的进步，3D模型在我们的生活中无处不在。然而，创建3D形状是艰巨的，需要大量的专业知识。同时，深度学习可以从各种来源实现高质量的3D形状重建，使其成为一种以最小effort获取3D形状的可行方法。重要的是，为了在常见的3D应用中使用，重建的形状需要表示为多边形网格，由于网格镶嵌的不规则性，这对神经网络来说是一个挑战。在本调查中，我们提供了由机器学习驱动的网格重建方法的全面审查。我们首先描述了深度学习环境中3D形状的各种表示。然后回顾了体素、点云、单图像和多视图图像的三维网格重建方法的发展。最后，我们确定了该领域的几个挑战，并提出了潜在的未来方向。</p><p>KW：3D shape, mesh, and representation; reconstruction from voxels, point clouds, and images; machine learning</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>随着近年来硬件和渲染技术的进步，3D模型在我们的生活中无处不在:电影、视频游戏、AR/VR、自动驾驶，甚至在网站上[1]。与人们可以用相机或智能手机轻松创建的图像和视频相比，建模3D内容需要专业知识。就像编程一样，只有经过繁琐的培训和实践的人才能够在3D建模软件中创建3D模型和场景。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920132726.png" alt="image.png"></p><p>尽管如此，以更简单的方式创建3D模型是可能的。理想情况下，用户可以提供一些易于获取的数据作为输入，并依靠复杂的三维重建算法获得三维模型</p><p>因此，拥有强大的工具来执行从这些输入的三维形状重建是非常重要的。它们在人们的日常生活中非常有用，并使娱乐业受益。在相关领域已经投入了大量的研究和努力。许多经典的重建方法，如用于等面重建的Marching Cubes[96]，用于点云重建的screening Poisson[74]，以及用于多视图重建的COLMAP[129,130]，都是非常鲁棒的算法，直到今天仍在使用。然而，正是深度学习革命为人们带来了工具，使他们能够完成许多以前无法完成的任务。<strong>基于深度学习的方法不仅放松了输入约束，允许我们从稀疏或有噪声的数据中重建形状，而且将重建质量提高到一个全新的水平</strong>。</p><p>尽管如此，许多挑战仍然存在，大多数方法还远远不能应用于实际产品。一个很大的挑战是，几乎所有的软件和硬件都只能支持三角形网格，这应该是这些重建方法的理想输出。<strong>然而，三角形镶嵌的非均匀性和不规则性自然不支持传统的卷积操作，因此新的网络架构和新的表示已经被设计出来，以与神经网络兼容的方式表示3D形状</strong>。</p><p>这项调查提供了一个全面的审查这些网格重建方法由机器学习。在第2节中，我们描述了深度学习环境中3D形状的各种表示。然后在第3、4、5和6节中，我们分别回顾了从体素、点云、单图像和多视图图像重建表面的3D重建方法。最后，在第7节中，我们确定了该领域的几个挑战，并提出了潜在的未来方向。</p><h1 id="Representations"><a href="#Representations" class="headerlink" title="Representations"></a>Representations</h1><p>任何算法的基础都是数据表示。对于图像，像素是学术界和工业界都使用的表示。不幸的是，3D模型没有这样的统一表示。事实上，研究人员已经为3D生成任务提出了广泛的表示。<br>在这个调查中，我们关注的是本质上是三角形网格的表示。我们还考虑了可以很容易地转换为三角形网格的表示，例如CSG(Constructive Solid Geometry)树和参数曲面。我们还将讨论隐式表示，如体素网格和神经隐式，因为它们是最流行的表示，尽管它们在转换为三角形网格时可能面临问题，例如创建过多的顶点和三角形。</p><p>CSG模型 ：<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231128153207.png" alt="image.png|666"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920133657.png" alt="image.png"></p><p>表2.1列出了深度学习网格重建方法中使用的表示的总结。请注意，虽然没有考虑输出点云的方法，但其中一些方法使用点来表示隐式字段，因此可以很容易地提取3D形状。例如，“Shape As Points”[117]提出了一个点到网格层，使用泊松曲面重构的可微公式[73,74]，将形状表示为一组具有法线的点的隐式场。另一方面，“Analytic Marching”[81]等作品可以从神经隐式表示中提取精确的多边形网格，未来神经隐式和显式网格之间的差距可能会缩小。</p><p><strong>Deform one template</strong><br>变形单一模板网格生成不同姿态的形状<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920134633.png" alt="image.png"></p><p><strong>Retrieve and deform template</strong><br>首先为目标对象检索最合适的模板，然后对模板进行变形，以获得最佳性能<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920134714.png" alt="image.png"></p><p><strong>Deform one primitive</strong><br>对于一般的形状重建任务，特别是在只有二维图像监督的情况下重建三维形状时，可以使用<strong>原始initial形状</strong>作为初始形状，并对其进行变形以接近目标形状<br><strong>最常用的primitive是一个简单的球体</strong><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920135135.png" alt="image.png"><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920135203.png" alt="image.png"></p><p><strong>Deform multiple primitives</strong><br>单一原始形状的变形限制了重构形状的拓扑结构和表示能力，因此自然的解决方案是拟合多个原始形状。然而，这种表现方式主要用于直接的3D监控;从二维图像监督重建三维形状可能过于复杂和不可控<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920140614.png" alt="image.png"></p><p><strong>Set of primitives</strong><br>关键的区别在于，在“变形多个基元”中，基元网格通过神经网络进行变形。在“基元集合”中，每个基元只是由一组参数定义的基元，因此不存在变形网络</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920143643.png" alt="image.png"></p><p><strong>Primitive detection</strong><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920144012.png" alt="image.png"></p><p><strong>Grid mesh</strong> 点云MC到表面mesh<br>受行军立方体(Marching Cubes, MC)[96]、双重轮廓(Dual contoring, DC)[66]和行军四面体(Marching Tetrahedra, MT)[35]等在规则网格结构上运行的经典等曲面算法的启发，已经提出了几种方法来生成规则的参数网格，以便可以一个单元一个单元地提取表面<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920144055.png" alt="image.png"></p><p><strong>Grid polygon soup</strong><br>八叉树状网络结构<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920144236.png" alt="image.png"></p><p><strong>Grid voxels</strong><br>voxels ：Occupancy or signed distance grids<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920144420.png" alt="image.png"></p><p><strong>Neural implicit</strong><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920144750.png" alt="image.png"></p><p><strong>Primitive CSG</strong><br>构造立体几何CSG(Constructive Solid Geometry)，它是CAD(计算机辅助设计)模型的常见表示，其中原始形状(如多面体、椭球体和圆柱体)通过布尔运算符(如并、交和差)合并<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920144930.png" alt="image.png"></p><p><strong>Sketch and extrude</strong><br>“素描和挤压”也是CSG的一种表现形式。然而，与“原始CSG”不同的是，在“原始CSG”中，CSG操作只使用原始3D形状，这种表示方式更类似于创建CAD模型的工作流程，通过重复绘制2D轮廓，然后将该轮廓挤压到3D体中</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920145435.png" alt="image.png"></p><p><strong>Connect given vertices</strong></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920145520.png" alt="image.png"></p><p>从点云重建网格，因为它只连接点云中的给定顶点。这些方法可以通过是否推断出形状的内外区域来进行分类，从而生成封闭的网格</p><p><strong>Generate and connect vertices</strong><br>该表示首先通过神经网络生成一组网格顶点，然后有选择地将这些顶点连接起来，形成另一个神经网络的网格面。该方法可以直接生成三维网格作为索引面集，<strong>但由于其极高的复杂性，很少被使用</strong>。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920145630.png" alt="image.png"></p><p><strong>Sequence of edits</strong></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920145912.png" alt="image.png"></p><h1 id="Reconstruction-from-Voxels"><a href="#Reconstruction-from-Voxels" class="headerlink" title="Reconstruction from Voxels"></a>Reconstruction from Voxels</h1><p>在本节中，我们回顾了从占位网格或符号距离重建形状的作品。基于动机，我们将作品集合分为两类:形状超分辨率和形状解析，其中</p><ul><li><strong>形状超分辨率从输入体素中重建更详细和视觉上令人愉悦的形状</strong></li><li><strong>形状解析将输入体素分解为原语和CSG序列，用于逆向工程CAD形状</strong>。</li></ul><p>请注意，从部分体素重建完整形状，即形状补全，也是一个活跃的研究领域，由于本调查的范围，我们将不讨论。</p><h2 id="Shape-super-resolution"><a href="#Shape-super-resolution" class="headerlink" title="Shape super-resolution"></a>Shape super-resolution</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920150106.png" alt="image.png"></p><h2 id="Shape-parsing"><a href="#Shape-parsing" class="headerlink" title="Shape parsing"></a>Shape parsing</h2><h1 id="Reconstruction-from-Point-Clouds"><a href="#Reconstruction-from-Point-Clouds" class="headerlink" title="Reconstruction from Point Clouds"></a>Reconstruction from Point Clouds</h1><p>在本节中，我们回顾了从点云重建物体和场景的作品，有或没有点法线。我们将这些方法分为两类:一类基于显式表示，另一类基于隐式表示。</p><ul><li>具有显式表示的方法可以直接输出网格，但通常不能保证表面质量，例如，它们可能不是水密的，可能包含无流形和自相交。</li><li>采用隐式表示的方法可以保证生成无自交的水密流形网格，但它们需要一种等曲面算法来从隐式域中提取网格。</li></ul><h2 id="Explicit-representation"><a href="#Explicit-representation" class="headerlink" title="Explicit representation"></a>Explicit representation</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920150229.png" alt="image.png"></p><h2 id="Implicit-representation"><a href="#Implicit-representation" class="headerlink" title="Implicit representation"></a>Implicit representation</h2><ul><li>Overfit a single shape</li><li>Divide space into local cube patches</li><li>3D CNN then local neural implicit</li><li>Point cloud encoder then local neural implicit</li><li>Implicit field defined by points</li><li>Octrees</li></ul><h1 id="Reconstruction-from-Single-Images"><a href="#Reconstruction-from-Single-Images" class="headerlink" title="Reconstruction from Single Images"></a>Reconstruction from Single Images</h1><p>从单个图像重构形状的方法可以根据它们在训练过程中接受的监督分为两类。</p><ul><li>一类是用地面真实三维形状作为监督来训练的。这类方法通常在ShapeNet上进行训练[17]。</li><li>另一类是只训练单视图图像作为监督。单视图图像意味着每个对象只有一个图像用于训练，而多视图图像则是每个对象有来自不同视点的多个图像。这类方法通常在具有球形或圆盘拓扑形状的鸟、车、马和脸的图像数据集上进行训练。</li></ul><h2 id="With-3D-supervision"><a href="#With-3D-supervision" class="headerlink" title="With 3D supervision"></a>With 3D supervision</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920150654.png" alt="image.png"></p><h2 id="With-2D-supervision"><a href="#With-2D-supervision" class="headerlink" title="With 2D supervision"></a>With 2D supervision</h2><h1 id="Reconstruction-from-Multi-View-Images"><a href="#Reconstruction-from-Multi-View-Images" class="headerlink" title="Reconstruction from Multi-View Images"></a>Reconstruction from Multi-View Images</h1><p>大多数方法是使用基于网格或神经隐式的可微渲染算法，或基于NeRF的射线推进体渲染公式的方法，对多个输入图像过度拟合单个形状或场景[101]。</p><h2 id="Differentiable-rendering-on-explicit-representation"><a href="#Differentiable-rendering-on-explicit-representation" class="headerlink" title="Differentiable rendering on explicit representation"></a>Differentiable rendering on explicit representation</h2><h2 id="Surface-rendering-on-implicit-representation"><a href="#Surface-rendering-on-implicit-representation" class="headerlink" title="Surface rendering on implicit representation"></a>Surface rendering on implicit representation</h2><p>本节中的方法都有一个可微分的渲染公式，假设每个输入图像都给出了对象分割掩码，并且每个光线最多与表面相交一次(每条光线只有一个交点用于梯度传播)。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230920150758.png" alt="image.png"></p><h2 id="Volume-rendering-on-implicit-representation"><a href="#Volume-rendering-on-implicit-representation" class="headerlink" title="Volume rendering on implicit representation"></a>Volume rendering on implicit representation</h2><p>本节的方法采用nerf风格的射线推进体绘制算法。对于每一个像素，相机都会发射一束穿过它的光线。沿着射线对许多点进行采样。每个采样点携带密度(“不透明度”)和亮度(视相关的RGB颜色)，由MLP预测。最后的像素颜色是所有采样点相对于其密度的累积亮度，类似于alpha合成。这些方法通常不需要目标分割蒙版，它们以某种方式用定义良好的神经隐式场表示点密度，从而可以通过等曲面提取形状的表面。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Review </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Review </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CVPR 2023</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Review/2023%20Conf%20about%20NeRF/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Review/2023%20Conf%20about%20NeRF/</url>
      
        <content type="html"><![CDATA[<p>ref:<br><a href="https://github.com/lif314/awesome-NeRF-papers/blob/main/NeRFs-CVPR2023.md">awesome-NeRF-papers/NeRFs-CVPR2023.md at main · lif314/awesome-NeRF-papers (github.com)</a><br><a href="https://github.com/lif314/awesome-NeRF-papers/blob/main/NeRFs-ICCV2023.md">awesome-NeRF-papers/NeRFs-ICCV2023.md at main · lif314/awesome-NeRF-papers (github.com)</a><br><a href="https://public.tableau.com/views/CVPR2023SubjectAreasbyTeamSize/Dashboard2a?:showVizHome=no">工作簿: CVPR 2023 Subject Areas by Team Size (tableau.com)</a><br><a href="https://github.com/yangjiheng/nerf_and_beyond_docs">yangjiheng/nerf_and_beyond_docs (github.com)</a></p><span id="more"></span><p>反射 | 低光 | 阴影<br>稀疏视图<br>编码方式<br>去模糊anti-alias<br><strong>无位姿情况</strong><br>快速渲染<br>节省内存</p><p>应用：</p><ul><li>物体探测</li><li><strong>分割</strong></li><li>RGBD实时跟踪</li><li>机器人感知</li><li>Human：人体，人脸，人手</li><li>其他</li></ul><p>扩散模型<br>3D风格迁移<br>text to 3D<br>可编辑<br>动态场景</p><p><a href="https://zju3dv.github.io/autorecon/">AutoRecon: Automated 3D Object Discovery and Reconstruction (zju3dv.github.io)</a></p><h1 id="反射-低光-阴影"><a href="#反射-低光-阴影" class="headerlink" title="反射 | 低光 | 阴影"></a>反射 | 低光 | 阴影</h1><h2 id="光照反射"><a href="#光照反射" class="headerlink" title="光照反射"></a>光照反射</h2><p><a href="https://github.com/hirotong/ReNeuS">https://github.com/hirotong/ReNeuS</a>3D重建,<br>NeFII: Inverse Rendering for Reflectance Decomposition with Near-Field Indirect Illumination光照反射渲染<br><a href="https://ktiwary2.github.io/objectsascam/">https://ktiwary2.github.io/objectsascam/</a>辐射场相机,物体反射<br><a href="https://zx-yin.github.io/msnerf/">MS-NeRF (zx-yin.github.io)</a>穿过镜像物体的复杂光路，高质量场景渲染<br><a href="https://github.com/JiaxiongQ/NeuS-HSR">JiaxiongQ/NeuS-HSR: Looking Through the Glass: Neural Surface Reconstruction Against High Specular Reflections (CVPR 2023) (github.com)</a> 高镜面反射</p><h2 id="阴影射线监督"><a href="#阴影射线监督" class="headerlink" title="阴影射线监督"></a>阴影射线监督</h2><p><a href="https://gerwang.github.io/shadowneus/">https://gerwang.github.io/shadowneus/</a></p><h2 id="Low-Light低光场景"><a href="#Low-Light低光场景" class="headerlink" title="Low Light低光场景"></a>Low Light低光场景</h2><p><a href="https://www.whyy.site/paper/llnerf">https://www.whyy.site/paper/llnerf</a></p><h2 id="水下或雾气弥漫场景"><a href="#水下或雾气弥漫场景" class="headerlink" title="水下或雾气弥漫场景"></a>水下或雾气弥漫场景</h2><p><a href="https://sea-thru-nerf.github.io/">SeaThru-NeRF: Neural Radiance Fields in Scattering Media (sea-thru-nerf.github.io)</a></p><h2 id="光流监督"><a href="#光流监督" class="headerlink" title="光流监督"></a>光流监督</h2><p><a href="https://mightychaos.github.io/projects/fsdnerf/">https://mightychaos.github.io/projects/fsdnerf/</a></p><h1 id="稀疏视图"><a href="#稀疏视图" class="headerlink" title="稀疏视图"></a>稀疏视图</h1><p><a href="https://scade-spacecarving-nerfs.github.io/">https://scade-spacecarving-nerfs.github.io/</a>(深度监督)<br><a href="https://github.com/google-research/nerf-from-image">https://github.com/google-research/nerf-from-image</a>(单视图)<br><a href="https://flex-nerf.github.io/">https://flex-nerf.github.io/</a>(人体建模)<br><a href="http://prunetruong.com/sparf.github.io/">http://prunetruong.com/sparf.github.io/</a>(位姿不准)<br><a href="https://jiawei-yang.github.io/FreeNeRF/">https://jiawei-yang.github.io/FreeNeRF/</a>稀疏视图<br><a href="https://github.com/ShuhongChen/panic3d-anime-reconstruction">https://github.com/ShuhongChen/panic3d-anime-reconstruction</a>单视图3D重建<br>MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs稀疏视图,深度监督<br>NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real Image Animation<br><a href="https://sparsefusion.github.io/">https://sparsefusion.github.io/</a>扩散模型<br><a href="https://sparsenerf.github.io/">https://sparsenerf.github.io/</a>depth-based, Few-shot<br><a href="https://fwmb.github.io/bts/">Behind the Scenes: Density Fields for Single View Reconstruction | fwmb.github.io</a><br><a href="https://zero123.cs.columbia.edu/">Zero-1-to-3: Zero-shot One Image to 3D Object (columbia.edu)</a><br><a href="https://arxiv.org/abs/2306.00965">[2306.00965] BUOL: A Bottom-Up Framework with Occupancy-aware Lifting for Panoptic 3D Scene Reconstruction From A Single Image (arxiv.org)</a></p><h1 id="编码方式"><a href="#编码方式" class="headerlink" title="编码方式"></a>编码方式</h1><p><a href="https://github.com/KostadinovShalon/exact-nerf">https://github.com/KostadinovShalon/exact-nerf</a><br><a href="https://3d-front-future.github.io/neuda/">https://3d-front-future.github.io/neuda/</a>保真表面重建</p><h1 id="去模糊"><a href="#去模糊" class="headerlink" title="去模糊"></a>去模糊</h1><p><a href="https://github.com/TangZJ/able-nerf">https://github.com/TangZJ/able-nerf</a>(自注意力)<br><a href="https://dogyoonlee.github.io/dpnerf/">https://dogyoonlee.github.io/dpnerf/</a>(物理场景先验)<br><a href="https://github.com/BoifZ/VDN-NeRF">https://github.com/BoifZ/VDN-NeRF</a><br><a href="https://jonbarron.info/zipnerf/">https://jonbarron.info/zipnerf/</a>Anti-Aliased, Grid-Based<br><a href="https://wbhu.github.io/projects/Tri-MipRF/">https://wbhu.github.io/projects/Tri-MipRF/</a>Anti-Aliasing, Faster<br><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Isaac-Medina_Exact-NeRF_An_Exploration_of_a_Precise_Volumetric_Parameterization_for_Neural_CVPR_2023_paper.html">CVPR 2023 Open Access Repository (thecvf.com)</a></p><h1 id="没有位姿-NeRF-without-pose"><a href="#没有位姿-NeRF-without-pose" class="headerlink" title="没有位姿 NeRF without pose"></a>没有位姿 NeRF without pose</h1><p><a href="https://rust-paper.github.io/">https://rust-paper.github.io/</a><br><a href="https://nope-nerf.active.vision/">https://nope-nerf.active.vision/</a><br><a href="https://henry123-boy.github.io/level-s2fm/">https://henry123-boy.github.io/level-s2fm/</a>增量重建,无位姿SfM</p><p>大规模街景<br><a href="https://city-super.github.io/gridnerf/">https://city-super.github.io/gridnerf/</a><br><a href="https://dnmp.github.io/">https://dnmp.github.io/</a>Urban Reconstruction<br><a href="https://astra-vision.github.io/SceneRF/">https://astra-vision.github.io/SceneRF/</a>Urban Reconstruction</p><p>联合估计位姿，增量重建<br><a href="https://localrf.github.io/">https://localrf.github.io/</a></p><p>Bundle-Adjusting<br><a href="https://rover-xingyu.github.io/L2G-NeRF/">https://rover-xingyu.github.io/L2G-NeRF/</a><br><a href="https://aibluefisher.github.io/dbarf/">https://aibluefisher.github.io/dbarf/</a><br><a href="https://github.com/WU-CVGL/BAD-NeRF">https://github.com/WU-CVGL/BAD-NeRF</a>去模糊</p><p>可泛化<br><a href="https://xhuangcv.github.io/lirf/">https://xhuangcv.github.io/lirf/</a></p><p>点云渲染<br><a href="https://arxiv.org/pdf/2303.16482.pdf">https://arxiv.org/pdf/2303.16482.pdf</a><br><a href="https://jkulhanek.com/tetra-nerf/">https://jkulhanek.com/tetra-nerf/</a>Point-Based, Tetrahedra-Based<a href="https://visual.ee.ucla.edu/alto.htm/">ALTO: Alternating Latent Topologies for Implicit 3D Reconstruction - Visual Machines Group (ucla.edu)</a>交替潜在拓扑，从嘈杂的点云中高保真地重建隐式3D表面</p><p>逆渲染<br><a href="https://haian-jin.github.io/TensoIR/">https://haian-jin.github.io/TensoIR/</a></p><p>逼真合成<br><a href="https://redrock303.github.io/nerflix/">https://redrock303.github.io/nerflix/</a><br>ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-real Novel View Synthesis via Contrastive Learning真实渲染<br><a href="https://robustnerf.github.io/public/">https://robustnerf.github.io/public/</a>真实渲染，场景包含分散物体，<strong>去除floaters</strong><br><a href="https://yifanjiang19.github.io/alignerf">AligNeRF (yifanjiang19.github.io)</a>高保真度、输入高分辨率图像和有校准误差的相机</p><p>3D边缘重建<br><a href="https://yunfan1202.github.io/NEF/">https://yunfan1202.github.io/NEF/</a></p><p>3D扫描中重建2D楼层平面图<br><a href="https://ywyue.github.io/RoomFormer/">Connecting the Dots: Floorplan Reconstruction Using Two-Level Queries (ywyue.github.io)</a></p><p>nerf2mesh, nerf-texture<br><a href="https://me.kiui.moe/nerf2mesh/">https://me.kiui.moe/nerf2mesh/</a></p><p>高质量重建SDF Based Reconstruction / Other Geometry Based Reconstruction<br><a href="https://xmeng525.github.io/xiaoxumeng.github.io/projects/cvpr23_neat">https://xmeng525.github.io/xiaoxumeng.github.io/projects/cvpr23_neat</a><br><a href="https://github.com/yiqun-wang/PET-NeuS">yiqun-wang/PET-NeuS: PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces (CVPR 2023) (github.com)</a><br><a href="https://sarahweiii.github.io/neumanifold/">NeuManifold (sarahweiii.github.io)</a><br><a href="https://vcai.mpi-inf.mpg.de/projects/NeuS2/">NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction (mpg.de)</a></p><p>立体深度<br><a href="https://nerfstereo.github.io/">https://nerfstereo.github.io/</a></p><p>视图合成<br>Enhanced Stable View Synthesis(室外)</p><p>事件相机<br><a href="https://4dqv.mpi-inf.mpg.de/EventNeRF/">https://4dqv.mpi-inf.mpg.de/EventNeRF/</a></p><p>3D GAN,生成辐射场<br><a href="https://www.computationalimaging.org/publications/singraf/">https://www.computationalimaging.org/publications/singraf/</a>SinGRAF：学习单个场景的3D生成辐射场</p><h1 id="快速渲染"><a href="#快速渲染" class="headerlink" title="快速渲染"></a>快速渲染</h1><p><a href="https://mobile-nerf.github.io/">https://mobile-nerf.github.io/</a>(移动设备)<br><a href="https://snap-research.github.io/MobileR2L/">https://snap-research.github.io/MobileR2L/</a><br><a href="https://totoro97.github.io/projects/f2-nerf/">https://totoro97.github.io/projects/f2-nerf/</a>(任意相机路径,快速)<br><a href="https://radualexandru.github.io/permuto_sdf/">https://radualexandru.github.io/permuto_sdf/</a>快速渲染, 30 fps on an RTX 3090<br><a href="https://arxiv.org/pdf/2212.08476.pdf">https://arxiv.org/pdf/2212.08476.pdf</a><br><a href="https://gymat.github.io/SurfelNeRF-web/">SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic Reconstruction of Indoor Scenes (gymat.github.io)</a><br>SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory<br><a href="https://arxiv.org/abs/2305.13220">[2305.13220] Fast Monocular Scene Reconstruction with Global-Sparse Local-Dense Grids (arxiv.org)</a> 快速重建<br><a href="https://jefftan969.github.io/dasr/">Distilling Neural Fields for Real-Time Articulated Shape Reconstruction (jefftan969.github.io)</a>实时从视频中重建关节式三维模型的方法</p><h1 id="节省内存"><a href="#节省内存" class="headerlink" title="节省内存"></a>节省内存</h1><p><a href="https://daniel03c1.github.io/masked_wavelet_nerf/">https://daniel03c1.github.io/masked_wavelet_nerf/</a><br><a href="https://github.com/AlgoHunt/VQRF">https://github.com/AlgoHunt/VQRF</a><br><a href="https://plenvdb.github.io/">https://plenvdb.github.io/</a>快速渲染<br><a href="https://sarafridov.github.io/K-Planes/">https://sarafridov.github.io/K-Planes/</a>快速渲染<br><a href="https://paperswithcode.com/paper/nerflight-fast-and-light-neural-radiance">NeRFLight: Fast and Light Neural Radiance Fields Using a Shared Feature Grid | Papers With Code</a></p><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><h2 id="物体探测"><a href="#物体探测" class="headerlink" title="物体探测"></a>物体探测</h2><p><a href="https://github.com/lyclyc52/NeRF_RPN">https://github.com/lyclyc52/NeRF_RPN</a><br><a href="https://chenfengxu714.github.io/nerfdet/">https://chenfengxu714.github.io/nerfdet/</a>3D Object Detection<br><a href="https://zju3dv.github.io/autorecon/">AutoRecon: Automated 3D Object Discovery and Reconstruction (zju3dv.github.io)</a> 点云中发现物体并重建</p><h2 id="分割"><a href="#分割" class="headerlink" title="分割"></a>分割</h2><p><a href="https://rahul-goel.github.io/isrf/">https://rahul-goel.github.io/isrf/</a>交互式场景<br><a href="https://liuff19.github.io/S-Ray/">https://liuff19.github.io/S-Ray/</a>(可泛化语义分割)<br>Unsupervised Continual Semantic Adaptation through Neural Rendering 语义分割<br><a href="https://github.com/xxm19/jacobinerf">https://github.com/xxm19/jacobinerf</a>语义分割<br><a href="https://nihalsid.github.io/panoptic-lifting/">Panoptic Lifting (nihalsid.github.io)</a>从野外场景的图像中学习全景三维体积表示的新方法<br><a href="https://arxiv.org/abs/2303.05251">[2303.05251] Masked Image Modeling with Local Multi-Scale Reconstruction (arxiv.org)</a>本地多尺度重构</p><h2 id="RGBD实时跟踪与3D重建"><a href="#RGBD实时跟踪与3D重建" class="headerlink" title="RGBD实时跟踪与3D重建"></a>RGBD实时跟踪与3D重建</h2><p><a href="https://bundlesdf.github.io/">https://bundlesdf.github.io/</a><br><a href="https://rllab-snu.github.io/projects/RNR-Map/">https://rllab-snu.github.io/projects/RNR-Map/</a>(视觉导航)</p><h2 id="机器人感知-抓取感知"><a href="#机器人感知-抓取感知" class="headerlink" title="机器人感知,抓取感知"></a>机器人感知,抓取感知</h2><p><a href="https://bland.website/spartn/">https://bland.website/spartn/</a></p><h2 id="场景识别recognition"><a href="#场景识别recognition" class="headerlink" title="场景识别recognition"></a>场景识别recognition</h2><p><a href="https://alexeybokhovkin.github.io/neural-part-priors/">Neural Part Priors: Learning to Optimize Part-Based Object Completion in RGB-D Scans (alexeybokhovkin.github.io)</a>利用大规模合成的3D形状数据集，其中包含部分信息的注释，来学习神经部分先验（NPPs）</p><h2 id="视觉重定位器"><a href="#视觉重定位器" class="headerlink" title="视觉重定位器"></a>视觉重定位器</h2><p><a href="https://nianticlabs.github.io/ace/">ACE: Accelerated Coordinate Encoding (nianticlabs.github.io)</a></p><h2 id="人类"><a href="#人类" class="headerlink" title="人类"></a>人类</h2><h3 id="人体重建"><a href="#人体重建" class="headerlink" title="人体重建"></a>人体重建</h3><p><a href="https://github.com/JanaldoChen/GM-NeRF">https://github.com/JanaldoChen/GM-NeRF</a>(可泛化)<br><a href="https://yzmblog.github.io/projects/MonoHuman/">https://yzmblog.github.io/projects/MonoHuman/</a>(,文本交互)<br>HumanGen: Generating Human Radiance Fields with Explicit Priors<br><a href="https://paperswithcode.com/paper/personnerf-personalized-reconstruction-from">PersonNeRF: Personalized Reconstruction from Photo Collections | Papers With Code</a><br><a href="https://grail.cs.washington.edu/projects/personnerf/">https://grail.cs.washington.edu/projects/personnerf/</a><br><a href="https://zju3dv.github.io/mlp_maps/">https://zju3dv.github.io/mlp_maps/</a>动态人体建模<br><a href="https://skhu101.github.io/SHERF/">https://skhu101.github.io/SHERF/</a></p><h3 id="人脸渲染"><a href="#人脸渲染" class="headerlink" title="人脸渲染"></a>人脸渲染</h3><p><a href="https://kunhao-liu.github.io/StyleRF/">https://kunhao-liu.github.io/StyleRF/</a>(高质量人脸)<br><a href="https://yudeng.github.io/GRAMInverter/">https://yudeng.github.io/GRAMInverter/</a>(单视图人像合成)<br>NeRF-Gaze: A Head-Eye Redirection Parametric Model for Gaze Estimation(视线重定向)<br><a href="https://malteprinzler.github.io/projects/diner/diner.html">https://malteprinzler.github.io/projects/diner/diner.html</a>人脸建模,深度监督</p><h3 id="Gaze-redirection-视线重定向"><a href="#Gaze-redirection-视线重定向" class="headerlink" title="Gaze redirection 视线重定向"></a>Gaze redirection 视线重定向</h3><p><a href="https://github.com/AlessandroRuzzi/GazeNeRF">https://github.com/AlessandroRuzzi/GazeNeRF</a></p><h3 id="手部重建"><a href="#手部重建" class="headerlink" title="手部重建"></a>手部重建</h3><p>HandNeRF: Neural Radiance Fields for Animatable Interacting Hands</p><h2 id="自拍VR"><a href="#自拍VR" class="headerlink" title="自拍VR"></a>自拍VR</h2><p><a href="https://changwoon.info/publications/EgoNeRF">https://changwoon.info/publications/EgoNeRF</a></p><h2 id="电影剪辑"><a href="#电影剪辑" class="headerlink" title="电影剪辑"></a>电影剪辑</h2><p><a href="https://www.lix.polytechnique.fr/vista/projects/2023_cvpr_wang/">https://www.lix.polytechnique.fr/vista/projects/2023_cvpr_wang/</a></p><h2 id="6-DoF-Video"><a href="#6-DoF-Video" class="headerlink" title="6-DoF Video"></a>6-DoF Video</h2><p><a href="https://hyperreel.github.io/">HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling</a></p><h1 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h1><p><a href="https://omniobject3d.github.io/">OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation</a><br><a href="https://gaplab.cuhk.edu.cn/projects/MVImgNet/#table_stat">MVImgNet (cuhk.edu.cn)</a><br><a href="https://eyecan-ai.github.io/rene/">ReNé (eyecan-ai.github.io)</a><br><a href="https://arxiv.org/abs/2303.01932">[2303.01932] MobileBrick: Building LEGO for 3D Reconstruction on Mobile Devices (arxiv.org)</a></p><h1 id="扩散模型"><a href="#扩散模型" class="headerlink" title="扩散模型"></a>扩散模型</h1><p><a href="https://sirwyver.github.io/DiffRF/">https://sirwyver.github.io/DiffRF/</a><br><a href="https://github.com/nianticlabs/diffusionerf">https://github.com/nianticlabs/diffusionerf</a><br>NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors扩散模型,语言引导<br><a href="https://research.nvidia.com/labs/toronto-ai/NFLDM/">https://research.nvidia.com/labs/toronto-ai/NFLDM/</a>场景生成<br><a href="https://poseguided-diffusion.github.io/">https://poseguided-diffusion.github.io/</a>视图合成 NeRF-GAN NeRF-Diffusion<br><a href="https://make-it-3d.github.io/">https://make-it-3d.github.io/</a>3D Creation, NeRF-Diffusion<br><a href="https://samsunglabs.github.io/NeuralHaircut/">https://samsunglabs.github.io/NeuralHaircut/</a>Hair Reconstruction, NeRF-Diffusion</p><h1 id="3D风格迁移"><a href="#3D风格迁移" class="headerlink" title="3D风格迁移"></a>3D风格迁移</h1><p><a href="https://github.com/sen-mao/3di2i-translation">https://github.com/sen-mao/3di2i-translation</a><br><a href="https://kunhao-liu.github.io/StyleRF/">https://kunhao-liu.github.io/StyleRF/</a><br>Transforming Radiance Field with Lipschitz Network for Photorealistic 3D Scene Stylization<br><a href="https://ref-npr.github.io/">https://ref-npr.github.io/</a>(3D场景风格化)</p><h1 id="text-to-3D"><a href="#text-to-3D" class="headerlink" title="text to 3D"></a>text to 3D</h1><p><a href="https://research.nvidia.com/labs/dir/magic3d/">https://research.nvidia.com/labs/dir/magic3d/</a><br><a href="https://github.com/eladrich/latent-nerf">https://github.com/eladrich/latent-nerf</a><br><a href="https://bluestyle97.github.io/dream3d/">https://bluestyle97.github.io/dream3d/</a>CLIP, Text-to-3D,扩散模型,零样本<br><a href="https://dreambooth3d.github.io/">https://dreambooth3d.github.io/</a><br><a href="https://sked-paper.github.io/">https://sked-paper.github.io/</a><br><a href="https://avatar-craft.github.io/">https://avatar-craft.github.io/</a>Text-to-Avatars<br><a href="https://arxiv.org/abs/2305.02541">[2305.02541] Catch Missing Details: Image Reconstruction with Frequency Augmented Variational Autoencoder (arxiv.org)</a></p><h1 id="可编辑"><a href="#可编辑" class="headerlink" title="可编辑"></a>可编辑</h1><p><a href="https://ktertikas.github.io/part_nerf">https://ktertikas.github.io/part_nerf</a>(部分)<br><a href="https://zju3dv.github.io/sine/">https://zju3dv.github.io/sine/</a><br><a href="https://spinnerf3d.github.io/">https://spinnerf3d.github.io/</a>(移除物体)<br><a href="https://jetd1.github.io/nerflets-web/">https://jetd1.github.io/nerflets-web/</a>(高效和结构感知的三维场景表示, 大规模，室内室外，场景编辑，全景分割)<br><a href="https://jingsenzhu.github.io/i2-sdf/">https://jingsenzhu.github.io/i2-sdf/</a>(室内重建,可编辑,重光照)<br><a href="https://github.com/yizhangphd/FreqPCR">https://github.com/yizhangphd/FreqPCR</a>(可编辑,点云渲染,实时)<br><a href="https://chengwei-zheng.github.io/EditableNeRF/">https://chengwei-zheng.github.io/EditableNeRF/</a><br><a href="https://snap-research.github.io/discoscene/">https://snap-research.github.io/discoscene/</a><br><a href="https://nianticlabs.github.io/nerf-object-removal/">https://nianticlabs.github.io/nerf-object-removal/</a><br><a href="https://palettenerf.github.io/">https://palettenerf.github.io/</a>外观编辑<br><a href="https://zju3dv.github.io/intrinsic_nerf/">https://zju3dv.github.io/intrinsic_nerf/</a></p><h1 id="动态场景"><a href="#动态场景" class="headerlink" title="动态场景"></a>动态场景</h1><p><a href="https://caoang327.github.io/HexPlane/">https://caoang327.github.io/HexPlane/</a><br><a href="https://dylin2023.github.io/">https://dylin2023.github.io/</a><br><a href="https://haithemturki.com/suds/">https://haithemturki.com/suds/</a>(城市动态场景)<br><a href="https://github.com/JokerYan/NeRF-DS">https://github.com/JokerYan/NeRF-DS</a>(光照反射)<br><a href="https://robust-dynrf.github.io/">https://robust-dynrf.github.io/</a><br><a href="https://limacv.github.io/VideoLoop3D_web/">https://limacv.github.io/VideoLoop3D_web/</a><br><a href="https://nowheretrix.github.io/Instant-NVR/">https://nowheretrix.github.io/Instant-NVR/</a>人机交互,动态场景<br><a href="https://sungheonpark.github.io/tempinterpnerf/">https://sungheonpark.github.io/tempinterpnerf/</a><br><a href="https://aoliao12138.github.io/ReRF/">https://aoliao12138.github.io/ReRF/</a><br><a href="https://fengres.github.io/mixvoxels/">https://fengres.github.io/mixvoxels/</a><br><a href="https://robust-dynrf.github.io/">RoDynRF: Robust Dynamic Radiance Fields (robust-dynrf.github.io)</a><br><a href="https://sarafridov.github.io/K-Planes/">K-Plane (sarafridov.github.io)</a><br><a href="https://caoang327.github.io/HexPlane/">HexPlane (caoang327.github.io)</a></p><h1 id="多模态"><a href="#多模态" class="headerlink" title="多模态"></a>多模态</h1><p><a href="https://yccyenchicheng.github.io/SDFusion/">SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation (yccyenchicheng.github.io)</a></p><p>混合表达SDF,高效密集SLAM<br><a href="https://www.idiap.ch/paper/eslam/">https://www.idiap.ch/paper/eslam/</a><br><a href="https://hengyiwang.github.io/projects/CoSLAM">https://hengyiwang.github.io/projects/CoSLAM</a>NeRF-based SLAM<br><a href="https://kxhit.github.io/vMAP">https://kxhit.github.io/vMAP</a>NeRF-based SLAM, RGBD<br>Audio Driven<br><a href="https://github.com/Fictionarry/ER-NeRF">https://github.com/Fictionarry/ER-NeRF</a></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Review </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Review </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FlexiCubes</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/FlexiCubes/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/FlexiCubes/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Flexible Isosurface Extraction for Gradient-Based Mesh Optimization</th></tr></thead><tbody><tr><td>Author</td><td>Shen, Tianchang and Munkberg, Jacob and Hasselgren, Jon and Yin, Kangxue and Wang, Zian and Chen, Wenzheng and Gojcic, Zan and Fidler, Sanja and Sharp, Nicholas and Gao, Jun</td></tr><tr><td>Conf/Jour</td><td>ACM Trans. on Graph. (SIGGRAPH 2023)</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://research.nvidia.com/labs/toronto-ai/flexicubes/">Flexible Isosurface Extraction for Gradient-Based Mesh Optimization (FlexiCubes) (nvidia.com)</a></td></tr><tr><td>Paper</td><td><a href="https://nv-tlabs.github.io/flexicubes_website/FlexiCubes_paper.pdf">Flexible Isosurface Extraction for Gradient-Based Mesh Optimization (nv-tlabs.github.io)</a></td></tr></tbody></table></div><p>一种新的Marching Cube的方法<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230917211425.png" alt="image.png"></p><span id="more"></span><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>这项工作考虑了基于梯度的mesh优化，其中我们通过将其表示为标量场的等值面来迭代优化3D表面mesh，这是一种越来越普遍的应用范例，包括摄影测量，生成建模和逆物理。现有的实现采用经典的等值面提取算法，如Marching Cubes或Dual contoring;这些技术旨在从固定的、已知的区域中提取mesh，在优化设置中，它们缺乏自由度来表示高质量的特征保留mesh，或者遭受数值不稳定性的影响。我们介绍FlexiCubes，这是一种专为优化几何、视觉甚至物理目标的未知mesh而设计的等面表示。我们的主要见解是在表示中引入额外的精心选择的参数，这允许对提取的mesh几何形状和连接性进行局部灵活调整。当针对下游任务进行优化时，这些参数通过自动微分与底层标量字段一起更新。<strong>我们基于双行军立方体的提取方案来改进拓扑特性，并提供扩展以可选地生成四面体和层次自适应mesh</strong>。大量的实验验证了FlexiCubes在合成基准测试和实际应用中的应用，表明它在mesh质量和几何保真度方面提供了显着的改进。</p><p>KW：</p><ul><li>Computing methodologies → Mesh geometry models;Shape representations; Reconstruction.</li><li>isosurface extraction, gradient-based mesh optimization, photogrammetry摄影测量, generative models</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>从计算机图形学到机器人技术，表面mesh在表示、传输和生成3D几何图形方面发挥着无处不在的作用。在许多其他优点中，表面mesh提供了任意表面的简明而准确的编码，受益于高效的硬件加速渲染，并支持在物理模拟和几何处理中求解方程</p><p>然而，并不是所有的mesh都是一样的——上面的属性通常只有在高质量的mesh上才能实现。事实上，mesh中有过多的元素，suffer from self-intersections和sliver elements，或poorly捕获底层几何，可能完全不适合下游任务。<strong>因此，生成特定形状的高质量mesh非常重要，但远非微不足道，通常需要大量的手工工作</strong>。</p><p>最近算法内容创建和生成式3D建模工具的爆炸式增长导致对自动mesh生成的需求增加。事实上，制作高质量mesh的任务，传统上是熟练的技术艺术家和建模者的领域，越来越多地通过自动算法管道来解决。这些通常基于可微分mesh生成，即参数化三维表面mesh空间，并通过基于梯度的技术对各种目标进行优化。例如，逆渲染等应用[Hasselgren et al. 2022;Munkberg et al. 2022]，结构优化[Subedi et al. 2020]，生成式3D建模[Gao et al. 2022;Lin等人。2022]都利用了这个基本构建块。在一个完美的世界里，这样的应用程序将简单地对一些mesh表示执行naïve梯度下降来优化他们想要的目标。<strong>然而，从如何优化不同拓扑的mesh的基本问题到现有公式缺乏稳定性和鲁棒性导致不可挽回的低质量mesh输出，许多障碍阻碍了这种工作流程的实现</strong>。在这项工作中，我们提出了一种新的公式，使我们更接近这一目标，显着提高了各种下游任务中可微mesh生成的易用性和质量。</p><p>直接优化mesh的顶点位置很容易成为退化和局部最小值的受害者，除非非常仔细地初始化，重新mesh化和正则化使用[Liu et al. 2019;Nicolet et al. 2021;Wang et al. 2018]。<strong>因此，一个常见的范例是在空间中定义和优化标量场或符号距离函数(SDF)，然后提取一个接近该函数的水平集的三角形mesh</strong>。标量函数表示和mesh提取方式的选择对管道整体优化的性能影响很大。从标量场中提取mesh的一个微妙但重要的挑战是可能生成的mesh空间可能受到限制。正如我们稍后将展示的那样，<strong>用于提取三角形mesh的特定算法的选择直接决定了生成形状的属性</strong>。</p><p>为了解决这些问题，我们确定了mesh生成过程应该提供的两个关键属性，以便对下游任务进行简单、高效和高质量的优化:</p><ul><li>Grad：对于mesh的微分定义良好，并且基于梯度的优化在实践中有效收敛。</li><li>Flexible：mesh顶点可以单独和局部调整，以适应表面特征，并找到具有少量元素的高质量mesh。</li></ul><p>然而，这两个属性本质上是冲突的。<strong>增加的灵活性</strong>提供了更多的能力来表示退化几何和自交，这<strong>阻碍了基于梯度的优化的收敛</strong>。<br>因此，现有的技术[Lorensen and Cline 1987;Remelli et al. 2020;Shen et al. 2021]通常会忽略两个属性中的一个(表1)。</p><ul><li>例如，广泛使用的Marching Cubes过程[Lorensen and Cline 1987]并不灵活，因为顶点总是沿着固定的晶格，因此生成的mesh永远不会与非轴向对齐的尖锐特征对齐(图1)。</li><li>广义的Marching技术会使底层grid变形[Gao et al. 2020;Shen et al. 2021]，但仍然不允许调整单个顶点，导致sliver elements和不完美拟合。另一方面，双轮廓(Dual contourting) [Ju et al. 2002]因其捕捉尖锐特征的能力而广受欢迎，但缺乏grad。使用线性系统定位顶点会导致不稳定和无效的优化。第2节和表1对过去的工作进行了详细分类。</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230915151732.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230915151827.png" alt="image.png"></p><p>在这项工作中，我们提出了一种名为FlexiCubes的新技术，它满足了这两个期望的特性。我们的见解是采用特定的双行进立方体公式(Dual Marching Cubes)，并引入额外的自由度，以灵活地定位每个提取的顶点在其双单元内。我们仔细地约束了公式，使其在绝大多数情况下仍然产生无相交的流形和水密mesh，从而实现相对于底层mesh的良好微分(Grad.)。</p><p>该公式最重要的特性是基于梯度的mesh优化在实践中始终成功。为了评估这种固有的经验问题，我们将本工作的重要部分用于FlexiCubes在几个下游任务上的广泛评估。具体来说，我们证明了我们的配方为各种mesh生成应用提供了显着的好处，包括反向渲染，优化物理和几何能量，以及生成3D建模。所得的mesh在低元素计数下简洁地捕获所需的几何形状，并易于通过梯度下降进行优化。此外，我们还提出了FlexiCubes的扩展，如通过分层细化自适应调整mesh分辨率，并自动对域内部进行四面体化。与过去的方法相比，基准测试和实验显示了该技术的价值，我们相信它将成为许多应用领域中高质量mesh生成的有价值的工具。 </p><h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h2><h3 id="Isosurface-Extraction"><a href="#Isosurface-Extraction" class="headerlink" title="Isosurface Extraction"></a>Isosurface Extraction</h3><p>传统的等值面方法提取一个表示标量函数的水平集的多边形mesh，这个问题已经在多个领域得到了广泛的研究。在这里，我们回顾了特别相关的工作，并建议读者参考De Araújo等人[2015]的优秀调查(# A Survey on Implicit Surface Polygonization)，以获得全面的概述。根据De Araújo等人[2015]，我们将等表面处理方法分为三类，并将最常用的方法分类在表1中。</p><ul><li>Spatial Decomposition.第一类方法通过空间分解获得等值面，将空间划分为立方体或四面体等单元，并在包含曲面的单元内创建多边形[Bloomenthal 1988;Bloomenthal et al. 1997]。<ul><li>行进立方体(Marching Cubes, MC) [Lorensen and Cline 1987]是这一类中最具代表性的方法。正如最初提出的那样，Marching Cubes遭受拓扑模糊性的困扰，难以表示尖锐的特征。</li><li>随后的工作改进了为立方体分配多边形类型的查找表[Chernyaev 1995;Hege et al. 1997;Lewiner et al. 2003;Montani et al. 1994;尼尔森2003;Scopigno 1994]或将立方体划分为四面体[Bloomenthal 1994]，并使用类似的Marching tetrahedra [Doi and Koide 1991]来提取等值面。</li><li>为了更好地捕捉尖锐特征，Dual Contouring (DC) [Ju et al. 2002]将mesh顶点提取到每个单元格的双重表示，并提出根据局部等值面细节估计顶点位置。双轮廓扩展到自适应mesh划分[Azernikov和Fischer 2005]，可以输出四面体mesh。</li><li>另一种改进的方法是双行进立方体(DMC) [Nielson 2004]，它利用了行进立方体和双轮廓的好处。</li><li>最近，Neural Marching Cubes [Chen and Zhang 2021]和Neural Dual contourting (NDC) [Chen et al. 2022b]提出了一种数据驱动的方法，将提取的mesh定位为输入域的函数。<strong>尽管在已知标量场的提取方面取得了很大进展，但将等曲面方法应用于基于梯度的mesh优化仍然具有挑战性</strong>。</li></ul></li><li>Surface Tracking.第二类方法利用曲面跟踪，利用曲面样本之间的相邻信息提取等值面。<ul><li>行进三角形[Hilton et al. 1996,1997]是最早的代表性方法之一，它在Delaunay约束下从初始点迭代地对表面进行三角化。以下工作旨在纳入适应性[Akkouche和Galin 2001;Karkanis and Stewart 2001]或与尖锐特征对齐[McCormick and Fisher 2002]。<strong>然而，在曲面跟踪框架中基于梯度的mesh优化需要通过离散的迭代更新过程进行微分，这是一项非常重要的工作</strong>。</li></ul></li><li>Shrink Wrapping.第三类的方法依赖于缩小球面mesh[Van Overveld and Wyvill 2004]，或者膨胀临界点[Stander and Hart 1995]来匹配等值面。默认情况下，这些方法仅适用于有限的拓扑情况，并且需要手动选择临界点[Bottino et al. 1996]以支持任意拓扑。此外，通过收缩过程的微分也不是直截了当的，因此<strong>这些方法不太适合基于梯度的优化</strong>。</li></ul><h3 id="Gradient-Based-Mesh-Optimization-in-ML"><a href="#Gradient-Based-Mesh-Optimization-in-ML" class="headerlink" title="Gradient-Based Mesh Optimization in ML"></a>Gradient-Based Mesh Optimization in ML</h3><p>随着机器学习(ML)的最新进展，一些研究探索了用神经网络生成3Dmesh，神经网络的参数通过基于梯度的优化在一些损失函数下进行优化。早期的方法试图预先定义生成形状的拓扑结构，例如球体[Chen等人，2019;Hanocka et al. 2020;Kato et al. 2018;Wang et al. 2018]，原语联合[Paschalidou et al. 2021;Tulsiani et al. 2017]或一组分段部分[Sung et al. 2017;Yin et al. 2020;Zhu et al. 2018]。<strong>然而，它们泛化到具有复杂拓扑的对象的能力有限</strong>。</p><ul><li>为了解决这个问题，AtlasNet [Groueix等人，2018]将3D形状表示为参数表面元素的集合，尽管它不编码连贯表面。Mesh R-CNN [Gkioxari等人，2019]首先预测粗结构，然后细化为表面mesh。这种两阶段方法可以生成具有不同拓扑的mesh，<strong>但由于第二阶段仍然依赖于mesh变形，因此无法纠正第一阶段的拓扑误差</strong>。</li><li>PolyGen [Nash et al. 2020]渐进式生成mesh顶点和边缘，<strong>但它们在需要3D地面真实数据方面受到限制</strong>。</li><li>cvnet [Deng等人，2019]和BSPNet [Chen等人，2020]试图使用形状或二进制平面的凸分解来进行空间划分，<strong>但是将它们扩展到mesh上定义的各种目标是非常重要的</strong>。</li></ul><p>最近，许多研究探索了可微mesh重建方案，该方案从隐函数中提取等值面，通常通过卷积网络或隐神经场进行编码。</p><ul><li>Deep Marching Cubes [Liao et al. 2018]计算立方体内可能拓扑的期望，随着grid分辨率的增加，其可扩展性很差。</li><li>MeshSDF [Remelli et al. 2020]通过mesh提取提出了一种专门的梯度采样方案，而Mehta et al.[2022]则仔细阐述了神经环境下的水平集进化。</li><li>Def Tet [Gao et al. 2020]预测了一个可变形的四面体grid来表示3D对象。</li><li>与我们的方法最相似的是DMTet [Shen et al. 2021]，它利用可微的Marching Tetrahedra层来提取mesh。第3节提供了对DMTet的深入分析。</li></ul><h1 id="BACKGROUND-AND-MOTIVATION"><a href="#BACKGROUND-AND-MOTIVATION" class="headerlink" title="BACKGROUND AND MOTIVATION"></a>BACKGROUND AND MOTIVATION</h1><p>在这里，我们首先讨论了常见的现有等面提取方案，以了解它们的缺点并激励我们在第4节中提出的方法。</p><p>Problem Statement.<br>如第1节所述，我们寻求可微mesh优化的表示，其中基本管道是:<br>i)在空间中定义标量带符号距离函数<br>ii)将其0-等值面提取为三角形mesh<br>iii)评估该mesh上的目标函数<br>iv)将梯度反向传播到底层标量函数。<br>目前广泛应用于等值面提取的几种流行算法在这种可微环境下仍然存在显著的问题。主要的挑战是，<strong>基于梯度的优化的有效性很大程度上取决于等值面提取的特定机制</strong>:在基于梯度的优化中使用时，限制性参数化、数值不稳定表达式和拓扑障碍都会导致失败和工件。<br>我们强调，我们的FlexiCubes表示不是用于从固定的、已知的标量场中提取等值面，这是过去工作中考虑的主要情况。相反，<strong>我们特别考虑可微mesh优化，其中底层标量场是未知的，并且在基于梯度的优化过程中执行多次提取</strong>。这种设置提供了新的挑战，并激发了专门的方法。</p><p>Notation.<br>我们考虑的所有方法都是从标量函数中提取等值面s: $\mathbb{R}^3\to\mathbb{R},$，在规则grid的顶点采样，并在每个单元内插值。函数s可以直接离散为grid顶点处的值，或者从底层神经网络中评估等，精确参数化s对等值面提取没有影响。为了清楚起见，集合X用单元格C表示grid的顶点，而$M=(V,F)$表示最终提取的mesh与顶点V和面F． 我们含蓄地超载了$v\in V{\mathrm{~or~}}x\in X$指一个逻辑顶点，或者指那个顶点在空间中的位置，$x\in\mathbb{R}^3.$。</p><h2 id="Marching-Cubes-amp-Tetrahedra四面体"><a href="#Marching-Cubes-amp-Tetrahedra四面体" class="headerlink" title="Marching Cubes &amp; Tetrahedra四面体"></a>Marching Cubes &amp; Tetrahedra四面体</h2><p>最直接的方法是提取grid上有顶点的mesh，每个grid单元内有一个或多个mesh面，如Marching Cubes [Lorensen and Cline 1987]， Marching Tetrahedra [Doi and Koide 1991]，以及许多推广方法。mesh顶点沿着grid边缘提取，其中线性插值的标量函数改变符号<br>$u_e=\frac{x_a\cdot s(x_b)-x_b\cdot s(x_a)}{s(x_b)-s(x_a)}.$ Eq.1</p><p>廖等[2018];Remelli等人[2020]观察到这个表达式包含一个奇点，当$s(v_a)=s(v_b)$，这可能会阻碍微分优化，尽管Shen等人[2021]注意到，在提取过程中，等式1从未在奇异条件下求值。生成的mesh总是无自交(self-intersection-free)和流形(manifold)的<br>然而，通过构造，通过marching提取得到的mesh顶点只能位于grid边缘的稀疏格上。这样可以防止mesh适合尖锐的特征，并且不可避免地在等面线经过顶点附近时创建质量较差的三角形。最近的方法提出了超越朴素自微分的方案来计算底层标量场的改进梯度[Mehta et al. 2022;Remelli等人2020]，但这并没有解决mesh有限的输出空间。<br>一种有希望的补救措施是允许底层grid顶点变形[Gao等人，2020;Shen et al. 2021]。虽然这种泛化显著提高了性能，但提取的mesh顶点仍然不能独立移动，导致星形的窄三角形伪像，因为mesh顶点围绕grid上的一个自由度聚集。我们的方法受到Shen等人[2021]的启发，也利用了grid变形，但增加了额外的自由度，以允许顶点的独立重新定位，如图4所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230915162911.png" alt="image.png"></p><h2 id="Dual-Contouring"><a href="#Dual-Contouring" class="headerlink" title="Dual Contouring"></a>Dual Contouring</h2><p>顾名思义，双重轮廓(Dual Contouring, DC) [Ju et al. 2002]转向双重表示，提取通常可以定位在grid单元内的mesh顶点，以更好地捕捉尖锐的几何特征。每个mesh顶点的位置是通过最小化局部二次误差函数(QEF)来计算的，这取决于标量函数的局部值和空间梯度。<br>$v_{d}=\underset{v_{d}}{\mathrm{argmin}}\sum_{u_{e}\in\mathcal{Z}_{e}}\nabla s(u_{e})\cdot(v_{d}-u_{e}).$ Eq.2<br>$u_{e}\in\mathcal{Z}_{e}$是线性插值标量函数沿单元格边缘的过零点。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230915170011.png" alt="image.png"><br><em>Grad issue in DC左:当求解二次误差函数(QEF)时，结果顶点不能保证在立方体内部。这导致几何和拓扑情况之间的差异。此外，当法线共面时，QEF中存在一个奇异点。虽然已有技术可以通过约束QEF的解空间或使QEF具有正则化损失的偏置来提高直流电的稳定性，但它们在优化设置中不容易适应。前者(第二种)在某些方向上将梯度归零。后者(第三种)很难调整，并且具有很强的正则化将降低DC在灵活性方面的优势。我们的版本(第四版)FlexiCubes提供了额外的自由度，这样对于这个特定的配置，双顶点可以放置在绿色三角形内的任何地方</em></p><p>当从固定的标量函数中提取单个mesh时，双轮廓擅长于拟合尖锐特征，但一些特性阻碍了它在微分优化中的使用。最重要的是，<strong>公式2不能保证提取的顶点位于grid单元内</strong>。事实上，共面梯度向量$\nabla s(u_{e})$创建退化配置，其中顶点爆炸到一个遥远的位置，导致自相交和数值上不稳定的优化，通过公式进行微分。明确地将顶点限制在单元格中，使梯度归零，并对方程2进行正则化以解决这个问题，从而消除了拟合尖锐特征的能力(图2和图4)。此外，得到的mesh连通性可能是非流形的，输出mesh包含非平面四边形，当它们被分割成三角形时，会引入误差(图3)。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230915164347.png" alt="image.png"></p><p>最近关于对双轮廓的推广[Chen et al. 2022b]用学习的神经网络代替了方程2，提高了从不完美但固定的标量函数中提取的质量。然而，当针对底层函数进行优化时，通过额外的神经网络进行区分会进一步使优化环境复杂化，并阻碍收敛(图4)。</p><p>我们的方法从这些方法中获得灵感，并且在单元格内自由定位每个顶点的重要性。然而，我们没有明确地将提取的顶点定位为标量场的函数，而是引入了额外的精心选择的自由度，这些自由度被优化为局部调整顶点位置。我们可以通过将我们的方案基于类似但不太为人所知的对偶行进立方体来解决流形问题。</p><h2 id="Dual-Marching-Cubes"><a href="#Dual-Marching-Cubes" class="headerlink" title="Dual Marching Cubes"></a>Dual Marching Cubes</h2><p>就像双重轮廓一样，双重移动立方体[Nielson 2004]提取grid单元内的顶点。然而，它不是沿着grid的双重连通性提取mesh，而是沿着mesh的双重连通性提取mesh，这些mesh将由Marching Cubes提取。这允许所有配置的流形mesh输出，通过在需要时在单个grid单元内发射多个mesh顶点。提取的顶点位置被定义为类似于双重轮廓的QEF的最小化器[Schaefer等人，2007]，或者作为原始mesh几何的几何函数[Nielson 2004]，如面部质心。</p><p>一般来说，与双轮廓相比，双行进立方体提高了提取mesh连通性，但如果使用QEF进行顶点定位，它会受到许多与双轮廓相同的缺点的影响。如果顶点位于原始mesh的质心，则该公式缺乏拟合单个尖锐特征的自由度。在随后的文本中，除非另有说明，否则每当我们提到双行进立方体时，我们指的是质心方法。</p><p>我们的方法建立在Dual Marching Cube提取的基础上，但<strong>我们引入了额外的参数来定位顶点，从而推广了质心方法</strong>。基于一种即使在困难的配置中也能发出正确拓扑的方案是我们成功的关键之一。</p><h1 id="METHOD"><a href="#METHOD" class="headerlink" title="METHOD"></a>METHOD</h1><p>我们提出了可微mesh优化的FlexiCubes表示。该方法的核心是grid上的标量函数，通过双步立方提取三角形mesh。我们的主要贡献是引入了三组额外的参数，精心选择以增加mesh表示的灵活性，同时保持鲁棒性和易于优化:</p><ul><li>Interpolation weights: $\alpha\in\mathbb{R}_{&gt;0}^8,\beta\in\mathbb{R}_{&gt;0}^{12}$ per grid cell, to position dual vertices in space <strong>4.2</strong></li><li>Splitting weights: $\gamma\in\mathbb{R}_{&gt;0}$ per grid cell, to control how quadrilaterals四边形 are split into triangles <strong>4.3</strong></li><li>Deformation vectors：$\delta\in\mathbb{R}^{3}$ per vertex of the underlying grid for spatial alignment <strong>4.4</strong></li></ul><p>这些参数与标量函数一起优化s通过自动分化拟合一个mesh到所需的目标。我们还提出了FlexiCubes表示的扩展，以提取体积的四面体mesh(第<strong>4.5</strong>节)，并表示具有自适应分辨率的分层meshes(第<strong>4.6</strong>节)。</p><h2 id="Dual-Marching-Cubes-Mesh-Extraction"><a href="#Dual-Marching-Cubes-Mesh-Extraction" class="headerlink" title="Dual Marching Cubes Mesh Extraction"></a>Dual Marching Cubes Mesh Extraction</h2><p>我们首先根据每个grid顶点x标量函数s(x)的值提取Dual Marching Cubes mesh的连通性。正如Nielson [2004];Schaefer等[2007]。The signs of s(x)在立方体角处确定连通性和邻接关系(图7)。与沿着grid边缘提取顶点的普通行军立方体不同，双行军立方体为单元格中的每个原始面提取一个顶点;通常是单个顶点，但也可能多达四个(图7，案例C13)。相邻单元中提取的顶点通过边连接形成双mesh，由四边形面组成(图5)。所得mesh保证是流形的，尽管由于下面描述的额外自由度，它可能很少包含自交;参见7.2节。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230915170320.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230915170327.png" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IsosurfaceExtraction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BakedSDF</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/BakedSDF/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/BakedSDF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis</th></tr></thead><tbody><tr><td>Author</td><td>Lior Yariv and Peter Hedman and Christian Reiser and Dor Verbin and Pratul P. Srinivasan and Richard Szeliski and    Jonathan T. Barron and Ben Mildenhall</td></tr><tr><td>Conf/Jour</td><td>SIGGRAPH</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://bakedsdf.github.io/">BakedSDF</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4762044324229677057&amp;noteId=1959289127671095296">BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230913154905.png" alt="image.png"></p><ul><li>对前景物体采用类似VolSDF的方法训练<em>Modeling density with an SDF</em></li><li>使用Marching Cube 的方法来提取高分辨率网格<em>Baking a high-resolution mesh</em></li><li><em>Modeling view-dependent appearance</em>，对baked后的高分辨率网格上顶点：采用漫反射颜色和球形高斯叶（前景3个波瓣，远处背景使用1个波瓣）<ul><li>$\mathbf{C}=\mathbf{c}_{d}+\sum_{i=1}^{N}\mathbf{c}_{i}\exp\left(\lambda_{i}\left(\mu_{i}\cdot\mathbf{d}-1\right)\right).$ </li></ul></li></ul><span id="more"></span><h1 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h1><p>尽管我们的模型在实时渲染无界场景的任务中达到了最先进的速度和准确性，但仍有一些限制，这些限制代表了未来改进的机会:</p><ul><li>我们使用完全不透明的网格表示来表示场景，<strong>因此我们的模型可能难以表示半透明内容</strong>(玻璃，雾等)。与基于网格的方法一样，我们的模型<strong>有时无法准确地表示具有小或详细几何形状</strong>(茂密的树叶，薄结构等)的区域。图6描绘了额外提取的网格可视化，展示了我们的表面重建限制及其对渲染重建的影响。<strong>这些问题也许可以通过增加网格的不透明度值来解决，但是允许连续的不透明度将需要一个复杂的多边形排序过程，这很难集成到实时光栅化管道中</strong>。</li><li>我们技术的另一个限制是，我们模型的输出网格<strong>占用了大量的磁盘空间</strong>(每个场景约430兆字节)，这可能对某些应用程序的存储或流式传输具有挑战性。这可以通过网格简化和UV绘图来改善。<strong>然而，我们发现现有的简化和绘图工具主要是为艺术家制作的3D资产而设计的，对于我们通过移动立方体提取的网格并不适用</strong>。</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230913204555.png" alt="image.png"></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了一个系统，产生一个高质量的网格实时渲染的大型无界现实世界场景。我们的技术首先优化了用于精确表面重建的场景的混合神经体-表面表示。从这种混合表示中，我们提取了一个三角形网格，其顶点包含与视图相关的外观的有效表示，然后优化这个网格表示以最佳地再现捕获的输入图像。这使得网格在速度和准确性方面都能产生最先进的实时视图合成结果，并且具有足够高的质量以支持下游应用。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>我们提出了一种适合于真实感<strong>新视图</strong>合成的大型<strong>无界现实场景</strong>的<strong>高质量网格</strong>重建方法。我们首先优化了一个<strong>混合神经体-表面场景表示</strong>，它被设计成具有与场景中的表面对应的行为良好的水平集。然后，我们将这种表示烘烤成一个高质量的三角形网格，我们配备了一个简单而快速的基于球面高斯的视图依赖外观模型。最后，我们优化了这种烘焙表示，以最好地再现捕获的视点，从而产生一个可以利用加速多边形光栅化管道在消费级硬件上进行实时视图合成的模型。我们的方法在准确性、速度和功耗方面优于以前的实时渲染场景表示，并产生高质量的网格，使外观编辑和物理模拟等应用成为可能。</p><p>Key Words：</p><ul><li>Reconstruction; Neural networks; Volumetric models.</li><li>Neural Radiance Fields, Signed Distance Function, Surface Reconstruction, Image Synthesis, Real-Time Rendering, Deep Learning.</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>目前用于新视图合成(使用捕获的图像恢复从未观察到的视点渲染的3D表示)的最佳性能方法主要基于神经辐射场(NeRF) [Mildenhall等人，2020]。通过将场景表示为由多层感知器(MLP)参数化的连续体积函数，NeRF能够生成逼真的效果图，展示详细的几何图形和依赖于视图的效果。<strong>由于计算NeRF下的MLP成本很高，并且每个像素必须查询数百次，因此从NeRF渲染高分辨率图像通常很慢</strong>。</p><p>最近的工作通过将计算繁重的mlp替换为离散的体积表示(如体素网格)，提高了NeRF渲染性能。然而，这些方法需要大量的GPU内存和自定义的体积射线行进代码，并且不适合在商用硬件上进行实时渲染，因为现代图形硬件和软件面向的是呈现多边形表面而不是体积场。</p><p>虽然目前类似nerf的方法能够恢复具有简单几何形状的单个对象的高质量实时可渲染网格[Boss等人，2022]，但从现实世界无界场景的捕获(例如Barron等人的“360度捕获”)中重建详细且行为良好的网格被证明更加困难。最近，MobileNeRF [Chen et al. 2022a]通过训练NeRF来解决这个问题，该NeRF的体积内容被限制在多边形网格的表面上，然后将NeRF烘焙成纹理图。虽然这种方法产生了合理的图像质量，但MobileNeRF将场景几何初始化为一组axis-aligned tiles，优化后会变成一个纹理多边形“汤soup”。生成的几何图形不太适合常见的图形应用程序，如纹理编辑、重光照和物理模拟。</p><p><strong>在这项工作中，我们演示了如何从类似nerf的神经体积表示中提取高质量的网格</strong>。我们的系统，我们称之为BakedSDF，扩展了VolSDF的混合体面神经表示[Yariv等人，2021]，以表示无界的现实世界场景。<strong>这种表示被设计成具有与场景中的表面相对应的良好表现的零水平集，这让我们可以使用行进立方体提取高分辨率的三角形网格</strong>。</p><p>我们的关键思想是在收缩contracted坐标空间中定义SDF [Barron et al. 2022]，因为它具有以下优点:它更强地正则化远距离内容，并且它还允许我们在收缩空间中提取网格，从而更好地分配三角形预算(中心多，外围少)。</p><p>然后，我们为这个网格配备了一个基于球面高斯的快速有效的视图依赖外观模型，该模型经过微调以再现场景的输入图像。我们系统的输出可以在商品设备上以实时帧速率呈现，并且我们表明，我们的实时渲染系统在真实感，速度和功耗方面优于先前的工作。此外，我们表明(不像以前的工作)，我们的模型产生的网格是准确和详细的，使标准的图形应用程序，如外观编辑和物理模拟。</p><p>总而言之，我们的主要贡献是:</p><ul><li>无界现实场景的高质量神经表面重建;</li><li>一个在浏览器中实时渲染这些场景的框架，以及</li><li>我们证明了球面高斯函数是视图合成中视图依赖外观的实际表示(practical representation)。</li></ul><h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h2><p>View synthesis, i.e.，在给定一组捕获的图像的情况下渲染场景的新视图，是计算机视觉和图形学领域的一个长期存在的问题。</p><ul><li>在观测视点采样密集的场景中，可以通过光场渲染来合成新的视点——直接插值到观测光线集合中</li><li>然而，在实际环境中，观察到的视点被捕获得更稀疏，重建场景的3D表示对于呈现令人信服的新视图至关重要。大多数经典的视图合成方法使用三角形网格(通常使用由多视图立体MVS组成的管道重建。泊松地表重建，以及Marching Cube作为底层3D场景表示，并通过将观察到的图像重新投影到每个新视点中并使用启发式定义或learned 混合权重将它们混合在一起来呈现新视图。</li><li>尽管基于网格的表示非常适合使用加速图形管道进行实时渲染，但这些方法产生的网格在具有精细细节或复杂材料的区域中往往具有不准确的几何形状，从而导致渲染新视图时出现错误。</li><li>Alternatively，基于点的表示更适合于建模薄几何，但如果没有可见的裂缝或相机移动时不稳定的结果，则无法有效地渲染。</li></ul><p>最近的视图合成方法通过使用几何和外观的体积表示(如体素网格)来回避高质量网格重建的困难或多平面图像。这些表示非常适合基于梯度的渲染损失优化，因此它们可以有效地优化以重建输入图像中看到的详细几何形状。这些体积方法中最成功的是神经辐射场(NeRF) ，它构成了许多最先进的视图合成方法的基础(参见Tewari等人[2022]进行综述)。NeRF将场景表示为发射和吸收光的连续的物质体积场，并使用体积光线跟踪渲染图像。NeRF使用MLP从空间坐标映射到体积密度和发射亮度，并且MLP必须沿着射线在一组采样坐标上进行评估，以产生最终的颜色。</p><p>随后的工作建议修改NeRF的场景几何和外观的表示，以提高质量和可编辑性。</p><ul><li>Ref-NeRF [Verbin et al. 2022]重新参数化NeRF的视图依赖外观，以实现外观编辑并改进镜面材料的重建和渲染。</li><li>其他作品[Boss et al. 2021;Kuang等。2022;Srinivasan等人。2021;Zhang等。2021a,b]尝试将场景的视图依赖外观分解为材料和照明属性。</li><li>除了修改NeRF的外观表示外，包括UNISURF [Oechsle等人]在内的论文。[2021]， VolSDF [Yariv等。2021]，neus [Wang等。2021]，MetaNLR++和NeuMesh用混合体面模型增强NeRF的全体积表示，<strong>但不以实时渲染为目标，只显示对象和有界场景的结果</strong></li></ul><p>用于表示场景的MLP NeRF通常是大型且昂贵的评估，这意味着NeRF的训练速度很慢(每个场景数小时或数天)，渲染速度也很慢(每百万像素数秒或数分钟)。虽然可以通过减少每条光线的MLP查询的采样网络来加速渲染，最近的方法通过用体素网格取代大型MLP来改善训练和渲染时间，小型mlp网格， low-rank或sparse grid表示，或者使用小MLP进行多尺度哈希编码。</p><p>虽然这些表示减少了训练和渲染所需的计算(以增加存储为代价)，但渲染可以通过预计算和存储进一步加速，即“烘烤”，训练好的NeRF变为更有效的表示。FastNeRF ，Plenoctrees 和可扩展神经室内场景渲染(Scalable Neural Indoor Scene Rendering)[Wu et al. 2022]都将训练过的nerf烤成稀疏的体积结构，并使用简化的视图依赖外观模型，以避免沿着每条光线对每个样本进行MLP评估。这些方法可以在高端硬件上实现nerf的实时渲染，<strong>但它们对体积射线推进的使用妨碍了普通硬件的实时性能</strong>。在我们工作的同时，开发了内存高效辐射场(MERF)，这是一种用于无界场景的压缩表示体积，有助于在商用硬件上快速渲染。与我们的网格相比，这种体积表示达到了更高的质量分数，<strong>但需要更多的内存，需要一个复杂的渲染器，并且不直接用于下游图形应用程序，如物理模拟</strong>。请参考MERF论文与我们的方法进行直接比较。</p><h1 id="PRELIMINARIES"><a href="#PRELIMINARIES" class="headerlink" title="PRELIMINARIES"></a>PRELIMINARIES</h1><p>在本节中，我们描述了神经容量表示，NeRF 用于视图合成以及mip-NeRF 360引入的改进，因为它代表了unbounded “360度”场景。</p><p>NeRF的渲染方程与loss函数<br>$\mathbf{C}=\sum_{i}\exp\left(-\sum_{j&lt;i}\tau_{j}\delta_{j}\right)\left(1-\exp\left(-\tau_{i}\delta_{i}\right)\right)\mathbf{c}_{i},\quad\delta_{i}=t_{i}-t_{i-1}.$ Eq.1</p><p>$\mathcal{L}_{\mathrm{data}}=\mathbb{E}\left[\left|\mathbf{C}-\mathbf{C}_{\mathrm{gt}}\right|^{2}\right].$ Eq.2</p><p>Mip-NeRF 360<br>将无界的x位置contract到有界域<br>$\operatorname{contract}(\mathbf{x})=\begin{cases}\mathbf{x}&amp;|\mathbf{x}|\leq1\\\left(2-\frac{1}{|\mathbf{x}|}\right)\frac{\mathbf{x}}{|\mathbf{x}|}&amp;|\mathbf{x}|&gt;1\end{cases},$ Eq.3</p><h1 id="METHOD"><a href="#METHOD" class="headerlink" title="METHOD"></a>METHOD</h1><p>我们的方法由三个阶段组成，如图2所示。</p><ul><li>首先，我们使用类似nerf的体渲染优化基于表面的几何形状表示和场景外观。</li><li>然后，我们将几何形状“烘烤”到一个网格中，我们证明它足够精确，可以支持令人信服的外观编辑和物理模拟。</li><li>最后，我们训练了一个新的外观模型，该模型使用球面高斯(SGs)嵌入到网格的每个顶点中，取代了第一步中昂贵的类似nerf的外观模型。</li></ul><p>这种方法产生的3D表示可以在商品设备上实时渲染，因为渲染只需要对网格进行栅格化并查询少量球面高斯函数。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230913164451.png" alt="image.png"></p><h2 id="Modeling-density-with-an-SDF"><a href="#Modeling-density-with-an-SDF" class="headerlink" title="Modeling density with an SDF"></a>Modeling density with an SDF</h2><p>我们的表示结合了mip-NeRF 360表示无界场景的优点，以及VolSDF混合体-面表示的良好表面属性[Yariv等人。2021]。VolSDF将场景的体积密度建模为mlp参数化有符号距离函数(SDF)的参数函数f，它返回带符号的距离$f(\mathbf{x})$从每一点$\mathbf{x}\in\mathbb{R}^3$到曲面。因为我们的重点是重建无界的现实世界场景，我们参数化f在收缩空间(公式3)而不是世界空间。场景的下表面是f的零水平集，即距离曲面为零的点的集合:<br>$\mathcal{S}=\{\mathbf{x}:f(\mathbf{x})=0\}.$ Eq.4<br>与VolSDF一样，将体密度定义为：<br>$\tau(\mathbf{x})=\alpha\Psi_{\beta}\left(f(\mathbf{x})\right),$ Eq.5<br>其中$\Psi_{\beta}$是带尺度参数$\beta&gt;0$的零均值拉普拉斯分布的累积分布函数。注意随着$\beta$趋近于0，体积密度接近于一个函数，它返回$\alpha$在任意对象内，自由空间为0。鼓励f近似一个有效的带符号距离函数(即一个f(x)返回对所有x的f水平集的有符号欧氏距离)。，我们惩罚f的偏差满足Eikonal方程：<br>$\mathcal{L}_{\mathrm{SDF}}=\mathbb{E}_{\mathrm{x}}\left[(|\nabla f(\mathrm{x})|-1)^{2}\right].$ Eq.6<br>注意f在收缩空间中定义，这个约束也作用于收缩空间。</p><p>最近，Ref-NeRF [Verbin et al. 2022]通过将其参数化为反映表面法线的视图方向的函数来改进视图依赖外观。我们使用sdf参数化的密度使得这很容易被采用因为sdf具有定义良好的表面法线:$\mathbf{n}(\mathbf{x})=\nabla f(\mathbf{x})/|\nabla\hat{f}(\mathbf{x})|.$ 因此，在训练我们模型的这一阶段时，我们采用Ref-NeRF的外观模型，并使用单独的漫射和镜面分量来计算颜色，其中镜面分量是由法线方向反射的视图方向的拼接、法线方向与视图方向之间的点积以及MLP输出的256个元素的bottleneck向量来参数化的。</p><p>我们使用mip-NeRF 360的变体作为我们的模型(具体训练细节见补充材料中的附录a)。与VolSDF [Yariv et al. 2021]类似，我们将密度比例因子参数化为$\alpha=\beta^{-1}$式5。然而，我们find调度$\beta$而不是把它作为一个自由的可优化参数，结果是更稳定的训练。我们因此退火$\beta$根据$\beta_{0}\left(1+\frac{\beta_{0}-\beta_{1}}{\beta_{1}}t^{0.8}\right)^{-1},$其中$t$在训练过程中从0到1，$\beta_{0}=0.1$，和$\beta_{1}$三个分层采样阶段分别为0.015、0.003和0.001。由于密度SDF参数化所需的Eikonal正则化已经消除了floaters体并导致了良好的常态，我们发现没有必要使用Ref-NeRF的orientation损失或预测法向量，或mip - nerf 360的distortion损失。</p><h2 id="Baking-a-high-resolution-mesh"><a href="#Baking-a-high-resolution-mesh" class="headerlink" title="Baking a high-resolution mesh"></a>Baking a high-resolution mesh</h2><p>优化神经体积表示后，我们通过在常规3D网格上查询恢复的mlp参数化SDF创建三角形网格，然后运行Marching Cubes [Lorensen and Cline 1987]。请注意，VolSDF使用超出SDF零交叉点的密度下降来建模边界(参数化为$\beta$)。我们在提取网格时考虑到这种扩散，并选择0.001作为表面交叉的等值，否则我们会发现场景几何形状会被轻微侵蚀。</p><p>Visibility and free-space culling.当运行Marching Cubes时，MLP参数化的SDF可能在被观测视点遮挡的区域以及建议MLP标记为“自由空间”的区域中包含虚假的表面交叉。在训练期间，这两种类型区域中的SDF MLP值都不受监督，因此我们必须剔除任何可能在重建网格中显示为虚假内容的表面交叉点。为了解决这个问题，我们检查沿着我们的训练数据中的射线拍摄的3D样本。我们计算每个样本的体积渲染权重，即它对训练像素颜色的贡献。然后，我们将任何具有足够大的渲染权重(&gt; 0.005)的样本放入3D网格中，并将相应的单元标记为表面提取的候选单元。</p><p>Mesh extraction.<strong>我们在收缩空间中以均匀间隔的坐标对SDF网格进行采样</strong>，从而在世界空间中产生不均匀间隔的非轴向坐标。<strong>这具有为靠近原点的前景内容创建较小三角形(在世界空间中)和为较远的内容创建较大三角形的理想属性</strong>。有效地，我们利用收缩算子作为细节级策略:因为我们想要渲染的视图接近场景原点，并且因为收缩的形状被设计为撤销透视投影的效果，所以所有三角形在投影到图像平面上时将具有大约相等的面积。<br>Region growing.在提取三角形网格后，我们使用区域生长过程来填充可能存在于输入视点未观察到或在烘烤过程中被提议MLP遗漏的区域中的小洞。我们在当前网格周围的邻域中迭代标记体素，并提取这些新活动体素中存在的任何表面交叉点。<strong>这种区域增长策略有效地解决了SDF MLP中存在曲面但由于训练视图覆盖不足或提议MLP中存在错误而无法通过行进立方体提取的情况</strong>。然后我们将网格转换为世界空间，这样它就可以通过在欧几里德空间中操作的传统渲染引擎进行光栅化。<br>Implementation.我们使用$2048^{3}$网格进行可见性和自由空间筛选和移动立方体。最初，我们只在未被剔除的体素上运行行进立方体，即可见且非空的体素。然后我们用32个区域增长迭代来完成网格，在那里我们在当前网格中的顶点周围的$8^{3}$个体素附近重新运行行进立方体。最后，我们使用顶点顺序优化对网格进行后处理[Sander等2007]，<strong>它通过允许顶点着色器输出在相邻三角形之间缓存和重用来加快现代硬件上的渲染性能</strong>。在附录B中，我们详细介绍了网格提取的其他步骤，这些步骤并不严格提高重建精度，但可以提供更令人愉悦的交互式观看体验。</p><h2 id="Modeling-view-dependent-appearance"><a href="#Modeling-view-dependent-appearance" class="headerlink" title="Modeling view-dependent appearance"></a>Modeling view-dependent appearance</h2><p>上面描述的烘焙过程从我们基于mlp的场景表示中提取高质量的三角形网格几何。为了模拟场景的外观，包括与视图相关的效果，如镜面，我们为每个网格顶点配备漫射颜色$c_{d}$和一组球形高斯叶。由于遥远的区域只能从有限的一组视图方向观察到，我们不需要在场景中以相同的保真度对视图依赖性进行建模。在我们的实验中，我们在中心区域$(|\mathbf{x}|\leq1)$使用三个球形高斯波瓣，在外围区域使用一个波瓣。图3展示了我们的外观分解。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230913202957.png" alt="image.png"></p><p>这种外观表示满足我们对计算和内存的效率目标，因此可以实时呈现。每个球面高斯叶有七个参数:一个三维单位向量$\mu$对于瓣的均值，一个三维矢量c表示瓣的颜色，和一个标量$\lambda$对于叶的宽度。这些叶是由视图方向向量d参数化的，所以一条射线与任何给定顶点相交的渲染颜色C可以计算为:<br>$\mathbf{C}=\mathbf{c}_{d}+\sum_{i=1}^{N}\mathbf{c}_{i}\exp\left(\lambda_{i}\left(\mu_{i}\cdot\mathbf{d}-1\right)\right).$ Eq.7</p><p>为了优化这种表示，我们首先将网格栅格化到所有训练视图中，并存储与每个像素相关的顶点索引和质心坐标。在此预处理之后，我们可以通过将重心插值应用于学习到的每个顶点参数，然后运行我们的视图依赖外观模型(模拟fragment着色器的操作)来轻松渲染像素。因此，我们可以通过最小化每个像素的颜色损失来优化每个顶点的参数，如公式2所示。如附录B所述，我们还优化了背景清晰的颜色，以便为交互式查看器提供更愉悦的体验。为了防止这种优化被网格几何形状没有很好建模的像素(例如，软对象边界和半透明对象的像素)所影响，我们使用鲁棒损失$\rho(\cdot,\alpha,c)$来代替VolSDF最小化的L2损失。鲁棒损失超参数$\alpha=0,c={}^{1}/5$，这使得优化对离群值更具鲁棒性[Barron 2019]。我们还使用直通估计器对量化建模[Bengio等人]。[2013]，确保8位精度很好地表示视图相关外观的优化值。<br>我们发现直接优化这种逐顶点表示会使GPU内存饱和，这阻碍了我们扩展到高分辨率网格。我们转而优化了一个基于Instant NGP的压缩神经哈希网格模型。在优化过程中，我们在训练批内的每个3D顶点位置查询该模型，以产生我们的漫反射颜色和球面高斯参数。<br>优化完成后，我们通过在每个顶点位置查询NGP模型以获得与外观相关的参数，烘烤出哈希网格中包含的压缩场景表示。最后，我们使用gLTF格式[ISO/IEC 12113:2022 2022]导出生成的网格和逐顶点外观参数，并使用gzip压缩它，这是web协议原生支持的格式。</p><h1 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h1><p>我们通过输出效果图的准确性以及速度、能量energy和内存需求来评估方法的性能。<br>对于准确性，我们测试了两个版本的模型: 第4.1节中描述的中间体绘制结果，我们称之为“离线”模型，以及第4.2节和4.3节中描述的烘焙实时模型，我们称之为“实时”模型。作为基线，我们使用先前的离线模型[Barron等人，2022;米尔登霍尔等人。2020;Müller等。2022;Riegler和Koltun 2021;Zhang等人。2020]designed for fidelity,，以及之前的实时方法[Chen等人。2022年;海德曼等人。2018]designed for performance.<br>我们还将我们的方法恢复的网格与COLMAP [Schönberger等人]，mip-NeRF 360 [Barron等。2022]，和MobileNeRF [Chen等。2022年]提取的网格进行了比较。所有FPS(帧/秒)测量都是在1920 × 1080分辨率下渲染的。</p><h2 id="Real-time-rendering-of-unbounded-scenes"><a href="#Real-time-rendering-of-unbounded-scenes" class="headerlink" title="Real-time rendering of unbounded scenes"></a>Real-time rendering of unbounded scenes</h2><h2 id="Mesh-extraction"><a href="#Mesh-extraction" class="headerlink" title="Mesh extraction"></a>Mesh extraction</h2><h2 id="Appearance-model-ablation"><a href="#Appearance-model-ablation" class="headerlink" title="Appearance model ablation"></a>Appearance model ablation</h2><h1 id="实验-非官方code"><a href="#实验-非官方code" class="headerlink" title="实验(非官方code)"></a>实验(非官方code)</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install torch torchvision</span><br><span class="line">pip install git+https://github.com/NVlabs/tiny-cuda-nn/<span class="comment">#subdirectory=bindings/torch</span></span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>colmap生成pose<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/imgs2poses.py ./load/bmvs_dog <span class="comment"># images are in ./load/bmvs_dog/images</span></span><br></pre></td></tr></table></figure></p><p>run:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python launch.py --config configs/neus-colmap.yaml --gpu 0 --train dataset.root_dir=<span class="variable">$1</span></span><br><span class="line">python launch.py --config configs/bakedsdf-colmap.yaml --gpu 0 --train dataset.root_dir=<span class="variable">$1</span> --resume_weights_only --resume latest</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Real-time </tag>
            
            <tag> Efficiency </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ref-NeuS</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/Ref-NeuS/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/Ref-NeuS/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection</th></tr></thead><tbody><tr><td>Author</td><td>Wenhang Ge1 Tao Hu 2 Haoyu Zhao 1 Shu Liu 3 Ying-Cong Chen1,∗</td></tr><tr><td>Conf/Jour</td><td>ICCV Oral</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://g3956.github.io/">Ref-NeuS (g3956.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4735800840636350465&amp;noteId=1957762733774455552">Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection (readpaper.com)</a></td></tr></tbody></table></div><ul><li>Anomaly Detection for Reflection Score + Visibility Identification for Reflection Score</li><li>Reflection Direction Dependent Radiance反射感知的光度损失</li></ul><span id="more"></span><h1 id="Limitation-and-Conclusion"><a href="#Limitation-and-Conclusion" class="headerlink" title="Limitation and Conclusion"></a>Limitation and Conclusion</h1><p>Limitation：尽管我们的方法在带反射的多视图重建中显示出良好的效果，但仍然存在一些局限性。</p><ul><li>首先，<strong>估计反射分数</strong>不可避免地<strong>增加了计算成本</strong>。</li><li>其次，简单地用反射方向取代辐射网络的依赖关系，<strong>而不管物体的材质如何，在某些情况下都会导致伪影</strong>。我们在补充材料中给出了这样一个人工制品的例子。</li></ul><p>Conclusion：本文研究了具有反射表面的物体的多视点重建问题，这是一个重要但尚未得到充分研究的问题。反射引起的模糊会严重破坏多视图的一致性，但我们提出的Ref-NeuS方法<strong>通过引入反射感知的光度损失来解决这个问题，其中反射像素的重要性使用高斯分布模型衰减</strong>。此外，我们的方法采用了反射方向相关的辐射，这进一步改善了几何形状，具有更好的辐射场，包括几何形状和表面法线。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>神经隐式表面学习在多视图三维重建中取得了重大进展，其中物体由MLP表示，这些感知器提供连续的隐式表面表示和与视图相关的亮度。然而，目前的方法往往不能准确地重建反射表面，导致严重的模糊性。为了克服这个问题，我们提出了Ref-NeuS，<strong>旨在通过降低反射表面的重要性来减少模糊性</strong>。具体来说，我们利用<strong>异常检测器</strong>在多视图context的指导下估计显式反射分数来定位反射表面。之后，我们设计了一个<strong>反射感知的光度损失</strong>，通过将渲染颜色建模为高斯分布，以反射分数表示方差variance，自适应地减少模糊。我们表明，与反射方向相关的辐射一起，我们的模型在反射表面上实现了高质量的表面重建，并且在很大程度上优于最先进的技术。此外，我们的模型在一般表面上也具有可比性。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>三维重建是计算机视觉中的一项重要任务，是计算机辅助设计[20,8]、计算机动画[33,23]、虚拟现实[40]等多个领域的基础。在各种三维重建技术中，基于图像的三维重建技术尤其具有挑战性，其目的是从构成的二维图像中恢复三维结构。传统的多视角立体(MVS)方法[11,39,51]通常需要一个多步骤的管道，并带有监督，这可能很麻烦。最近，神经隐式表面学习[45,52,31]因其能够以支持端到端和无监督训练的整洁公式实现卓越的重建质量而受到越来越多的关注。然而，如图1所示，<strong>现有的方法往往在反射表面(如镜面高光)产生错误的结果</strong>。由于这些方法推断的几何信息具有多视图一致性，因此<strong>由于反射表面上几何网络的表面预测不明确而影响了多视图一致性</strong>。因此，在反射不可避免的情况下，它们的实用性受到限制。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230912143310.png" alt="image.png"></p><p>最近的一些研究[3,42,55,57,43]研究了神经辐射场的反射建模。这些方法通常将对象的外观分解为几个物理组件，从而允许显式地表示反射。通过消除反射分量的影响，可以更好地估计3D几何形状。<strong>然而，物理分解可能是高度不适定的[14]，不准确的分解会严重限制性能</strong>。例如，如图1所示，Ref-NeRF[43]中预测的norm不够准确，从而导致性能次优。</p><p>在本文中，我们提出了一个简单而有效的解决方案，<strong>不依赖于具有挑战性的物理分解</strong>。<br>相反，<strong>我们建议通过引入反射感知光度损失来减少模糊性，该光度损失可根据反射分数自适应地降低拟合反射表面的权重。通过这样做，我们避免了破坏性的多视图一致性</strong>。此外，受Ref-NeRF[43]和NeuralWarp[7]的启发，我们表明我们可以通过用反射方向代替辐射依赖关系来进一步改进几何形状，以获得更准确的辐射场。如图1所示，我们的模型在预测表面几何形状(顶部行)和表面法线(中间行)方面优于其他竞争方法。此外，通过估算更精确的表面法线来确定反射方向的准确性，我们还可以实现有希望的渲染真实感，作为一个额外的benefit(底部行)。</p><p>虽然上面讨论的想法很简单，但设计反射感知的光度损失是非常重要的。一种直接的方法是遵循NeRF-W[27]，其中vanilla光度损失被扩展到贝叶斯学习框架[19]。它将辐射表示为高斯分布，学习到的不确定性表示方差，期望不确定性可以在野外定位图像的瞬态分量，消除其对静态分量学习的影响。<strong>然而，这种方法并不适用于反射场景</strong>，因为它学习了只考虑单一光线信息而忽略多视图context的隐含不确定性。</p><p>为了解决这个问题，<strong>我们建议定义一个明确的反射分数，该分数利用参考同一表面点的多视图图像的像素颜色获得的多视图context</strong>。首先，我们确定给定一个表面点的所有源视图的可见性。接下来，我们将点投影到可见图像中以获得像素颜色。在此基础上，我们使用异常检测器来估计反射分数，该分数作为方差。通过<strong>最小化颜色高斯分布的负对数似然</strong>，较大的方差会减弱其重要性。我们进一步证明，通过使用反射方向相关的辐射，我们的模型在具有更好的辐射场的多视图重建中取得了令人满意的结果。</p><p>贡献：</p><ul><li>据我们所知，我们提出了第一个用于重建具有反射表面的物体的神经隐式表面学习框架。</li><li>我们提出了一种简单而有效的方法，使神经隐式表面学习处理反射表面。我们的方法可以产生高质量的表面几何形状和表面法线。</li><li>在多个数据集上进行的大量实验表明，所提出的框架在反射表面上的性能明显优于最先进的方法。</li></ul><h2 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h2><h2 id="Multi-View-Stereo-for-3D-Reconstruction"><a href="#Multi-View-Stereo-for-3D-Reconstruction" class="headerlink" title="Multi-View Stereo for 3D Reconstruction"></a>Multi-View Stereo for 3D Reconstruction</h2><p>多视图立体(MVS)是一种旨在从多视图图像中重建细粒度场景几何结构的技术。传统的MVS方法可以根据输出场景的表示分为四类: 基于体积的方法[6,41]、基于网格的方法[10]、基于点云的方法[11,24]和基于深度图的方法[4,12,38,39,47]。<strong>其中，基于深度图的方法最为灵活，利用参考图像和相邻图像的光度一致性估计每个视图的深度图[11]，然后将所有深度图融合成密集的点云</strong>。然后采用表面重建方法[6,18,22]，如筛选泊松表面重建[18]，从点云重建表面。<br><strong>然而，基于学习的MVS方法在某些情况下仍然可能产生令人不满意的结果</strong>，例如镜面反射的表面，低纹理的区域和非兰伯特区域。在这些情况下，不能保证多视图图像的光度一致性，这可能导致重建结果中出现严重的伪影和缺失部分。</p><h2 id="Neural-Implicit-Surface-for-3D-Reconstruction"><a href="#Neural-Implicit-Surface-for-3D-Reconstruction" class="headerlink" title="Neural Implicit Surface for 3D Reconstruction"></a>Neural Implicit Surface for 3D Reconstruction</h2><p>近年来，人们提出了基于学习的隐式表面表示方法。在这些表示中，神经网络将三维空间中的连续点映射到占用场[29,36]或有符号距离函数(SDF)[34]。这些方法执行多视图重建，并根据每个点的占用值或SDF进行额外的监督。<strong>然而，对这些方法的监督并不总是适用于only多视图2D图像，这限制了它们的可扩展性</strong>。</p><p>在NeRF[30]中引入的体积方法将经典的体绘制[16]与用于新型视图合成的隐式函数相结合，引起了人们对使用神经隐式表面表示和体绘制进行三维重建的大量关注[31,52,45]。与NeRF不同，NeRF旨在呈现新颖的视图图像，同时保持几何形状不受约束，这些方法以更明确的方式定义表面，因此更适合表面提取。UNISURF[31]使用占用域[29]，而IDR[53]、VolSDF[52]和NeuS[45]使用SDF域[34]作为隐式表面表示。<strong>尽管这些方法在三维重建方面表现良好，但它们无法恢复具有反射的物体的正确几何形状，导致表面优化模糊不清</strong>。我们的方法建立在NeuS[45]的基础上，但我们相信它可以适应任何体积神经隐式框架</p><h2 id="Modeling-for-Object-with-Reflection"><a href="#Modeling-for-Object-with-Reflection" class="headerlink" title="Modeling for Object with Reflection"></a>Modeling for Object with Reflection</h2><p>我们讨论了具有反射的物体的渲染和重建。最近的作品[3,42,55,57,43]研究了通过将场景分解为形状、反射率和照明来渲染视图依赖的反射外观，以实现新的视图合成和重照明。<strong>然而，恢复的网格没有明确验证，几何形状往往不令人满意</strong>。另一方面，重建旨在恢复显式几何，由于固有的挑战，这一领域仍未得到充分探索。例如，PM-PMVS[5]将重建任务定义为曲面几何和反射率的联合能量最小化，而nLMVS-Net[49]将MVS定义为端到端可学习网络，利用表面法线作为与视图无关的表面特征进行成本体积构建和过滤。<strong>然而，这些方法都没有将神经隐式曲面与体绘制相结合进行重建</strong>。</p><h2 id="Warping-based-Consistency-Learning"><a href="#Warping-based-Consistency-Learning" class="headerlink" title="Warping-based Consistency Learning"></a>Warping-based Consistency Learning</h2><p>基于翘曲的一致性学习广泛应用于MVS[44,48,47,54]和神经隐式表面学习[7,9]中，通过利用可微翘曲操作的图像间对应关系进行三维重建。<strong>通常，基于mvs的管道中的一致性学习是在CNN特征级进行的</strong>。例如，MVSDF[54]将预测的表面点扭曲到相邻视图，并强制逐像素特征一致性，而ACMM[47]将粗预测深度扭曲，形成多视图聚合的几何一致性成本，以细化更精细的尺度。另一方面，基于神经隐式表面的管道中的一致性学习通常在图像级别进行。NeuralWarp[7]将采样点沿射线弯曲到源图像，获得其RGB值，并与辐度网络联合优化，Geo-Neus[9]将以预测表面点为中心的灰度斑块弯曲到邻近图像，以保证多视图几何一致性。然而，它们忽略了与视图相关的辐射，并且当由于反射而导致多视图一致性不合理时受到限制，这可能导致在最小化贴片相似性时产生伪影，而不管反射如何。此外，可见性识别处理得不好，两者都依赖于繁琐的预处理来确定源图像。Alternatively，我们利用不一致性来减少高保真重建的模糊性。</p><h1 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h1><p>给定具有反射表面的物体的N幅校准多视图图像$\mathcal{X}=\left\{\mathbf{I}_{i}\right\}_{i=1}^{N}$，我们的目标是通过神经隐式表面学习来重建表面。</p><ul><li>第3.1节介绍了我们用于重建的基线Neus。</li><li>第3.2节介绍了反射感知的光度损失。它通过将渲染颜色表述为高斯分布并考虑多视图context的显式方差估计来减少反射的影响。</li><li>第3.3节讨论了我们如何识别源视图的可见性以获得无偏反射分数。</li><li>第3.4节显示，与反射方向相关的辐射一起，我们的模型通过更好的辐射场获得更好的几何形状。</li><li>最后，第3.5节给出了完整的优化。图2概述了我们的方法。</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230912150124.png" alt="image.png"></p><h2 id="Volume-Rendering-with-Implicit-Surface"><a href="#Volume-Rendering-with-Implicit-Surface" class="headerlink" title="Volume Rendering with Implicit Surface"></a>Volume Rendering with Implicit Surface</h2><p>体绘制[16]在NeRF[30]中被用于新视图合成。这个想法是用神经网络来表示3D场景的连续属性(如密度和亮度)。α合成[28]沿着射线r聚集这些属性，通过以下方式近似像素RGB值:<br>$\hat{\mathbf{C}}(\mathbf{r})=\sum_{i=1}^{P}T_{i}\alpha_{i}\mathbf{c}_{i},$ Eq.1</p><p>$T_i=\exp\left(-\sum_{j=1}^{i-1}\alpha_j\delta_j\right)$, $\alpha_i=1-\exp{(-\sigma_i\delta_i)}$分别表示采样点的透光率和alpha值<br>$δ_{i}$是相邻采样点之间的距离。P是沿一条射线采样点的个数。在位置x = (x, y, z)和视场方向$d = (θ， φ)$的条件下，神经网络预测属性$σ_{i}$和$c_{i}$。NeRF的训练对象$\mathcal{L}$是真实像素颜色C(r)与渲染颜色$\hat{\mathrm{C}}(\mathbf{r})$之间的均方误差，表示为<br>$\mathcal{L}_\mathrm{color}=\sum_{\mathbf{r}\in\mathcal{R}}|\mathrm{C}(\mathbf{r})-\hat{\mathbf{C}}(\mathbf{r})|_2^2,$ Eq.2</p><p>其中R为从相机中心到图像像素的所有光线的集合。<br>然而，基于密度的体绘制缺乏清晰的表面定义，这使得提取精确的几何形状变得困难。另外，有符号距离函数(Signed Distance Function, SDF)将曲面明确定义为零水平集，使得基于SDF的体绘制在曲面重建中更加有效。根据NeuS [45]， 3D场景的属性包括带符号的距离和亮度，由几何网络f和亮度网络c参数化:<br>$s=f(\mathbf{x}),\quad\mathbf{c}=c(\mathbf{x},\mathbf{d}),$ Eq.3</p><p>其中几何网络f将空间位置x映射到其到对象的带符号距离f (x)，而辐射网络c预测位置x和视图方向d的颜色，以模拟与视图相关的辐射。为了沿着射线r聚合采样点的带符号距离和颜色以进行像素颜色近似，我们使用了类似于NeRF的体渲染。关键的区别在于$\alpha_{i},$的公式，它是从带符号的距离f (x)而不是密度$\sigma_{i}$计算出来的<br>$\alpha_{i}=\max\left(\frac{\Phi_{s}\left(f(\mathbf{x}_{i})\right)-\Phi_{s}\left(f(\mathbf{x}_{i+1})\right)}{\Phi_{s}\left(f(\mathbf{x}_{i})\right)},0\right),$ Eq.4</p><p>$\Phi_s(x)=(1+e^{-sx})^{-1}$,1/s是一个可训练参数，表示$\Phi_s(x).$的标准差。</p><h2 id="Anomaly-Detection-for-Reflection-Score"><a href="#Anomaly-Detection-for-Reflection-Score" class="headerlink" title="Anomaly Detection for Reflection Score"></a>Anomaly Detection for Reflection Score</h2><p>对于多视图重建，多视图一致性是精确重建曲面的保证。然而，对于反射像素，几何网络经常预测模糊的表面，这破坏了多视图的一致性。为了克服这个问题，我们建议<strong>通过反射感知光度损失</strong>来减少反射表面的影响，<strong>该损失自适应地降低分配给反射像素的权重</strong>。为了实现这一点，我们首先定义<strong>反射分数</strong>，它<strong>允许我们识别反射像素</strong><br>一种naive的解决方案是将NeRF-W[27]中定义的不确定性作为反射评分。该方法将场景的radiance值建模为高斯分布，并将预测的不确定性视为方差。<strong>通过最小化高斯分布的负对数似然</strong>，大方差降低了具有高不确定性的像素的重要性。理想情况下，应该为反射像素分配较大的方差，以减弱其对重建的影响。<strong>然而，MLP学习到的隐式不确定性是在单个光线上定义的，而没有考虑多视图context。因此，如果没有明确的监督，它可能无法准确定位反射表面</strong>。<br>与NeRF-W[27]类似，我们也将渲染光线的颜色表述为高斯分布$\hat{\mathbf{C}}(\mathbf{r})\sim (\overline{\mathbf{C}}(\mathbf{r}),\overline{\mathbf{\beta}}^2(\mathbf{r}))$，其中$\mathrm{\overline{C}(r)}$和$\overline{\beta}^2(\mathbf{r})$分别是均值和方差。我们采用Eq.(1)来查询$\mathrm{\overline{C}(r)}$。然而，与NeRF-W不同的是，NeRF-W仅根据单个光线的信息定义隐式方差，我们基于多视图context显式定义方差。具体来说，<strong>我们利用多视图像素的颜色来确定相同的表面点的方差</strong>。</p><p>为了获得多视图像素颜色$\{\mathbf{C}_i(\mathbf{r})\}_{i=1}^N$，我们将曲面点x投影到所有N张图像$\{\mathbf{I}_i\}_{i=1}^N$上，并使用双线性插值得到相应的像素颜色$\{\hat{\mathbf{C}_i}(\mathbf{r})\}_{i=1}^N$，其中$\{\hat{\mathbf{C}_i}(\mathbf{r})\}_{i=1}^{N}= \left\{\mathbf{C}_i(\mathbf{r}),\{\mathbf{C}_j(\mathbf{r})\}_{j=1}^{N-1}\right\}$表示参考像素颜色和源像素颜色。为简单起见，省略下标，像素颜色C由<br>$\begin{aligned}\mathcal{G}&amp;=\mathbf{K}\cdot\left(\mathbf{R}\cdot\mathbf{x}+\mathbf{T}\right),\\\mathbf{C}&amp;=\operatorname{interp}(\mathbf{I},\mathcal{G}),\end{aligned}$ Eq.5<br>式中，interp为双线性插值，K为内标定矩阵，R为旋转矩阵，t为平移矩阵，·为矩阵乘法。<br>考虑到只有部分图像的局部区域包含反射，我们将反射定位视为异常检测问题，期望将反射表面视为异常并赋予较高的反射分数。为此，我们利用马氏距离[26]作为反射分数(即方差)，通过异常检测器经验估计出视点相关反射分数$\cdot\overline{\beta}^{2}(\mathbf{r})$，如下所示:<br>$\overline{\beta}^{2}(\mathbf{r})=\gamma\frac{1}{N-1}\sum_{j=1}^{N-1}\sqrt{\left(\mathbf{C}_{i}(\mathbf{r})-\mathbf{C}_{j}(\mathbf{r})\right)^{T}\mathbf{\Sigma}^{-1}\left(\mathbf{C}_{i}(\mathbf{r})-\mathbf{C}_{j}(\mathbf{r})\right)},$Eq.6<br>其中γ为控制反射分数尺度的尺度因子，$\Sigma^{-1}$为经验协方差矩阵。由于反射并不占大多数训练图像的主导地位，如果当前渲染的像素颜色$\mathbf{C}_i(\mathbf{r})$受到反射的污染，则由于大多数相对散度变大，生成的反射分数也会变大。<br>然后，我们通过最小化类似于NeRF-W[27]和ActiveNeRF[32]的批次$\mathcal{R}$中射线r分布的负对数似然，将Eq.(2)中的光度损失扩展为反射感知的损失，如下所示:<br>$\mathcal{L}_{\mathrm{color}}=-\log p(\hat{\mathbf{C}}(\mathbf{r}))=\sum_{\mathbf{r}\in\mathcal{R}}\frac{|\mathbf{C}(\mathbf{r})-\overline{\mathbf{C}}(\mathbf{r})|_{2}^{2}}{2\overline{\beta}^{2}(\mathbf{r})}+\frac{\log\overline{\beta}^{2}(\mathbf{r})}{2}.$ Eq.7</p><p>由于${\overline{\beta}}^{2}(\mathbf{r})$是由Eq.(6)显式估计的，而不是通过MLP隐式学习，因此它是一个常数，可以从目标函数中去除。此外，根据之前的工作[45,52,9]，我们使用L1损耗代替L2损耗进行多视图重建。最后，我们的反射感知光度损失是相当简单的公式为<br>$\mathcal{L}_{\mathrm{color}}=\sum_{\mathrm{r}\in\mathcal{R}}\frac{|\mathbf{C}(\mathbf{r})-\overline{\mathbf{C}}(\mathbf{r})|}{\overline{\beta}^2(\mathbf{r})}$ Eq.8</p><h2 id="Visibility-Identification-for-Reflection-Score"><a href="#Visibility-Identification-for-Reflection-Score" class="headerlink" title="Visibility Identification for Reflection Score"></a>Visibility Identification for Reflection Score</h2><p>使用源图像的所有像素颜色$\{\mathbf{C}_{j}(\mathbf{r})\}_{j=1}^{N-1}$计算反射分数时，假设表面上的点在所有源图像上都有有效的投影。然而，在实践中，由于自我聚焦，这种假设并不正确。如果该点在源图像中不可见，则投影的像素颜色没有意义，在式(6)中不应使用相应的像素颜色。<br>为了解决这个问题，我们设计了一个可见性识别模块，它利用中间重构网格来识别可见性，如图3所示。具体来说，给定一条射线$r_{i}$对应的像素$p_{i}$，则射线$r_{i}$上的隐式曲面可以根据采样点x的带符号距离表示为:$\hat{S}_i=\{\mathbf{x}\mid f(\mathbf{x})=0\}.$ Eq.9</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230912152320.png" alt="image.png"><br>由于一条射线上有无数个点，我们需要对这条射线上的离散点进行采样。根据采样点和它们的符号距离，我们可以通过<br>$T_i=\left\{\mathbf{x}_j\mid f\left(\mathbf{x}_j\right)\cdot f\left(\mathbf{x}_{j+1}\right)&lt;0\right\}.$ Eq.10<br>如果采样点$f(\mathbf{x}_j)$的符号与下一个采样点$f(\mathbf{x}_{j+1})$的符号不同，则区间$[\mathbf{x}_j,\mathbf{x}_{j+1}]$与曲面相交。交点集$\hat{S}_{i}$可由线性插值得到<br>$\hat{S}_i=\left\{\mathbf{x}\mid\mathbf{x}=\frac{f(\mathbf{x}_j)\mathbf{x}_{j+1}-f(\mathbf{x}_{j+1})\mathbf{x}_j}{f(\mathbf{x}_j)-f(\mathbf{x}_{j+1})},\mathbf{x}_j\in T_i\right\}$ Eq.11</p><p>实际上，射线$r_{i}$可以在多个表面与物体相交。对于我们的反射分数计算，只有第一个交集是有意义的，它被表示为<br>$\mathbf{x}_i^<em>=\operatorname{argmin}\mathcal{D}(\mathbf{x},\mathbf{o}_i),$ Eq.12<br>$\mathbf{x}\in\hat{S}_{i}$和$\mathcal{D}(\cdot,\mathbf{o}_i)$分别表示点x和射线原点$r_{i}$之间的距离。<br>捕获预测的曲面点$\mathrm{x}_i^</em>$后，我们可以计算该点与所有源相机位置$\{\mathbf{o}_{j}\}_{j=1}^{N-1}$之间的距离$\left\{d_j^<em>\right\}_{j=1}^{N-1}$，如下所示:<br>$d_j^</em>=\frac{\mathbf{x}_i^<em>-\mathbf{o}_j}{\mathrm{norm}(\mathbf{x}_i^</em>-\mathbf{o}_j)},$ Eq.13<br>其中norm表示将向量转换为单位向量的归一化操作。同时，我们通过光线投射计算所有源摄像机位置$\{\mathbf{o}_{j}\}_{j=1}^{N-1}$到中间重建网格的第一个交叉点的距离$\{d_j\}_{j=1}^{N-1}$[37]。基于这两个距离，源图像的可见性$\mathrm{I}_{j}$由<br>$v_j=\mathbb{I}(d_j^*\leq d_j)$ Eq.14<br>其中$\mathbb{I}(\cdot)$为指示函数。在近似可见性的基础上，我们剔除公式(6)中用于计算反射分数的不可见像素颜色$\{\mathbf{C}_j(\mathbf{r})\mid v_j=0\}$，然后将反射分数细化如下:<br>$\overline{\beta}^2(\mathbf{r})=\gamma\frac{1}{\sum_{j=1}^{N-1}v_j}\sum_{j=1}^{N-1}v_j\text{Mdis},$<br>$\mathbf{Mdis}=\sqrt{\left(\mathbf{C}_{i}(\mathbf{r})-\mathbf{C}_{j}(\mathbf{r})\right)^{T}\mathbf{\Sigma}^{-1}\left(\mathbf{C}_{i}(\mathbf{r})-\mathbf{C}_{j}(\mathbf{r})\right)}.$ Eq.15</p><p>我们在图4中提供了一些例子来说明估计的反射得分。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230912153803.png" alt="image.png"><br><em>反射分数的可视化，可以定位反射面。黑色表示高分</em></p><h2 id="Reflection-Direction-Dependent-Radiance"><a href="#Reflection-Direction-Dependent-Radiance" class="headerlink" title="Reflection Direction Dependent Radiance"></a>Reflection Direction Dependent Radiance</h2><p>如Ref-NeRF[43]所示，在反射场景中，根据反射方向调节亮度可以获得更精确的亮度场，这已被证明有利于在NeuralWarp中进行重建[7]。受此启发，我们将辐射网络重新参数化为表面法线反射方向的函数，公式(3)为<br>$\mathbf{c}=c(\mathbf{x},\hat{\mathbf{d}}),$ Eq.16<br>reflection direction: $\hat{\mathbf{d}}=2(-\mathbf{d}\cdot\mathbf{\hat{n}})\mathbf{\hat{n}}+\mathbf{d},$ Eq.17<br>surface normal: $\hat{\mathbf{n}}=\frac{\nabla f(\mathbf{x})}{||\nabla f(\mathbf{x})||}.$ Eq.18</p><p>与Ref-NeRF[43]相比，<strong>我们的框架中的反射方向更加精确，因为表面法线估计得很好，这导致了更精确的辐射场</strong>。与在其框架中忽略反射的NeuralWarp[7]相比，我们考虑了与视图相关的亮度，并估计了更准确的亮度场。结果表明，该方法对于具有反射的物体的多视图重建更为可靠和有前景</p><h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p>损失函数：<br>$\mathcal{L}=\mathcal{L}_{\mathrm{color}}+\alpha\mathcal{L}_{\mathrm{eik}}.$</p><ul><li>反射感知光度损失$\mathcal{L}_{\mathrm{color}}$ Eq.8</li><li>$\mathcal{L}_{\mathrm{eik}}=\frac{1}{P}\sum_{i=1}^{P}\left(\left|\nabla f\left(x_{i}\right)\right|-1\right)^{2}.$</li><li>$\alpha = 0.1$</li></ul><h1 id="Experimetns"><a href="#Experimetns" class="headerlink" title="Experimetns"></a>Experimetns</h1><p>Datasets</p><ul><li>Shiny Blender</li><li>Blender</li><li>SLF</li><li>Bag of Chips</li></ul><p>Evaluation Protocol.</p><ul><li>Chamfer Distance</li><li><strong>Shiny Blender</strong><ul><li>MAE: mean angular error</li><li>PSNR</li></ul></li></ul><p>Implementation Details </p><ul><li>same as NeuS</li><li>3090Ti 7h</li></ul><p>Comparison with State-of-the-Art Methods</p><ul><li>IDR、UNISURF、VolSDF、NeuS</li><li>NeuralWarp、Geo-NeuS</li><li>Ref-NeRF、PhySG<ul><li>quantitative results</li><li>qualitatively</li></ul></li></ul><p>Ablation Study</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Shadow&amp;Highlight </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neus </tag>
            
            <tag> Shadow&amp;Highlight </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IDR</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/IDR/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/IDR/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance</th></tr></thead><tbody><tr><td>Author</td><td>Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, Yaron Lipman</td></tr><tr><td>Conf/Jour</td><td>NeurIPS</td></tr><tr><td>Year</td><td>2020</td></tr><tr><td>Project</td><td><a href="https://lioryariv.github.io/idr/">Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance (lioryariv.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4545153915959271425&amp;noteId=1815797245432115712">Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230906183157.png" alt="image.png"></p><p>端到端的IDR：可以从masked的2D图像中学习3D几何、外观，<em>允许粗略的相机估计</em></p><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们介绍了隐式可微分渲染器(IDR)，这是一个端到端的神经系统，可以从masked 2D图像和噪声相机初始化中学习3D几何、外观和相机。仅考虑粗略的相机估计允许在现实场景中，准确的相机信息不可用稳健的3D重建。<br>我们的方法的一个<strong>限制</strong>是，它<strong>需要一个合理的相机初始化</strong>，不能工作在随机相机初始化。<br>未来有趣的工作是</p><ul><li>将IDR与直接从图像中预测相机信息的神经网络结合起来。</li><li>另一个有趣的未来工作是进一步将表面光场(公式5中的M0)分解为材料(BRDF, B)和场景中的光(Li)。</li></ul><p>最后，我们希望将IDR整合到其他计算机视觉和学习应用中，例如3D模型生成，以及从野外图像中学习3D模型。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>在这项工作中，我们解决了多视图三维表面重建的挑战性问题。我们引入了一个神经网络架构，它可以<strong>同时学习未知的几何形状、相机参数和一个神经渲染器，它可以近似地从表面反射到相机的光</strong>。几何图形表示为神经网络的零水平集，而从渲染方程导出的神经渲染器能够(隐式地)对各种照明条件和材料进行建模。我们在来自DTU MVS数据集的具有不同材质属性、光照条件和噪点相机初始化的物体的真实世界2D图像上训练我们的网络。我们发现我们的模型可以产生高保真度、分辨率和细节的最先进的3D表面重建。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>NeRF：基于NN的方法可以从2D图像中学习3D形状</p><ul><li>Differential rendering: 基于ray casting/tracing或rasterization光栅化</li><li>3D geometry represent : pointcloud , triangle meshes , implicit representations defined over volumetric grids, <strong>neural implicit representations</strong></li></ul><p><strong>neural implicit representations</strong>主要优点是它们在表示任意形状和拓扑的表面方面的灵活性，以及无网格性(即，没有固定的先验离散化，如体积网格或三角形网格)。<strong>but</strong>: 到目前为止，具有隐式神经表征的可微渲染系统[30,31,40]并没有纳入在图像中产生faithful<strong>3D几何外观</strong>所需的照明和反射特性，也没有处理可训练的摄像机位置和方向。</p><p>目标是设计一个<strong>end-to-end neural architecture system</strong>该系统可以从masked的2D图像和粗略的相机估计中学习3D几何形状，并且不需要额外的监督。为了实现这一目标，我们将像素的颜色表示为场景中三个未知数的可微分函数:<strong>几何、外观和相机</strong>。在这里，外观是指定义表面光场的所有因素的总和，不包括几何形状，即表面双向反射分布函数(BRDF)和场景的照明条件。我们称这种架构为<strong>隐式可微分渲染器</strong>(IDR)。我们表明，IDR能够近似从3D形状反射的光，3D形状表示为神经网络的零水平集。该方法可以处理某一限定族的表面外观，即所有表面光场都可以表示为表面上的点、其法线和观测方向的连续函数。此外，将<strong>全局形状特征向量</strong>合并到IDR中可以增加其处理更复杂外观(例如，间接照明效果)的能力。</p><p>与我们的论文最相关的是DVR[40]，该论文首先引入了隐式神经占用函数的完全可微渲染器Occupy Network[37]，这是上文定义的隐式神经表示的一个特定实例。虽然他们的模型可以表示任意的颜色和纹理，但它不能处理一般的外观模型，也不能处理未知的、有噪声的相机位置。例如，我们表明[40]中的模型以及其他几个基线无法生成Phong反射模型[8]。<strong>此外，我们通过实验证明，IDR可以从2D图像以及精确的相机参数中产生更精确的3D形状重建</strong>。值得注意的是，虽然基线在高光场景中经常产生形状伪影，但IDR对这种照明效果具有鲁棒性。我们的代码和数据可在<a href="https://github.com/lioryariv/idr上获得。">https://github.com/lioryariv/idr上获得。</a></p><p>贡献：</p><ul><li>端到端架构，处理未知的几何形状、外观和相机。</li><li>表达<strong>神经隐式曲面对摄像机参数的依赖关系</strong>。</li><li>从现实生活中的2D图像，通过精确和noise的相机信息，产生具有广泛外观的不同物体的最先进的3D表面重建。</li></ul><h2 id="Previous-work"><a href="#Previous-work" class="headerlink" title="Previous work"></a>Previous work</h2><p>用于学习几何的可微渲染系统(主要)有两种风格:<strong>可微光栅化</strong>[32,23,10,29,4]和<strong>可微光线投射</strong>。由于目前的工作属于第二类，我们首先集中讨论这一类工作。然后介绍了多视图曲面重建和神经视图合成的相关工作。</p><p><strong>Implicit surface differentiable ray casting</strong><br>可微光线投射主要用于隐式形状表示，如<strong>在体积网格上定义的隐式函数或隐式神经表示</strong>，其中隐式函数可以是占位函数[37,5]，有符号距离函数(SDF)[42]或任何其他有符号隐式[2]。</p><ul><li>在一项相关工作中，[20]SDFDiff使用体积网格来表示SDF并实现光线投射可微分渲染器。它们近似于每个体积单元中的SDF值和表面法线。</li><li>[31]DIST使用预训练的DeepSDF模型[42]的球体跟踪，并通过区分球体跟踪算法的各个步骤来近似深度梯度与DeepSDF网络的潜在代码;</li><li>[30]使用现场探测来促进可微射线投射。与这些作品相比，<strong>IDR利用了精确可微的曲面点和隐曲面的法线，并考虑了更一般的外观模型，并处理了噪声相机</strong>。</li></ul><p><strong>Multi-view surface reconstruction</strong><br>在图像的捕获过程中，深度信息会丢失。假设已知摄像机，经典的多视点立体(Multi-View Stereo, MVS)方法[9,48,3,54]试图通过匹配视图间的特征点来重现深度信息。<strong>然而，要产生有效的三维水密表面重建，需要深度融合[6,36]的后处理步骤，然后是泊松表面重建算法[24]</strong>。</p><ul><li>最近的方法使用场景集合来训练深度神经模型，用于MVS管道的子任务，例如特征匹配[27]或深度融合[7,44]，或用于端到端MVS管道[16,56,57]。当相机参数不可用时，给定一组来自特定场景的图像，应用运动结构(SFM)方法[51,47,22,19]来重现相机并进行稀疏的3D重建。</li><li>Tang和Tan[53]BA-Net使用具有集成可微束调整[55]层的深度神经结构提取参考帧深度的线性基础，并从附近图像中提取特征，并优化每个前向通道的深度和相机参数。<strong>与这些工作相反，IDR使用来自单个目标场景的图像进行训练，从而产生精确的水密3D表面重建</strong>。</li></ul><p><strong>Neural representation for view synthesis</strong>.<br>最近的工作训练神经网络从已知相机的有限图像集预测 3D 场景或对象的新视图和一些几何表示。</p><ul><li>[50]SRN使用 LSTM 对场景几何进行编码以模拟光线行进过程。</li><li>[38]NeRF使用神经网络来预测体积密度和视相关的发射辐射，从一组具有已知相机的图像中合成新的视图。</li><li>[41]Learning Implicit Surface Light Fields使用神经网络从输入图像和几何中学习表面光场，并预测未知视图和/或场景照明。<strong>与IDR不同的是，这些方法不会产生场景几何的三维表面重建，也不处理未知的相机</strong></li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>我们的目标是从具有可能粗糙或嘈杂的相机信息的masked 2D 图像中重建物体的几何形状。有三个未知数:<br>(i)几何，由参数$\theta\in\mathbb{R}^m$表示;<br>(ii)外观，用$\gamma\in\mathbb{R}^n$表示;<br>(iii)由$\tau\in\mathbb{R}^k.$表示的摄像机。符号和设置如图 2 所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230906183258.png" alt="image.png"></p><p>我们将几何图形表示为神经网络 (MLP) f 的零水平集，$\mathcal{S}_\theta=\left\{x\in\mathbb{R}^3\mid f(\mathbf{x};\theta)=0\right\},$ Eq.1</p><p>可学习参数$\theta\in\mathbb{R}^m$。为了避免任何地方 0 解，f 通常进行正则化 [37, 5]。我们选择 f 将符号距离函数 (SDF) 建模为其零水平集 $s_{\theta}$[42]。我们使用隐式几何正则化 (IGR) [11] 强制执行 SDF 约束，稍后详细介绍。<br>SDF在我们的上下文中有两个好处：首先，它允许使用球体跟踪算法进行有效的光线投射[12，20]；其次，IGR具有有利于光滑和逼真的表面的隐式正则化。</p><p><strong>IDR forward model.</strong><br>给定一个由 p 索引的像素，与某个输入图像相关联，让 $R_p(\tau)=\{c_p+t\mathbf{v}_p\mid t\geq0\}$ 通过像素 p 表示射线，其中 $c_p=c_p(\tau)$ 表示相应相机的未知中心，$\mathbf{v}_p=\mathbf{v}_p(\tau)$ 射线的方向（即从$c_{p}$指向像素p的向量）。令$\hat{\mathbf{x}}_{p}=\hat{\mathbf{x}}_{p}(\mathbf{\theta},\tau)$表示射线 $R_{p}$ 和表面$\mathcal{S}_{\theta}$ 的第一个交集。沿$R_{p}$的传入射线，它确定像素 $L_{p}=L_{p}(\theta,\gamma,\tau)$的呈现颜色，是$\hat{x}_{p}$处表面属性的函数，$\hat{x}_{p}$处的传入辐射，以及观看方向 $v_{p}$。反过来，我们假设表面属性和传入的辐射是表面点$\hat{x}_{p}$的函数，及其对应的表面正态$\hat{\mathbf{n}}_{p}=\hat{\mathbf{n}}_{p}(\theta),$查看方向$v_{p}$和全局几何特征向量$\hat{\mathbf{z}}_p=\hat{\mathbf{z}}_p(\hat{\mathbf{x}}_p;\theta)$。因此，IDR 前向模型是：</p><p>$L_p(\theta,\gamma,\tau)=M(\hat{x}_p,\hat{n}_p,\hat{z}_p,v_p;\gamma),$ Eq.2</p><p>其中 M 是第二个神经网络 (MLP)。我们在比较 $L_{p}$和像素输入颜色$I_{p}$的损失中使用 $L_{p}$ 来同时训练模型的参数 $\theta,\gamma,\tau.$。接下来，我们提供有关等式 2 中模型不同组件的更多详细信息。</p><h2 id="Differentiable-intersection-of-viewing-direction-and-geometry"><a href="#Differentiable-intersection-of-viewing-direction-and-geometry" class="headerlink" title="Differentiable intersection of viewing direction and geometry"></a>Differentiable intersection of viewing direction and geometry</h2><p>此后（直到第 3.4 节），我们假设一个固定的像素 p，并删除下标 p 符号以简化符号。第一步是将交点 $\hat{\mathbf{x}}(\theta,\tau):$表示为参数为 θ, τ 的神经网络。这可以通过对几何网络 f 稍作修改来完成。</p><p>令$\hat{\mathbf{x}}(\theta,\tau)=c+t(\theta,c,v)\mathbf{v}$表示交点。由于我们的目标是在类似梯度下降的算法中使用 $\hat{x}$，我们需要确保我们的推导在当前参数的值和一阶导数中是正确的，用 $\theta_0,\tau_0$表示；因此，我们将$c_{0}=c(\tau_{0}),$ $v_0=v(\tau_0),$ $t_0=t(\theta_0,c_0,v_0),$ $\mathrm{and~}$ $x_0=\hat{x}(\theta_0,\tau_0)=c_0+t_0v_0.$</p><p>设$S_{\theta}$定义为式1。射线$R(\tau)$与表面$\mathcal{S}_{\theta}$的交点可以用公式表示<br>$\hat{x}(\theta,\tau)=c+t_0\mathbf{v}-\frac v{\nabla_xf(x_0;\theta_0)\cdot v_0}f(\mathbf{c}+t_0\mathbf{v};\theta),$ Eq.3<br>在$\theta=\theta_{0}.$和$\tau=\tau_{0}.$时，θ和τ的值和一阶导数是准确的。</p><p>为了证明$\hat{x}$对其参数的功能依赖性，我们使用隐式微分 [1, 40]，即区分方程 $f(\hat{\mathbf{x}};\theta)\equiv0\text{ w.r.t. }v,c,\theta$并求解 t 的导数。然后，可以检查等式 3 中的公式具有正确的导数。更多细节在补充材料中。<strong>我们将方程 3 实现为神经网络</strong>，即我们添加了两个线性层（参数为 c, v）：<br>one before and one after the MLP f .等式 3 将 [1] 中的样本网络公式和 [40]DVR 中的可微深度统一起来，并将它们推广到考虑未知相机。$\hat{\mathbf{x}}$处的${\mathcal{S}}_{\theta}$的法向量可由下式计算:</p><p>$\hat{n}(\theta,\tau)=\nabla_{\mathbf{x}}f(\hat{x}(\theta,\tau),\theta)/\left|\nabla_{\mathbf{x}}f(\hat{\mathbf{x}}(\theta,\tau),\theta)\right|_{2}.$ Eq.4</p><p>请注意，对于 SDF，分母为 1，因此可以省略</p><h2 id="Approximation-of-the-surface-light-field"><a href="#Approximation-of-the-surface-light-field" class="headerlink" title="Approximation of the surface light field"></a>Approximation of the surface light field</h2><p>表面光场辐射 L 是从$\mathcal{S}_{\theta}$在$\hat{x}$处反射的光量，方向为 -v 到达 c。它由两个函数决定：描述表面的反射率和颜色属性的双向反射率分布函数 (BRDF) 和场景中发出的光（即光源）。</p><p>BRDF函数$B(x,\mathbf{n},\mathbf{w}^{o},\mathbf{w}^{i})$描述了反射亮度(即光通量)在某些波长(即颜色)下的比例，相对于从方向$w^{i}$的入射辐射的方向$w^{o}$处法向n离开表面点x。我们让 BRDF 也依赖于一个点处的法线到表面。场景中的光源由函数 $L^e(x,\mathbf{w}^o)$ 描述，该函数测量点 x 处某个波长的光在方向 $w^{o}$处的发射辐射。方向v到达c的光量等于方向$\mathbf{w}^o=-\mathbf{\upsilon}$中$\hat{x}$反射的光量，用所谓的渲染方程[21,17]描述:<br>$L(\hat{x},w^o)=L^e(\hat{x},w^o)+\int_{\Omega}B(\hat{x},\hat{n},w^i,w^o)L^i(\hat{x},w^i)(\hat{n}\cdot w^i)dw^i=M_0(\hat{x},\hat{n},v),$ Eq.5</p><p>其中$L^i(\hat{\mathbf{x}},\mathbf{w}^i)$编码方向$w^{i},$$\hat{x}$处的入射辐射，项$\hat{n}\cdot w^{i}$补偿光不正交地撞击表面的事实；Ω是以$\hat{n}$为中心的半球体。函数 $M_{0}$ 将表面光场表示为局部表面几何 $\hat{\mathbf{x}},\hat{\mathbf{n}},$和观察方向 v 的函数。该渲染方程适用于每个光波长；如后面所述，我们将将其用于红色、绿色和蓝色 (RGB) 波长。<br>我们将注意力限制在可以用连续函数 M0 表示的光场上。我们用 $\mathcal{P}=\{M_{0}\}$表示这种连续函数的集合（有关 P 的更多讨论，请参阅补充材料）。用(足够大的)MLP近似M(神经渲染器)替换$M_0$提供了光场近似:<br>$L(\theta,\gamma,\tau)=M(\hat{x},\hat{n},v;\gamma).$ Eq.6</p><p>几何和外观的解纠缠需要可学习的M来近似所有输入x, n, v的$M_0$，而不是记忆特定几何的亮度值。给定任意选择光场函数$M_{o}\in\mathcal{P}$，存在权重$\gamma=\gamma_{0}$的选择，使得M对于所有x, n, v(在某些有界集合中)近似于$M_0$。这可以通过 MLP 的标准普适定理来证明（详见补充）。然而，M 可以学习正确的光场函数 $M_0$ 的事实并不意味着在优化过程中保证学习它。然而，能够近似任意 x, n, v 的 M0 是解开几何（用 f 表示）和外观（用 M 表示）的必要条件。我们将此必要条件命名为 P-university。</p><p><strong>Necessity of viewing direction and normal</strong></p><p>对于 M 能够表示从表面点 x 反射的正确光，即 P-universal，它必须作为参数 v, n 接收。即使我们期望 M 适用于固定几何形状，观察方向 v 是必要的；例如，用于模拟镜面反射。另一方面，正常的 n 可以被 M 记忆为 x 的函数。然而，为了解开几何，即允许 M 独立于几何学习外观，还需要结合法线方向。这可以在图 3 中看到：</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230906192338.png" alt="image.png"></p><p>没有法线信息的渲染器M将在情况(A)和(b)中产生相同的光估计，而没有观看方向的渲染器M将在情况(A)和(c)中产生相同的光估计。在补充中，我们提供了这些渲染器在Phong反射模型下无法产生正确亮度的详细信息[8]。先前的作品，例如[40]，已经考虑了$L(\theta,\gamma)=M(\hat{x};\gamma).$如上所述，从M中省略n和/或v将导致non-P-universal。<strong>在实验部分，我们证明了在渲染器M中加入n确实可以成功地解除几何和外观的纠缠，而忽略它则会损害解除纠缠</strong>。</p><p><strong>Accounting for global light effects</strong>.</p><p>P-universal是学习一个可以从集合P中模拟外观的神经渲染器M的必要条件。然而，P不包括二次照明和自阴影等全局照明效果。我们通过引入一个全局特征向量来进一步提高IDR的表达能力。这个特征向量允许渲染器对几何$S_{\theta}$进行全局推理。为了得到向量z，我们将网络f扩展如下$F(x;\theta)=\left[f(x;\theta),z(x;\theta)\right]\in\mathbb{R}\times\mathbb{R}^{\ell}.$。一般来说，z可以编码相对于表面样本x的几何$S_{\theta}$;Z被馈送到渲染器中:$\hat{\mathbf{z}}(\theta,\tau)=\mathbf{z}(\hat{\mathbf{x}};\theta)$，以考虑与当前感兴趣的像素p相关的表面样本$\hat{\mathbf{x}}$。我们现在已经完成了IDR模型的描述，如公式2所示。</p><h2 id="Masked-rendering"><a href="#Masked-rendering" class="headerlink" title="Masked rendering"></a>Masked rendering</h2><p>另一种用于重建3D几何体的2D监督类型是遮罩;掩码是二值图像，表示对于每个像素p，感兴趣的对象是否占用该像素。掩码<strong>可以在数据中提供</strong>(如我们假设的那样)，<strong>也可以使用掩码或分割算法计算</strong>。我们想考虑以下指示函数来识别某个像素是否被渲染对象占用(记住我们假设某个固定像素p):</p><p>$S(\theta,\tau)=\begin{cases}1&amp;R(\tau)\cap\mathcal{S}_\theta\neq\emptyset\\0&amp;\text{otherwise}\end{cases}$<br>由于这个函数在θ上不可微，在τ上也不可连续，我们使用一个几乎处处可微的近似:<br>$S_\alpha(\theta,\tau)=\text{sigmoid}\left(-\alpha\min_{t\geq0}f(c+t\mathbf{v};\theta)\right),$ Eq.7</p><p>其中α &gt; 0是一个参数。<strong>由于按照惯例，几何内部f &lt; 0，外部f &gt; 0</strong>，因此可以证明$\begin{aligned}S_\alpha(\theta,\tau)\xrightarrow{\alpha\to\infty}S(\theta,\tau)\end{aligned}$。请注意，微分方程7w.r.t. ${\mathbf{c}},v$可以使用包络定理来完成，即$\partial_{\mathbf{c}}\operatorname{min}_{t\geq0}f(\mathbf{c}+t\mathbf{v};\theta)=\partial_{\mathbf{c}}f(\mathbf{c}+t_{x}\mathbf{v};\theta),$<br>其中$t_{x}$是达到最小值的参数，即$f(c_0+t_{x}v_0;\theta)=\min_{t\geq0}f(c_0+tv_0;\theta)$，$∂_{v}$也是类似的。因此，我们将$S_{\alpha}$实现为神经网络$\text{sigmoid}(-\alpha f(\mathbf{c}+t_{x}\mathbf{v};\theta)).$。注意这个神经网络在$c = c_{0}$和$v = v_{0}$处有精确的值和一阶导数。</p><h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p>设$\begin{aligned}I_p\in[0,1]^3,O_p\in\{0,1\}\end{aligned}$为相机$c_{p}(\tau)$和方向$v_{p}(\tau)$拍摄的图像中像素p对应的RGB和掩码值(resp.)，其中$p∈P$表示图像输入集合中的所有像素，$\tau\in\mathbb{R}^k$表示场景中所有相机的参数。损失函数的形式是<br>$\mathrm{loss}(\theta,\gamma,\tau)=\mathrm{loss}_{\mathrm{RGB}}(\theta,\gamma,\tau)+\rho\mathrm{loss}_{\mathrm{MASK}}(\theta,\tau)+\lambda\mathrm{loss}_{\mathrm{E}}(\theta)$ Eq.8</p><p>我们在P的小批量像素上训练这个损失;为了使记号简单，我们用P表示当前的小批。对于每个$p∈P$，我们使用球体跟踪算法[12,20]来计算射线$R_p(\tau)$与$S_θ$的第一个交点$c_p+t_{p,0}v_p,$设$P^{\mathrm{in}}\subset P$是像素P的子集，其中存在交集且$O_p=1.\text{ Let }L_p(\theta,\gamma,\tau)=M(\hat{x}_p,\hat{n}_p,\hat{z}_p,v_p;\gamma),$其中，$\hat{x}_{p},\hat{n}_{p}$定义为式3和式4,$\hat{\mathbf{z}}_{p}=\hat{\mathbf{z}}(\hat{x}_{p};\theta)$如3.2节和公式2所示。<br>RGB损失为<br>$\mathrm{loss}_{\mathrm{RGB}}(\theta,\gamma,\tau)=\frac{1}{|P|}\sum_{p\in P^{\mathrm{in}}}\left|I_{p}-L_{p}(\theta,\gamma,\tau)\right|,$ Eq.9</p><p>$P^\mathrm{out}=P\setminus P^\mathrm{in}$表示小批中没有射线几何相交或$O_p = 0$的指标。掩膜损失是<br>$\mathrm{loss}_{\mathrm{MASK}}(\theta,\tau)=\frac1{\alpha|P|}\sum_{p\in I^{\mathrm{pout}}}\mathrm{CE}(O_{p},S_{p,\alpha}(\theta,\tau)),$ Eq.10<br>CE是交叉熵损失</p><p>最后，我们通过隐式几何正则化(IGR)使f近似为带符号距离函数[11]，即结合Eikonal正则化<br>$\mathrm{loss}_{\mathrm{E}}(\theta)=\mathbb{E}_{\mathbf{x}}\big(\left|\nabla_{\mathbf{x}}f(x;\theta)\right|-1\big)^2$ Eq.11, 其中x均匀分布在场景的边界框中。</p><p><strong>Implementation details</strong></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Multiview-3D-reconstruction"><a href="#Multiview-3D-reconstruction" class="headerlink" title="Multiview 3D reconstruction"></a>Multiview 3D reconstruction</h2><p>Dataset</p><ul><li>DTU</li><li>MVS</li></ul><p>Evaluation</p><ul><li>PSNR</li><li>Chamfer-$L_1$</li><li>compare with DVR、Colmap、Furu<ul><li>Quantitative results</li></ul></li></ul><p>Small number of cameras.</p><h2 id="Disentangling-geometry-and-appearance"><a href="#Disentangling-geometry-and-appearance" class="headerlink" title="Disentangling geometry and appearance"></a>Disentangling geometry and appearance</h2><h2 id="Ablation-study"><a href="#Ablation-study" class="headerlink" title="Ablation study"></a>Ablation study</h2>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Geo-Neus</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Geo-Neus/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Geo-Neus/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for Multi-view Reconstruction</th></tr></thead><tbody><tr><td>Author</td><td>Fu, Qiancheng and Xu, Qingshan and Ong, Yew-Soon and Tao, Wenbing</td></tr><tr><td>Conf/Jour</td><td>NeurIPS</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://github.com/GhiXu/Geo-Neus">GhiXu/Geo-Neus: Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for Multi-view Reconstruction (NeurIPS 2022) (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4629958250540843009&amp;noteId=1943200084633808128">Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for Multi-view Reconstruction (readpaper.com)</a></td></tr></tbody></table></div><p>几何先验：使用COLMAP产生的稀疏点来作为SDF的显示监督—&gt;可以捕获强纹理的复杂几何细节<br>具有多视图立体约束的隐式曲面上的几何一致监督—&gt;大面积的光滑区域</p><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了 Geo-Neus，这是一种通过执行显式 SDF 优化来执行神经隐式表面学习的新方法。在本文中，我们首先提供了理论分析，即体绘制集成和神经SDF学习之间存在差距。有了这个理论支持，我们建议<strong>通过引入两个多视图几何约束来显式优化神经 SDF 学习</strong>：来自SFM的稀疏 3D 点和多视图立体中的光度一致性。<br>通过这种方式，Geo-Neus 在复杂的薄结构和大的光滑区域生成高质量的表面重建。因此，它大大优于最先进的技术，包括传统和神经隐式表面学习方法。<strong>我们注意到，虽然我们的方法大大提高了重建质量，但其效率仍然有限</strong>。未来，通过超快逐场景辐射场优化方法[22，32]探索通过体绘制加速神经隐式表面学习将是有趣的。我们没有看到我们工作的直接负面社会影响，但准确的 3D 模型可以从malevolence.中使用。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>近年来，通过体绘制的神经隐式曲面学习成为多视图重建的热门方法。然而，一个关键的挑战仍然存在: <strong>现有的方法缺乏明确的多视图几何约束，因此通常无法生成几何一致的表面重建</strong>。为了解决这一挑战，我们提出了几何一致的神经隐式曲面学习用于多视图重建。我们从理论上分析了体绘制积分与基于点的有符号距离函数(SDF)建模之间的差距。为了弥补这一差距，我们直接定位SDF网络的零级集，并通过利用多视图立体中的结构来自运动的稀疏几何(SFM)和光度一致性显式地执行多视图几何优化。这使得我们的SDF优化无偏，并允许多视图几何约束专注于真正的表面优化。大量实验表明，我们提出的方法在复杂薄结构和大面积光滑区域都能实现高质量的表面重建，从而大大优于目前的技术水平。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>传统方法管线：需要深度图或点云来生成表面网格。这些中间表示不可避免地为最终重建的几何引入累积误差。</li><li>新的方法：从图像中直接重建曲面，有可能减轻累积误差并产生高质量的重建。为了实现这一点，现有的方法将表面表示为神经隐式表示，并利用体积渲染[19]来优化它们。</li></ul><p>受神经体绘制[21,42]同时从输入图像中学习体密度和辐射场的启发，最近的作品[33,39]使用有符号距离函数(SDF)[25]进行表面表示，并引入SDF诱导的密度函数，使体绘制能够学习隐式的SDF表示。<strong>本质上，这些工作仍然侧重于通过体绘制积分直接进行色场建模，而不是显式的多视图几何优化。因此，现有的方法通常不能产生几何一致的表面重建</strong><br>直观上，体绘制沿着每条光线对多个点进行采样，并将输出的像素颜色表示为亮度场的积分，或沿光线采样颜色的加权和(参见图1(a))。这意味着体绘制积分直接优化几何积分，而不是沿着射线的单一曲面相交。<strong>这显然引入了几何建模的偏差，从而阻碍了真正的表面优化</strong>。</p><p>在图1(b)中，我们展示了NeuS的重建案例[33]，其中可以直观地观察到渲染颜色与物体几何形状之间的偏差。渲染的颜色由颜色网络通过体绘制获得。表面颜色由SDF值为零的表面的预测颜色形成。<strong>可以很容易地看到，渲染颜色和表面颜色之间存在差距</strong>。因此尽管高质量的渲染图像，重建的表面是不精确的，表明色彩渲染和隐式几何之间的bias。(详细的理论分析将在后面阐述)。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230911164541.png" alt="image.png"></p><p>为了解决上述问题，我们提出了Geo-Neus，设计一个明确和精确的神经几何优化模型，用于几何一致的神经隐式曲面的体绘制学习，从而实现更好的多视图三维重建。<strong>具体而言，我们直接定位SDF网络的零水平集，并通过利用多视图立体中的结构运动稀疏几何(SFM)和光度一致性明确地进行多视图几何优化</strong>。<br>这种模式有几个好处。</p><ul><li>首先，直接定位SDF网络的零水平集保证了我们的几何建模是无偏的。这使我们的方法能够专注于真正的表面优化。</li><li>其次，我们证明了在SDF网络的定位零水平集上显式地强制多视图几何约束使我们的方法能够生成几何一致的表面重建。以往的神经隐式曲面学习主要利用渲染损失来隐式优化SDF网络。这导致了训练优化过程中的几何模糊。<strong>我们引入的两种类型的明确的多视图约束鼓励我们的SDF网络推理正确的几何形状，包括复杂的薄结构和大的光滑区域</strong>。</li></ul><p>综上所述，我们的贡献是:<br>1)我们从理论上分析了体绘制积分和基于点的SDF建模之间存在差距。这表明直接监督SDF网络是促进神经内隐表面学习的必要条件。<br>2)在理论分析的基础上，提出直接定位SDF网络的零水平集，利用多视图几何约束明确监督SDF网络的训练。通过这种方式，SDF网络被鼓励专注于真正的表面优化。<br>大量的实验进一步验证了我们的理论分析和提出的SDF网络直接优化的有效性。我们的研究表明，我们提出的Geo-Neus能够重建复杂的薄结构和大的光滑区域。因此，它大大优于目前最先进的表面重建方法，包括传统方法和神经隐式表面学习方法。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p><strong>Traditional multi-view 3D reconstruction</strong></p><p>传统的多视图三维重建是多视图图像表面重建的经典流水线。在给定多视图输入图像的情况下，传统的多视图三维重建使用<strong>SFM (structure from motion)</strong>[27,31]来提取和匹配相邻视图的特征，并估计相机参数和稀疏的三维点。然后，应用<strong>多视图立体(multi-view stereo, MVS)</strong>[8,28,34,35]估计每个视图的密集深度图，然后将所有深度图融合成密集点云。最后，采用表面重建方法[6,13,15]，如筛选泊松表面重建<strong>screened Poisson Surface Reconstruction</strong>[13]，从点云重建表面。传统的方法在各种场合都取得了很大的成功，<strong>但由于它们的多个中间步骤没有构成一个整体，在某些情况下存在表面不完备性</strong>。随着深度学习的发展，人们对基于学习的多视图重建进行了很多尝试[12,20,25,36,37]，但问题依然存在。</p><p><strong>Implicit representation of surface</strong>.<br>根据表面的表示形式，表面重建方法一般可分为显式方法和隐式方法。显式表示包括体素[5,29]和三角网格[3,4,14]，它们受到分辨率的限制。<strong>隐式表示法使用隐式函数来表示曲面，因此是连续的</strong>。可以使用隐式函数在任意分辨率下提取曲面。传统的重建方法，如筛选泊松曲面重建[13]，使用基本函数构成隐函数。在基于学习的方法中，最常用的形式是占用函数[20,26]和以网络为代表的有符号距离函数(SDF) [25]</p><p><strong>Neural implicit surface reconstruction</strong>.</p><p>神经隐式field是一种表示物体几何形状的新方法。随着NeRF[21]首次在新颖视图合成中使用多层感知机(Multi-Layer Perceptron, MLP)代表的神经辐射场，大量使用神经网络来表示场景的作品[16,18,30]层出不穷。</p><ul><li>IDR[40]通过将几何表示为被认为是SDF的MLP的零水平集，用神经网络重构曲面。</li><li>MVSDF[41]从MVS网络中导入信息以获得更多的几何先验。</li><li>VolSDF[39]和Neus[33]在渲染过程中<strong>使用涉及SDF的权重函数</strong>使颜色和几何更接近。</li><li>UNISURF[24]探索了表面绘制和体绘制之间的平衡。</li><li>与传统的多视图重建方法相比，神经网络重建的曲面具有更好的完备性，特别是在处理非兰伯特情况<strong>non-Lambertian cases</strong>时。然而，复杂的结构并没有得到很好的处理。同时，平面和尖角也无法保证。</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>给定目标的多视图图像，我们的目标是在没有遮罩监督的情况下，通过神经体绘制重建表面。目标的空间场用有符号距离函数(SDF)表示，并利用SDF的零水平集提取相应的表面。<strong>在体绘制过程中，我们的目标是优化有符号距离函数</strong>。<br>在本节中，我们首先<strong>分析了导致渲染颜色与隐式几何之间不一致的颜色渲染中的固有偏差</strong>。然后<strong>引入显式SDF优化来实现几何一致性</strong>。我们的方法概述如图2所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230911165550.png" alt="image.png"></p><h2 id="Bias-in-color-rendering"><a href="#Bias-in-color-rendering" class="headerlink" title="Bias in color rendering"></a>Bias in color rendering</h2><p>在体渲染的过程中，渲染的颜色与物体的几何形状之间存在间隙。渲染的颜色与表面的真实颜色不一致。<br>对于不透明实体$\Omega\in\mathbb{R}^3,$其不透明度可以用指标函数$\mathcal{O}(p)$表示:<br>$\mathcal{O}(\boldsymbol{p})=\left\{\begin{array}{l}1,\boldsymbol{p}\in\Omega\\0,\boldsymbol{p}\notin\Omega\end{array}\right..$ Eq.1</p><p>当我们看到一些颜色或用相机捕捉到一些颜色时，这些颜色就是沿着光线进入我们眼睛或相机的光。基于不透明固体物体固有的光学特性，我们近似假设图像集$\{I_{i}\}$的颜色C为与对应相机位置o的光线v相交的物体的颜色C:<br>$C\left(\boldsymbol{o},\boldsymbol{v}\right)=\left.c\left(\boldsymbol{o}+t^*\boldsymbol{v}\right),\right.$ Eq.2</p><p>其中$t^<em>=\operatorname{argmin}\{t|\boldsymbol{o}+t\boldsymbol{v}=\boldsymbol{p},\boldsymbol{p}\in\partial\Omega,t\in(0,\infty)\}.$ ∂Ω表示几何曲面。这个<strong>假设</strong>是合适的，因为透过不透明物体的光可以被忽略。<em>*光的强度在穿过不透明物体表面时急剧衰减到零左右</em></em>。让我们用带符号的距离函数在数学上表示物体的表面。有符号距离函数sdf (p)是空间点p与曲面∂Ω之间的有符号距离。这样，曲面∂Ω可以表示为:<br>$\partial\Omega=\left\{\boldsymbol{p}|sdf\left(\boldsymbol{p}\right)=0\right\}.$ Eq.3</p><p>通过神经体绘制，我们通过多层感知器(MLP)网络$F_{\Theta}\mathrm{and} G_{\Phi}$估计有符号距离函数$\hat{sdf}$和$\hat{c}$ </p><p>$\hat{sdf}\left(p\right)=F_{\Theta}\left(p\right),$ Eq.4<br>$\hat{c}\left(\boldsymbol{o},\boldsymbol{v},t\right)=G_{\Phi}\left(\boldsymbol{o},\boldsymbol{v},t\right).$ Eq.5</p><p>因此，相机位置为$\text{o}$的图像的估计颜色可以表示为:$\hat{C}=\int_{0}^{+\infty}w\left(t\right)\hat{c}\left(t\right)dt,$ Eq.6<br>其中，t是从0点向v方向的射线的深度，$w(t)$是t点的权值。为简单起见，省略了注释o和v。为了得到$w$和$\hat{c}$的离散对应项，我们也沿着射线对$t_{i}$进行离散采样，并使用黎曼和:<br>$\hat{C}=\sum_{i=1}^nw\left(t_i\right)\hat{c}\left(t_i\right).$ Eq.7</p><p>值得注意的是，新视图合成的目标是准确预测颜色$\hat{C},$并努力最小化地面真实图像C与预测图像$\hat{C}$之间的颜色差异:<br>$C=\hat{C}=\sum_{i=1}^{n}w\left(t_{i}\right)\hat{c}\left(t_{i}\right).$ Eq.8<br>在表面重建任务中，我们更关注的是物体的表面而不是颜色。这样，上式可改写为:</p><p><script type="math/tex">\begin{aligned}\text{C}& \begin{aligned}=\sum_{i=1}^{j-1}w\left(t_{i}\right)\hat{c}\left(t_{i}\right)+w\left(t_{j}\right)\hat{c}\left(\hat{t^{*}}\right)+w\left(t_{j}\right)\left(\hat{c}\left(t_{j}\right)-\hat{c}\left(\hat{t^{*}}\right)\right)+\sum_{i=j+1}^{n}w\left(t_{i}\right)\hat{c}\left(t_{i}\right)\end{aligned}  \\&=w\left(t_j\right)\hat{c}\left(\hat{t^*}\right)+\varepsilon_{sample}+\sum_{\overset{i=1}{\operatorname*{i\neq j}}}^nw\left(t_i\right)\hat{c}\left(t_i\right) \\&=w\left(t_j\right)\hat{c}\left(\hat{t^*}\right)+\varepsilon_{sample}+\varepsilon_{weight},\end{aligned}</script> Eq.9</p><p>式中，$\hat{sdf}(\hat{t^<em>})=0,$,$t_{\boldsymbol{j}}$表示离$\hat{t^{</em>}}$最近的样本点，$\varepsilon_{sample}$表示采样操作引起的偏差，$\varepsilon_{weight}$表示体积绘制<strong>加权和操作</strong>引起的偏差。由式(2)可改写为:<br>$w\left(t_{j}\right)\hat{c}\left(\hat{t^{<em>}}\right)+\varepsilon_{sample}+\varepsilon_{weight}=c\left(t^{</em>}\right),$ Eq.10</p><p>$\hat{c}\left(\hat{t^{<em>}}\right)=\frac{c\left(t^{</em>}\right)-\varepsilon_{sample}-\varepsilon_{weight}}{w\left(t_{j}\right)}.$ Eq.11</p><p>其中，物体表面颜色与估计表面颜色的总偏差为:<br>$\Delta c=\hat{c}\left(\hat{t^{<em>}}\right)-c\left(t^{</em>}\right)=\frac{\left(1-w\left(t_{j}\right)\right)c\left(t^{*}\right)-\varepsilon_{sample}-\varepsilon_{weight}}{w\left(t_{j}\right)}.$ Eq.12</p><p>相对偏差是:<br>$\delta c=\frac{\Delta c}{c\left(t^{<em>}\right)}=\frac1{w\left(t_{j}\right)}-1-\frac{\varepsilon_{sample}+\varepsilon_{weight}}{w\left(t_{j}\right)c\left(t^{</em>}\right)}.$ Eq.13</p><p>当$w\left(t_{j}\right)$趋于1时, $\varepsilon_{weight}$趋于0，$\delta c$趋于$\varepsilon_{sample}/c(t^{<em>}).$。<em>*在这种情况下，总偏差仅由离散抽样引起，它很小(但仍然存在)</em></em>。现有几种神经重构方法的模拟权值如图3所示。可以看出，在实践中几乎不可能做到这一点，特别是在没有任何几何约束的情况下。此外，当处理occlusion的情况时，这个问题变得更加棘手。因此，体绘制积分的加权方式引入了隐式几何建模的bias。由于整个网络的监督几乎完全依赖于渲染颜色和地面真色的差异，这种bias会使表面的颜色和SDF网络难以监督，导致颜色和几何之间的gap。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230911171816.png" alt="image.png"></p><p>一个简单的解决方案是直接监督对象的几何形状。<strong>通过这种方式，我们设计了SDF网络上的显式监督和具有多视图约束的几何一致监督</strong>。</p><h2 id="Explicit-supervision-on-SDF-network"><a href="#Explicit-supervision-on-SDF-network" class="headerlink" title="Explicit supervision on SDF network"></a>Explicit supervision on SDF network</h2><p>SDF网络估计从任意空间点到物体表面的带符号距离，是我们需要优化的关键网络。因此，我们<strong>提出了一种对SDF网络进行显式监督的方法</strong>，直接利用三维空间中的点来保证其精度。</p><p>为了减少额外成本，我们使用由SFM生成的点[27,31]来监督SDF网络。实际上，SFM是一种计算输入图像相机参数的规范解决方案，其中2D特征匹配X和稀疏的3D点P也作为副产品生成。因此，这些稀疏的三维点可以作为“自由的”显式几何信息。<strong>近似地，我们假设这些稀疏点在物体表面</strong>。即稀疏点的SDF值为零:$sdf\left(\boldsymbol{p}_{i}\right)=0$，其中$\boldsymbol{p}_{i}\in\boldsymbol{P}.$。在实际中，在获得稀疏的3D点后，使用<strong>半径滤波器</strong>来排除一些离群点[43]。</p><p><strong>Occlusion handling</strong><br>因为我们关注的是不透明的物体，所以物体的某些部分从相机的特定位置来看是不可见的。因此，每个视图中只有一些稀疏点可见。对于相机位置为$o_{i}$的图像$I_{i}$，可见点$P_{i}$与$I_{i}$的特征点$X_{i}$一致:</p><p>$X_{i}=K_{i}\left[R_{i}|t_{i}\right]P_{i},$ Eq.14</p><p>其中$K_{i}$为内部定标矩阵，$R_i$为旋转矩阵，$t_i$为图像$I_{i}$的平移向量。$X_i\mathrm{~and~}P_i$的坐标都是齐次坐标。为简单起见，省略了$X_{\boldsymbol{i}}$之前的标度指数。根据每张图像的特征点，<strong>我们得到每个视图的可见点，并在从对应视图渲染图像时使用它们来监督SDF网络</strong>。</p><p><strong>View-aware SDF loss</strong>.<br>在从视图$V_{i}$渲染图像$I_i$时，我们使用SDF网络估计可见点$P_{i}\mathrm{~of~}V_{i}$的SDF值。基于稀疏点的SDF值为零的近似，我们提出了视图感知的SDF损失:</p><p>$\mathcal{L}_{SDF}=\sum_{p_j\in\boldsymbol{P}_i}\frac{1}{N_i}|\hat{sd}f\left(\boldsymbol{p}_j\right)-sdf\left(\boldsymbol{p}_j\right)|=\sum_{\boldsymbol{p}_j\in\boldsymbol{P}_i}\frac{1}{N_i}|\hat{sd}f\left(\boldsymbol{p}_j\right)|,$ Eq.15<br>式中，$N_{i}$为$P_{i}$中点的个数，|·|为L1距离。值得注意的是，<strong>我们用来监督SDF网络的损失根据所呈现的视图而变化</strong>。这样，引入的SDF损耗与显色过程是一致的。</p><p>通过对SDF网络的显式监督，我们的网络可以更快地收敛，<strong>因为使用了几何先验</strong>。此外，由于纹理强烈的复杂几何结构是稀疏点的集中分布区域，因此我们的方法可以捕捉到更细致的几何形状。</p><p><strong>Geometry-consistent supervision with multi-view constraints</strong></p><p>使用SDF损失，我们的网络可以捕获具有强纹理的复杂几何细节。由于稀疏的3D点主要对纹理丰富的区域提供显式约束，<strong>因此大面积的光滑区域仍然缺乏显式的几何约束</strong>。为了更进一步，我们设计了具有多视图立体约束的隐式曲面上的几何一致监督</p><p><strong>Occlusion-aware implicit surface capture</strong></p><p>我们使用曲面的隐式表示，并利用隐式函数的零水平集提取曲面。隐曲面根据式(3)，估计曲面为:<br>$\hat{\partial\Omega}=\left\{\boldsymbol{p}|\hat{sdf}(\boldsymbol{p})=0\right\}.$Eq.16</p><p>我们的目标是在不同视图之间使用几何一致的约束来优化$\partial\hat{\Omega}$。因为曲面上的点的数量是无限的，所以在实践中我们需要从$\partial\hat{\Omega}$中采样点。为了与使用视图光线的显色过程保持一致，我们对这些光线上的表面点进行采样。正如3.1中提到的，我们沿着视图射线离散采样，并使用黎曼和来获得渲染的颜色。在采样点的基础上，采用线性插值方法得到曲面点。</p><p>在射线上采样点t，对应的三维点为$p=o+tv,$ 预测的SDF值为$\hat{sdf(p)}$，为简单起见，我们进一步将$\hat{sdf(p)}$表示为 $\hat{sdf(t)}$，这是t的函数。我们找到样本点$t_{i}$，其SDF值的符号与下一个样本点$t_{i+1}$不同。由$t_{i}$形成的样本点集T为:<br>$T=\left\{t_i|\hat{sdf}(t_i)\cdot\hat{sdf}(t_{i+1})&lt;0\right\}.$ Eq.17</p><p>在这种情况下，线$t_it_{i+1}$与曲面$\partial\hat{\Omega}.$相交。相交点集合$\hat{T^<em>}$为:<br>$\hat{T^</em>}=\left\{t|t=\frac{\hat{sd}f(t_i)t_{i+1}-\hat{sd}f(t_{i+1})t_i}{\hat{sd}f(t_i)-\hat{sd}f(t_{i+1})},t_i\in T\right\}.$ Eq.18</p><p>与物体相互作用的光线可能与物体表面有不止一个交点。具体来说，可能至少有两个交叉点。与SDF监督机制类似，考虑到遮挡问题，我们只使用沿光线的第一个交点<br>$t^<em>=\operatorname{argmin}\left\{t|t\in\hat{T^</em>}\right\}.$ Eq.19<br>$t^*$的选择保证隐式曲面的样本点在对应的视图中都是可见的，并且使监督与显色过程一致。</p><p><strong>Multi-view photometric consistency constraints</strong><br>我们捕获估计的隐式曲面，其几何结构在不同的视图中应该是一致的。基于这种直觉，我们使用多视图立体(MVS)中的<strong>光度一致性约束</strong>[8,9,34]来监督我们提取的隐式表面。</p><p>对于表面上的一个小面积s, s在图像上的投影是一个小的像素斑q。除了遮挡情况外，s对应的斑在不同视图之间应该是几何一致的。与传统MVS方法中的补片变形类似，我们用中心点及其法线表示s。为方便起见，我们在参考图像$I_{r}$的相机坐标中表示s的平面方程:<br>$n^Tp+d=0,$ Eq.20</p><p>式中，p为式(19)计算得到的交点，$n^{T}$为SDF网络在p点自动微分计算得到的法线，则参考图像$I_{r}$的像素patch $q_{i}$中的图像点x与源图像$I_{s}$的像素patch $q_{i}$中的对应点x′通过平面诱导的单应性H相关联[11]<br>$x=Hx^{\prime},H=K_s(R_sR_r^T-\frac{R_s(R_s^Tt_s-R_r^Tt_r)n^T}{d})K_r^{-1},$ Eq.21<br>其中K为内部校准矩阵，R为旋转矩阵，t为平移向量。索引显示捐赠属于哪个图像。为了集中几何信息，我们将彩色图像$\left\{I_{i}\right\}$转换为灰度图像$\left\{I’_{i}\right\}$，并利用$\left\{I’_{i}\right\}$中斑块间的光度一致性来监督隐式表面。</p><p><strong>Photometric consistency loss</strong>.<br>为了测量光度一致性，我们使用参考灰度图像$\left\{I’_{r}\right\}$和源灰度图像$\left\{I’_{s}\right\}$中斑块patches的归一化互相关(NCC):<br>$NCC(I_{r}’(q_{i}),I_{s}’(q_{is}))=\frac{Cov(I_{r}’(q_{i}),I_{s}’(q_{is}))}{\sqrt{Var(I_{r}’(q_{i}))Var(I_{s}’(q_{is}))}},$ Eq.22<br>其中Cov表示协方差，var表示方差。在对图像进行颜色渲染时，我们使用以被渲染像素为中心的patch, patch的大小为11 × 11。我们将渲染图像作为参考图像，并计算其采样补丁与所有源图像上相应补丁之间的NCC分数。为了处理遮挡，<strong>我们为每个采样patch找到计算出的NCC分数中最好的四个</strong>[9]，并使用它们来计算相应视图的光度一致性损失:</p><p>$\mathcal{L}_{photo}=\frac{\sum_{i=1}^{N}\sum_{s=1}^{4}1-NCC(I_{r}^{\prime}(q_{i}),I_{s}^{\prime}(q_{is}))}{4N},$ Eq.23</p><p>其中N是渲染图像上采样像素的数量。在光度一致性损失的情况下，保证了隐式曲面在多视图间的几何一致性。</p><h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>在渲染特定视图的颜色时，我们的总损失是:<br>$\mathcal{L}=\mathcal{L}_{color}+\alpha\mathcal{L}_{reg}+\beta\mathcal{L}_{SDF}+\gamma\mathcal{L}_{photo}.$ Eq.24</p><p>$\mathcal{L}_{color}=\frac1N\sum_{i=1}^N|C_i-\hat{C}_i|.$<br>$\mathcal{L}_{reg}=\frac1N\sum_{i=1}^{N}(|\nabla\hat{sd}f(\boldsymbol{p}_{i})|-1)^{2}.$ eikonal项用来正则化SDF网络的梯度<br>在我们的实验中，我们选择α， β和γ分别为0.3,1.0和0.5。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>Datasets</p><ul><li>DTU</li><li>BlendedMVS</li></ul><p>Baselines</p><ul><li>COLMAP</li><li>IDR</li><li>VolSDF</li><li>NeuS </li><li>NeuralWarp </li></ul><p>Implementation details.</p><ul><li>MLP SDF ： 256x8</li><li>MLP color：256x4</li><li>PE：L=6—&gt;x , L=4—&gt;dirs</li><li>bathsize rays = 512</li><li>单个2080Ti 16h</li><li>提取mesh resolution = $512^{3}$</li></ul><p>Comparisons</p><ul><li>quantitative results</li><li>Qualitative results</li></ul><p>Analysis</p><ul><li>Ablation study</li><li>Geometry bias of volumetric integration.</li><li>Convergence speed.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HF-NeuS</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/HF-NeuS/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/HF-NeuS/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>HF-NeuS: Improved Surface Reconstruction Using High-Frequency Details</th></tr></thead><tbody><tr><td>Author</td><td>_Yiqun Wang, Ivan Skorokhodov, Peter Wonka_</td></tr><tr><td>Conf/Jour</td><td>NeurIPS</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://github.com/yiqun-wang/HFS">yiqun-wang/HFS: HF-NeuS: Improved Surface Reconstruction Using High-Frequency Details (NeurIPS 2022) (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4670872824890540033&amp;noteId=1943282597784032256">Improved surface reconstruction using high-frequency details (readpaper.com)</a></td></tr></tbody></table></div><p>贡献：</p><ul><li>新的SDF与透明度$\alpha$关系函数，相较于NeuS更简单</li><li>将SDF分解为两个独立隐函数的组合：基和位移。并利用自适应尺度约束对隐函数分布不理想的区域进行重点优化，可以重构出比以往工作更精细的曲面</li></ul><span id="more"></span><h1 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h1><p>如图5所示，我们的方法仍然存在挑战。我们给出了参考地面真值图像、相应的重建图像和重建表面。对于船舶的绳索网格，仍然观察到对地面真实辐射的一些过拟合。<strong>具体来说，绳子的网格在图像中是可见的，但表面没有精确地重建</strong>。另一个限制是缺少单根细绳。我们还可视化了表1的一个坏情况，其中误差大于其他方法的误差，如补充材料中的图14 DTU Bunny所示。在这种情况下，这个模型的光线变化，纹理不那么明显，因此很难重建腹部的细节。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230904154600.png" alt="image.png"></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>介绍了一种基于高频细节的多视点表面重建新方法HF-NeuS。<strong>我们提出了一个新的推导来解释符号距离和透明度之间的关系</strong>，并提出了一类可以使用的函数。<strong>通过将符号距离场分解为两个独立隐函数的组合，并利用自适应尺度约束对隐函数分布不理想的区域进行重点优化，可以重构出比以往工作更精细的曲面</strong>。<br>实验结果表明，该方法在定量重建质量和视觉检测方面都优于目前的技术水平。目前的一个限制是，HF-NeuS需要优化一个额外的隐式函数，因此它需要更多的计算资源，并产生额外的编码复杂性。此外，由于缺乏3D监督，我们仍然在一定程度上观察到对地面真实度的过拟合。<br>未来工作的一个有趣的方向是探索不同照明方式下场景的重建。<em>最后，我们不期望与我们的研究直接相关的负面社会影响。不过，一般来说，地表重建可能产生负面的社会影响</em></p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>神经渲染可以在没有三维监督的情况下重建形状的隐式表示。然而，目前的神经表面重建方法难以学习高频几何细节，因此重建的形状往往过于光滑。本文提出了一种提高神经绘制中表面重建质量的新方法HF-NeuS。我们遵循最近的工作，将曲面建模为有符号距离函数(sdf)。<br><strong>首先，我们推导了SDF、体积密度、透明度函数和体积渲染方程中使用的加权函数之间的关系，并提出了将透明度建模为转换后的SDF</strong>。<br>其次，我们观察到，试<strong>图在单个SDF中联合编码高频和低频分量会导致不稳定的优化</strong>。<br>我们提出将SDF分解为基函数和位移函数，采用由粗到精的策略，逐步增加高频细节。最后，我们设计了一种自适应优化策略，使训练过程专注于改进那些靠近表面的sdf有伪像的区域。我们的定性和定量结果表明，我们的方法可以重建细粒度的表面细节，并获得比现有技术更好的表面重建质量</p><p>NeRF —&gt; Neus、VolSDF表面重建 —&gt; HF-Neus</p><ul><li>首先，我们分析了符号距离函数与体积密度、透明度和权重函数之间的关系。我们从我们的推导中得出结论，最好是建模一个将有符号距离映射到透明度的函数，并提出一类满足理论要求的函数</li><li>其次，我们观察到，如图2所示，单符号距离函数很难直接学习高频细节。因此，我们建议在相关工作之后将SDF分解为基函数和位移函数。我们将这一思想应用于可微分的NeRF渲染框架和NeRF训练方案。</li><li>第三，可以选择将距离转换为透明度的函数有一个参数，我们称之为尺度s，它控制函数的斜率(或导数的偏差)。参数s控制表面被定位的精确程度，以及远离表面的颜色对结果的影响程度。<ul><li>在之前的工作中，这个参数s是全局设置的，但它是可训练的，因此它可以在迭代之间改变。<strong>我们提出了一种新的空间自适应加权方案来影响该参数，使优化更加关注距离场中的问题区域</strong></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230904145337.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230904145342.png" alt="image.png"></p><p>RW:</p><ul><li>Multi-view 3D reconstruction.</li><li>Neural implicit surfaces.<ul><li>DVR、IDR、NeRF++、UNISURF、<strong>VolSDF、Neus</strong>、NeuralPatch</li></ul></li><li>High-frequency detail reconstruction.<ul><li>SIREN、MipNeRF、</li></ul></li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>输入N张图片$I=\{I_1,I_2…I_N\}$及对应相机内外参$\Pi=\{\pi_1,\pi_2…\pi_N\}.$</p><p>1)首先，我们展示了<strong>如何将带符号的距离函数嵌入到体绘制的公式中</strong>，并讨论了<strong>如何建模距离和透明度之间的关系</strong>。<br>2)然后，我们提出<strong>利用额外的位移符号距离函数将高频细节添加到基本符号距离函数中</strong>。<br>3)最后，我们观察到将带符号距离映射到透明度的函数由一个参数s控制，<strong>该参数s决定了函数的斜率</strong>。我们提出了一种方案，根据距离场的梯度范数以空间变化的方式设置该参数s，而不是在单个训练迭代中对整个体积保持恒定。</p><h2 id="Modeling-transparency-as-transformed-SDF"><a href="#Modeling-transparency-as-transformed-SDF" class="headerlink" title="Modeling transparency as transformed SDF"></a>Modeling transparency as transformed SDF</h2><p>NeRF<br>光线$\mathbf{r}(t)=\mathbf{o}+\mathbf{t}\mathbf{d},$<br>对应像素的颜色$C(\mathbf{r})=\int_{t_n}^{t_f}T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t),\mathbf{d})dt$<br>透明度$T(t)=\exp\left(-\int_{t_{n}}^{t}\sigma(\mathbf{r}(s))ds\right),$是一个单调递减函数</p><p>为了将SDF转换为密度，需要找到一个函数</p><ul><li>VolSDF中提出的密度函数：$\sigma(\mathbf{r}(t))=\Psi\left(f\left(\mathbf{r}(t)\right)\right)$ </li><li>Neus中提出的权重函数：$w((t))=\Psi\left(f\left(\mathbf{r}(t)\right)\right)$</li><li>本文给出了密度函数σ表达式的一个复杂推导<ul><li>$T(t)=\Psi\left(f\left(\mathbf{r}(t)\right)\right),$</li><li>$\sigma(\mathbf{r}(t_{i}))=s\left(\Psi\left(f\left(\mathbf{r}(t_{i})\right)\right)-1\right)\nabla f\left(\mathbf{r}(t_{i})\right)\cdot\mathbf{d}$</li><li>$\alpha_i=1-exp\left(-\sigma_i\left(t_{i+1}-t_i\right)\right)$，并将$\alpha$clamp在0,1之间</li></ul></li></ul><p>与NeuS相比，我们得到了更简单的密度σ的离散化计算公式，减少了NeuS中除法带来的数值问题。此外，我们的方法不需要涉及两个不同的采样点，即截面点和中点，这使得它更容易满足无偏加权函数。由于不需要为两个不同的点集分别计算SDF和颜色，因此与NeuS相比，颜色和几何形状更加一致。与VolSDF[32]相比，由于透明度函数是显式的，因此我们的方法可以使用逆CDF计算的逆分布抽样来满足近似质量。因此不需要像VolSDF那样复杂的采样方案。图3显示了一个直观的比较。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230904145240.png" alt="image.png"></p><h2 id="Implicit-displacement-field-without-3D-supervision"><a href="#Implicit-displacement-field-without-3D-supervision" class="headerlink" title="Implicit displacement field without 3D supervision"></a>Implicit displacement field without 3D supervision</h2><p>为了实现多尺度拟合框架，我们建议将SDF建模为<strong>基距离函数</strong>和<strong>沿基距离函数法线的位移函数</strong>的组合。隐式位移函数是一个附加的隐式函数。这样设计的原因是单个隐式函数很难同时学习低频和高频信息。隐式位移函数可以补充基本隐式函数，从而更容易学习高频信息。<br>与从点云中学习隐式函数的任务相比，从多幅图像中重建三维形状使得高频内容的学习更加困难。<strong>我们建议使用神经网络在多个尺度上学习频率，并以由粗到细的方式逐步增加频率内容</strong>。</p><p>基面的隐式函数$f_{b}$<br>位移隐式函数$f_{d’}$：将基面上点$x_{b}$沿着法向$n_{b}$，映射到表面点x<br>$f_d$将基面上点x沿着法向$n_{b}$映射到表面点$x_{b}$，因此$\mathop{:}f_{d^{\prime}}(\mathbf{x}_{b})=f_{d}(\mathbf{x}).$<br>由于隐函数的性质，这两个函数之间的关系可以表示为:$f_b(\mathbf{x}_b)=f(\mathbf{x}_b+f_{d^{\prime}}\left(\mathbf{x}_b\right)\mathbf{n}_b)=0$</p><p>$\mathbf{n}_b=\frac{\nabla f_b(\mathbf{x}_b)}{|\nabla f_b(\mathbf{x}_b)|},$ 是$x_{b}$在基面上的法线。为了计算隐函数f的表达式，引入$\mathbf{x}_b=\mathbf{x}-f_{d^{\prime}}\left(\mathbf{x}_b\right)\mathbf{n}_b$并得到组合隐函数的表达式：$f(\mathbf{x})=f_b(\mathbf{x}-f_d\left(\mathbf{x}\right)\mathbf{n}_b)$</p><p>因此，我们可以用基隐函数和位移隐函数来表示组合隐函数。<br>然而，出现了两个挑战</p><ul><li>首先，只有当点x在表面上时，才满足式10。</li><li>其次，当只知道位置x时，很难估计点$x_{b}$处的法线。</li></ul><p>我们依靠两个假设来解决问题</p><ul><li>一个假设是，这种变形可以应用于所有等面，即$f_b(\mathbf{x}_b)=f(\mathbf{x}_b+f_{d^{\prime}}\left(\mathbf{x}_b\right)\mathbf{n}_b)\stackrel{\cdot}{=}c.$。这样，假设方程对体积中的所有点都有效，而不仅仅是在表面上。</li><li>另一假设是$x_{b}$和x距离不太远，则可以将式(10)中x点上的$\mathbf{n}_b$替换为法向$\mathbf{n}$。我们使用位移约束$4\Psi_{s}^{\prime}(f_{b})$来控制隐式位移函数的大小。</li></ul><p>$f(\mathbf{x})=f_b(\mathbf{x}-4\Psi_{(0.01s)}^{\prime}(f_b)f_d\left(\mathbf{x}\right)\mathbf{n})$</p><p>为了精确控制频率，我们采用位置编码对基隐函数和位移隐函数分别进行编码。频率可以通过一种从粗到精的策略来明确控制，而不是简单地使用两个具有两个不同频率级别的Siren网络</p><p>$\gamma(\mathbf{x})=[\gamma_0(\mathbf{x}),\gamma_1(\mathbf{x}),…,\gamma_{L-1}(\mathbf{x})]$<br>每个分量由不同频率的sin和cos函数组成$\gamma_j(\mathbf{x})=\left[\sin\left(2^j\pi\mathbf{x}\right),\cos\left(2^j\pi\mathbf{x}\right)\right]$</p><p>直接学习高频位置编码会使网络容易受到噪声的影响，因为错误学习的高频会阻碍低频的学习。如果有三维监控，这个问题就不那么明显了，但是图像的高频信息很容易以噪声的形式引入到表面生成中。我们使用Park等人Nerfies[24]提出的从粗到精的策略，逐步增加位置编码的频率。</p><p>$\gamma_{j}(\mathbf{x},\mathbf{\alpha})=\omega_{j}\left(\mathbf{\alpha}\right)\gamma_{j}(\mathbf{x})=\frac{\left(1-\cos\left(clamp\left(\alpha L-j,0,1\right)\pi\right)\right)}2\gamma_{j}(\mathbf{x})$<br>$\alpha\in[0,1]$是控制所涉及频率信息的参数。在每次迭代中，α增加$1/n_{max}$，直到它接近1，其中$n_{max}$是最大迭代次数。<br>我们利用两种不同参数$\alpha_b.$和$\alpha_d.$的位置编码$\gamma(\mathbf{x},\alpha_b),\gamma(\mathbf{x},\alpha_d)$。为简单起见，我们设$\alpha_{b}=0.5\alpha_{d}$，只控制$\alpha_d$。我们还使用了两个MLP函数$MLP_{b},MLP_{d}$来拟合基函数和位移函数。</p><p>$f(\mathbf{x})=MLP_{b}(\gamma(\mathbf{x},\alpha_{b})-4\Psi_{s}^{\prime}(f_{b})MLP_{d}\left(\gamma(\mathbf{x},\alpha_{d})\right)\mathbf{n}),$</p><p>$\begin{array}{rcl}\mathbf{n}&amp;=&amp;\frac{\nabla f_b(\mathbf{x})}{|\nabla f_b(\mathbf{x})|}\end{array}$ 可以通过MLP b的梯度来计算<br>$\Psi_s^{\prime}(f_b)\quad=\Psi_s^{\prime}(MLP_b(\gamma(\mathbf{x},\alpha_b))).$<br>训练时应clamp位移约束的s。</p><p>我们将这个隐式函数带入Eq.(6)来计算透明度，这样图像的亮度(颜色)$\hat{C}_{s}$就可以通过体积渲染方程来计算。</p><p>为了训练网络，我们使用了损失函数$\mathcal{L}=\mathcal{L}_{rad}+\mathcal{L}_{reg}.$，它包含了带符号距离函数的辐射损失和Eikonal正则化损失。对于正则化损失，我们同时约束了基本隐函数和详细隐函数。$\mathcal{L}=\frac{1}{M}\sum_{s}\left|\hat{C}_{s}-C_{s}\right|_{1}+\frac{1}{N}\sum_{k}\left[\left(|\nabla f_{b}(\mathbf{x}_{k})|_{2}-1\right)^{2}+\left(|\nabla f(\mathbf{x}_{k})|_{2}-1\right)^{2}\right]$</p><h2 id="Modeling-an-adaptivate-transparency-function"><a href="#Modeling-an-adaptivate-transparency-function" class="headerlink" title="Modeling an adaptivate transparency function"></a>Modeling an adaptivate transparency function</h2><p>在前面的小节中，透明度函数被定义为sigmoid函数，由一个标度s控制。该参数控制着sigmoid函数的斜率，同时也是导数的标准差。我们也可以说它控制着函数的平滑度。当s较大时，随着位置远离表面，s型函数的值急剧下降。相反，当s较小时，该值平稳减小。然而，每次迭代选择单个参数会在体块的所有空间位置产生相同的行为。<br>由于需要重构两个带符号的距离函数，特别是在高频叠加之后，<strong>很容易出现Eikonal方程不满足的情况</strong>，即SDF的梯度范数在某些位置不为1。即使有正则化损失，也不可能避免这个问题。<br>我们建议<strong>使用带符号距离场的梯度范数以空间变化的方式对参数s进行加权</strong>。当沿射线方向的梯度范数大于1时，我们增加s。这意味着当梯度的范数大于1时，隐函数的变化更剧烈，这表明了一个需要改进的区域。在某些区域使距离函数更大，需要更精确的距离函数，并且由于不正确的距离函数而放大误差，特别是在表面附近。为了自适应地修改量表s，我们提出如下公式:</p><p>$T(t)=\left(1+e^{-s\exp\left(\sum_{i=1}^{K}\omega_i|\nabla f_i|-1\right)f(\mathbf{r}(t))}\right)^{-1},$<br>其中∇f为带符号距离函数的梯度，K为采样点个数<br>$ω_i$ 为归一化后的$\Psi_s^{\prime}(f_i)$作为权值，$\sum_{i=1}^{K}\omega_{i}=1.$</p><p>该方法既可用于控制透明度函数，也可用于标准NeRF提出的分层采样阶段。通过局部增大s，<strong>在距离值变化较快的表面附近会产生更多的样本</strong>。这种机制也有助于优化集中在这些区域的体积。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>Baselines：Neus、VolSDF、NeRF</p><p>Datasets：DTU、BlendedMVS</p><p>Evaluation metrics：CD(chamfer distance)、PSNR</p><p>Implementation details:</p><ul><li>GPU A100 40GB</li><li>lr = 5e-4</li><li>首先64个均匀采样点，计算点的SDF和梯度，然后计算s参数增益，根据增益自适应更新权重</li><li>然后根据权重额外采样64个点</li><li>当$\alpha^{0}_{d}=0$时产生光滑的结果，本文使用$\alpha^{0}_{d}=0.5$ ,$\alpha^{0}_{b}=0.5 \alpha^{0}_{d}=0.25$</li><li>位置编码L=16</li></ul><p>Comparison：</p><ul><li>定性+定量</li></ul><p>Ablation study.</p><ul><li>Coarse2Fine module</li><li>Implicit displacement function module</li><li>Position-adaptive s module</li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Color-NeuS</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Color-NeuS/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Color-NeuS/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Color-NeuS: Reconstructing Neural Implicit Surfaces with Color</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://colmar-zlicheng.github.io/">Licheng Zhong</a>1 , <a href="https://lixiny.github.io/">Lixin Yang</a>1,2 , <a href="https://kailinli.top/">Kailin Li</a>1, <a href="https://haoyuzhen.com/">Haoyu Zhen</a>1, <a href="https://joymei.github.io/">Mei Han</a>3, <a href="https://www.mvig.org/">Cewu Lu</a>1,2</td></tr><tr><td>Conf/Jour</td><td>arXiv</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://colmar-zlicheng.github.io/color_neus/">Color-NeuS (colmar-zlicheng.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4789058885507874817&amp;noteId=1941703036570922240">Color-NeuS: Reconstructing Neural Implicit Surfaces with Color (readpaper.com)</a></td></tr></tbody></table></div><p>集成了与视图无关的全局颜色变量和与视图相关的relight效果<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230901123131.png" alt="image.png"></p><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>本文介绍了一种新的三维隐式纹理表面重建方法Color-NeuS，该方法与任何类似Neus的模型兼容。通过从神经辐射场中分离出依赖于视图的元素，并采用重光照网络来维持体绘制，Color-NeuS能够有效地检索表面颜色，同时准确地重建表面细节。我们使用我们个人收集的序列以及几个公共数据集，将Color-NeuS测试放在要求苛刻的手持对象扫描任务上。结果表明，Color-NeuS具有重建具有准确颜色表示的神经隐式表面的能力。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>从多视图图像或单目视频中重建物体表面是计算机视觉中的一个基本问题。然而，最近的许多研究集中在通过隐式或显式方法重建几何。在本文中，我们将重点转向<strong>与颜色相结合的网格重建</strong>。我们从神经体绘制中去除与视图相关的颜色，同时通过重光照网络保持体绘制性能。<strong>从表面的符号距离函数(SDF)网络中提取网格，并从全局颜色网络中绘制每个表面顶点的颜色</strong>。为了评估我们的方法，我们设想了一个手持物体扫描任务，该任务具有许多遮挡和光照条件的急剧变化。我们为这项任务收集了几个视频，结果超过了任何现有的能够重建网格和颜色的方法。此外，使用公共数据集(包括DTU、BlendedMVS和OmniObject3D)评估了我们的方法的性能。结果表明，我们的方法在所有这些数据集上都表现良</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>从二维图像重建三维物体是计算机视觉和图形学领域的一个关键和持续的挑战。此前，SFM方法[20,21]被广泛用于从2D图像重建3D物体。然而，<strong>此外，它经常很难处理缺乏纹理或重复模式的场景，导致模糊的对应关系和不正确的深度估计。SFM的另一个限制是它不能有效地处理闭塞</strong>。当场景中的物体被部分遮挡时，往往会导致重建错误。SFM的另一个局限性在于它对点云表示的依赖，无法实现完全密集的重建。最近，这一领域的前景一直在发展，人们对通过体渲染来研究隐式神经表面的兴趣日益浓厚Neus[23]，它可以表示像素级的精细表面。这是基于神经辐射场的工作[18]NeRF。</p><p>像NeRF[18]及其后继者[1,4,16,30,36]这样的开创性作品令人信服地证明了神经网络在表示连续3D场景方面的强大功能。这是通过学习从3D坐标到体密度和视图相关颜色的映射来实现的，从而实现高效的新视图合成。随后，Neus[23]扩展了这一概念，利用有符号距离函数(SDF)重建神经隐式曲面。<strong>然而，NeuS的范围仍然局限于无顶点颜色的网格重建</strong>。这个限制出现在Neus的神经体渲染中，每个点的颜色都是由它的位置和观察光线方向决定的。因此，<strong>在没有视图方向的网格重建环境中，NeuS无法为每个点分配特定的颜色值</strong>。为了解决这一问题，本文提出了一种基于视觉无关颜色的神经隐式曲面重建方法。</p><p>我们提出了Color-NeuS，这是一种与NeuS兼容的方法，可以独立于视点，促进3D表面和全局顶点颜色的重建。同时，它支持NeuS在渲染2D图像和执行3D表面重建方面的强大功能。为此，<strong>我们用与视图无关的全局颜色变量和与视图相关的重光照效果的集成取代了神经体渲染中与视图相关的单一颜色分量</strong>，如图1所示。这不仅使我们的模型能够进行标准体渲染的训练，而且为全局顶点颜色的学习铺平了道路。重要的是，<strong>Color-NeuS自然地处理物体表面的大量反射，并且可以处理物体之间动态交互过程中的间歇性遮挡</strong>(见图7)。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230901150726.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230901150740.png" alt="image.png"></p><p>我们的Color-NeuS的有效性已经在各种数据集上得到了广泛的验证，包括DTU[13]、BlendedMVS[32]和OmniObject3D[29]数据集。为了证明我们的方法的优越性，我们与激光扫描和成熟的方法如COLMAP[20,21]和HHOR[12]进行了比较评估。结果表明，Color-NeuS在提取合理纹理的同时，成功地重建了物体表面。为了强调其实际应用，我们将Color-NeuS应用于现实世界中具有挑战性的任务:手持物体扫描[10,12]。作为这个过程的一部分，我们收集一个数据集进行验证，其中包括物体的3D扫描，手持移动物体的视频，以及以物体为中心的相机转换。</p><p>贡献：</p><ul><li>我们提出了一种新的方法来重建一个具有颜色的神经隐式表面，可以应用于任何类Neus模型。</li><li>我们解耦了神经体渲染中依赖于视图的过程，在获得全局顶点颜色的同时能够处理遮挡和反射。</li><li>我们设计了一个具有挑战性的手持物体扫描任务，用于重建带颜色的网格，并为此任务编译了一个真实世界的数据集。</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p><strong>Neural Radiance Field</strong>.<br>继NeRF的开创性工作[18]之后，神经隐式场出现了各种各样的研究。</p><ul><li>其中值得注意的是，NeRF−−[26]、NoPe-NeRF[2]、BARF[15]和GNeRF[17]一直在研究在训练隐式场时估计相机姿势的方法。</li><li>与此同时，Pixel-NeRF[36]、MVSNeRF[4]和IBRNet[24]等研究的重点是神经辐射场的泛化。</li><li>与此同时，KiloNeuS[7]、NeX[28]、NeRV[22]、NeRD[3]等研究项目也在关注神经领域的照明和再照明。</li><li>该方法兼容神经体绘制，保留了新视图合成的能力。</li></ul><p><strong>Surface Reconstruction</strong>.</p><ul><li>隐式可微分渲染器(IDR)[33]通过利用隐式几何正则化(IGR)[8]，将几何表示为有符号距离函数(SDF)的零水平集。</li><li>Neus[23]结合了SDF场和体绘制来重建曲面。</li><li>VolSDF[34]和UNISURF[19]都将隐式表面表示与体绘制结合起来:<ul><li>VolSDF[34]将外观与几何分离，</li><li>而UNISURF[19]以内聚的方式制定隐式表面模型和亮度场。</li></ul></li><li>PET-NeuS[25]是NeuS[23]的扩展，引入了新的组件，如独特的位置编码类型、三平面表示和可学习的卷积操作。<strong>然而，这些方法没有考虑到全局视图无关的表面颜色</strong>。</li><li>我们的方法侧重于从全局颜色网络中提取表面颜色，同时保持从重光照网络中学习几何和图像渲染的能力。</li></ul><p><strong>In-hand Object Scanning</strong>.</p><ul><li>ObMan[11]提出了一种端到端可学习的模型，该模型采用独特的接触损失，鼓励物理上合理的手-物配置。</li><li>对于手持物体扫描任务，IHOI[35]利用估计的手部姿态，从单个RGB图像进行重建。</li><li>另一方面，BundleSDF[27]使用RGBD输入序列图像估计目标姿态，同时重建由有符号距离函数(Signed Distance Function, SDF)表示的隐式表面。</li><li>HHOR[12]也利用SDF来表示物体表面，但它是与手一起重建物体的，而手是牢牢握住物体的。</li><li>最近的一项工作[9]从RGB序列重建物体，并通过使用占用场来表示表面来同时估计相机姿势。<strong>但是，它假设光源距离较远，并且光的方向保持不变</strong>。</li><li>相反，我们的方法可以处理任意光照条件来模拟物体的外观。</li></ul><h1 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h1><p>我们首先引入神经隐式曲面(Neural Implicit Surface, NeuS)[23]，这是我们方法的基础。给定一个具有已知内在参数的相机，我们可以将相机坐标系中的光线表示为 : $p(z)=\mathbf{o}+z\mathbf{d},$ Eq.1</p><p>其中o和d分别是原点和射线的方向，z是原点到射线上一点的距离。然后，使用MLP网络$\mathcal{G}$将p编码为其带符号距离函数(SDF) s(p)和特征向量f (p)，分别为:$[s(p),f(p)]=\mathcal{G}(p).$ Eq.2</p><p>以位置p、方向d、特征向量f (p)、梯度g(p)为输入，另一个MLP $\mathcal{M}$输出查询点的颜色为:$c(p,d)=\mathcal{M}(p,d,f(p),g(p)),$ Eq.3</p><p>其中$g(p) =∇s(p)$为SDF在p点处的梯度。最后，对查询像素C沿射线的颜色进行积分，得到查询像素C的颜色，<br>$C=\int_{z_n}^{z_f}w(z)c(\mathbf{p},\mathbf{d})dz,$ Eq.4<br>$w(z)=\exp\big(-\int_{z_n}^z\sigma(t)dt\big)*\sigma(z),$ Eq.5<br>$\sigma(\mathbf{p})=\frac{\alpha e^{-\alpha s(\mathbf{p})}}{(1+e^{-\alpha s(\mathbf{p})})^2},$ Eq.6</p><p>其中σ(p)表示以$\alpha\in\mathbb{R}^1$作为可学习参数的p的密度。$w(z)$表示在z点处赋予颜色的权重。另外，$z_{n}$和$z_{f}$分别表示相机的近、远平面。<br>基于Eq.(4)中的输入d，输出的每个顶点颜色与视图相关。因此，原始的Neus避免了incorporating color,，只专注于表面形状的重建。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>我们的目标是提取物体的颜色和几何形状，<br>$c_g(\mathbf{x}),x\in\mathcal{S},$ Eq.7<br>其中$\mathcal{S}=\left.\{x\in\mathbb{R}^3|s(\mathbf{x})=0\right\}$表示物体表面(网格顶点的集合)，其特征为一组具有零级带符号距离值的点。</p><p><strong>Property1</strong>:<br>([23, Sec.3.1]) NeuS具有一个有利的性质，其中密度σ(p)的标准差由可训练参数1/α决定，随着网络训练达到收敛，该参数逐渐接近零。</p><p>根据性质1，神经辐射场的密度σ(p)基本上集中在表面s上，这为我们在网络训练达到收敛时提取物体的表面(每个顶点)颜色提供了一个焦点。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230901154617.png" alt="image.png"></p><h2 id="Naive-Solution"><a href="#Naive-Solution" class="headerlink" title="Naive Solution"></a>Naive Solution</h2><p>一种看似直观的解决方案是简单地删除式(3)中的视图相关项，如:<br>$c(p)=\mathcal{M}(p,f(p),g(p)).$ Eq.8<br>遗憾的是，进行这个过程可能会潜在地损害所学到的几何形状和神经辐射场内的外观。这是由于当视点相关项缺失时，神经辐射场无法准确表达不同方向点的光变化。此外，这种方法也可能最终导致SDF场的破裂，随后引发地表的破碎。朴素解的定性结果如图3所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230901152222.png" alt="image.png"></p><h2 id="Intermediate-Solution"><a href="#Intermediate-Solution" class="headerlink" title="Intermediate Solution"></a>Intermediate Solution</h2><p>中间的解决方案是利用一个恒定的方向作为颜色网络的输入，以便从表面提取颜色信息。例如，在HHOR[12]中，<strong>顶点颜色是根据表面的法线方向来定义的</strong>。对于任意x∈S，法线方向由- g(x)给出。因此，可以使用</p><p>$c(x)=\mathcal{M}(x,-g(x),f(x),g(x)).$Eq.9</p><p>然而，考虑到物体表面颜色在不断变化的光照条件下会发生相当大的变化，用单一的“方向色”来表示物体的真实颜色可能是不够的。与光表面相互作用相关的固有复杂性要求更细致入微的方法来提取颜色。</p><h2 id="Color-NeuS"><a href="#Color-NeuS" class="headerlink" title="Color-NeuS"></a>Color-NeuS</h2><p>在我们的方法Color-NeuS中，我们提出了一个工作流来将全局颜色从视图相关的公式中分离出来，同时保留适当的几何形状并为每个顶点获得合理的视图无关的颜色。具体来说，我们用一个<strong>独立于视图的全局颜色变量</strong>(第4.3.1节)和一个<strong>依赖于视图的重光照效果</strong>(第4.3.2节)的学习取代了单个依赖于视图的颜色组件的学习(如Eq.(3)所示)。</p><h3 id="Removing-View-Dependence"><a href="#Removing-View-Dependence" class="headerlink" title="Removing View-Dependence"></a>Removing View-Dependence</h3><p>我们提出的解决方案首先从Eq.(3)中删除视图依赖项，将模型转换为全局颜色网络，如下:</p><p>$c_{g}(\mathbf{p})=\mathcal{M}_{g}(\mathbf{p},f(\mathbf{p}),g(\mathbf{p})).$ Eq.10</p><p>该公式与Eq.(8)中提出的朴素解相匹配，后者是提取依赖于视图的顶点颜色的理想起点。然而，仅仅使用这个方程(忽略视图依赖性)来训练网络收敛将违反性质1。因此，这种矛盾可以阻止密度σ(p)凝聚到物体表面，无意中影响学到的几何。</p><p>因此，我们提出的解决方案(Color-NeuS)是<strong>将推理过程与训练分开</strong>。</p><ul><li>在训练过程中，我们将全局颜色与与视图方向不一致的残差项重新整合。该集成的结果满足体绘制公式Eq.(4)中提出的视图要求，并且在训练阶段保持属性1。</li><li>在推理过程中，一旦密度σ(p)浓缩到物体表面，就可以保证物体的形状(s(p) = 0)。另外，$c_{g}(p)$获得了表示顶点颜色的能力。</li></ul><p>因此，我们可以在推理过程中提取曲面(其中$x\in\mathcal{S}$，即$s(\mathbf{x})=0$)上的顶点颜色<br>$c_g(\mathbf{x})=\mathcal{M}_g(\mathbf{x},f(\mathbf{x}),g(\mathbf{x})).$ Eq.11</p><h3 id="Coupling-Relighting-Effect"><a href="#Coupling-Relighting-Effect" class="headerlink" title="Coupling Relighting Effect"></a>Coupling Relighting Effect</h3><p>为了在$c_{g}(x)$中去除视图依赖项后保持模型的性能，我们引入了一个重光照网络来补偿丢弃的视图依赖项。重照明网络根据位置、方向和与视图无关的颜色发挥作用，<strong>为每个点生成一个小的与视图相关的颜色调整</strong>。这可以表示为:</p><p>$c_{r}(\mathbf{p},\mathbf{d})=\mathcal{R}_{g}(c_{g}(\mathbf{p}),\mathbf{p},\mathbf{d},g(\mathbf{p})).$Eq.12</p><p>然后，将重光照效果与全局颜色进行积分，计算出每个点的最终颜色，如下:</p><p>$c(\mathbf{p},\mathbf{d})=\Psi(\Psi^{-1}(c_g(\mathbf{p}))+c_r(\mathbf{p},\mathbf{d})),$Eq.13</p><p>其中$Ψ$和$Ψ^{−1}$分别表示sigmoid函数和逆sigmoid函数。目前，颜色是全局颜色(与视图无关)和重光照效果(与视图相关)的融合。然后将$c(\mathbf{p},\mathbf{d})$纳入方程(4)中计算体积积分c。</p><h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p>沿着采样光线的渲染颜色C可以使用公式计算。(4) ~(6)。设$\widehat{C}$为地面真色，可将颜色损失定义为:</p><p>$\mathcal{L}_{c}=\frac{1}{N_{r}}\sum_{i=1}^{N_{r}}|\widehat{C}_{i}-C_{i}|_{2}^{2},$ Eq.14<br>式中，$N_{r}$为采样射线的个数。</p><p>为了使优化后的神经表示满足有效的SDF，我们对SDF预测进行了eikonal正则化[8]，如下:<br>$\mathcal{L}_{e}=\frac{1}{N_{r}N_{p}}\sum_{i,j}^{N_{r},N_{p}}(|\nabla s(p_{i,j})|_{2}-1)^{2},$ Eq.15<br>其中$N_{p}$为每条射线上的采样点数。</p><p>为了使全局颜色更接近实际颜色，我们对relight颜色$c_{r}$的平均值施加约束为零，如下所示:<br>$\mathcal{L}_{r}=\frac{1}{N_{r}N_{p}}\sum_{i,j=1}^{N_{r},N_{p}}c_{ri,j}(p,d).$Eq.16<br>这种策略促使全局颜色网络在平均光照条件下学习颜色。换句话说，<strong>这样可以最小化重光照网络对全局颜色的影响</strong>。这个损失项是必要的，因为我们不能直接监督全局颜色，原因在幼稚方案4.1中提到。</p><p>在物体重建的背景下，一种直观的方法是使用物体(前景)分割来消除背景元素。在前景分割可用且没有物体-场景遮挡的情况下，我们建议合并掩码损失$L_{m}$<br>$\mathcal{L}_{m}=\frac{1}{N_{r}}\sum_{N_{r}}BCE(M,\hat{O}),$ Eq.17</p><p>其中，$\hat{O}=\sum_{j}w_{j}$是沿相机光线的累积权值，M是表示光线是否在目标分割边界内的二进制掩码。</p><p>综上所述，我们的训练损失按照Eq.(18)计算，其中$λ_{c}， λ_{e}， λ_{r}， λ_{m}$为超参数。<br>$\mathcal{L}=\lambda_{c}\mathcal{L}_{c}+\lambda_{e}\mathcal{L}_{e}+\lambda_{r}\mathcal{L}_{r}+\lambda_{m}\mathcal{L}_{m}.$</p><p>除了优化上面提到的网络参数外，对于我们的野生数据集，<strong>我们还根据NeRF−−[26]优化了相机姿势</strong>。与GNeRF[17]一样，我们使用连续的6D向量$r\in\mathbb{R}^6$来表示三维旋转，这被证明更适合于学习[37]。联合优化公式可表示为:<br>$\Theta^{<em>},\Pi^{</em>}=\operatorname*{arg}_{\Theta,\Pi}\mathcal{L}(\Theta,\Pi),$<br>其中$Θ$为网络参数，$Π$为摄像机姿态。</p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><ul><li>8张图片，每批1024条射线</li><li>当个A10 GPU 10 iter</li><li>在前5k次迭代中，学习率首先从0线性升温到$5×10^{−4}$，然后通过余弦衰减调度控制到最小学习率$2.5×10^{−5}$</li><li>对于所有数据集，我们设置λc为1.0，λe为0.1，λr为1.0。对于没有对象分割的数据集(OmniObject3D)或发生对象-场景遮挡的数据集(IHO-Video)，我们将λm设置为0.0。相反，对于其他数据集，我们设置λm为0.1</li><li>如果分割蒙版可用，我们应用采样策略，以便在训练期间落在蒙版内的光量从50%逐渐增加到80%。</li></ul><h2 id="Empirical-Evaluation-Hand-held-Object-Scan"><a href="#Empirical-Evaluation-Hand-held-Object-Scan" class="headerlink" title="Empirical Evaluation - Hand-held Object Scan"></a>Empirical Evaluation - Hand-held Object Scan</h2><p>“手持物体扫描” HOS<br>手持对象视频’ (IHO-Video)。</p><ul><li>IHO-Video Dataset</li><li>HOS Task Evaluation</li></ul><h2 id="Evaluation-on-Public-Datasets"><a href="#Evaluation-on-Public-Datasets" class="headerlink" title="Evaluation on Public Datasets"></a>Evaluation on Public Datasets</h2><ul><li>OmniObject3D<ul><li>OmniObject3D是一个全面的3D对象数据集，其特点是具有广泛的词汇表和大量高质量，实时扫描的3D对象。</li></ul></li><li>DTU</li><li>BlendedMVS</li><li>HOD Hand-held Object Dataset (HOD) </li></ul><h2 id="Quantitative-Results"><a href="#Quantitative-Results" class="headerlink" title="Quantitative Results"></a>Quantitative Results</h2><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230901154219.png" alt="image.png"></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/Colmar-zlicheng/Color-NeuS.git</span><br><span class="line"><span class="built_in">cd</span> Color-NeuS</span><br><span class="line">conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia</span><br><span class="line">pip install -r requirement.txt</span><br><span class="line">pip install <span class="string">&quot;git+https://github.com/facebookresearch/pytorch3d.git&quot;</span></span><br></pre></td></tr></table></figure><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train</span></span><br><span class="line">python train.py -g 0 --config configs/Color_NeuS_<span class="variable">$&#123;DATASET&#125;</span>.yaml -obj <span class="variable">$&#123;OBJECT_NAME&#125;</span> --exp_id <span class="variable">$&#123;EXP_ID&#125;</span></span><br><span class="line"></span><br><span class="line">eg:</span><br><span class="line"><span class="comment"># DTU: dtu_scan83</span></span><br><span class="line">python train.py -g 0 --config configs/Color_NeuS_dtu.yaml -obj 83 --exp_id Color_NeuS_dtu_83</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">python evaluation.py -g 0 --config configs/Color_NeuS_<span class="variable">$&#123;DATASET&#125;</span>.yaml -obj <span class="variable">$&#123;OBJECT_NAME&#125;</span> -rr 512 --reload <span class="variable">$&#123;PATH_TO_CHECKPOINT&#125;</span></span><br><span class="line"></span><br><span class="line">eg:</span><br><span class="line">python evaluation.py -g 0 --cfg configs/Color_NeuS_<span class="variable">$&#123;DATASET&#125;</span>.yaml -obj <span class="variable">$&#123;OBJECT_NAME&#125;</span> -rr 512 --reload <span class="variable">$&#123;PATH_TO_CHECKPOINT&#125;</span></span><br></pre></td></tr></table></figure><h3 id="Miku"><a href="#Miku" class="headerlink" title="Miku"></a>Miku</h3><p><code>python train.py -g 0 --cfg ./config/Color_NeuS_dtu.yml -obj Miku</code><br>大概需要8h左右</p><p>恢复训练，添加关机指令： &amp;&amp; /usr/bin/shutdown<br><code>python train.py -g 0 --cfg ./config/Color_NeuS_dtu.yml -obj Miku --resume /root/autodl-tmp/Color-NeuS/exp/default_2023_0917_2027_15 &amp;&amp; /usr/bin/shutdown</code></p><p>对自定义数据集的重建效果很差，无法实现很好的渲染，而且所需时间很长</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
            <tag> Texture </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeuFace</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/NeuFace/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/NeuFace/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://github.com/MingwuZheng">Mingwu Zheng</a>, <a href="https://github.com/aejion">Haiyu Zhang</a>, <a href="https://scholar.google.com/citations?user=dnbjaWIAAAAJ&amp;hl=zh-CN">Hongyu Yang</a>, <a href="https://irip.buaa.edu.cn/dihuang/index.html">Di Huang</a></td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://github.com/aejion/NeuFace">aejion/NeuFace: Official code for CVPR 2023 paper NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images. (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4738284856274862081&amp;noteId=1937572464869295872">NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230829163918.png" alt="image.png"></p><p>贡献：</p><ul><li><strong>BRDF+SDF+PBR</strong>新框架，端到端训练，重建出Face的外观和几何</li><li>简单而新的低秩先验，镜面反射部分的Material Integral. 表示为线性组合的BRDF基</li></ul><span id="more"></span><h1 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h1><p>由于NeuFace主要侧重于复杂面部皮肤的反射建模，而<strong>没有明确解决</strong>更具挑战性的<strong>次表面散射问题</strong>，仅采用简化的阴影模型，因此本研究中获得的渲染结果的<strong>保真度可以进一步提高</strong>。<br>此外，NeuFace目前提供的是一个<strong>静态的3D人脸</strong>，<strong>而不是一个可驱动的人脸</strong>，虽然几何模型，即ImFace，是一个非线性变形模型，理论上支持可控的表情编辑，但需要更多的工作来提高性能。</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>本文提出了一种新的三维神经渲染模型，即:NeuFace，可以同时从多视图图像中捕获逼真的面部外观和几何形状。<br>为了处理复杂的面部皮肤反射率，我们将PBR与神经BRDFs结合起来，以获得更准确和更有物理意义的3D表示。<br>此外，为了促进底层brdf的优化，我们引入了一种split积分技术以及一种简单而新的低秩先验，这大大提高了恢复性能。<br>大量的实验证明了NeuFace在人脸绘制方面的优越性，以及对普通物体良好的泛化能力。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>多视点图像的真实感人脸绘制对各种计算机视觉和图形学应用都有很大的帮助。然而，由于人脸具有复杂的空间变化的反射率特性和几何特征，在目前的研究中，如何真实有效地恢复三维人脸表征仍然是一个挑战。<br>本文提出了一种新的三维人脸绘制模型NeuFace，该模型通过神经绘制技术来学习精确且有物理意义的底层3D表示。它自然地将神经BRDFs合并到基于物理的渲染PBR中，以协作的方式捕获复杂的面部几何和外观线索。具体来说，<strong>我们引入了一个近似的BRDF积分和一个简单而新的低秩先验，有效地降低了模糊性，提高了面部BRDF的性能</strong>。大量的实验证明了NeuFace在人脸渲染方面的优势，以及对常见对象的良好泛化能力。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><strong>Face Rendering</strong>问题<br>根据摄影测量学，在这个问题上的开创性研究通常利用<strong>复杂的主动照明设置</strong>，例如:LightStage[9]等，从个人的多张照片中构建3D人脸模型，其中准确的形状属性和高质量的漫反射和镜面反射属性通常被认为是其成功的前提。需要一个精心设计的工作流程，通常涉及相机校准、动态数据采集、多视角立体、材料估计和纹理参数化等一系列阶段[44]。虽然最终可以获得令人信服的3D人脸模型，<strong>但这种输出在很大程度上取决于工程师和美工的专业知识，因为多步骤的过程不可避免地会带来不同的优化目标</strong>。<br>最近，<strong>3D神经渲染</strong>提供了一种端到端替代方案，在从现实世界图像中恢复场景属性方面表现出了很好的性能，例如视图相关的亮度[27,29,37,39,49]和几何形状[34,50,51,56,57]。这主要归功于可学习的3D表示和可微分的图像形成过程的解开，摆脱了繁琐的摄影测量管道。<strong>然而，像经典的函数拟合一样，反向渲染从根本上是不受约束的，这可能会导致底层3D表示的条件拟合不良，特别是对于复杂的情况，例如，具有视图依赖高光的非兰伯曲面</strong>。随着计算机图形学与学习技术相结合的趋势，一些尝试利用物理动机的归纳偏差，并提出了基于物理的渲染(PBR)[15,32,49,58]，其中双向反射分布函数(brdf)被广泛采用。通过明确地模拟环境光与场景的相互作用，它们促进了网络优化并提供了可观的收益。<strong>不幸的是，被利用的物理先验要么是启发式的，要么是分析性的</strong>[8,21,46]，<strong>仅限于一小部分现实世界的材料，例如金属，无法描述人脸</strong>。<br>对于逼真的<strong>人脸渲染</strong>，<strong>最根本的问题在于准确建模多层面部皮肤的光学特性</strong>[22]。特别是，分布不均的细尺度油层和表皮不规则地反射入射光，导致复杂的视依赖和空间变化的高光。这种特征和面部表面的低纹理性质强烈地放大了形状-外观的模糊性。此外，真皮层和其他皮肤层之间的表皮下散射也使这一问题更加复杂。</p><p>在本文中，我们遵循了PBR范式在学习3D表示方面的潜力，并迈出了逼真的3D神经面部绘制的第一步，主要针对复杂的皮肤反射建模。我们的方法，即NeuFace，能够从多视图图像中恢复忠实的面部反射率和几何形状。具体来说，我们建立了一个PBR框架来学习神经BRDFs来描述面部皮肤，它模拟了物理正确的光传输，具有更高的表示能力。通过使用一种基于可微分符号距离函数(SDF)的表示方法，即ImFace[63]作为形状先验，<strong>可以在反渲染中同步优化面部外观和几何场</strong>。<br>与分析型BRDFs相比，神经型BRDFs可以更丰富地表征面部皮肤等复杂材料。尽管有这种优势，但这种表示对训练期间的计算成本和数据需求提出了挑战。为了解决这些困难，实时渲染技术[1]被用来分离神经 BRDFs 的半球积分，其中物质积分和光积分被单独学习，绕过了数值解法所需的大量 Monte-Carlo 采样阶段[35]。此外，在空间变化的面部BRDFs中引入了低秩先验，极大地限制了解空间，从而减少了对大规模训练观测的需求。这些模型设计确实使NeuFace能够像在真实的3D空间中一样，准确而稳定地描述光与面部表面的相互作用。图1显示了一个示例。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230829161944.png" alt="image.png"></p><p>本研究的主要贡献包括:<br>1)一个具有自然结合的PBR和神经BRDF表示的<strong>新框架</strong>，协同捕获复杂面部皮肤的面部几何和外观属性。<br>2)一种<strong>新的、简单的低秩先验</strong>，极大地促进了神经brdf的学习，提高了外观恢复性能。<br>3)令人印象深刻的人脸渲染结果仅来自多视图图像，适用于各种应用，如重照明，以及对常见物体的良好泛化能力。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>我们将讨论具体限于<strong>静态面部几何和外观捕获</strong>以及<strong>3D神经渲染</strong>。请参考[10,22,45]进行更深入的讨论。</p><p><strong>Face Capturing</strong><br>它的目标是在任意光照条件下渲染一个逼真的3D人脸。</p><ul><li>现有的方法通常利用摄影测量技术来估计面部几何形状和外观，需要大量的人工工作。在这种情况下，它们通常分解问题，其中面部几何形状是通过复杂的多视图立体过程预先捕获的[3,4]。<strong>然而，由于光与皮肤之间复杂的相互作用，具有反射率的面部外观仍然难以获得</strong>。</li><li>最初的尝试[9,17,53]通过密集地捕获每像素的面部反射率来解决这一挑战，<strong>这需要大量的数据采集和专门的设备</strong>。随后，梯度[12,20,26]或偏光[12,13,26,40]被探索以减少记忆成本，<strong>其中大部分精力都花在brdf的良好条件拟合上</strong>。</li><li>与上述研究相比，我们的解决方案是真正的端到端，<strong>只在单一的、未知的照明条件下观察面部皮肤，没有繁琐的捕捉设置</strong>。</li></ul><p><strong>3D Neural Rendering</strong></p><ul><li>该领域的最新进展，如NeRF[30]，已经彻底改变了多视图重建的范式。使用可学习的体积表示(例如，神经场[30,57]，网格[11]和混合[31])和解析可微前向映射，仅从2D图像就可以直接推断出场景属性。虽然在新的视图合成[27,29,37,39,49]或几何重建[34,50,51,56,57]方面取得了令人印象深刻的成果，<strong>但研究仍然存在2D-3D模糊性，导致难以同时建立真实的外观和精确的几何形状</strong>[59]。<strong>由于面部皮肤的反射特性比较复杂，这个问题更加突出</strong>。</li><li>Ref-NeRF[49]向精确表面法线和光滑外观的目标迈进了一步，这是通过用反射方向重新参数化经典NeRF中的辐射来实现的。它验证了物理定律在模糊中的意义。[15,23,32,43,58]进一步采用PBR管道，同时提供更高的质量和支持重照明。<strong>然而，使用或假设的简化材料模型无法处理皮肤等复杂材料</strong>。</li><li>NeRFactor等[25,61,62]从测量数据训练神经材料模型。<strong>但是获取这样的数据对于活的生物组织来说是不切实际的</strong></li></ul><p>相比之下，<strong>我们的方法直接构建神经皮肤brdf</strong>，重建精确的几何线索，而<strong>不需要任何外部数据</strong>。</p><h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><p>我们采用PBR[19]来显式预测复杂的skin reflectances，其中表面位置x处的辐射度$L_{o}$沿方向$\omega_{o}$可以表示为: $L_o(x,\omega_o)=\int_{S^2}L_i(\omega_i)f(x,\omega_i,\omega_o)(\omega_i\cdot\mathbf{n})^+\mathrm{d}\omega_i,$ Eq.1</p><p>其中，$L_{o}$的计算方法是对从$\omega_{i}$方向入射亮度$L_{i}$、皮肤BRDF f和表面法向$\mathbf{n}$与$\omega_{i}$球面$S^2$之间的半余弦函数$(\omega_i\cdot\mathbf{n})^+$的乘积进行积分。<strong>假设无阴影的单弹直接照明</strong>，因此$L_{i}$与x无关。此外，由于<strong>假设像素级的次表面散射</strong>，$L_{o}$只考虑单个小区域。</p><p>球面谐波(SH)[6]和球面高斯(SG)[58]通常被用作入射照明的有效表示。<strong>本研究选择SH是因为它的完备性和紧凑性</strong>。更重要的是，它可以隐式地促进低阶的镜面分离[47]。因此，未知$L_{i}$可以用前十阶的球形基函数$Y_{\ell m}$乘以相应的可学习系数$c_{\ell m}$来近似:$L_i(\omega_i)\approx\sum_{\ell=0}^{10}\sum_{m=-\ell}^{\ell}c_{\ell m}Y_{\ell m}(\omega_i),$ Eq.2</p><p>f被建模为两个组件，类似于现有技术<br>$f\left(x,\omega_i,\omega_o\right)=\frac{\mathbf{a}(x)}\pi+\varrho f_\mathrm{s}\left(x,\omega_i,\omega_o\right),$ Eq.3</p><p>其中左项是漫反射分量(Lambertian)，仅由反照率a(x)决定，剩余项主要负责光滑反射，由$f_\mathrm{s}\left(x,\omega_i,\omega_o\right)$表示，其比例因子$\varrho$表示镜面强度。因此，渲染方程(Eq.(1))可分为两项:<br>Eq.4:<br>$L_o(x,\omega_o)=\underbrace{\frac{\mathbf{a}(x)}{\pi}\int_{S^{2}}L_i(\omega_i)(\omega_i\cdot\mathbf{n})^{+}\mathrm{d}\omega_i}_{\text{diffuse term }L_\mathrm{d}} +\underbrace{\varrho\int_{S^2}L_i(\omega_i)f_\mathrm{s}(x,\omega_i,\omega_o)(\omega_i\cdot\mathbf{n})^+\mathrm{d}\omega_i}_{residual (specular) term L_{8}}$</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>我们将神经brdf整合到PBR中，以<strong>协同学习复杂的外观属性(即漫射反照率和镜面反照率)和复杂的几何属性</strong>。提出的NeuFace从多视图图像中进行3D人脸捕获，<strong>只需要额外的相机姿势</strong>。如图2所示，它利用Spatial MLP和Integrated Basis MLP来学习皮肤BRDFs，并通过将其明确分解为漫反射反照率、光积分和BRDFs积分来模拟物理正确的光传输。面部外观建模可以用Eq.(4)表示，我们努力解决学习过程中计算成本高和数据需求大带来的挑战。为了获得更准确的估计，NeuFace进一步利用基于sdf的表示，即imface[63]，作为前向映射的面部几何形状。<strong>通过端到端的逆渲染，将人脸外观和几何场联合恢复</strong>。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230829163918.png" alt="image.png"></p><h2 id="Specular-Modeling"><a href="#Specular-Modeling" class="headerlink" title="Specular Modeling"></a>Specular Modeling</h2><p>对于带有BRDF $f_\mathbf{s}(x,\omega_i,\omega_o),$的镜面项$L_{\mathrm{s}}$，由于$\omega_{o}$的视域依赖特性，难以求解。在先前处理非面部神经渲染的研究中，分析BRDFs，例如microfacet模型[32,42,58]，已被用于近似积分。<strong>然而，这些显式模型只能恢复一小部分材料，而不包括具有相当独特属性的面部皮肤</strong>[33]。为了获得更强的表示能力，NeRFactor[61]采用了从真实世界捕获的MERL数据库[28]中学习到的数据驱动BRDFs，<strong>但面部皮肤等具有空间变化BRDFs的活组织在体内[16]极难测量</strong>。此外，在NeRFactor中必须执行大量蒙特卡罗采样来求解渲染方程，这在我们的情况下是不实用的，因为面部几何形状最初是未知的。<br>在这项研究中，我们提出了一种<strong>不需要外部数据和数值积分的方法</strong>来渲染复杂的面部高光。特别地，受到实时渲染[1]中的分裂积分近似的启发，对镜面进行了分解，其中$L_{\mathrm{s}}$可以近似为: $L_{\mathrm{s}}\approx\varrho\int_{\mathrm{s}^{2}}f_{\mathrm{s}}\left(x,\omega_{i},\omega_{o}\right)(\omega_{i}\cdotp\mathrm{n})^{+}\mathrm{d}\omega_{i}\int_{\mathrm{s}^{2}}D(h)L_{i}\left(\omega_{i}\right)\mathrm{d}\omega_{i}$ Eq.5</p><p>其中D(h)是一个分布函数，表示光在半向量$h=\frac{\omega_i+\omega_o}{|\omega_i+\omega_o|_2}$处的反射。整个方程两侧分别是材料和光的积分项，它们在后面分别求解。</p><p><strong>Material Integral</strong>.<br>Eq.(5)中的第一个分裂积分项仅与材料属性有关，在我们的方法中，材料属性由可学习网络参数化，以获得更高的面部皮肤表示能力。考虑到二维观测只能约束积分值，而不是像[61,62]那样建模$f_{\mathrm{s}}(\tilde{x,\omega_{i}},\omega_{o})$，我们直接将整个积分表示为更平滑的函数F: $\int_{\mathrm{s}^{2}}f_{\mathrm{s}}\left(x,\omega_{i},\omega_{o}\right)\left(\omega_{i}\cdot\mathbf{n}\right)^{+}\mathrm{d}\omega_{i}=F(x,\omega_{o},\mathbf{n}).$ Eq.6</p><p>需要注意的是，对于这样一个空间变化的9变量函数，使用MLP实现鲁棒拟合需要大量的真实样本，这在实践中是不可用的。基于之前对人脸外观测量的研究[13,53]，<strong>我们提出了一个关键假设，即个体的所有面部表面位置都具有相似的镜面结构，因此空间变化的特性可以由少数(低秩)可学习BRDFs的不同线性组合表示</strong>:<br>$f_\mathrm{s}\left(x,\omega_i,\omega_o\right)\approx\sum_{j=1}^{k}c_j(x)b_j(\omega_i,\omega_o),$ Eq.7<br>其中$\{b_j(\omega_i,\omega_o)\}_{j=1}^k$表示<strong>k个全局与空间无关的BRDF基</strong>，$\mathbf{c}(x)=[c_1(x),c_2(x),…,c_k(x)]^T$表示每个表面位置x对应的线性系数。</p><p>因此，材料积分可表示为:<br>$\int_{\mathrm{s}^2}f_\mathrm{s}\left(x,\omega_i,\omega_o\right)(\omega_i\cdot\mathbf{n})^+\mathrm{d}\omega_i=\mathbf{c}(x)\cdot\mathbf{B}(\omega_o,\mathbf{n}),$ Eq.8<br>式中$\mathbf{B}(\omega_o,\mathbf{n})=[B_1,B_2,…,B_k]^T$表示k积分BRDF基$b_{j}$乘以半余弦函数$(\omega_{i}\cdot\mathbf{n})^{+}$:<br>$B_j(\omega_o,\mathbf{n})=\int_{\text{s}^2}b_j\left(\omega_i,\omega_o\right)(\omega_i\cdot\mathbf{n})^+\mathrm{d}\omega_i,j=1,2,…,k.$ Eq.9</p><p>我们利用一个MLP，即Integrated Basis MLP，来拟合$\mathbf{B}(\omega_o,\mathbf{n})$。$\omega_{o}\cdot\mathbf{n}$也被输入其中，以考虑菲涅耳效应。同时，以x为输入的Spatial MLP预测系数向量$\mathbf{c}(x)$。请注意，面部皮肤是介电的，不能对高光着色，因此我们使用单个通道$B_{j}$来表示单色面部brdf 。<br>如图 2 (c) 所示，低秩先验全局限制了所有面部镜面的解空间，而不强制任何空间平滑性[55]或采样位置聚类[32]。有了这样的先验，材料积分项就更容易拟合和插值，只需适量的训练数据就能产生令人印象深刻的结果。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230829163918.png" alt="image.png"></p><p><strong>Light Integral</strong>.<br>对于光滑表面，反射能量主要集中在反射方向附近，用$\omega_{r}$表示，如图2 (b)所示。因此，我们将$D(h)$建模为<strong>三维归一化球面高斯分布</strong>(vMF)，以$\omega_{r}$为中心，浓度参数κ表示亮度: $D(h)\approx\mathrm{vMF}(\omega_i;\omega_r,\kappa),$<br>κ是由Spatial MLP预测的，较大的值表明指向$\omega_{r}$的BRDF波瓣更尖锐，使面部皮肤看起来更有光泽。正如我们补充材料中的证明所示，方程(5)中描述镜面光输运的第二次分裂积分可以近似为:<br>$\int_{\mathrm{s}^2}D(h)L_i\left(\omega_i\right)\mathrm{d}\omega_i\approx\sum_{\ell=0}\sum_{m=-\ell}^{\ell}e^{-\frac{\ell(\ell+1)}{2\kappa}}c_{\ell m}Y_{\ell m}(\omega_r).$ Eq.11</p><p>然后，神经brdf (Eq.(4))中的整个镜面项可以有效地微分计算为:<br>$L_\mathrm{s}\approx\varrho\left(\mathbf{c}(x)\cdot\mathbf{B}(\omega_o,\mathbf{n})\right)\sum_{\ell=0}\sum_{m=-\ell}^{\ell}e^{-\frac{\ell(\ell+1)}{2\kappa}}c_{\ell m}Y_{\ell m}(\omega_r).$ Eq.12</p><h2 id="Diffuse-Modeling"><a href="#Diffuse-Modeling" class="headerlink" title="Diffuse Modeling"></a>Diffuse Modeling</h2><p>由式(2)可知，表面位置x处的漫射亮度$L_{\mathrm{d}}$可改写为:<br>$L_{\mathrm{d}}(x)=\frac{\mathbf{a}(x)}{\pi}\sum_{\ell=0}\sum_{m=-\ell}^{\ell}c_{\ell m}\int_{\mathrm{s}^{2}}Y_{\ell m}\left(\omega_{i}\right)\left(\omega_{i}\cdot\mathbf{n}\right)^{+}\mathrm{d}\omega_{i}.$Eq.13</p><p>根据Funk-Hecke定理[2]，$(\omega_{i}\cdot\mathbf{n})^{+}$与球次谐波$Y_{\ell m}$的卷积可解析计算为:<br>$\int_{\mathrm{s}^{2}}Y_{\ell m}\left(\omega_{i}\right)\left(\omega_{i}\cdot\mathrm{n}\right)^{+}\mathrm{d}\omega_{i}\approx\Lambda_{\ell m}Y_{\ell m}(\mathrm{n}),$ Eq.14</p><p>$\Lambda_{\ell m}=\begin{cases}\frac{2\pi}{3},&amp;\mathrm{if~\ell=1,}\\\frac{(-1)^{\ell/2+1}\pi}{2^{\ell-1}(\ell-1)(\ell+2)}\binom{\ell}{\ell/2},&amp;\mathrm{if~\ell~is~even},\\0,&amp;\mathrm{if~\ell~is~odd},\end{cases}$ Eq.15</p><p>详见[2]。在NeuFace中，漫射反照率a(x)由Spatial MLP建模。对于可学习的SH照明系数$c_{lm}$，漫射项可直接计算为: $L_{\mathrm{d}}(x)\approx\frac{\mathbf{a}(x)}{\pi}\sum_{\ell=0}\sum_{m=-\ell}^{\ell}\mathbf{\Lambda}_{\ell m}c_{\ell m}Y_{\ell m}\left(\mathbf{n}\right).$ Eq.16</p><p>综上所述，NeuFace的外观分量由一个Spatial MLP: $\begin{aligned}x\mapsto(\varrho,\mathbf{c},\kappa,\mathbf{a})\end{aligned}$，一个Integrated Basis MLP:$(\omega_o,\mathbf{n},\omega_o\cdot\mathbf{n})\mapsto\mathbf{B}$，以及可学习的环境光系数$c_{\ell m}.$组成。最后可以通过Eq.(12)和Eq.(16)估算出Radiance $L_{o}$。</p><h2 id="Geometry-Modeling"><a href="#Geometry-Modeling" class="headerlink" title="Geometry Modeling"></a>Geometry Modeling</h2><p>为了实现端到端训练，一个可微的几何表示是必不可少的。与大多数神经渲染实践类似[50,56 - 58]，神经SDF可用于隐式定义面部几何形状。在这里，我们利用ImFace[63]作为直接的面部SDF，以便于采样和训练。为了捕获先验分布之外的几何形状，我们对ImFace I: $x\mapsto\mathrm{SDF}$进行微调，并引入神经位移场$\mathcal{D}(x)$来纠正最终结果:$\mathrm{SDF}(x)=\mathcal{I}(x)+\mathcal{D}(x).$ Eq.17<br>根据SDF的性质，可以通过自梯度提取表面法向n: $\mathbf{n}=\nabla\text{SDF}(x).$。</p><h2 id="Sampling-and-Rendering"><a href="#Sampling-and-Rendering" class="headerlink" title="Sampling and Rendering"></a>Sampling and Rendering</h2><p>为了处理多层面部皮肤，我们像VolSDF[56]一样对亮度值进行体积渲染。对于从相机位置$\mathbf{o}\in\mathbb{R}^3$向$\omega_{o}$方向发射的射线$x(t)$，定义为$x(t)=\mathbf{o}+t\omega_{o},t&gt;0,$，密度函数定义为$\sigma(t)=\beta^{-1}\Psi_{\beta}(-\mathrm{SDF}(x(t))).$。$\Psi_{\beta}$为零均值拉普拉斯分布的累积分布函数和学习到的尺度参数β。因此，每条射线的综合辐射度由以下公式计算:<br>$\mathbf{I}(\mathbf{o},\omega_o)=\int_{t_a}^{t_b}L_o(x(t),\omega_o)\sigma(t)T(t)\mathrm{d}t,$ Eq.18<br>其中$T(t)=\exp(-\int_{0}^{t}\sigma(s)\mathrm{d}s)$为透明度。</p><p>为了加速渲染，我们不像[56]那样沿着射线密集采样点，而是首先进行积极的球体追踪[24]，快速找到表面附近的位置$t_{0}$(阈值为0.05mm)，然后从$t\in[t_a,t_b]$均匀采样32个点，其中$\begin{aligned}t_a=t_0-0.5mm\end{aligned}$,$t_{b}=t_{0}+0.5mm.$。如图3所示，对于未击中表面的光线，我们从ImFace[63]中稀疏采样由先前几何定义的球体Ω内的点，并计算累积，绕过需要大量采样的掩膜损失[57]。通过结合体和表面渲染，它在渲染质量、几何精度和采样效率之间取得了很好的平衡。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230829190928.png" alt="image.png"><br><em>抽样策略的说明。采用侵略性球体跟踪来加速体绘制的采样过程。对于未击中表面的光线，使用ImFace的几何先验定义的球体来约束采样空间。</em></p><p><strong>Neural Photometric Calibration</strong><br>为了自动校准相机之间不一致的色彩响应和白平衡，我们将每个图像的线性映射矩阵$\mathbf{A}_n\in \mathbb{R}^{3\times3}$应用于渲染的亮度值: $\mathbf{I}_n=\mathbf{A}_n\mathbf{I},$ Eq.19<br>其中$\mathbf{A}_{n}$由基于可学习的每幅图像嵌入的轻量级MLP预测。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230829195532.png" alt="image.png"></p><h2 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h2><p>NeuFace 通过复合批评器进行训练，以学习准确的外观和几何属性: $\mathcal{L}=\mathcal{L}_{RGB}+\mathcal{L}_{white}+\mathcal{L}_{spec}+\mathcal{L}_{geo}.$ Eq.20<br>设$P_{n}$为第n张多视图图像$\bar{\mathbf{I}}_n.$的像素。$\mathcal{L}$的每一项描述如下，其中λ表示权衡超参数。</p><ul><li>Reconstruction Loss.它用于监督渲染的2D人脸是否接近真实观测值:<ul><li>$\mathcal{L}_{RGB}=\lambda_{1}\sum_{n}\sum_{\omega_{o}\in P_{n}}|\mathbf{I}_{n}-\bar{\mathbf{I}}_{n}|.$ Eq.21</li></ul></li><li>Light Regularization.我们假设捕获的环境光接近白色，通过以下方式实现:<ul><li>$\mathcal{L}_{white}=\lambda_{2}\sum_{n}\sum_{\omega_{o}\in P_{n}}|L_{i}-\bar{L}_{i}|,$ Eq.22</li><li>其中，$\bar{L}_i$是通过平均RGB通道来计算的。</li></ul></li><li>Facial Specular Regularization.只有一小部分入射光在表面上(大约6%[48])直接反射，因此我们通过以下方式惩罚镜面能量:<ul><li>$\mathcal{L}_{spec}=\lambda_3\sum_n\sum_{\omega_o\in P_n}L_s.$ Eq.23</li></ul></li><li>Geometry Losses. Following ImFace，利用嵌入正则化、Eikonal正则化和一种新的残差约束进行精确几何建模:<ul><li>$\mathcal{L}_{geo}=\lambda_{4}|\mathbf{z}|^{2}+\lambda_{5}\sum_{x\in\Omega}||\nabla\mathrm{SDF}(x)|-1|+\lambda_{6}\sum_{x\in\Omega}|\mathcal{D}(x)|,$ Eq.24</li><li>其中z代表ImFace的嵌入</li></ul></li></ul><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>Dataset: FaceScape</p><p>Comparison</p><ul><li>NeRF, Ref-NeRF</li><li>Volume Rendering of Neural Implicit Surfaces</li><li>Extracting Triangular 3D Models, Materials, and Lighting From Images</li><li>PhySG</li><li>DIFFREC</li></ul><p>metrics: PSNR、SSIM[52]和LPIPS[60]、Chamfer distance</p><p>Ablation Study</p><ul><li>On Shading Model.</li><li>On Neural Integrated Basis.</li></ul><p>Extension to Common Objects</p><ul><li>DTU</li><li></li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Shadow&amp;Highlight </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
            <tag> Face </tag>
            
            <tag> Shadow&amp;Highlight </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FreeNeRF</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Sparse/FreeNeRF/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Sparse/FreeNeRF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://jiawei-yang.github.io/">Jiawei Yang</a> <a href="https://web.stanford.edu/~pavone/index.html">Marco Pavone</a> <a href="https://yuewang.xyz/">Yue Wang</a></td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://jiawei-yang.github.io/FreeNeRF/">FreeNeRF: Frequency-regularized NeRF (jiawei-yang.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4733566158242856961&amp;noteId=1937442832522335232">FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization (readpaper.com)</a></td></tr></tbody></table></div><p><strong>Fre</strong>qu<strong>e</strong>ncy regularized <strong>NeRF</strong> (FreeNeRF)<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230829140344.png" alt="image.png"><br>T为正则化持续时间，t为当前训练迭代，L为输入位置编码的长度</p><p>How：</p><ul><li>High-frequency inputs cause the catastrophic failure in few-shot neural rendering.<ul><li>位置编码中高频信号可以使高频分量更快收敛，但是过快收敛将导致少样本神经渲染中灾难性的过拟合</li><li>测试：将高频位置编码位设置为0，<code>pos_enc[int(L * x%): ] = 0,</code> ， L为位置编码的长度，x是可见比率</li></ul></li><li>Frequency regularization enjoys the benefits of both high-frequency and low-frequency signals.<ul><li>频率正则化：根据训练时间steps，线性增加的频率mask，来正则化可见频谱。<strong>即刚开始使用低频，逐步增加高频信号的可见性</strong></li><li>频率正则化有助于降低在开始时导致灾难性故障的过度拟合风险，并避免在最终导致过度平滑的欠拟合</li></ul></li><li>Occlusion regularization addresses the near-camera floaters.<ul><li>遮挡正则化，对相机附近密集场进行乘法</li></ul></li></ul><span id="more"></span><h1 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h1><p>我们的FreeNeRF有两个限制</p><ul><li>首先，较长的频率curriculum可以使场景更流畅，但可能会降低LPIPS分数，尽管达到了具有竞争力的PSNR分数。</li><li>其次，遮挡正则化会导致DTU数据集中近相机对象的过度正则化和不完整表示。<strong>每个场景调整正则化范围可以缓解这个问题</strong>，但我们选择不在本文中使用它。</li></ul><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了FreeNeRF，一种简化的少镜头神经渲染方法。<strong>我们的研究揭示了输入频率与少镜头神经渲染失败之间的深层关系</strong>。一个简单的频率正则化器可以彻底解决这个挑战。FreeNeRF在多数据集上的性能优于现有的最先进的方法，开销最小。<br>我们的研究结果为未来的研究提供了几个方向。例如，将FreeNeRF应用于受高频噪声影响的其他问题是有趣的，例如NeRF in the wild，in the dark，以及野外更具有挑战性的图像，例如来自自动驾驶场景的图像。<br>此外，<strong>在附录中，我们展示了频率正则化的NeRF产生更平滑的正频估计</strong>，这可以促进处理光滑表面的应用，如RefNeRF[32]。我们希望我们的工作能激发对少镜头神经渲染的进一步研究，并在神经渲染中更普遍地使用频率正则化。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>具有<strong>稀疏输入</strong>的新颖视图合成是神经辐射场 (NeRF) 的一个具有挑战性的问题。最近的工作通过引入外部监督（例如预训练模型和额外的深度信号）或使用非平凡的基于补丁的渲染来缓解这一挑战。在本文中，我们提出了<strong>频率正则化 NeRF</strong> (FreeNeRF)，这是一个令人惊讶的简单基线，在对普通 NeRF 进行最小修改的情况下优于以前的方法。<br>我们分析了少镜头神经渲染的关键挑战，发现频率在NeRF的训练中起着重要作用。基于该分析，我们提出了两个正则化项：一个用于正则化 NeRF 输入频率范围，另一个用于惩罚近相机密度场。这两种技术都是”free lunches”，无需额外的计算成本。<br>我们证明了即使只有一行代码更改，原始 NeRF 也可以在少样本设置中实现与其他复杂方法相似的性能。FreeNeRF 在不同的数据集上实现了最先进的性能，包括 Blender、DTU 和 LLFF。我们希望这个简单的基线能够<strong>激发人们重新思考频率在 NeRF 训练中的基本作用，在低数据模式下和更高模式下</strong></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li><strong>NeRF稀疏视图</strong><ul><li>迁移学习方法<ul><li>PixelNerf</li><li>MVSNeRF</li></ul></li><li>深度监督方法</li><li>基于补丁的正则化方法<ul><li>语义一致性正则化</li><li>几何正则化</li><li>外观正则化</li></ul></li></ul></li></ul><p>我们提出<strong>两个正则化项</strong>：</p><ul><li>频率正则化</li><li>遮挡正则化</li></ul><p>我们的方法FreeNeRF<strong>特点</strong>: </p><ul><li>无依赖，不需要昂贵的预训练，也不需要额外的监督信号</li><li>无开销，不需要额外的基于补丁的正则化训练时间</li></ul><p>贡献：</p><ul><li>我们揭示了少镜头神经渲染失败与位置编码频率之间的联系，并通过实证研究进一步验证了这一点，并通过我们提出的方法解决了。据我们所知，我们的方法是<strong>首次尝试从频率的角度解决少镜头神经渲染</strong>。</li><li>我们从稀疏输入中确定了学习NeRF的另一个常见失败模式，并通过新的<strong>遮挡正则化器</strong>来缓解它。这个正则化器有效地提高了性能并泛化了数据集。</li><li>结合，我们引入了一个简单的基线 FreeNeRF，它可以通过几行代码修改来实现，同时优于以前的最先进方法。我们的方法是无依赖和无开销的，使其成为这个问题的实用且有效的解决方案。</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p><strong>Neural fields</strong> 神经场[36]使用深度神经网络将2D图像或3D场景表示为连续函数。开创性的工作神经辐射场(Neural Radiance Fields, NeRF)[21]在各种应用中得到了广泛的研究和推进[2,3,32,19,23,13,25]，包括新型视图合成[21,18]、3D生成[25,10]、变形[23,26,28]、视频[15,35,7,24,14]。尽管取得了巨大的进步，<strong>NeRF仍然需要数百个输入图像来学习高质量的场景表示</strong>; 它无法用一些输入视图(例如3,6和9视图)合成新的视图，<strong>从而限制了它在现实世界中的潜在应用</strong>。</p><p><strong>Few-shot Neural Rendering</strong>. <strong>许多作品试图通过利用额外的信息</strong>来解决具有挑战性的少数镜头神经渲染问题。例如，外部模型可用于获得归一化——流正则化[22]、感知正则化[38]、深度监督[29,6,34]和跨视图语义一致性[11]。另一项研究[5,37,4]试图通过在一个大型的、精心策划的数据集上训练来学习可转移模型，而不是使用外部模型。最近的研究认为几何是少镜头神经渲染中最重要的因素，并提出几何正则化[22,1,8]以获得更好的性能。然而，这些方法需要在定制的多视图数据集上进行昂贵的预训练[5,37,4]或昂贵的训练时间补丁渲染[11,22,1,8]，<strong>这在方法、工程实施和训练预算方面带来了巨大的开销</strong>。在这项工作中，我们表明，通过结合我们的频率正则化和遮挡正则化，简单的NeRF可以通过最小的修改(几行代码)出奇地好地工作。与大多数以前的方法不同，我们的方法保持了与原始NeRF相同的计算效率</p><p><strong>Frequency in neural representations</strong> <strong>位置编码是NeRF成功的核心</strong>[21,31]。先前的研究[31,30]表明，神经网络通常难以从低维输入中学习高频函数。用不同频率的正弦函数编码输入可以缓解这个问题。最近的研究表明，在非刚性场景变形[23]、束调整[16]、曲面重构[33]和更宽频带的拟合函数[9]等不同应用中，逐渐增加输入频率是有好处的。我们的工作利用频率课程来解决少数镜头的神经渲染问题。值得注意的是，我们的方法不仅证明了频率正则化在从稀疏输入中学习方面的惊人有效性，而且揭示了这个问题背后的失效模式以及频率正则化为什么有帮助。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><p><strong>Neural radiance fields</strong>. 神经辐射场(NeRF)[21]使用多层感知器(MLP)将场景表示为体积密度场σ和场景中每个点的相关RGB值c。它以三维坐标$\mathrm{x~\in~\mathbb{R}^3}$和观看方向单位矢量$\mathbf{d}\in\mathbb{S}^{2},$作为输入，输出相应的密度和颜色。在其最基本的形式中，NeRF学习一个连续函数$f_\theta(\mathbf{x},\mathbf{d})=(\sigma,\mathbf{c})$，其中θ表示MLP参数。</p><p><strong>Positional encoding</strong>. 直接优化原始输入(x, d)上的NeRF通常会导致合成高频细节的困难[31,21]。为了解决这个问题，最近的研究使用不同频率的正弦函数将输入映射到高维空间[21]<br>$\gamma_{L}(\mathbf{x})=\left[\sin(\mathbf{x}),\cos(\mathbf{x}),…,\sin(2^{L-1}\mathbf{x}),\cos(2^{L-1}\mathbf{x})\right],$  Eq.1</p><p>其中L是一个控制最大编码频率的超参数，对于坐标x和方向向量d可能会有所不同。一种常见的做法是将原始输入与频率编码输入连接起来:$\mathbf{x’}=[\mathbf{x},\gamma_L(\mathbf{x})]$ Eq.2，此连接应用于坐标输入和视图方向输入。</p><p><strong>Rendering</strong>. 为了在NeRF中渲染一个像素，从相机原点o沿着方向d投射一条射线$\mathbf{r}(t)=\mathbf{o}+t\mathbf{d}$穿过像素，其中t是到原点的距离。在投射光线的近界和远界$[t_\mathrm{near},t_\mathrm{far}]$内，NeRF使用K个采样点$\mathbf{t}_{K}=\{t_{1},\ldots,t_{K}\}$的求积来计算该光线的颜色：<br>$\hat{\mathbf{c}}(\mathbf{r};\theta,\mathbf{t}_K)=\sum_{K}T_k(1-\exp(-\sigma_k(t_{k+1}-t_k)))\mathbf{c_k},$<br>$\text{with}\quad T_{k}=\exp\left(-\sum_{k’&lt;k}\sigma_{k}^{\prime}\left(t_{k’+1}-t_{k’}\right)\right),$ Eq.3</p><p>$\hat{\mathbf{c}}(\mathbf{r};\theta,\mathbf{t}_K)$为最终的积分色。注意，采样点$\mathbf t_{K}$是按远近顺序排列的，即指数k较小的点离相机原点更近。</p><h2 id="Frequency-Regularization"><a href="#Frequency-Regularization" class="headerlink" title="Frequency Regularization"></a>Frequency Regularization</h2><p>少镜头神经渲染最常见的失效模式是过拟合。NeRF从一组没有明确的3D几何图形的2D图像中学习3D场景表示。通过优化2D投影视图中的外观，隐式地学习3D几何。然而，由于只有少数输入视图，NeRF很容易过度拟合这些2D图像，损失很小，同时不能以多视图一致的方式解释3D几何。从这些模型中综合新的观点会导致系统失败。如图1左侧所示，在合成新视图时，没有NeRF模型能够成功恢复场景几何形状。</p><p><strong>高频输入可能会加剧少数镜头神经渲染中的过拟合问题</strong>。[31]表明，<strong>高频映射使高频分量的收敛速度更快</strong>。然而，高频上的过快收敛阻碍了NeRF对低频信息的探索，并显著地使NeRF倾向于不需要的高频伪影(图1中的喇叭和房间示例)。在少拍场景中，NeRF对易受影响的噪声更敏感，因为需要学习相干几何的图像更少。因此，我们<strong>假设高频成分是在少量神经渲染中观察到的失效模式的主要原因</strong>。我们在下面提供了经验证据</p><p>我们研究了当输入由不同数量的频带编码时，普通NeRF是如何执行的。为了实现这一点，我们使用掩码(集成)位置编码来训练mipNeRF[2]。具体来说，我们设置<code>pos enc[int(L*x%]):]=0</code>，其中L表示经过位置编码(Eq.(1))后的频率编码坐标的长度，x为可见比。我们在这里简单地说明我们的观察结果，并把实验的细节推迟到§4.1。图2显示了DTU数据集在3个输入视图设置下的结果。正如预期的那样，我们观察到当向模型提供更高频率的输入时，mipNeRF的性能显着下降。当使用总嵌入位的10%时，mipNeRF实现了17.62的高PSNR，而普通mipNeRF本身仅实现9.01的PSNR(在100%可见比下)。这两种模型之间的唯一区别是是否使用掩码位置编码。虽然去除高频成分的很大一部分避免了在训练开始时的灾难性故障，但它不会导致竞争性的场景表示，因为渲染的图像通常是过度平滑的(如图2放大补丁所示)。尽管如此，值得注意的是，<strong>在少数场景中，使用低频输入的模型可能比使用高频输入的模型产生更好的表示</strong>。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230829144515.png" alt="image.png"></p><p>基于这一经验发现，我们提出了一种频率正则化方法。给定长度为L + 3的位置编码(Eq.(2))，我们<strong>使用线性增加的频率掩码</strong>α来<strong>根据训练时间步长调节可见频谱</strong>，如下所示:</p><p>$\gamma_{L}^{\prime}(t,T;\mathbf{x})=\gamma_{L}(\mathbf{x})\odot\mathbf{\alpha}(t,T,L),$ Eq.4<br>$\text{with }\alpha_i(t,T,L)=\begin{cases}1&amp;\text{if }i\le\frac{t\cdot L}{T}+3\\\dfrac{t\cdot L}{T}-\lfloor\dfrac{t\cdot L}{T}\rfloor&amp;\text{if }\frac{t\cdot L}{T}+3<i\le\frac{t\cdot L}{T}+6\\0&\text{if }i>\frac{t\cdot L}{T}+6\end{cases}$ Eq.5</p><p>式中$\alpha_i(t,T,L)$为$\alpha(t,T,L)$的第i位值，t和T分别为频率正则化的当前训练迭代和最终迭代。具体地说，我们从没有位置编码的原始输入开始，随着训练的进行，每次线性增加3位的可见频率。这个时间表也可以简化为一行代码，如图1所示。我们的频率正则化绕过了训练开始时不稳定、易受影响的高频信号，逐步提供NeRF高频信息，避免过平滑。<br>我们注意到，我们的频率正则化与其他工作中使用的粗到细频率调度有一些相似之处[23,16]。<strong>与他们不同的是，我们的工作侧重于少镜头神经渲染问题，揭示了高频输入引起的灾难性故障模式及其对该问题的影响</strong>。</p><h2 id="Occlusion-Regularization"><a href="#Occlusion-Regularization" class="headerlink" title="Occlusion Regularization"></a>Occlusion Regularization</h2><p>频率正则化并不能解决少镜头神经渲染中的所有问题。由于训练视图的数量有限和问题的不适定性质，在新视图中可能仍然存在某些特征伪象。这些失效模式通常表现为靠近相机的“墙壁”或“漂浮物”，如图3底部所示。即使有足够数量的训练视图[3]，仍然可以观察到这样的工件。为了解决这些问题，Mip-NeRF 360[3]提出了失真损失。然而，我们的实验表明，<strong>这种正则化在很少的镜头设置中没有帮助，甚至可能加剧问题</strong>。</p><p>我们发现大多数的失败模式源自训练视图中重叠最少的区域。图3显示了3个训练视图和2个带有“白墙”的新视图的示例。为了演示(图3中的(a)和(b))，我们手动标注了训练视图中重叠最少的区域。由于可用信息极其有限(一次性)，这些区域很难从几何角度进行估计。因此，<strong>NeRF模型将这些未开发区域解释为位于相机附近的密集体积漂浮物</strong>。我们怀疑在[3]中观察到的漂浮物也来自这些重叠最少的区域。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230829144547.png" alt="image.png"><br><em>遮挡正则化示意图。我们展示了3个训练视图(实心矩形)和2个新视图(虚线矩形)，这些视图由频率正则化的NeRF渲染。在训练视图(虚线圈)中，新视图中的漂浮物似乎是靠近相机的密集场，因此我们可以直接惩罚它们，而不需要在[11,22]中进行昂贵的新视图渲染。</em></p><p>如上所述，新视图中漂浮物和墙壁的存在是由不完善的训练视图引起的，因此可以在训练时直接解决，而不需要进行新姿态采样[22,11,37]。为此，我们提出了一种简单而有效的“遮挡”正则化方法，对相机附近的密集场进行惩罚。我们定义: $\mathcal{L}_{occ}=\frac{\sigma_{K}^{\mathsf{T}}\cdot\mathrm{m}_{K}}{K}=\frac{1}{K}\sum_{K}\sigma_{k}\cdot m_{k},$ Eq.6</p><p>其中$m_k$是一个二进制掩模向量，它决定了一个点是否会被惩罚，$\sigma_{K}$表示沿着射线采样的K个点的密度值，其顺序从近到远。为了减少相机附近的固体漂浮物，我们将指数 M以下的$m_k$值(正则化范围)，设置为1，其余为0。闭塞正则化损失易于实现和计算。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>Datasets &amp; metrics</p><ul><li>NeRF Blender合成数据集(Blender) DietNeRF</li><li>DTU数据集 RegNeRF</li><li>LLFF数据集 RegNeRF</li><li>metrics：PSNR, SSIM和LPIPS <ul><li>and $\mathrm{MSE~=~10^{-PSNR/10}},$$\sqrt{1-\mathrm{SSIM}},$LPIPS,</li></ul></li></ul><p>Implementations<br>直接改进NeRF和mipNeRF</p><p>Hyper-parameters</p><ul><li>$T=\lfloor90\%*\text{total-iters}\rfloor$ to  3-view setting</li><li>$T=\lfloor70\%*\text{total-iters}\rfloor$ to  6-view setting</li><li>$T=\lfloor20\%*\text{total-iters}\rfloor$ to  9-view setting</li><li>$\mathcal{L}_{occ}$ 使用0.01权重</li><li>LLFF和Blender的正则化范围设置为M = 20, DTU的正则化范围设置为M = 10</li></ul><p>Comparing methods.<br>除非另有说明，否则我们直接使用<strong>DietNeRF</strong>[11]和<strong>RegNeRF</strong>[22]中报告的结果进行比较，因为我们的方法是使用它们的代码库实现的。我们还包括我们的复制结果供参考。</p><h2 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h2><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p>Frequency curriculum.<br>Occlusion regularization</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Sparse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tools </tag>
            
            <tag> Sparse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Floaters No More</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Sampling/Floaters%20No%20More/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Sampling/Floaters%20No%20More/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Floaters No More: Radiance Field Gradient Scaling for Improved Near-Camera Training</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://julienphilip.com/">Julien Philip</a>1, <a href="https://valentin.deschaintre.fr/">Valentin Deschaintre</a>1</td></tr><tr><td>Conf/Jour</td><td>The Eurographics Association</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://gradient-scaling.github.io/#Code">Floaters No More: Radiance Field Gradient Scaling for Improved Near-Camera Training (gradient-scaling.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4767319201526317057&amp;noteId=1933542681554555392">Floaters No More: Radiance Field Gradient Scaling for Improved Near-Camera Training (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828181707.png" alt="image.png"></p><p><strong>消除</strong>由近平面过度采样导致的<strong>摄像头附近漂浮物</strong><br>可以通过几行代码简单的用于：</p><ul><li>Mip-NeRF 360</li><li>InstantNGP</li><li>DVGO</li><li>TensoRF</li></ul><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了一种简单而有效的梯度缩放方法，<strong>在防止背景塌陷的同时，消除了NeRF类方法中对近平面设置的需要</strong>。<br>我们的方法计算效率高，解决了大多数已发表方法中存在的采样不平衡问题。这在物体距离相机任意近或距离不同的捕捉场景中尤为重要。我们的缩放直接适用于大多数类似NeRF的表示，并且可以很容易地与几行代码集成</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>NeRF采集通常需要仔细选择不同相机的近平面，否则会遭受背景崩溃，在捕获场景的边缘产生浮动伪影。这项工作的关键见解是，背景塌陷是由靠近相机的区域的高密度样本引起的。由于这种采样不平衡，近相机体积接收到更多的梯度，导致不正确的密度积累。<strong>我们提出了一种梯度缩放方法来平衡这种采样不平衡</strong>，<strong>消除了对近平面的需要，同时防止背景崩溃</strong>。我们的方法可以在几行代码中实现，不会产生任何显著的开销，并且与大多数NeRF实现兼容。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>神经辐射场(Neural Radiance Fields, NeRF) [MST <em> 20]引入了一种新的方法，对给定一组多视图图像的真实捕获物体和场景进行3D重建和渲染。NeRF方法<strong>基于可微分的体积渲染</strong>，在体积的每个点上指导密度和辐射的存在或不存在。这种灵活的表示和令人印象深刻的重建质量激发了新的研究方向，在重建质量、重建速度、渲染速度、模型压缩或所需内存等方面改进了原始公式。<em>*尽管如此，一些问题，特别是背景崩溃和浮动</em></em>仍然存在于大多数方法的重建中，特别是对于真实场景，这指向了一个更根本的问题。在这项工作中，我们研究并提出了一个假设和一个简单的解决方案来解决背景崩溃和近相机漂浮物的问题。</p><p>背景塌缩症状是非常明显的浮动伪影，出现在训练摄像机附近，<strong>错误地将一些背景烤成前景密度</strong>。</p><ul><li>先前的研究Mip-NeRF 360[BMV∗22]已经确定了这个问题，该研究在强迫密度集中并接近狄拉克[BMV∗22]的损失中增加了一个项来解决这个问题。<strong>虽然这个项确实减少了背景崩溃，但它并没有解释它，并且在优化中存在一些先验，这可能不适合所有场景</strong>。</li><li>另一个经常被掩盖的在减少背景塌缩中起作用的细节是<strong>在射线行进过程中使用近平面</strong>。它完全防止梯度反向传播到近相机区域，因为它们没有采样，<strong>但它是场景相关的，必须手动调整，并且在物理上是不准确的</strong>。此外，对于与被摄对象距离不同的复杂捕获，可能不存在良好的全景式近平面。</li><li>相反，我们认为任何NeRF方法都应该能够直接跟踪来自相机的光线，而不会导致背景塌陷或漂浮物，并且<strong>这种伪影的一个可能原因是近相机体积元素接收的梯度不成比例</strong>。因此，我们建议在反向传播期间缩放梯度，以使用非常简单的近似来补偿这种不平衡。这种缩放允许我们完全消除对近平面的需求，同时防止背景塌陷。</li></ul><p>基于NeRF的方法主要在其底层体积数据结构上有所不同，这些数据结构可以简单到多层感知机[BMT∗21,BMV∗22,MST∗20]，体素哈希网格[MESK22]，张量分解[CXG∗22]甚至是普通体素[SSC22a]。<strong>我们表明，无论选择的数据结构如何，重构都受益于我们的梯度缩放方法</strong>。</p><p>贡献：</p><ul><li>确定一个背景塌陷可能的根本原因：近相机区域梯度接收的不平衡。</li><li>提出一个轻量级的梯度缩放解决方案。</li><li>演示了它在具有广泛不同数据结构的几种方法中的有效性。</li></ul><h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p>我们证明我们的方法对许多现有的NeRF [MST <em> 20]相关方法是有益的。我们首先<strong>介绍基于辐射场的视图合成方法</strong>，然后<strong>进一步讨论辐射场文献中的背景塌陷和近相机采样问题</strong>，以及迄今为止<strong>如何解决这个问题</strong>。为了更详尽地了解最近快节奏的文献，我们推荐Tewari等人最近对神经渲染的调查Advances in Neural Rendering[TTM </em> 22]。</p><p><strong>NeRF representations</strong><br>NeRF [MST * 20]为新视图合成引入了一种不同的表示。与依赖于预计算几何和重投影的基于网格的方法[OCDD15, HRDB16, HPP∗18,RK20, RK21,PMGD21]或基于点云的方法[ASK∗20,KPLD21, RALB22, KLR∗22]相反，辐射场通过可微射线推进和体绘制共同优化三维体积密度场和六维辐射场。这种方法经过调整，可以使用各种数据结构来存储底层字段，从而提供不同的质量、紧凑性、优化速度和呈现速度权衡。</p><ul><li>最初的工作线[MST∗20,ZRSK20]使用mlp来表示场景，后来扩展到防止混叠[BMT∗21]，处理无界场景[BMV∗22]并更好地表示反射[VHM∗22]。<strong>虽然这些方法提供了一些最高质量的结果，但它们的优化速度相对较慢，渲染速度也很慢，通常每帧需要几秒钟</strong>。</li><li>为了克服这些限制，一些作品在表示中重新引入了一些局部性，以避免对每个空间点评估一个大的MLP。KiloNeRF [RPLG21]提出先训练NeRF，然后再用微小的局部mlp再现优化后的场，从而加快渲染速度。以类似的精神，PlenOctrees [YLT <em> 21]在训练原始NeRF后烘烤一个八叉树。Hedman等人提出烘烤[HSM </em> 21]预训练的NeRF来提高渲染速度，<strong>然而，它并没有提高优化时间</strong>。</li><li>加速优化已被证明可以通过直接优化网格或基于体素的数据结构[STC∗22,SSC22a,SSC22b]。在这种快速优化的背景下，紧凑性也得到了改进，使用张量分解[CXG∗22]或散列体素[MESK22]和自定义CUDA内核来实现极快的优化和渲染。</li></ul><p>我们表明，这种表示的选择与我们的贡献是正交的，并且我们的梯度缩放可以很容易地集成，通过对具有不同表示的突出方法进行评估来解决背景崩溃。</p><p><strong>Floaters, Background collapse and Sampling</strong><br>一些工作观察并提出了解决基于辐射场的方法中浮子和背景塌缩问题的方法。</p><ul><li>Roessle等人[RBM∗22]提出在稀疏捕获的背景下使用深度先验来解决这个问题，而在NeRFShop中，Jambon等人[JKK∗23]承认浮动的问题，并提出了一种编辑方法来去除它们。MipNeRF360 [BMV <em> 22]提出了一种损耗，鼓励密度沿着射线集中在单个点周围，有效地减少近相机的半透明辐射。这种相对较大的损失进一步提高了效率[SSC22b]。NeRF in the Dark[MHMB </em> 22]也建议减少权重方差以减少浮动。FreeNeRF [YPW23]详细讨论了这个问题，将其称为“墙壁”和“漂浮物”，注意到它们存在于相机附近，因此建议用遮挡损失来惩罚相机附近的密度。<strong>然而，使用这些损失会对场景密度分布施加先验，这可能不适合所有内容</strong>。<strong>此外，这些方法并不能解释这种现象的根本原因，也不能解释为什么密度在相机附近而不是在其他地方积聚</strong>。</li></ul><p>在这项工作中，我们提供了一种可能的解释和解决方案，通过注意到近相机区域是过度采样的，因此接收到更强烈和更频繁的梯度。<br>Nimier-David等人[NDMKJ22]在体积效应优化(例如烟雾)的背景下发现了类似的采样问题，<strong>其中与透射率和密度的积成比例的采样导致密度累积斑块，而仅与透射率成比例的采样可显着减少伪影</strong>。<br>在<strong>我们</strong>的工作中，我们没有修改采样，而是<strong>考虑了它在反向传播步骤中的不平衡</strong>。</p><h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><p>在下一节中，我们简要回顾了辐射场优化(第3.1节)，我们定义了问题(第3.2节)，并提出了我们对该问题原因的假设(第3.3节)。</p><h2 id="Neural-Radiance-Fields-optimization"><a href="#Neural-Radiance-Fields-optimization" class="headerlink" title="Neural Radiance Fields optimization"></a>Neural Radiance Fields optimization</h2><p>大多数NeRF方法共享共同的组件和假设，以优化其体积表示。从校准相机捕获的一组图像开始，目标是优化发射emissive体积密度，以便在使用分段恒定体积渲染渲染时再现输入图片的外观。一般优化框架在输入的训练图像中选择像素点，生成一条从摄像机出发，朝向所选像素点的射线，并沿着射线在离散位置对数据结构进行采样，从而进行射线行进，从而获得颜色和密度。这些样本的颜色和密度被整合，以获得通过该像素投射的每条光线的颜色。最后将这些颜色的聚合与原始像素值进行比较，导致使用随机梯度下降进行优化的损失。</p><h2 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h2><p>仅给定一组校准的输入图像，NeRF重建问题是不适定的，存在幼稚的解决方案。例如，靠近每个相机的一个平面，再现它们的图像内容，可能导致重建损失为0。在实践中，数据结构和损失正则器的性质部分地阻止了这种情况的发生。然而，一些人工制品经常保留下来:最突出的两个是漂浮物和一种被称为背景塌陷的现象，在这种现象中，一些几何形状在相机附近被重建。反过来，从其他角度来看，<strong>这种错误重建的几何形状被视为浮动几何形状</strong>。<strong>请注意，虽然背景塌陷会导致浮动，但一些浮动可能有不同的来源，我们的方法无法解决</strong>。</p><p>在图2中，我们用false颜色可视化深度图，从深蓝色(近)到红色(远)。我们可以在深度图中看到没有正则化器损失的深蓝色区域(a)，表明在相机附近重建了一些几何形状。在MipNeRF360中，作者提出了一个相对复杂的loss $\mathcal{L}_{\mathrm{dist}}$，以部分解决浮动和背景塌陷问题——这些问题与近距离摄像机密度在其他视角下表现为浮动几何相关联。对于给定的射线，损失的目的是将样本权值合并到尽可能小的区域内</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828161137.png" alt="image.png"></p><p>虽然这种损失部分地阻止了背景崩溃，但它并不能解释它。此外，它推动密度集中，这可能是一个问题，如果半透明的表面出现在场景中，因为它们是用部分密度表示的。<br>另一种减轻背景塌缩的简单方法是为superior 零的光线设置一个近平面，即光线不是从相机中心开始，而是从它有一定距离。实际上，这个技巧在大多数nerf相关的工作中都有使用，但很少讨论，也很少讨论它的含义。在优化像素时，<strong>使用非零近平面可防止任何梯度影响由相机中心和近平面形成的金字塔</strong>。<strong>另一方面，它防止在这个金字塔中重建和渲染，这意味着应该仔细选择近平面距离</strong>。如果它离相机太近，可能会出现背景塌陷，如果它太远，在重建过程中会丢失一些几何形状，并可能导致其他伪影。确实，当近平面位置过远时，模型必须在近平面之后用密度表示训练像素的颜色。虽然通常可以为近平面距离找到合理的值，<strong>但在一般情况下，它需要对每个场景进行调整</strong>。在从不同距离拍摄主体的情况下，这个近平面距离可能需要为每台相机独立设置。<strong>此外，当某些内容在离相机很近的地方被捕获时，可能就没有什么好的价值了</strong>。</p><h2 id="Cause"><a href="#Cause" class="headerlink" title="Cause"></a>Cause</h2><p>我们假设背景塌缩主要是由接收到的近相机体积梯度不成比例引起的。如图3所示，来自相机的光线投射类似于光的传播，并且遭受类似的二次衰减。给定一个相机和一个可见的体积元素，并假设沿光线的采样间隔相等，<strong>则落在体积元素中的采样密度与到该相机的距离的平方的反比成正比</strong>。这意味着靠近相机的体积比它的其余部分采样更多，<strong>并且靠近相机的区域每个体积元素接收到更多的梯度，从而鼓励密度的快速建立</strong>，并创建浮动伪影。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828162428.png" alt="image.png"><br><em>当光线从相机向场景传播时，采样点的密度降低:第一个矩形中所有区域都被采样，而第二个矩形(用红色圆圈标记)中$\frac{1}{9}$只有个采样点。</em></p><p>事实上，由于用于表示亮度场的数据结构通常是连续的，体积元素较高的采样率直接转化为用于表示体积的密度和颜色的变量的更强和更频繁的梯度。例如，在使用类体素结构[STC <em> 22, MESK22, SSC22a, SSC22b]的半离散表示中，权值具有局部排列。在直接体素网格优化(DVGO) [SSC22a]和后续工作(DVGOv2) [SSC22b]中，只有与包含采样点的体素角相关的8个权重受到反向传递的影响。在这些情况下，<strong>更高的采样密度直接转化为更频繁地接收梯度，从而更快地更新</strong>。同样的推理也可以应用于NGP [MESK22]中不同级别的哈希网格。在类似MLP的隐式表示[MST∗20,BMT∗21,BMV∗22]的情况下，<em>*更高的采样率意味着MLP在近相机空间接收到比其他地方更多的信号</em></em>。</p><p>我们还注意到，当低频尚未拟合时，这种采样不平衡在训练早期具有最强的影响。在这个早期训练阶段，梯度可能在局部非常一致，因为它们都朝着同一个全局方向推进。例如，如果在早期迭代中预测的小体积的颜色在灰色周围变化，但目标是红色，则所有点都将获得近似相同的梯度以将颜色更改为更红。<strong>在这种情况下，这意味着影响体积元素的权重的梯度随该体积的采样密度线性缩放，并且权重变化得更快</strong>。<br>密度大的近相机部分，梯度应该进行缩小</p><h2 id="Sampling-in-NeRF"><a href="#Sampling-in-NeRF" class="headerlink" title="Sampling in NeRF"></a>Sampling in NeRF</h2><p>给定一个针孔相机$c_i$，视点方向为$\vec{d}_i,$，对图像平面上的像素进行均匀采样。沿着这些射线，假设对点进行线性采样，则给定点p处的采样密度为:<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828163528.png" alt="image.png"></p><ul><li>其中$\nu_i(p)$是可见性函数(如果p在相机视场中，则为1，否则为0)。</li><li>第二项解释了边界上光线的空间密度较低</li><li>第三项解释了光线随距离的扩散。对于合理的相机视场，与距离衰减相比，第二项的影响可以忽略不计，我们可以近似地计算出$\frac{|p-c_i|}{\vec{d}_i\cdot(p-c_i)}\approx1$，得到:</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828164357.png" alt="image.png"><br>$\delta_{p}^i$ 是$c_i$与p之间的距离</p><p>对于有n个摄像机的完整场景，我们可以计算给定点p处的采样密度为所有摄像机的密度之和：$\mathsf{p}(p)\approx\sum_{i=0}^{n}\nu_{i}(p)\times\frac{1}{(\delta_{p}^{i})^{2}}$ Eq.3</p><p>由公式3中的总和给出的主要直觉是，对于一个点，可见且靠近给定的相机，总和由单个相机项主导，而对于距离相机较远且距离大致相等的点，可见性项起着重要作用。</p><ul><li>对于靠近摄像机的点，距离的平方反比有非常显著的影响，而这些点往往只有少数摄像机可见。</li><li>另一方面，拍摄主体周围的点往往会被更多的相机看到。这种相机可见现象如图4a所示。</li><li>在图4b中，我们以对数尺度说明了沿相机光线的平均采样密度。我们可以看到，在相机附近，密度呈二次衰减，尽管能见度较低，<strong>但靠近相机的体积的采样密度不成比例，导致这些区域的梯度不成比例</strong>。</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828164654.png" alt="image.png"><br><em>将看到光线上一个点的相机的平均数量可视化为到光线原点距离的函数。从各种方法[BMV∗22,CXG∗22,SSC22a,MESK22]对12个场景中所有摄像机的得分进行平均。大多数靠近摄像机的点只能被少数摄像机看到。能见度增加，直到达到被摄物的平均距离，然后再次下降</em></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828164757.png" alt="image.png"><br><em>可视化的平均体积采样的光线在盆景场景作为一个函数到相机的距离。我们看到，尽管能见度很低，但靠近相机的体积单位仍被过度采样</em></p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>为了补偿靠近相机的采样密度不平衡，我们建议在反向传递期间缩放每个点特征(如密度或颜色)反向传播到NeRF表示(MLP，体素网格等)的梯度。我们建议采用以下梯度缩放:</p><p>$s_{\nabla p}=min(1,(\mathbf{\delta}_{p}^{i})^{2})$ Eq.4</p><p>也就是我们用$\nabla p\times s_{\nabla p}.$来代替$\nabla_{p}$。其中$\delta_{p}^{i}$是点到相机光线原点的距离。</p><p>这种缩放补偿了靠近相机的主要正方形密度，同时保持其余的梯度不变。<strong>请注意，给定点的缩放取决于光线投射的相机</strong>。</p><p>对于提出的方法，我们假设相机和捕获内容之间的典型距离为1个单位的距离。我们使用这个假设来推导Eq. 4，并且在我们的实验中没有调整场景比例，因为大多数场景都遵循这个假设。如果场景尺度与此假设有显著差异，并且捕获的内容处于σ单位距离的数量级，则权重可以替换为:$s_{\nabla P}=min(1,\frac{(\delta_{p}^{i})^{2}}{\sigma^{2}})$ Eq.5</p><p>在摄像机标定的基础上，有可能自动估计出σ。<br><strong>直接作用于梯度是不常见的，但设计一个损失来达到类似的效果将是具有挑战性的</strong>: 损失需要访问样本的单个密度/颜色，因为不可能根据体积积分后的距离对单个点产生不同的影响。正则化单个密度/颜色会对它们的值施加先验，而缩放梯度会降低它们变化的速度。<strong>此外，密度为0的样品对前向颜色没有贡献，但可能会收到显著的梯度</strong>。添加新的损失意味着将该损失的梯度添加到其他损失/正则化器的梯度(微分求和规则)，使其难以重现缩放的效果。<br><strong>相比之下，梯度缩放是直接的，它改变了相机附近样本的密度/颜色的更新幅度，有助于避免局部最小值</strong>。</p><h2 id="Non-linear-space-parameterization"><a href="#Non-linear-space-parameterization" class="headerlink" title="Non-linear space parameterization"></a>Non-linear space parameterization</h2><p>一些方法[BMV * 22]使用坐标$f(p)\in\mathbb{R}^3\rightarrow\mathbb{R}^3$的非线性参数化将无界场景拟合到有界坐标中。在这种情况下，应该考虑空间的体积收缩来缩放梯度。这个收缩因子是雅可比矩阵$\mathbb{J}_{f}$ of f的行列式的绝对值。这样缩放就变成:<br>$s_{\nabla p}=min(1,\frac{(\delta_{p}^{i})^{2}}{|\det(\mathbb{J}_{f}(p)|})$</p><p>根据映射，$\det(\mathbb{J}_f(p))$计算起来可能不容易。在我们的实验中，我们没有使用它，但未来的工作可能需要它。事实上，在MipNeRF360中，场景的中心空间，包含大多数相机不受映射影响，因此雅可比矩阵是恒等式。</p><h2 id="Implementation-and-performance"><a href="#Implementation-and-performance" class="headerlink" title="Implementation and performance"></a>Implementation and performance</h2><p>在PyTorch中使用自定义autograd实现此操作非常简单。函数，如图5中的10行代码所示。我们还在补充材料中提供了一个JAX实现，它直接与multinerf [MVS * 22]代码库兼容。使用此操作，可以在数据结构(MLP，哈希网格，体素等)采样之后，以及沿着射线的点积分之前插入对它的单个调用。这确保了每个点梯度是独立缩放的，同时影响控制其特征(密度和颜色)的权重。第4.2节中给出的代码在逆向传递300k个点时产生100μs的开销，这在~ 50ms迭代的上下文中可以忽略不计。它可以很容易地在大多数代码库中使用，只需进行最小的调整。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828170720.png" alt="image.png"></p><h1 id="Evaluations"><a href="#Evaluations" class="headerlink" title="Evaluations"></a>Evaluations</h1><p>我们对广泛的体积重建方法和表示进行了经验评估。虽然我们没有对改进优化的收敛性提供理论分析，但我们发现所提出的缩放减少了所有测试方法的浮动数，同时保留或改进了包括优化损失在内的定量度量。</p><h2 id="Clamped-quadratic-scaling"><a href="#Clamped-quadratic-scaling" class="headerlink" title="Clamped quadratic scaling"></a>Clamped quadratic scaling</h2><p>考虑到问题的性质，一个直接的候选方法可能是将梯度按$(\mathbf{\delta}_p^i)^2$缩放，以补偿采样密度的二次衰减。虽然这确实解决了近相机采样不平衡问题，但它会导致远离相机的非常强的梯度，从而无法正确学习如图6(中行)所示的曲面。<br>如式3所示，给定点的采样密度是所有摄像机采样密度之和的结果。这意味着在每个给定的相机附近，体积元素的采样密度的逆二次性质的假设大多是有效的，因为来自其他相机的采样在该区域可以忽略不计。因此，当到达场景的主要内容(深度为1左右)时，我们选择不修改梯度，让相机的分布引导这些区域的采样密度，如图4b所示。<br>让更多的相机看到一个点具有潜在的积极作用，因为它将样本集中在有趣的区域，而不是近相机采样不平衡，这纯粹是由于光线行进过程的性质而产生的伪影。我们在图6中说明，我们的梯度缩放方法有助于将梯度集中在场景中心附近，防止背景崩溃，并且使用纯二次缩放会导致更差的收敛和重建。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828170940.png" alt="image.png"></p><p>我们比较了使用NGP自定义实现的三种不同的梯度缩放方法[MESK22]。对于它们，我们设近平面为0。</p><ul><li>在最上面一行中，我们可以看到，在没有任何缩放的情况下，密度在训练摄像机附近建立得非常快，虽然在优化过程中有些密度被删除了，但一些近距离摄像机密度最终保留了下来。</li><li>在中间一行，我们可以看到，纯二次缩放导致远密度的偏向。Table首先在背景重建，优化不会从这个错误的初始化中恢复。</li><li>我们在下面一行展示了我们的<strong>闭合梯度缩放</strong>的结果。我们清楚地看到了该方法的优点，该表的几何形状被快速而良好地重建，并且在没有背景崩溃的情况下收敛到更好的估计。</li></ul><h2 id="Gradient-scaling-for-various-NeRF-representation"><a href="#Gradient-scaling-for-various-NeRF-representation" class="headerlink" title="Gradient scaling for various NeRF representation"></a>Gradient scaling for various NeRF representation</h2><p>在本节中，我们将<strong>展示添加梯度缩放到各种现有方法的效果</strong>，并展示它删除了所有这些方法的背景折叠效果。这在补充材料中的视频中尤为明显。对于每种方法，我们都尽可能地使用它们的实现，导致不同颜色的深度编码，我们为每个图形定义它。除非另有说明，否则我们呈现测试视图。</p><ul><li>DVGO使用标量scalar体素网格表示密度，使用带有浅MLP的特征网格表示颜色</li><li>Instant NGP使用多层哈希特征网格和浅MLP来表示场景的密度和颜色</li><li>TensoRF使用表示场景的4D张量的因式分解来模拟密度和颜色</li><li>MipNeRF360使用MLP和自定义频率编码在多个细节级别对场景进行建模，防止混叠，<strong>将MipNeRF360中的$\mathcal{L}_{dist}$与本文方法结合起来得到最好的效果</strong></li></ul><h2 id="Quantitative-evaluation-and-discussions"><a href="#Quantitative-evaluation-and-discussions" class="headerlink" title="Quantitative evaluation and discussions"></a>Quantitative evaluation and discussions</h2><p>PSNR ↑ SSIM ↑ LPIPS ↓</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Sampling </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sampling </tag>
            
            <tag> Tools </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeuS2</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/NeuS2/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/NeuS2/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://19reborn.github.io/">Yiming Wang</a><em> 1   Qin Han</em> 1   <a href="https://people.mpi-inf.mpg.de/~mhaberma/">Marc Habermann</a> 2   <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> 3   <a href="http://people.mpi-inf.mpg.de/~theobalt/">Christian Theobalt</a>2   <a href="https://lingjie0206.github.io/">Lingjie Liu</a> 2,3</td></tr><tr><td>Conf/Jour</td><td>ICCV</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://vcai.mpi-inf.mpg.de/projects/NeuS2/">NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction (mpg.de)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4791605939006341121&amp;noteId=1933174180423622400">NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230826151714.png" alt="image.png"></p><p>在Neus基础上添加了：</p><ul><li>哈希编码加速<ul><li>定制的二阶导数反向传播计算</li><li>渐进式学习策略(渐进添加高leve的哈希表)</li></ul></li><li>动态场景重建<ul><li>全局变换预测</li><li>增量训练策略</li></ul></li></ul><p>主要代码通过cuda c++编写</p><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>局限:<br>虽然我们的方法以高质量重建动态场景的每一帧，<strong>但帧之间没有密集的表面对应</strong>。一种可能的方法是使网格模板变形，以适应我们为每一帧学习的神经表面，就像[9，71]中使用的网格跟踪一样。目前，我们还需要保存每帧（25M）的网络参数。<strong>作为未来的工作，可以探索对动态场景进行编码的此类参数的压缩</strong></p><p>结论：<br>我们提出了一种基于学习的方法，以前所未有的运行时性能对静态和动态场景进行精确的多视图重建。为了实现这一点，我们将多分辨率哈希编码集成到神经SDF中，并根据我们的专用网络架构引入了二阶导数的简单计算。为了增强训练收敛性，我们提出了一种渐进式训练策略来学习多分辨率哈希编码。对于动态场景重建，我们提出了一种带有全局变换预测组件的增量训练策略，该策略利用了两个连续帧中的共享几何体和外观信息。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>最近的神经表面表示和渲染方法，例如Neus[62]，已经证明了静态场景的高质量重建。然而，Neus的训练时间非常长(8小时)，这使得它几乎不可能应用到数千帧的动态场景中。我们提出了一种快速的神经表面重建方法，称为NeuS2，在不影响重建质量的情况下，在加速方面实现了两个数量级的改进。为了加速训练过程，我们通过<strong>多分辨率哈希编码参数化</strong>神经表面表示，并提出了一种针对我们的网络<strong>定制的二阶导数的新型轻量级计算</strong>，以利用CUDA并行性，实现了两倍的速度提升。为了进一步稳定和加速训练，<strong>提出了一种渐进式学习策略，将多分辨率哈希编码从粗到细进行优化</strong>。<br>我们<strong>扩展了动态场景的快速训练方法，提出了一种增量训练策略和一种新的全局变换预测组件，使我们的方法能够处理具有大运动和变形的具有挑战性的长序列</strong>。<br>我们在各种数据集上的实验表明，无论是静态场景还是动态场景，NeuS2在表面重建精度和训练速度方面都明显优于目前的技术水平。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>Reconstructing the dynamic 3D world from 2D images的应用：AR/VR、3D电影、游戏、telepresence、3D打印等等<ul><li>传统方法：如特征匹配，仍然相对缓慢，难以重建高质量的结果。</li><li>基于隐式表示的三维重建：如Neus可以产生高质量的重建结果，但其训练过程非常缓慢</li><li>Instant-NGP通过利用多分辨率哈希表增强神经网络编码的辐射场，探索了神经辐射场(NeRF)的训练加速，但由于三维表示缺乏表面约束，从学习的密度场中提取的几何形状包含可识别的噪声</li></ul></li></ul><p>为了克服这些缺点，我们提出了NeuS2</p><ul><li>使用可学习特征向量的多分辨率哈希表参数化神经网络编码的SDF</li><li>推导了一个针对基于relu的mlp的二阶导数的简单公式，该公式可以在显着降低计算成本的情况下，以较小的内存占用实现高效的CUDA</li><li>引入了一种高效的渐进式训练策略，该策略以从粗到精的方式更新哈希表特征</li></ul><p>我们进一步将该方法扩展到多视图动态场景重建中。我们提出了一种新的增量学习策略，以有效地学习具有大运动和变形的物体的神经动态表示，而不是单独训练序列中的每一帧。虽然一般来说，这种策略效果很好，但我们观察到，当两个连续帧之间的运动相对较大时，大多数图像中未观察到的遮挡区域的预测SDF可能会卡在前一帧的学习SDF中。<br>为了解决这个问题，我们预测了一个全局转换，在学习新框架的表示之前大致对齐这两个框架。</p><p>总结贡献：</p><ul><li>我们提出了一种新的方法NeuS2，用于从静态和动态场景的多视图RGB输入中快速学习神经表面表示，该方法在实现前所未有的重建质量的同时实现了最先进的速度</li><li>提出了一种针对基于relu的mlp的二阶导数的简单公式，以实现GPU计算的高效并行化。</li><li>提出了一种从粗到细的多分辨率哈希编码学习的渐进式训练策略，以实现更快更好的训练收敛性。</li><li>我们设计了一种具有新的全局变换预测组件的增量学习方法，用于以高效和稳定的方式重建具有大运动的长序列(例如2000帧)。</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li>Multi-view Stereo.传统的多视图三维重建方法可分为基于深度和基于体素的方法。<ul><li>基于深度的方法[5,13,14,52]通过识别图像之间的点对应关系来重建点云。<strong>然而，对应匹配的准确性严重影响重构的质量</strong>。</li><li>基于体素的方法[11,6,54]通过使用光度一致性标准从多视图图像中恢复体素网格中的占用率和颜色，避开了显式对应匹配的困难。<strong>然而，由于体素网格的高内存消耗，这些方法的重建受到低分辨率的限制</strong>。</li></ul></li><li>Classical Multi-view 4D Reconstruction.在多视图四维重建中，大量的工作使用预先计算的可变形模型，然后将其拟合到多视图图像中。相比之下，我们的方法不依赖于预先计算的模型，可以重建详细的结果，并处理拓扑变化。与我们的工作最相关的也是一种无模型方法。他们利用RGB和深度输入为每帧重建高质量的点云，然后产生时间相干几何。相反，我们只需要RGB作为输入，并且可以在每帧20秒内以端到端方式学习每帧的高质量几何形状和外观。</li><li>Neural Implicit Representations<ul><li>NeRF[36]在新的视图合成任务中显示了高质量的结果，<strong>但由于几何表示缺乏表面约束，它无法提取高质量的表面</strong>。</li><li>Neus将3D表面表示为SDF，用于高质量的几何重建。<strong>但是，Neus的训练速度很慢，而且只适用于静态场景</strong>。</li><li>相反，我们的方法要快100倍，当应用于动态场景重建时，可以进一步加速到每帧20秒</li><li>一些基于nerf的作品[50,58,37]引入了voxelgrid特征来表示快速训练的3D属性。<strong>然而，这些方法不能提取高质量的表面，因为它们继承了体积密度场作为NeRF的几何表示</strong>[36]。</li><li>相比之下，我们的方法可以实现高质量的表面重建和快速训练。对于动态场景建模，许多作品[59,41,46,42,29,65,27]提出将4D场景分解为共享规范空间和每帧可变形场</li><li>[12]表示具有时间感知体素特征的4D场景。[30]提出了一种用于快速动态场景学习的静态到动态学习范式。[26]提出了一种基于网格的逐帧高效重建辐射场的方法。[61]提出了一种新颖的傅立叶八叉树方法将动态场景压缩到一个模型中。这四种方法都集中在新颖的视图合成上，因此，它们并不是为了重建高质量的表面而设计的，这与我们获得高质量的表面几何和外观模型的目标不同。<strong>虽然这些工作提高了动态场景的训练效率，但训练仍然很耗时。此外，这些方法不能处理大的运动，只能重建中等质量的表面</strong></li><li>在人类行为建模中的一些工作[57,32,44,8,66,39,20,45,25,63]可以通过引入可变形模板作为先验来建模大型动作。相比之下，我们的方法可以处理大的移动，不需要可变形的模板，因此，不局限于特定的动态对象。此外，我们可以在每帧20秒的时间内学习动态场景的高质量表面。</li><li>Instant-NSR[71]提出了一种人体建模和渲染的方法。它首先为每一帧重建一个神经表面表示;然后应用非刚性变形获得时间相干网格序列。我们的工作主要集中在第一部分，即动态场景的快速重建，我们利用两个连续帧之间的时间一致性来加速动态表示的学习。<strong>因此，我们的工作与[71]是正交的，可以作为第一步整合到[71]中</strong>。</li></ul></li><li>Concurrent Work<ul><li>Voxurf[64]提出了一种基于体素的表面表示，用于快速多视图三维重建。虽然它在基线上实现了20倍的加速(即news[62])，<strong>但我们提出的方法比Voxurf快3倍以上，并且与他们论文中报道的Voxurf结果相比，实现了更好的几何质量</strong>。</li><li>Neuralangelo[28]提出了一种利用多分辨率哈希网格和数值梯度计算进行神经表面重建的新方法。它可以在牺牲训练成本的同时，通过多个精致的设计从多视图图像中获得密集和高保真的大规模场景几何重建结果，<strong>比我们的速度慢100倍</strong>。</li><li>此外，<strong>Voxurf和Neuralangelo不是为动态场景重建而设计的</strong>。</li><li>最后，Unbiased4d[22]通过扩展弯曲射线的NeuS公式，提出了一种单眼动态表面重建方法。与我们的方法形成鲜明对比的是，<strong>他们的重点在于证明在光线弯曲和具有挑战性的单目环境下也能保持无偏性，而不是在最快的速度下尽可能高的质量</strong>。</li></ul></li></ul><h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p><strong>Neus</strong><br>给定标定过的静态场景多视图图像，NeuS[62]隐式地将场景的表面和外观表示为带符号的距离场$f(\mathbf{x}):\mathbb{R}^3\to \mathbb{R}$和辐射场$c(\mathbf{x},\mathbf{v}):\mathbb{R}^3\times\mathbb{S}^2\to\mathbb{R}^3$，其中x表示三维位置，$\textbf{v}\in\mathbb{S}^{2}$表示观看方向。通过提取$\text{SDF }\mathcal{S}=\{\mathbf{x}\in\mathbb{R}^3|f(\mathbf{x})=0\}$的零水平集，即可得到目标表面S。<br>为了将物体渲染成图像，Neus利用了体渲染。具体来说，对于图像的每个像素，我们采样点$\{\mathbf{p}(\mathbf{t_i})=\mathbf{o}+t_i\mathbf{v}|i=0,1,\ldots,n-1\}$，其中o为相机中心，v为观察方向。通过累积基于sdf的密度和样本点的颜色，我们可以计算光线的颜色。由于渲染过程是可微的，NeuS可以从多视图图像中学习符号距离场f和亮度场c。<strong>然而，训练过程非常缓慢，在单个GPU上需要大约8个小时</strong>。</p><p><strong>InstantNGP</strong><br>为了克服deep基于坐标的mlp训练时间慢的问题，这也是导致Neus性能缓慢的主要原因，最近，Instant-NGP[37]提出了一种多分辨率哈希编码，并证明了其有效性。具体来说，Instant-NGP假设要重建的对象在多分辨率体素网格中有界。每个分辨率的体素网格被映射到具有固定大小的可学习特征向量数组的哈希表。<br>对于一个3D位置$\mathrm{x\in\mathbb{R}^3}$，它在每一层得到一个哈希编码$h^i(\mathbf{x})\in\mathbb{R}^d$ (d是特征向量的维数，$i=1,…,L$)通过插值在这一层的周围体素网格上分配的特征向量。然后将所有L级别的哈希编码连接为多分辨率哈希编码$h(\mathbf{x})=\{h^i(\mathbf{x})\}_{i=1}^L\in \mathbb{R}^{L\times d}.$。除了哈希编码，训练加速的另一个关键因素是整个系统的CUDA实现，它利用了GPU的并行性。虽然运行时间得到了显著改善，但在几何重建精度方面，Instant-NGP仍然没有达到Neus的质量。</p><p><strong>Challenges.</strong><br>考虑到以上讨论，人们可能会问，Neus[62]和InstantNGP[37]的<strong>天真组合</strong>是否可以将两个世界的优点结合起来，即高的3D表面重建质量和高效的计算。我们强调，要实现像Instant-NGP[37]一样快的训练，同时实现像Neus[62]一样高质量的重建，绝非易事。具体来说，为了保证高质量的表面学习，Neus[62]中使用的<strong>Eikonal约束是必不可少的</strong>，如图2和图1，及补充材料。将Eikonal损失添加到基于cuda的mlp (Instant-NGP快速训练的关键因素[37])的<strong>关键挑战是如何有效地计算反向传播的二阶导数</strong>。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828134721.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828134728.png" alt="image.png"></p><p>InstantNSR[71]通过<strong>使用有限差分近似二阶导数解决了这一问题，这种方法存在精度问题</strong>，并且可能导致训练不稳定。相反，<strong>我们提出了一种简单、精确、高效的针对mlp的二阶导数公式</strong>(第4.2节)，从而实现了快速、高质量的重建。我们的方法相对于Instant-NSR的优势见表1和图4所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828134905.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828134937.png" alt="image.png"></p><p>对于动态场景重建，有两个关键的挑战:<strong>如何利用时间信息进行加速，以及如何处理具有大运动和变形的长序列</strong>。为了解决这些问题，我们首先提出了一种<strong>增量训练策略</strong>，利用两个连续帧共享的几何和外观信息的相似性，从而实现更快的收敛(第5.1节)。为了处理大的运动和变形，我们提出了一种<strong>新的全局变换预测组件</strong>，它可以防止预测的SDF训练陷入局部最小值。此外，它可以在小体积中绑定动态序列以节省内存并提高重建精度(第5.2节)。</p><h1 id="Static-Neural-Surface-Reconstruction"><a href="#Static-Neural-Surface-Reconstruction" class="headerlink" title="Static Neural Surface Reconstruction"></a>Static Neural Surface Reconstruction</h1><p>我们首先展示了我们的公式如何有效地从校准的多视图图像中学习静态场景的符号距离场(见图3a)。为了加速训练过程，我们首先演示了如何结合多分辨率哈希编码[37]来表示场景的SDF，以及如何应用体渲染将场景渲染成图像(第4.1节)。接下来，<strong>我们推导了针对基于relu的mlp的二阶导数的简化表达式，该表达式可以在定制的CUDA内核中有效地并行化</strong>(第4.2节)。最后，我们采用渐进式训练策略来学习多分辨率哈希编码，这导致更快的训练收敛和更好的重建质量(第4.3节)。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230826151714.png" alt="image.png"></p><h2 id="Volume-Rendering-of-a-Hash-encoded-SDF"><a href="#Volume-Rendering-of-a-Hash-encoded-SDF" class="headerlink" title="Volume Rendering of a Hash-encoded SDF"></a>Volume Rendering of a Hash-encoded SDF</h2><p>对于每个3D位置x，我们将其映射到具有可学习哈希表项Ω的多分辨率哈希编码$h_{\Omega}(\mathbf{x})$。由于$h_{\Omega}(\mathbf{x})$是空间位置的信息编码，因此将x映射到其SDF和颜色c的mlp可以非常浅，这将导致更有效的渲染和训练，而不会影响质量。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828143956.png" alt="image.png"></p><p><strong>SDF Network.</strong><br>$(d,\mathbf{g})=f_\Theta(\mathbf{e}),\quad\mathbf{e}=(\mathbf{x},h_\Omega(\mathbf{x})).$ Eq.1</p><p>权重为$\Theta$，以3D位置x及其哈希编码$h_{\Omega}(\mathbf{x})$作为输入，并输出SDF值d和几何特征向量$\mathbf{g}\in\mathbb{R}^{15}.$ Concatenating位置作为几何初始化[4]，导致更稳定的几何学习。</p><p><strong>Color Network.</strong><br>$\mathbf{n}=\nabla_{\mathbf{x}}d.$   Eq.2 法向$\nabla_{\mathbf{x}}d$表示SDF相对于x的梯度。<br>$\mathbf{c}=c_\Upsilon(\mathbf{x},\mathbf{n},\mathbf{v},d,\mathbf{g}),$ Eq.3预测了x的颜色c。</p><p><strong>Volume Rendering.</strong><br>为了渲染图像，我们采用了<strong>Neus的无偏体渲染</strong>[62]。此外，我们采用了在<strong>InstantNGP中使用的射线推进加速策略</strong>[37]。更多细节请参见support文档</p><p><strong>Supervision</strong><br>为了训练NeuS2，我们最小化渲染像素$\hat{C}_i$$i\in\{1,…,m\}$和相应的地面真值像素$C_{i}$之间的色差，无需任何3D监督。其中，m表示训练过程中的批大小。我们还使用了Eikonal项[15]来正则化学习到的带符号距离域，从而导致我们的最终损失为：$\mathcal{L}=\mathcal{L}_{\mathrm{color}}+\beta\mathcal{L}_{\mathrm{eikonal}},$ Eq.4</p><ul><li>$\begin{aligned}\mathcal{L}_{\mathrm{color}}=\frac{1}{m}\sum_{i}\mathcal{R}(\hat{C}_{i},C_{i}),\end{aligned}$</li><li>$\mathcal{L}_{\mathrm{eikonal}}=\frac{1}{mn}\sum_{k,i}(||\mathbf{n}_{k,i}||-1)^{2}.$<ul><li>k为沿着射线的第k个采样点$k\in\{1,…,n\}$</li><li>n为采样点数量</li><li>$\mathrm{n}_{k,i}$为采样点的法向量</li></ul></li></ul><h2 id="Efficient-Handling-of-Second-order-Derivatives"><a href="#Efficient-Handling-of-Second-order-Derivatives" class="headerlink" title="Efficient Handling of Second-order Derivatives"></a>Efficient Handling of Second-order Derivatives</h2><p>为了避免学习框架带来的计算开销，我们在CUDA中实现了整个系统。与Instant-NGP[37]在优化过程中只需要一阶导数相比，我们必须计算与输入到颜色网络$c_{\Upsilon}$(Eq. 3)的正态项$\mathbf{n}=\nabla_{\mathbf{x}}d$(Eq. 2)和Eikonal损失项$\mathcal{L}_{\mathrm{eikonal}}.$相关的参数的二阶导数。</p><p>二阶导数。为了加快计算速度，我们直接使用简化公式计算它们，而不是使用PyTorch的计算图[43]。具体来说，我们使用链式法则计算哈希表参数Ω和SDF网络参数Θ的二阶导数</p><ul><li>$\frac{\partial\mathcal{L}}{\partial\Omega}=\frac{\partial\mathcal{L}}{\partial\mathbf{n}}(\frac{\partial\mathbf{e}}{\partial\mathbf{x}}\frac{\partial\frac{\partial d}{\partial\mathbf{e}}}{\partial\mathbf{e}}\frac{\partial\mathbf{e}}{\partial\Omega}+\frac{\partial d}{\partial\mathbf{e}}\frac{\partial\frac{\partial\mathbf{e}}{\partial\mathbf{x}}}{\partial\Omega})$ Eq.5</li><li>$\frac{\partial\mathcal{L}}{\partial\Theta}=\frac{\partial\mathcal{L}}{\partial\mathbf{n}}(\frac{\partial\mathbf{e}}{\partial\mathbf{x}}\frac{\partial\frac{\partial d}{\partial\mathbf{e}}}{\partial\Theta}+\frac{\partial d}{\partial\mathbf{e}}\frac{\partial\frac{\partial\mathbf{e}}{\partial\mathbf{x}}}{\partial\Theta})$ Eq.6</li></ul><p>注意颜色网络$c_{\Upsilon}$只接受n作为输入，所以我们不需要计算颜色网络参数$\Upsilon$的二阶梯度。公式14和15的推导在补充文档中。<br>为了加快公式14和15的计算速度，<strong>我们发现基于relu的mlp可以大大简化上述项</strong>，从而减少计算开销。下面，我们将对此进行更详细的讨论，并在补充文件中提供该命题的证明。我们首先介绍如下一些有用的定义。</p><ul><li>定义1<ul><li>给定一个基于ReLU的MLP f，其中L个隐藏层以$x\in\mathbb{R}^{d}$为输入，它计算输出$y =H_{L}g(H_{L-1}\ldots g(H_{1}x))$，其中$\begin{aligned}H_l\in\mathbb{R}^{n_l}\times\mathbb{R}^{n_{l-1}}\end{aligned}$,$l \in \{1,\ldots,L\}$为层索引，g为ReLU函数。我们定义$P_l^j\in\mathbb{R}^{n_{l-1}}\times\mathbb{R}^1$, $\begin{aligned}S_l^i\in\mathbb{R}^1\times\mathbb{R}^{n_l}\end{aligned}$为：<ul><li>$P_l^j=G_lH_{l-1}\ldots G_2H_1^{(_,j)}$</li><li>$S_l^i=H_L^{(i,_)}G_L\ldots H_{l+1}G_{l+1}$ Eq.7<ul><li>$H_1^{(-,j)}$是$H_{1}$的第j列，$H_{L}^{(i,_)}$是$H_{L}$的第i行</li><li>$G_l=\begin{cases}1,H_{l-1}\ldots g(H_1x)&gt;0\\0,otherwise\end{cases}$</li></ul></li></ul></li><li>现在可以定义基于relu的MLP关于其输入层和中间层的二阶导数。</li></ul></li><li>定理1：基于relu的MLP二阶导数<ul><li>给定一个基于ReLU的MLP f 有L个隐藏层，与定义2中定义相同。MLP f的二阶导数为:<ul><li>$\frac{\partial\frac{\partial y}{\partial x}_{(i,j)}}{\partial H_l}=(P_l^jS_l^i)^T,$ $\frac{\partial^2y}{\partial\mathbf{x}^2}=0$ Eq.8<ul><li>$\frac{\partial y}{\partial x}_{(i,j)}$ 是$\frac{\partial\overline{y}}{\partial x}$的矩阵元素(i,j)</li></ul></li></ul></li><li>回到我们最初的二阶导数(公式14和15)，根据定理2，我们得到$\frac{\partial\frac{\partial d}{\partial\mathbf{e}}}{\partial\mathbf{e}}=0.$。因为$\frac{\partial\mathbf{e}}{\partial\mathbf{x}}$与$\Theta$无关，所以我们有$\frac{\partial\frac{\partial\mathbf{e}}{\partial\mathbf{x}}}{\partial\Theta}=0.$。结果是下面的简化形式:<ul><li>$\frac{\partial\mathcal{L}}{\partial\Omega}=\frac{\partial\mathcal{L}}{\partial\mathrm{n}}\frac{\partial d}{\partial\mathrm{e}}\frac{\partial\frac{\partial\mathbf{e}}{\partial\mathbf{x}}}{\partial\Omega}$ Eq.9</li><li>$\frac{\partial\mathcal{L}}{\partial\Theta}=\frac{\partial\mathcal{L}}{\partial\mathbf{n}}\frac{\partial\mathbf{e}}{\partial\mathbf{x}}\frac{\partial\frac{\partial d}{\partial\mathbf{e}}}{\partial\Theta}$ Eq.10</li></ul></li><li>对于二阶导数，这导致提高效率和更少的计算开销。$\frac{\partial\mathcal{L}}{\partial\Omega}$和$\frac{\partial\mathcal{L}}{\partial\Theta}$的最简单形式可以通过使用公式17替换$\frac{\partial\frac{\partial\mathbf{e}}{\partial\mathbf{x}}}{\partial\Omega}$和$\frac{\partial\frac{\partial d}{\partial\mathbf{e}}}{\partial\mathbf{\Theta}}$的项来获得。我们在图9中显示，使用Eq. 17的计算比PyTorch更有效。</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828141520.png" alt="image.png"></p><h2 id="Progressive-Training"><a href="#Progressive-Training" class="headerlink" title="Progressive Training"></a>Progressive Training</h2><p>虽然我们高度优化的梯度计算已经改善了训练时间，但我们发现在训练收敛性和速度方面仍有改进的空间。此外，我们通过经验观察到，<strong>在低分辨率下使用网格会导致数据的欠拟合，其中几何重建是光滑的，缺乏细节</strong>，而<strong>在高分辨率下使用网格会导致过拟合，导致结果的噪声和伪像增加</strong>。<strong>因此，我们引入一种渐进式训练方法</strong>，通过逐渐增加空间网格编码的带宽，表示为:</p><p>$h_{\Omega}(\mathbf{x},\lambda)=\big(w_{1}(\lambda)h_{\Omega}^{1}(\mathbf{x}),\ldots,w_{L}(\lambda)h_{\Omega}^{L}(\mathbf{x})\big),$ Eq.11</p><p>其中$h_{\Omega}^i$为level i 的哈希编码，每个网格编码层的权重$w_{i}$由$w_{i}(\lambda)=I[i\leq\lambda]$定义，参数λ调制应用于多分辨率哈希编码的低通滤波器的带宽。较小的参数λ导致更快的训练速度，但限制了模型模拟高频细节的能力。因此，我们初始化λ为2，然后在所有实验中每2.5%的总训练步长逐渐增加1。</p><h1 id="Dynamic-Neural-Surface-Reconstruction"><a href="#Dynamic-Neural-Surface-Reconstruction" class="headerlink" title="Dynamic Neural Surface Reconstruction"></a>Dynamic Neural Surface Reconstruction</h1><p>我们已经解释了NeuS2如何能够产生高度准确和快速的静态场景重建。接下来，我们将NeuS2扩展到动态场景重建。也就是说，<strong>给定一个运动物体的多视图视频和每个视图的摄像机参数</strong>，我们的目标是学习每个视频帧中物体的神经隐式表面(见图3b)。</p><h2 id="Incremental-Training"><a href="#Incremental-Training" class="headerlink" title="Incremental Training"></a>Incremental Training</h2><p>尽管我们的静态对象重建方法可以达到很好的效率和质量，但是通过单独训练每一帧来构建动态场景仍然很耗时。然而，从一帧到另一帧的场景变化通常很小。因此，我们提出了一种增量训练策略来利用两个连续帧之间共享的几何和外观信息的相似性，从而使我们的模型能够更快地收敛。具体来说，<strong>我们像静态场景重建中一样从头开始训练第一帧</strong>，<strong>然后根据学习到的前一帧的哈希网格表示对后续帧的模型参数进行微调</strong>。使用该策略，该模型能够对目标帧的神经表示进行良好的初始化，从而显著加快其收敛速度。</p><h2 id="Global-Transformation-Prediction"><a href="#Global-Transformation-Prediction" class="headerlink" title="Global Transformation Prediction"></a>Global Transformation Prediction</h2><p>我们在增量训练过程中观察到，预测的SDF很容易卡在前一帧学习到的SDF的局部极小值中，特别是当目标在相邻帧之间的移动比较大的时候。例如，当我们的模型从多视图图像中重建行走序列时，重建的表面看起来有许多孔，如图10所示。为了解决这个问题，我们提出了一个<strong>全局转换预测</strong>，<strong>在增量训练之前将目标SDF粗略地转换到一个规范空间</strong>。具体来说，我们预测了物体在两个相邻帧之间的旋转R和过渡T。对于坐标系i坐标空间中任意给定的三维位置$\mathrm{x}_i$，它被转换回前一坐标系i - 1的坐标空间，记作$\mathbf{x}_{i-1}$<br>$\mathbf{x}_{i-1}=R_i(\mathbf{x}_i+T_i).$ Eq.12</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230828142350.png" alt="image.png"></p><p>然后可以累积变换，将点xi转换回第一帧坐标空间中的$x_c$<br>$\mathbf{x}_{c}=R_{i-1}^{c}(\mathbf{x}_{i-1}+T_{i-1}^{c})=R_{i}^{c}(\mathbf{x}_{i}+T_{i}^{c}),$ Eq.13</p><ul><li>$R_i^c=R_{i-1}^cR_i$</li><li>$T_{i}^{c}=T_{i}+R_{i}^{-1}T_{i-1}^{c}.$</li></ul><p>全局变换预测还允许我们<strong>在小区域内对具有大移动的动态序列进行建模</strong>，而不是用大的哈希网格覆盖整个场景。由于哈希网格只需要对整个场景的一小部分进行建模，因此我们可以获得更准确的重建并降低内存成本。<br>值得注意的是，由于我们的方法的以下设计，我们的方法可以处理大的运动和变形，这对现有的动态场景重建方法[59]，[46]来说是具有挑战性的：<br>（1）全局变换预测，其解释了序列中的大的全局运动；<br>（2） 增量训练，学习两个相邻帧之间相对较小的可变形运动，而不是学习从每个帧到公共规范空间的相对较大的运动。<br>我们将增量训练与全局变换预测相结合作为一种端到端的学习方案来实现，如图所示。第3（b）段。在处理新帧时，<strong>我们首先独立预测全局变换</strong>，<strong>然后将模型的参数和全局变换一起微调</strong>，以有效地学习神经表示。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>3090 GPU</p><ul><li>Static Scene Reconstruction<ul><li>数据集：DTU</li><li>Baseline：Neus、InstantNGP、Instant-NSR、Voxurf</li><li>quantitative：Chamfer Distance、PSNR</li><li>qualitative</li></ul></li><li>Dynamic Scene Reconstruction<ul><li>数据集：<ul><li>Synthetic Scenes：NeRF共享的乐高场景、Artemis提供的狮子序列、RenderPeople中的人类角色</li><li>Real Scenes：Dynacap数据集中选择了三个序列</li></ul></li><li>Baseline：D-NeRF、TiNeuVox</li><li>quantitative：Chamfer Distance、PSNR</li><li>qualitative</li></ul></li><li>Ablation<ul><li>高效二阶导反向传播计算 VS Pytorch</li><li>with or without :<ul><li>GTP</li><li>PT</li></ul></li></ul></li></ul><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>PyTorch  1.11.0 | Python  3.8(ubuntu20.04) | Cuda  11.3</p><p>升级cmake 3.18以上<br><code>sudo apt-get install xorg-dev</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --recursive https://github.com/19reborn/NeuS2</span><br><span class="line"><span class="built_in">cd</span> NeuS2</span><br><span class="line"></span><br><span class="line">cmake . -B build</span><br><span class="line">cmake --build build --config RelWithDebInfo -j </span><br><span class="line"></span><br><span class="line"><span class="comment"># conda create env</span></span><br><span class="line">conda create -n neus2 python= your image</span><br><span class="line">conda activate neus2</span><br><span class="line">pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">commentjson~=0.9.0</span><br><span class="line">imageio~=2.16.0</span><br><span class="line">numpy~=1.21.2</span><br><span class="line">pybind11~=2.7.1</span><br><span class="line">scipy~=1.7.1</span><br><span class="line">tqdm~=4.62.2</span><br><span class="line">opencv-python~=4.5.5.62</span><br><span class="line">trimesh</span><br><span class="line">tensorboard</span><br><span class="line">https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line"><span class="comment"># install pytorch and pytorch3d</span></span><br><span class="line">pip install torch</span><br><span class="line">pip install <span class="string">&quot;git+https://github.com/facebookresearch/pytorch3d.git@stable&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p><a href="https://github.com/facebookresearch/pytorch3d/blob/main/INSTALL.md">pytorch3d/INSTALL.md at main · facebookresearch/pytorch3d (github.com)</a></p></blockquote><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run c++</span></span><br><span class="line">./build/testbed --scene $&#123;data_path&#125;/transform.json</span><br><span class="line"><span class="comment">## eg:</span></span><br><span class="line">./build/testbed --scene ./data/neus/dtu_scan114/transform_train.json -n dtu.json --no-gui</span><br><span class="line"><span class="comment">## questions:</span></span><br><span class="line">no output</span><br><span class="line"></span><br><span class="line"><span class="comment"># run python</span></span><br><span class="line">python scripts/run.py --scene $&#123;data_path&#125;/transform.json --name $&#123;your_experiment_name&#125; --network $&#123;config_name&#125; --n_steps $&#123;training_steps&#125;</span><br><span class="line"><span class="comment">## eg: autodl-tmp/NeuS2/data/neus/dtu_scan114/transform_train.json</span></span><br><span class="line">python scripts/run.py --scene ./data/neus/dtu_scan114/transform_train.json --name neus --network dtu.json --n_steps -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据转换</span></span><br><span class="line">python tools/data_format_from_neus.py --dataset_name dtu_scan114 --copy_image</span><br><span class="line"><span class="keyword">or</span> </span><br><span class="line">python tools/data_format_from_neus.py --dataset_all --copy_image </span><br></pre></td></tr></table></figure><h3 id="Miku"><a href="#Miku" class="headerlink" title="Miku"></a>Miku</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/run.py --scene ./data/neus/Miku/transform_train.json --name Miku --network dtu.json --n_steps <span class="number">20000</span></span><br></pre></td></tr></table></figure><p>neuspp —&gt; womask</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/run.py --scene ./data/neus/Miku/transform_train.json --name Miku --network womask.json --n_steps -<span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Efficiency </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
            <tag> Efficiency </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeUDF</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/NeUDF/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/NeUDF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>NeUDF: Leaning Neural Unsigned Distance Fields with Volume Rendering</th></tr></thead><tbody><tr><td>Author</td><td>Yu-Tao Liu1,2          Li Wang1,2          Jie Yang1,2          Weikai Chen3          Xiaoxu Meng3          Bo Yang3          Lin Gao1,2*</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="http://geometrylearning.com/neudf/">NeUDF (CVPR 2023) (geometrylearning.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4746971745559265281&amp;noteId=1931719993718658048">NeUDF: Leaning Neural Unsigned Distance Fields with Volume Rendering (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230825151238.png" alt="image.png"></p><p>解决了Neus中SDF的一个限制：仅限于封闭表面的重建，无法重建包含开放表面结构的广泛的现实世界对象<br>NeUDF使用UDF：仅从多视图监督中<strong>重建具有任意拓扑的表面</strong></p><ul><li>提出了两个专门为基于UDF的体渲染量身定制的权重函数的新公式<ul><li>$w_r(t)=\tau_r(t)e^{-\int_0^t\tau_r(u)du}$ Eq.4</li><li>$\tau_r(t)=\left|\frac{\frac{\partial(\varsigma_r\circ\Psi\circ p)}{\partial t}(t)}{\varsigma_r\circ\Psi\circ p(t)}\right|$ Eq.5<ul><li>$\varsigma_{r}(d) = \frac x{1+x}$</li><li>UDF: $d=\Psi_{\mathcal{O}}(x)$</li></ul></li></ul></li><li>为了应对开放表面渲染，当输入/输出测试不再有效时，我们提出了一种专用的<strong>法向正则化策略</strong>来解决表面方向模糊问题<ul><li>用邻近的插值法向替换原始采样的表面法向</li></ul></li></ul><p>局限：</p><ul><li>无法重建透明表面</li><li>平滑度和高频细节无法同时拥有</li><li>需要额外的网格划分工具，导致重构误差</li><li>展望：透明表面、稀疏视图</li></ul><span id="more"></span><h1 id="Discussions-amp-Conclusions"><a href="#Discussions-amp-Conclusions" class="headerlink" title="Discussions&amp;Conclusions"></a>Discussions&amp;Conclusions</h1><p><strong>局限性</strong></p><ul><li>首先，<strong>很难</strong>用我们的公式来<strong>模拟透明表面</strong>。当输入图像中没有足够的可见信息(例如视点稀疏或严重遮挡)时，重建质量会下降，图11给出了一个失败案例的例子。</li><li>由于正态正则化会累积附近信息以减轻表面法向模糊，<strong>因此在平滑度和高频细节之间也存在权衡</strong>。</li><li>此外，由于我们引入UDF是为了更好的表示能力，我们<strong>需要额外的网格划分工具</strong>，如MeshUDF[19]或SPSR[23]，这<strong>可能会引入更多的重构误差</strong>。</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230826144949.png" alt="image.png"></p><p>我们提出了NeUDF，这是一种新的基于UDF的体绘制方法，用于<strong>从有或没有掩码的2D图像中</strong>实现任意形状的<strong>高保真多视图重建</strong>。NeUDF 在定性和定量上都优于最先进的方法，<strong>尤其是在具有开放边界的复杂表面上</strong>。因此，我们的 NeUDF 可以在真实世界的 3D 应用程序中发挥关键作用。<br>在未来的工作中，我们可以扩展我们的公式以更好地<strong>重建透明表面</strong>。增强我们的 NeUDF 以支持<strong>稀疏输入图像</strong>也是一个有趣的未来方向。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>由于神经隐式表面绘制的最新进展，多视图形状重建取得了令人印象深刻的进展。然而，现有的基于符号距离函数(SDF)的方法<strong>仅限于封闭表面</strong>，无法重建包含开放表面结构的广泛的现实世界对象。在这项工作中，我们引入了一个新的神经渲染框架，编码为NeUDF，它可以仅从多视图监督中重建具有任意拓扑的表面。为了获得表示任意曲面的灵活性，NeUDF利用无符号距离函数(unsigned distance function, UDF)作为曲面表示。虽然基于sdf的神经渲染器的简单扩展不能扩展到UDF，但我们提出了两个专门为基于UDF的体渲染量身定制的权重函数的新公式。此外，为了应对开放表面渲染，当输入/输出测试不再有效时，我们提出了一种专用的正态正则化策略来解决表面方向模糊问题。我们在许多具有挑战性的数据集上广泛评估了我们的方法，包括DTU [21]， MGN[5]和Deep Fashion 3D[61]。实验结果表明，在多视图曲面重建任务中，特别是对于具有开放边界的复杂形状，NeUDF可以显著优于最先进的方法。</p><p>NeUDF建立在无符号距离函数(unsigned distance function, UDF)的基础上，UDF是一个简单的隐式函数，它返回从查询点到目标曲面的绝对距离。尽管它很简单，但我们表明，<strong>将基于sdf的神经渲染机制天真地扩展到无符号距离场并不能确保非水密表面的无偏渲染</strong>。特别是，如图2所示，基于sdf的加权函数会生成虚假曲面，其中渲染权重会在空洞区域触发不希望的局部最大值。为了解决这个问题，我们提出了一个新的无偏加权范式，专门为UDF量身定制，同时意识到表面遮挡。为了适应所提出的加权函数，我们进一步提出了一种定制的重要性采样策略，以确保非水密表面的高质量重建。此外，为了解决零等值面附近udf梯度不一致的问题，我们引入了一种正态正则化方法，利用曲面邻域的正态信息来增强梯度一致性。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230825153332.png" alt="image.png"></p><p>我们的贡献总结如下:</p><ul><li>第一个基于udf的神经体绘制框架，称为NeUDF，可用于具有任意拓扑的形状的多视图重建，包括具有开放边界的复杂形状。</li><li>针对UDF渲染提出了一种<strong>新的无偏加权函数和重要采样策略</strong>。</li><li>在具有<strong>非水密3D形状</strong>(带有孔洞)的许多具有挑战性的数据集上进行多视图表面重建的最新性能</li></ul><p>RW</p><ul><li>Neural Implicit Representation</li><li>Neural Rendering</li><li>Multi-view Reconstruction<ul><li>SDF+NR = Neus</li></ul></li></ul><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>给定一组物体或场景的<strong>校准图像</strong>$\{\mathcal{I}_k|1\leq k\leq n\}$，我们的目标是仅使用二维图像监督来重建任意表面，包括封闭和开放结构。在本文中，曲面被表示为无符号距离函数(udf)的零水平集。为了学习对象或场景的UDF表示，我们引入了一种新的神经渲染架构，该架构包含用于渲染的无偏权重公式。</p><ul><li>首先定义基于UDF的场景表示(第3.1节)。</li><li>然后，我们介绍了NeUDF，并为基于udf的体绘制专门定制了两个关键的权重函数公式(第3.2节)</li><li>最后，我们说明了用于减轻2D图像歧义的正常正则化(第3.3节)和我们的loss配置(第3.4节)。</li></ul><h2 id="Scene-Representation"><a href="#Scene-Representation" class="headerlink" title="Scene Representation"></a>Scene Representation</h2><p>与有符号距离函数(SDF)不同，无符号距离函数(UDF)是无符号的，能够表示任意拓扑的开放表面，除了水密表面。<br>给定一个三维物体$\mathcal{O}=\{V,F\}$，其中V和F是顶点和面的集合，物体$\mathcal{O}$的UDF可以表示为一个函数$d=\Psi_{\mathcal{O}}(x):\mathbb{R}^3\mapsto\mathbb{R}^+,$，它将一个点坐标映射到表面的欧几里得距离d。我们定义$\mathrm{UDF}_{\mathcal{O}}=\{\Psi_{\mathcal{O}}(x)|d&lt;\epsilon,d=\mathrm{argmin}_{f\in F}(|x-f|_2)\},$，其中ε是一个小阈值，目标表面可以被UDFO的零水平集调制。<br>我们引入了一个可微体绘制框架来从输入图像中预测UDF。该框架由神经网络ψ近似，该网络<strong>根据</strong>沿采样射线v的<strong>空间位置x预测UDF值d和渲染颜色c</strong>:<br>$(d,c)=\psi(v,x):\mathbb{S}^2\times\mathbb{R}^3\mapsto(\mathbb{R}^+,[0,1]^3)$ Eq.1</p><p>在体绘制的帮助下，权重通过最小化预测图像$\mathcal{I}_{k}^{\prime}$和真实图像$\mathcal{I}_{k}$之间的距离来优化<br>学习到的表面$\mathcal{S}_{\mathcal{O}}$可以用预测UDF的零水平集表示:$\mathcal{S}_{\mathcal{O}}=\{x\in\mathbb{R}^{3}|d=0,(d,c)=\psi(v,x)\}$ Eq.2</p><h2 id="NeUDF-Rendering"><a href="#NeUDF-Rendering" class="headerlink" title="NeUDF Rendering"></a>NeUDF Rendering</h2><p>渲染过程是学习准确UDF的关键，因为它通过沿射线v的积分将输出颜色和UDF值连接起来<br>$C(o,v)=\int_{0}^{+\infty}w(t)c(p(t),v)dt,$ Eq.3</p><p>其中C(o, v)为从相机原点0开始沿视点方向v的输出像素颜色，w(t)为点p(t)的权值函数，C(p(t)， v)为点p(t)沿视点方向v的颜色。<br>为了通过体绘制重建UDF，我们首先引入一个概率密度函数$\varsigma_r^{\prime}(\Psi(x)),$，称为<strong>U-density</strong>，其中Ψ(x)是x的无符号距离。密度函数$\varsigma_r^{\prime}(\Psi(x))$将UDF场映射到概率密度分布，该分布在表面附近具有显著的高值，以便准确重建。受Neus[53]的启发，我们推导了一个无偏和闭塞的权函数$w_{r}(t)$及其不透明密度$\tau_r(t)$</p><ul><li>$w_r(t)=\tau_r(t)e^{-\int_0^t\tau_r(u)du}$ Eq.4</li><li>$\tau_r(t)=\left|\frac{\frac{\partial(\varsigma_r\circ\Psi\circ p)}{\partial t}(t)}{\varsigma_r\circ\Psi\circ p(t)}\right|$ Eq.5</li></ul><p>其中◦是函数组合操作符，并且为了有效的UDF重建，必须满足以下规则:</p><ul><li>$\varsigma_r(0)=0,\lim_{d\to+\infty}\varsigma_r(d)=1$ Eq.6<ul><li>在d=0，即表面处，权重大，概率密度函数值为1</li></ul></li><li>$\varsigma_r’(d)&gt;0;\varsigma_r’’(d)<0,\forall d>0$ Eq.7</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230825160227.png" alt="image.png"><br>变量$\varsigma_r(d)$可以是图所示的任意函数。由于$\varsigma_{r}(d)$是u密度的累积分布函数，所以$\varsigma_r(0)=0$保证了负距离的点没有累积密度。此外，$\varsigma_r^{\prime}(d)&gt;0$和$\varsigma_{r}^{\prime\prime}(d)<0$确保靠近表面的点的u密度值为正且显著高。$\varsigma_r(d)$中参数r是可学习的，控制着密度的分布。该函数结构解决了体绘制和表面重建之间的体面差距，保证了整体无偏性。详细讨论请参考我们的补充我们认为，基于sdf的神经渲染器的幼稚扩展将违反上述一些规则。例如，**Neus**[53]中**u密度的累积分布函数**为$Φ_s$(Sigmoid function)， $Φ_s(0) > 0$<strong>违反式6</strong>。这种违背会导致权重渲染的偏差，从而<strong>导致多余的浮面和不规则的噪声</strong>，如图2所示。注意，Neus中提出的局部最大约束不能解决UDF中的这种呈现偏差。请查看我们补充资料中关于无偏性和全局/局部极大约束的详细讨论。和全局/局部最大约束的详细讨论。</p><p>在广泛评估了消融研究中不同形式的$\varsigma_{r}(d)$后(第4.3节)，我们最终选择$\varsigma_{r}(d) = \frac{rd}{1+rd}$, r初始化为0.05。进一步，我们采用α-合成对权函数进行离散化，对沿射线方向的点进行采样，并根据权积分对颜色进行累加。关于Eqn. 4和Eqn. 5的无偏和闭塞感知特性的详细离散化和证明，请参考我们的补充材料。</p><p>The choice of $\varsigma_{r}$ in $\tau_{r}$</p><p>尽管我们已经给出了 $\varsigma_{r}$ 应该满足的规则（Eq.6、Eq.7)，有一系列满足规则的函数。In the family, 所有函数都适用于UDF体绘制，因此我们对几种不同的候选函数进行验证，以检查每个函数的收敛能力进行网络优化，即在给定训练迭代中，网络收敛到最佳结果的函数。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230826150109.png" alt="image.png"><br>图 9 显示了遵循规则的三个候选函数$(1-e^{-x},\frac{2arctan(x)}\pi\text{ and }\frac x{1+x})$ 的视觉结果。在给定迭代(300k)之后，使用函数$\frac x{1+x}$的网络在定性和定量上都收敛到最佳结果，而其他函数不是完全收敛的，导致表面不完整，倒角距离略高。对不同形状的评估还表明，所有函数都运行良好，并且所选函数 $\frac x{1+x}$在我们的设置中效果最好（我们的：1.11 对 candidates：1.13/1.18）。</p><ul><li>红色：$\varsigma_{r}(d) = \frac x{1+x}$</li><li>蓝色：$\varsigma_r^{\prime}(d) = \frac{1}{(1+x)^{2}}$<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230826150617.png" alt="image.png"></li></ul><p><strong>Importance points sampling.</strong></p><p>适应渲染权重的点采样是体绘制的重要步骤。与SDF不同，为了实现UDF的无偏渲染，渲染函数应该在交点前分配更多的权重(图2(c))。因此，如果渲染和采样函数都使用相同的权重，则UDF梯度的正则化(Eikonal损失)将导致表面两侧的梯度幅度高度不平衡。这可能会严重影响重建UDF field的质量。因此，我们提出了一个专门定制的采样权函数(图2(c))，以实现整个空间的良好平衡正则化。重要性抽样$w_{s}(t)$的公式为:$w_s(t)=\tau_s(t)e^{-\int_0^t\tau_s(u)du},\tau_s(t)=\zeta_s\circ\Psi\circ p(t),$ Eq.8</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230825153332.png" alt="image.png"></p><p>其中，$\zeta_{s}(\cdot)$满足以下规则:$\zeta_s(d)&gt;0$且$\zeta_s^{\prime}(d) &lt; 0$，$∀d &gt; 0$。直观地说，$\zeta_{s}(\cdot)$在第一象限是一个单调递减的函数。在本文中，我们使用$\zeta_s(d)=\frac{se^{-sd}}{(1+e^{-sd})^2}$，其中$\zeta_{s}(d)$中的参数b控制x = 0处的强度。S从0.05开始，以$2^{z−1}$的速率改变每个采样步长z。任何可以与渲染函数实现平衡正则化的采样函数都与我们的框架兼容。有关上述规则的详细说明，请参阅我们的补充文件。此外，我们定性和定量地评估了在烧蚀研究中使用$\zeta_s(d)$的必要性(第4.3节)。</p><p>总体而言，在体绘制过程中，权重函数在渲染(Eqn. 4)和采样(Eqn. 8)中协同使用，实现了具有可微体绘制的高保真开放表面重建。</p><h2 id="Normal-Regularization"><a href="#Normal-Regularization" class="headerlink" title="Normal Regularization"></a>Normal Regularization</h2><p>由于UDF的零水平集中的点不是一阶可微的顶点，因此在学习表面附近的采样点的<strong>梯度在数值上不是稳定的</strong>(抖动的)。由于绘制权函数以UDF梯度为输入，不可靠的梯度会导致曲面重建不准确。为了缓解这一问题，我们引入了正态正则化来执行空间插值。<strong>法向正则化用邻近的插值法向替换原始采样的表面法向</strong>。图4给出了一个详细的说明。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230826143805.png" alt="image.png"><br><em>法向正则化图。我们使用与表面偏移的点的梯度(蓝色)来近似UDF表示的不稳定表面法线(绿色)。</em></p><p>由于不稳定法线只存在于曲面附近，我们使用与曲面有偏移的点法线来近似不稳定法线。我们在$p(t_i)$点离散地表示为:</p><p>$\mathbf{n}(p(t_i))=\frac{\sum_{k=1}^Kw_{i-k}\Psi^{\prime}(p(t_{i-k}))}{\sum_{k=1}^Kw_{i-k}}$ Eq.9</p><p>其中，$\begin{aligned}w_{i-k}=|p(i)-p(i-k)|_2^2\end{aligned}$是$p(i)$到$p(i-k)$的距离。$\Psi^{\prime}(\cdot)$是UDF $\Psi(\cdot)$的导数，返回UDF的梯度。<strong>通过利用法向正则化，我们的框架从2D图像中实现了更平滑的开放表面重建</strong>。我们可以调整法向正则化权值以获得更详细的几何形状。实验表明，法向正则化可以防止二维图像中高亮和高暗区域的高质量重建，如图10所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230826144049.png" alt="image.png"></p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>为了学习高保真开放表面重建，我们在没有任何3D监督的情况下，通过最小化渲染图像和已知相机姿势的ground truth图像之间的差异来优化网络。继Neus[53]之后，我们还应用了SDF体绘制中使用的三个损失术语: 颜色损失$\mathcal{L}_{c}$, Eikonal损失[58]$\mathcal{L}_{e}$和Mask损失$\mathcal{L}_{m}$。</p><ul><li>颜色损失衡量的是L1损失下渲染图像与输入图像之间的差异。</li><li>Eikonal损失对采样点上的UDF梯度进行了数值正则化。</li><li>如果提供了掩模，掩模损失也会促使预测掩模接近BCE测量下的真值掩模。</li></ul><p>总的来说，我们使用的损失由三部分组成:</p><p>$\mathcal{L}=\mathcal{L}_{c}+\alpha\mathcal{L}_{e}+\beta\mathcal{L}_{m}$ Eq.10</p><h1 id="实验和评估"><a href="#实验和评估" class="headerlink" title="实验和评估"></a>实验和评估</h1><h2 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h2><p>Datasets</p><ul><li>Multi-Garment Net数据集(MGN)[4]</li><li>Deep Fashion3D数据集(DF3D)[61]</li><li>DTU MVS数据集(DTU)[21]<ul><li>每个场景包含49或64张1600 × 1200分辨率的图像，蒙版来自IDR[58]。</li></ul></li></ul><p>Baselines</p><ul><li>Colmap</li><li>IDR</li><li>Neus</li><li>NeuralWarp</li><li>HF-Neus</li></ul><p>Metrics</p><ul><li>Chamfer Distance (CD) to quantitative</li></ul><h2 id="Comparisons-on-Multi-view-Reconstruction"><a href="#Comparisons-on-Multi-view-Reconstruction" class="headerlink" title="Comparisons on Multi-view Reconstruction"></a>Comparisons on Multi-view Reconstruction</h2><ul><li>Quantitative Results</li><li>Qualitative Results.</li><li>Captured Real Scenes</li></ul><h2 id="Further-Discussions-and-Analysis"><a href="#Further-Discussions-and-Analysis" class="headerlink" title="Further Discussions and Analysis"></a>Further Discussions and Analysis</h2><ul><li>The choice of $\varsigma_{r}$ in $\tau_{r}$</li><li>Necessity of Importance Points Sampling</li><li>Necessity of Normal Regularization.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
            <tag> UDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Point-NeRF</title>
      <link href="/3DReconstruction/Multi-view/Explicit%20Volumetric/Point-NeRF/"/>
      <url>/3DReconstruction/Multi-view/Explicit%20Volumetric/Point-NeRF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Point-NeRF: Point-based Neural Radiance Fields</th></tr></thead><tbody><tr><td>Author</td><td>Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin ShuKalyan Sunkavalli , Ulrich Neumann</td></tr><tr><td>Conf/Jour</td><td>CVPR 2022 Oral</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://xharlie.github.io/projects/project_sites/pointnerf/">Point-NeRF: Point-based Neural Radiance Fields (xharlie.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4734283362588442625&amp;noteId=1930369628502938624">Point-NeRF: Point-based Neural Radiance Fields (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230824165453.png" alt="image.png"></p><ul><li>生成初始点云(本文)，<em>此外还有Colmap和Metashape等方法生成</em><ul><li>基于MVS Net的$G_{p,γ}$，生成每个点的位置和置信度（点是否在表面上）</li><li>基于2D CNN的$G_f$，生成每个点的特征</li></ul></li><li>点云处理：排除降低渲染质量的孔和异常值 <strong>每10K次迭代</strong><ul><li>Point pruning 对置信度低于0.1的点进行删除</li><li>Point growing 当ray marching中密度最大点(表面附近的点)周围的点比较少时，添加点来填补空白</li></ul></li><li>MLP获取点x的信息<ul><li>MLP F：x点周围点的新特征$f_{i,x}=F(f_{i},x-p_{i}).$</li><li>MLP R：x点的辐射值(or颜色) $r=R(f_{x},d).$<ul><li>x点的聚合特征$f_{x}=\sum_{i}\gamma_{i}\frac{w_{i}}{\sum w_{i}}f_{i,x},\mathrm{~where~}w_{i}=\frac{1}{|p_{i}-x|}.$</li></ul></li><li>MLP T：x点周围点的密度 $\sigma_i=T(f_{i,x})$<ul><li>x点的聚合密度$\sigma=\sum_{i}\sigma_{i}\gamma_{i}\frac{w_{i}}{\sum w_{i}},w_{i}=\frac{1}{|p_{i}-x|}.$</li></ul></li></ul></li></ul><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>在本文中，我们提出了一种高质量神经场景重建和渲染的新方法。我们提出了一种新的神经场景表示Point-NeRF，用神经点云模拟体积辐射场。我们通过直接网络推理直接从输入图像重建了一个良好的Point-NeRF初始化，并表明我们可以有效地对场景的初始化进行微调。这使得高效的Point-NeRF重建只需要20 - 40分钟的每个场景优化，导致渲染质量可以媲美甚至超过需要更长的训练时间(20+小时)的NeRF。我们还为我们的每场景优化提出了新的有效的生长和修剪技术，显著改善了我们的结果，并使我们的方法在不同的点云质量下具有鲁棒性。我们的Point-NeRF成功地结合了经典点云表示和神经辐射场表示的优点，向高效和逼真的实用场景重建解决方案迈出了重要一步。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>像NeRF[35]这样的体积神经渲染方法可以生成高质量的视图合成结果，但对每个场景进行了优化，导致重建时间过长。另一方面，深度多视点立体视觉方法<strong>deep multi-view stereo methods</strong>可以通过直接网络推理快速重建场景几何。Point-NeRF结合了这两种方法的优点，通过使用神经3D点云和相关的神经特征来模拟辐射场。在基于光线行进的渲染管道中，通过聚集场景表面附近的神经点特征，可以有效地渲染点nerf。此外，可以通过预先训练的深度网络的直接推断来初始化点神经网络，从而生成神经点云;这个点云可以经过微调，以30倍快的训练时间超越NeRF的视觉质量。<strong>Point-NeRF可以与其他三维重建方法相结合，并通过一种新的修剪生长机制来处理这些方法中的误差和异常值</strong>。在DTU[18]、NeRF synthetic[35]、ScanNet[11]和Tanks and temple[23]数据集上的实验表明，Point-NeRF可以超越现有的方法，实现最先进的结果。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>从图像数据中建模真实场景并绘制逼真的新视图是计算机视觉和图形学的核心问题。NeRF[35]及其扩展[29,32,64]通过模拟神经辐射场在这方面取得了巨大成功。这些方法[35,38,64]通常通过射线marching，使用全局mlp对整个空间重建辐射场。<strong>由于每个场景网络拟合缓慢以及不必要的大量空白空间采样，这导致重建时间长</strong>。<br>我们使用Point-NeRF解决了这个问题，Point-NeRF是一种新的基于点的辐射场表示，它使用3D神经点来模拟连续的体积辐射场。与NeRF完全依赖于每个场景的拟合不同，PointNeRF可以通过前馈深度神经网络进行有效的初始化，跨场景进行预训练。此外，PointNeRF通过利用接近实际场景几何的经典点云来避免在空场景空间中进行光线采样。与其他神经辐射场模型相比，Point-NeRF的这一优势导致了更高效的重建和更准确的渲染[8,35,53,63]。<br>我们的Point-NeRF表示由具有点神经特征的点云组成: <strong>每个神经点编码其周围的局部3D场景几何形状和外观</strong>。先前的基于点的渲染技术[2]使用类似的神经点云，但在图像空间中使用栅格化和2D CNNs进行渲染。相反，我们将这些神经点视为3D中的局部神经基函数，以模拟连续的体辐射场，从而使用可微射线行进实现高质量的渲染。特别地，对于任何3D位置，我们建议使用MLP网络来聚集其邻域的神经点，以回归该位置的体积密度和视图依赖辐射。这表示一个连续的辐射场。<br>我们提出了一个基于学习的框架来有效地初始化和优化基于点的辐射场。为了生成<strong>初始场</strong>，我们利用深度多视图立体(MVS)技术[59]，即应用cost-volume-based的网络来预测深度，然后将其非投影到3D空间。此外，训练深度CNN从输入图像中提取2D特征映射，自然地提供点特征。这些来自多个视图的神经点被组合成一个神经点云，形成一个基于点的场景辐射场。我们使用基于点的体绘制网络对该点生成模块进行了端到端训练，以呈现新颖的视图图像并使用地面真实度对其进行监督。这导致了一个可推广的模型，可以直接预测基于点的辐射场在推理时间。一旦预测，初始的基于点的场将在短时间内进一步优化每个场景，以实现逼真的渲染。如图1(左)所示，使用Point-NeRF进行21分钟优化的效果优于经过数天训练的NeRF模型。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230824183903.png" alt="image.png"></p><p>除了使用内置的点云重建外，我们的方法是通用的，也可以基于其他重建技术的点云生成辐射场。然而，使用COLMAP[44]等技术生成的重建点云在实际中包含孔洞和异常值，这些孔洞和异常值会对最终渲染产生不利影响。为了解决这个问题，<strong>我们引入点生长和修剪作为我们优化过程的一部分</strong>。我们在体绘制过程中利用几何推理[13]，在高体密度区域中点云边界附近生长点，在低体密度区域中修剪点。该机制有效地提高了我们最终的重建和渲染质量。我们在图1(右)中展示了一个示例，其中我们将COLMAP点转换为辐射场，并成功填充大孔并生成逼真的渲染图。</p><p>我们在DTU数据集[18]上训练我们的模型，并在DTU测试场景、NeRF合成场景、Tanks &amp; Temples[23]和ScanNet[11]场景上进行评估。结果表明，我们的方法可以实现最先进的新视图合成，优于许多现有技术，包括基于点的方法[2]、NeRF、NSVF[29]和许多其他可推广的神经方法[8,53,63]，详细(见表1和表2)。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230824184217.png" alt="image.png"></p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li><strong>Scene representations.</strong><ul><li>传统方法和神经方法已经在不同的视觉和图形应用中研究了许多3D场景表示，包括体[19,25,41,46,56]、点云[1,40,51]、网格[20,52]、深度图[17,28]和隐式函数[9,33,37,60]。最近，各种神经场景表示已经被提出[4,30,47,67]，推进了新视图合成和逼真渲染的技术水平，体积神经辐射场(NeRF)[35]产生了高保真度的结果。NeRF通常被重构为编码整个场景空间的全局MLP[35,38,64];在重建复杂的大规模场景时，这种方法效率低下且成本高昂。相反，Point-NeRF是一种局部神经表示，将体积辐射场与经典用于近似场景几何的点云相结合。我们分配细粒度的神经点来模拟复杂的局部场景几何和外观，从而获得比NeRF更好的渲染质量(见图6,7)。</li><li>具有per-voxel神经特征的体素网格[8,16,29]也是一种局部神经辐射表示。然而，我们基于点的表示更适合实际表面，从而获得更好的质量。此外，我们直接预测了良好的初始神经点特征，绕过了大多数基于体素的方法所需的逐场景优化[16,29]。</li></ul></li><li><strong>Multi-view reconstruction and rendering</strong><ul><li>多视图三维重建已经得到了广泛的研究和解决，包括一些SFM[43,49,50]和MVS技术[10,14,25,44,59]。点云通常是MVS或深度传感器的直接输出，尽管它们通常被转换为网格[21,31]用于渲染和可视化。网格划分可能会引入误差，并且可能需要基于图像的渲染[6,12,66]才能实现高质量的渲染。我们直接使用深度MVS中的点云来实现逼真的渲染。</li><li>点云在渲染中得到了广泛的应用，通常是通过基于栅格化的点splatting，甚至是可微栅格化模块[26,55]。然而，重建的点云通常有孔洞和异常值，导致渲染中的伪影。基于点的神经渲染方法通过splatting神经特征并使用2D cnn来渲染它们来解决这个问题[2,24,34]。相比之下，我们的基于点的方法利用3D体渲染，导致比以前的基于点的方法明显更好的结果。</li></ul></li><li><strong>Neural radiance fields</strong><ul><li>NeRF[35]已经证明了新视图合成的显著高质量结果。它们已经扩展到实现动态场景捕获[27,39]、重光照[3,5]、外观编辑[57]、快速渲染[16,62]和生成模型[7,36,45]。然而，大多数方法[3,27,39,57]仍然遵循原始的NeRF框架，并训练每个场景的mlp来表示亮度场。我们利用场景中具有空间变化神经特征的神经点对其辐射场进行编码。与网络容量有限的纯mlp相比，这种本地化表示可以模拟更复杂的场景内容。更重要的是，我们证明了我们的基于点的神经场可以通过一个预训练的深度神经网络有效地初始化，该神经网络可以泛化整个场景，并导致高效的辐射场重建。</li><li>先前的工作也提出了基于辐射场的通用方法。PixelNeRF[63]和IBRNet[53]在每个采样的射线点聚合多视图2D图像特征，回归体渲染属性进行亮度场渲染。相反，<strong>我们利用场景表面周围的3D神经点的特征来模拟辐射场</strong>。这避免了在广阔的空白空间中采样点，并导致比PixelNeRF和IBRNet更高的渲染质量和更快的辐射场重建。MVSNeRF[8]可以实现非常快速的基于体素的辐射场重建。然而，其预测网络需要固定数量的三幅小基线图像作为输入，因此只能有效地重建局部辐射场。我们的方法可以从任意数量的视图融合神经点，实现完整的360度辐射场的快速重建，这是MVSNeRF无法支持的。</li></ul></li></ul><h1 id="Point-NeRF-Representation"><a href="#Point-NeRF-Representation" class="headerlink" title="Point-NeRF Representation"></a>Point-NeRF Representation</h1><p>我们提出了新的基于点的亮度场表示，设计用于高效重建和渲染(见图2 (b))。我们从一些预备开始。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230824165453.png" alt="image.png"><br><em>Point-NeRF概述(a)从多视图图像中，我们的模型通过使用基于cost volume的3D CNN Gp，γ来生成每个视图的深度，并通过2D CNN Gf从输入图像中提取2D特征。在对深度图进行聚合后，我们得到了一个基于点的辐射场，其中每个点具有空间位置pi、置信度γi和未投影图像特征fi。(b)为了合成一个新的视图，我们只在神经点云(例如xa, xb, xc)附近进行可微射线行进和计算阴影。在每个shading位置，point - nerf从它的K个神经点邻居中聚集特征，计算亮度r和体积密度σ，然后使用σ计算累加量。整个过程是端到端可训练的，基于点的亮度场可以优化渲染损失。</em></p><p><strong>Volume rendering and radiance fields</strong></p><p>基于物理的体绘制可以通过可微射线推进进行数值评估。具体来说，像素的亮度可以通过让光线穿过像素来计算，在$\{x_{j}\mid j=1,…,M\}$沿射线，并利用体积密度累积辐射，如:<br>$c=\sum_{M}\tau_{j}(1-\exp(-\sigma_{j}\Delta_{j}))r_{j},$<br>$\tau_{j}=\exp(-\sum_{t=1}^{j-1}\sigma_{t}\Delta_{t}).$<br>其中，τ表示体积透过率;$σ_j$、$r_j$为$x_j$处每个shading点j的体积密度和亮度，$∆_t$为相邻遮阳样本之间的距离。<br>辐射场表示任何3D位置的体积密度σ和视景相关的辐射r。NeRF[35]提出使用多层感知器(MLP)来回归这样的辐射场。我们提出了Point-NeRF，它利用神经点云来计算体积属性，从而实现更快、更高质量的渲染。</p><p><strong>Point-based radiance field.</strong></p><p>我们用$P=\{(p_i,f_i,\gamma_i)|i=1,…,N\},$其中每个点i位于$p_i$，并与编码局部场景内容的神经特征向量$f_i$相关联。我们还为每个点分配一个尺度置信度值$γ_i∈[0,1]$，它表示该点位于实际场景表面附近的可能性。我们从这个点云回归辐射场。<br>给定任何3D位置x，我们在一定半径r内查询它周围的K个相邻神经点。我们基于点的亮度场可以抽象为一个神经模块，它从相邻神经点回归任何阴影shading位置x的体积密度σ和视图依赖亮度r(沿任何观看方向d)，如下所示:<br>$(\sigma,r)=\text{Point-NeRF}(x,d,p_1,f_1,\gamma_1,…,p_K,f_K,\gamma_K).$<br>我们使用一个类似于pointnet的[40]神经网络，具有多个子mlp，来进行这种回归。总的来说，我们首先对每个神经点进行神经处理，然后将多个点的信息进行聚合，得到最终的估计值。</p><p><strong>Per-point processing.</strong></p><p>我们使用<strong>MLP F</strong>来处理每个相邻的神经点，通过以下方法预测阴影位置x的新特征向量:$f_{i,x}=F(f_{i},x-p_{i}).$<br>本质上，原始特征$f_i$编码了$p_i$周围的局部3D场景内容。该MLP网络表示一个局部3D函数，该函数输出特定的神经场景描述在x处$f_{i,x}$，由其局部帧中的神经点建模。相对位置x−p的使用使得网络对点平移具有不变性，从而更好地泛化。</p><p><strong>View-dependent radiance regression</strong></p><p>我们使用标准逆距离加权来聚合从这K个相邻点回归的神经特征$f_{i,x}$，以获得描述x处场景外观的单个特征$f_x$:<br>$f_{x}=\sum_{i}\gamma_{i}\frac{w_{i}}{\sum w_{i}}f_{i,x},\mathrm{~where~}w_{i}=\frac{1}{|p_{i}-x|}.$</p><p>然后一个<strong>MLP R</strong>，根据给定的观测方向d，从该特征中回归与视图相关的辐射:$r=R(f_{x},d).$</p><p>反距离权值$w_i$广泛应用于离散数据插值;我们利用它来聚合神经特征，使更接近的神经点对着色计算贡献更多。此外，我们在此过程中使用了<strong>逐点置信度</strong>γ;这在最后的重构中进行了优化，并减少了稀疏性损失，使网络能够灵活地拒绝不必要的点。</p><p><strong>Density regression</strong></p><p>为了计算x处的体积密度σ，我们遵循类似的多点聚合。然而，我们首先使用<strong>MLP T</strong>回归每个点的密度$σ_i$，然后做基于距离的逆加权，给出如下:$\sigma_i=T(f_{i,x})$<br>然后在x处的密度为（标准逆距离加权）<br>$\sigma=\sum_{i}\sigma_{i}\gamma_{i}\frac{w_{i}}{\sum w_{i}},w_{i}=\frac{1}{|p_{i}-x|}.$</p><p>因此，每个神经点直接贡献体积密度，点置信度$γ_i$与这种贡献明确相关。我们在点去除过程中利用了这一点(见第4.2节)。</p><p><strong>Discussion</strong></p><p>与之前基于神经点的方法[2,34]不同，这些方法将点特征栅格化，然后用2D cnn渲染它们，我们的表示和渲染完全是3D的。通过使用近似场景几何形状的点云，我们的表示自然而有效地适应场景表面，并避免在空场景空间中采样阴影位置。对于沿着每条射线的阴影点，我们实现了一种有效的算法来查询相邻的神经点;详情见补充材料。</p><h1 id="Point-NeRF-Reconstruction"><a href="#Point-NeRF-Reconstruction" class="headerlink" title="Point-NeRF Reconstruction"></a>Point-NeRF Reconstruction</h1><p>我们现在介绍我们的管道有效地重建基于点的辐射场。</p><ul><li>我们首先利用<strong>跨场景训练的深度神经网络</strong>，通过直接网络推理<strong>生成初始的基于点的场</strong>(第4.1节)。</li><li>用我们的<strong>点生长和修剪技术</strong>进一步优化每个场景的初始场，导致我们最终的高质量亮度场重建(第4.2节)。</li></ul><p>图3显示了这个工作流，以及用于初始预测和场景优化的相应梯度更新。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230825142715.png" alt="image.png"><br><em>虚线表示亮度场初始化和每个场景优化的梯度更新。</em></p><h2 id="Generating-initial-point-based-radiance-fields"><a href="#Generating-initial-point-based-radiance-fields" class="headerlink" title="Generating initial point-based radiance fields"></a>Generating initial point-based radiance fields</h2><p>给定一组已知图像$I_1，…，I_Q$和一个点云，我们的point -NeRF表示可以通过优化随机初始化的每点神经特征和具有渲染损失(类似于NeRF)的mlp来重建。然而，这种纯粹的逐场景优化依赖于现有的点云，并且可能会非常慢。因此，我们提出了一个<strong>神经生成模块</strong>，通过前馈神经网络预测所有神经点属性，包括点位置pi，神经特征fi和点置信度γi，以实现高效重建。网络的直接推理输出了一个良好的初始基于点的辐射场。然后可以对初始字段进行微调，以实现高质量的呈现。在很短的时间内，渲染质量更好或与NeRF相当，而NeRF需要更长的时间来优化(见表1和2)。</p><p><strong>Point location and confidence.</strong></p><p>我们利用深度MVS方法，使用基于成本体积的3D cnn生成3D点位置[10,59]。这样的网络产生高质量的密集几何，并在各个领域推广良好。对于每个在视点q处具有相机参数$Φ_q$的输入图像$I_q$，我们遵循MVSNet[17]，首先通过从邻近视点弯曲二维图像特征来构建平面扫描代价体积，然后使用深度3D cnn回归深度概率体积。深度图是通过线性组合每个平面的深度值加权概率来计算的。我们将深度图反投影到3D空间，得到每个视图q的一个点云${p_1，…， p_{N_{q}}}$。</p><p>由于深度概率描述了点在表面上的可能性，我们对深度概率体积进行三线性采样，以获得每个点$p_i$的点置信度γ。以上过程可以用<br>$\{p_{i},\gamma_{i}\}=G_{p,\gamma}(I_{q},\Phi_{q},I_{q_{1}},\Phi_{q_{1}},I_{q_{2}},\Phi_{q_{2}},…),$</p><p>其中$G_{p,γ}$为<strong>基于mvsnet的网络</strong>。$I_{q_{1}}， Φ_{q_{1}}，…$是MVS重建中使用的附加相邻视图;在大多数情况下，我们使用两个附加视图。</p><p><strong>Point features.</strong></p><p>我们使用<strong>2D CNN</strong> $G_f$从每个图像$I_q$中提取神经2D图像特征映射。这些特征映射与来自$G_{p,γ}$的点(深度)预测对齐，并用于直接预测每个点的特征$f_i$:$\{f_i\}=G_f(I_q).$<br>特别地，我们使用了一个有三个下采样层的VGG网络架构。我们将不同分辨率的中间特征组合为fi，提供了一个有意义的点描述来模拟多尺度场景的外观。(见图2(a))<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230825144059.png" alt="image.png"></p><p><strong>End-to-end reconstruction.</strong></p><p>我们结合来自多个视点的点云来获得我们最终的神经点云。我们训练点生成网络和表示网络，从头到尾都有渲染损失(见图3)。这使得我们的生成模块产生合理的初始辐射场。它还以合理的权重初始化Point-NeRF表示中的mlp，极大地节省了每个场景的拟合时间。<br>此外，除了使用完整的生成模块，我们的管道还支持使用从其他方法(如COLMAP[44])重建的点云，其中我们的模型(不包括MVS网络)仍然可以为每个点提供有意义的初始神经特征。详情请参考我们的补充资料。</p><h2 id="Optimizing-point-based-radiance-fields"><a href="#Optimizing-point-based-radiance-fields" class="headerlink" title="Optimizing point-based radiance fields"></a>Optimizing point-based radiance fields</h2><p><strong>上面的管道</strong>可以为一个新场景<strong>输出一个合理的基于初始点的亮度场</strong>。通过可微射线行进，对于特定场景，我们可以通过优化神经点云(点特征$f_i$and点置信度$γ_i$)和我们表示中的mlp来进一步改善亮度场(见图3)。</p><p>初始点云，特别是来自外部重建方法的点云(例如，图1中的Metashape或COLMAP)，通常会<strong>包含降低渲染质量的孔和异常值</strong>。在逐场景优化过程中，为了解决这个问题，我们发现直接优化现有点的位置会使训练不稳定，无法填充大洞(见1)。相反，<strong>我们采用了新的点修剪和生长技术，逐渐提高几何建模和渲染质量</strong>。</p><p><strong>Point pruning.</strong></p><p>如第3节所述，我们设计了点置信度值$γ_i$来描述神经点是否靠近场景表面。我们利用这些置信度值来修剪不必要的异常点。请注意，<strong>点置信度与体积密度回归中每个点的贡献直接相关</strong>(Eqn. 7);因此，低置信度反映了点局部区域的低体积密度，表明它是空的。因此，我们每10K次迭代就修剪$γ_i &lt; 0.1$的点。我们还对点置信度施加了稀疏性损失[30], $\mathcal{L}_{\mathrm{sparse}}=\frac{1}{|\gamma|}\sum_{\gamma_{i}}\left[log(\gamma_{i})+log(1-\gamma_{i})\right]$ 这迫使置信值要么接近0，要么接近1。如图4所示，这种剪枝技术可以去除离群点，减少相应的伪影。</p><p><strong>Point growing.</strong></p><p>我们还提出了一种新的技术来生长新的点来覆盖原始点云中缺失的场景几何。与直接利用现有点信息的点修剪不同，<strong>生长点需要在不存在点的空白区域恢复信息</strong>。我们通过基于我们的point - nerf表示建模的局部场景几何来逐步增长点云边界附近的点来实现这一点。<br>特别是，我们利用在光线行进中采样的每射线着色位置(Eqn. 1中的$x_j$)来识别新的候选点。具体来说，我们确定沿光线<strong>不透明度最高的阴影位置</strong>$x_{j_{g}}$:<br>$\alpha_j=1-\exp(-\sigma_j\Delta_j),j_g=\operatorname*{argmax}_j\alpha_j.$<br>我们将$\epsilon_{j_{g}}$计算为$x_{j_{g}}$到它最近的神经点的距离。</p><p>对于行进射线，如果$\alpha_{j_g}&gt;T_{\text{opacity}}$, $\epsilon_{j_{g}}&gt;T_{\mathrm{dist}}$，我们在$x_{j_{g}}$处生长一个神经点。这意味着该位置位于表面附近，但远离其他神经点。通过重复这种增长策略，我们的亮度场可以扩展到覆盖初始点云中的缺失区域。<strong>点增长尤其有利于通过COLMAP等方法重建的点云</strong>，这些方法不密集(见图4)。我们表明，即使在只有1000个初始点的极端情况下，我们的技术也能够逐步增长新的点并合理地覆盖物体表面(见图5)。</p><h1 id="Implementation-details"><a href="#Implementation-details" class="headerlink" title="Implementation details"></a>Implementation details</h1><p><strong>Network details</strong><br>我们对每点处理网络$G_f$的相对位置和每点特征以及网络r的观看方向应用<strong>频率位置编码</strong>。我们在network $G_f$中以<strong>不同分辨率从三层提取多尺度图像特征</strong>，从而得到一个具有56(8+16+32)个通道的向量。我们还从每个输入视点附加相应的观看方向，以处理与视图相关的效果。因此，<strong>我们最终的逐点神经特征是一个59通道向量</strong>。请参考我们的补充资料，了解网络架构和遮阳时的神经点查询的细节。</p><p><strong>Training and optimization details.</strong><br>我们在DTU数据集上训练我们的完整管道，使用与PixelNeRF和MVSNeRF相同的训练和测试分割。我们首先使用类似于原始MVSNet论文[59]的地面真值深度对基于MVSNet的深度生成网络进行预训练。然后，我们纯粹使用L2渲染损失$L_render$从端到端训练我们的完整管道，用ground truth监督我们从光线行进(通过Eqn. 1)中渲染的像素，以获得我们的Point-NeRF重建网络。我们使用Adam[22]优化器训练整个管道，初始学习率为5e−4。我们的前馈网络从三个输入视图生成一个点云需要0.2s。<br>在逐场景优化阶段，我们采用了一种结合了渲染和稀疏度损失的损失函数$\mathcal{L}_{\mathrm{opt}}=\mathcal{L}_{\mathrm{render}}+a\mathcal{L}_{\mathrm{sparse}},$<br>我们用a = 2e - 3来做所有的实验。我们<strong>每10K次迭代执行点生长和修剪</strong>，以实现最终的高质量重建。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><ul><li>Evaluation on the DTU testing set.</li><li>Evaluation on the NeRF Synthetic dataset.</li><li>Evaluation on the Tanks &amp; Temples and the ScanNet dataset.</li><li>Additional experiments.<ul><li>Converting COLMAP point clouds to Point-NeRF</li><li>Point growing and pruning.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Explicit Volumetric </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PointCloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learn-Colmap</title>
      <link href="/Learn/Learn-Colmap/"/>
      <url>/Learn/Learn-Colmap/</url>
      
        <content type="html"><![CDATA[<p><a href="https://colmap.github.io/tutorial.html">colmap tutorial</a></p><p>更好的重建效果需要拍摄的图片保证:</p><ul><li>好的纹理</li><li>相似的照明条件，避免高动态范围场景（例如，逆光阴影或透过门窗的照片），避免在光滑表面上出现反射光</li><li>高视觉重叠度，确保每个对象至少在 3 张图像中可见，图像越多越好</li><li>从不同的视角拍摄图像，不要只通过旋转相机来拍摄相同位置的图像</li></ul><span id="more"></span><p>Colmap 算法主要包括 SFM 和 MVS</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230717204531.png" alt="image.png|500"></p><h1 id="Structure-from-Motion-SFM"><a href="#Structure-from-Motion-SFM" class="headerlink" title="Structure-from-Motion(SFM)"></a>Structure-from-Motion(SFM)</h1><blockquote><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=553224841006579712&amp;noteId=1991438575104619008">Structure-from-Motion Revisited (readpaper.com)</a> &gt; <a href="https://jiajiewu.gitee.io/post/tech/slam-sfm/sfm-intro/">SFM 算法原理初简介 | jiajie (gitee.io)</a></p></blockquote><p>增量式 SFM：输入一系列从多个不同视角对相同物体 overlapping 的图像，输出稀疏 3D 点云和所有图像对应的相机内外参</p><ol><li>Feature detection and extraction</li><li>Feature matching and geometric verification</li><li>Structure and motion reconstruction</li></ol><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231005200141.png" alt="image.png|666"></p><p>SFM:</p><ul><li>Correspondence Search. <strong>输入图像</strong> —&gt; 一组经过几何验证的<strong>图像对</strong>，以及每个点的图像<strong>投影图</strong><ul><li>Feature Extraction. SIFT 算子(可以是任何一种特异性较强的特征)<ul><li>SIFT 算法(Scale-invariant feature transform)是一种电脑视觉的算法用来侦测与描述影像中的局部性特征，它在空间尺度中寻找极值点，并提取出其位置、尺度、旋转不变量，此算法由  David Lowe 在 1999 年所发表，2004 年完善总结, ref: <a href="https://blog.csdn.net/u010440456/article/details/81483145">非常详细的 sift 算法原理解析_可时间倒数了的博客-CSDN 博客</a></li></ul></li><li>Matching.</li><li>Geometric Verification. 如果一个有效的变换在图像之间映射了足够数量的特征，它们就被认为是几何验证的<ul><li>单应性H描述了纯旋转或移动摄像机捕捉平面场景的变换</li><li>对极几何通过essential矩阵E(校准)或fundamental矩阵F(未校准)描述了移动摄像机的关系，并且可以使用三焦张量扩展到三个视图</li></ul></li></ul></li><li>Incremental Reconstruction. <strong>scene graph.</strong> —&gt; <strong>pose estimates and reconstructed scene structure as a set of points</strong><ul><li>Initialization. 选择合适的初始pair至关重要</li><li>Image Registration. 配准：从度量重建开始，通过使用已配准图像中三角点的特征对应(2D-3D对应)来解决Perspective-n-Point (PnP)问题，可以将新图像配准到当前模型</li><li>Triangulation. 三角测量是SfM的关键步骤，因为它通过冗余增加了现有模型的稳定性</li><li>Bundle Adjustment. BA是摄像机参数Pc和点参数Xk的联合非线性细化，使重投影误差最小化</li></ul></li></ul><h1 id="Multi-View-Stereo-MVS-"><a href="#Multi-View-Stereo-MVS-" class="headerlink" title="Multi-View Stereo(MVS)"></a>Multi-View Stereo(MVS)</h1><blockquote><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=709983199334641664&amp;noteId=1991447797942823424">Pixelwise View Selection for Unstructured Multi-View Stereo (readpaper.com)</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Colmap </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF in the industrial and robotics domain</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Review/NeRF%20in%20the%20industrial%20and%20robotics%20domain/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Review/NeRF%20in%20the%20industrial%20and%20robotics%20domain/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Neural Radiance Fields in the Industrial and Robotics Domain: Applications, Research Opportunities and Use Cases</th></tr></thead><tbody><tr><td>Author</td><td>Eugen ˇSlapak, Enric Pardo, Mat ́uˇs Dopiriak, Taras Maksymyuk and Juraj Gazda</td></tr><tr><td>Conf/Jour</td><td>cs.RO</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://github.com/Maftej/iisnerf">Maftej/iisnerf (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4789767752768290817&amp;noteId=1925861245942862080">Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases (readpaper.com)</a></td></tr></tbody></table></div><p>探索了NeRF在工业和机器人领域的应用</p><ul><li>Instant-NGP 基于NeRF的视频压缩技术</li><li>D-NeRF 根据过去的arm位置来预测未来的arm运动</li></ul><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>在这项研究中，我们进行了全面的研究，并探讨了NeRF在工业部门的潜在应用。我们的研究表明，nerf可以克服传统3D表示和渲染方法的局限性，例如成本高、存储效率低、设备要求高和真实感有限。此外，我们研究了创新的NeRF在工业环境中的应用，并通过概念验证实验证实了它们的可行性。我们认为<strong>我们的工作是对不断扩大的关于nerf及其在各个领域的不同应用的文献的有价值的补充</strong>。此外，我们希望通过提供实用的指导和建议，帮助<strong>弥合学术研究与工业实践之间的差距</strong>，以利用nerf来应对现实世界的工业挑战。<br>作为一种新颖而有前途的方法，nerf为未来工业领域的研究提供了广泛的机会。</p><ul><li>我们的概念验证实验可以扩展到<strong>基于对未来机器人相机姿势的预测</strong>来<strong>探索视频帧的预测性推测生成</strong>，甚至可以预测使用这些帧进行其他下游任务，例如对象分类。推测性预测执行可以从根本上减少这类任务的延迟。</li><li>除了我们实验的扩展，还有许多未探索的主题。其中包括<strong>通过nerf对无线电频谱进行建模</strong>，以估计信号质量，或者<strong>通过静态外部摄像机观察蜂群及其周围环境的视频流</strong>，为机器人群中的单个机器人生成视图。<strong>嵌入语言的nerf</strong>，如[110]LERF中提出的nerf，也可以使基于语言模型的智能体能够在3D中进行正确的行动计划的常识性推理。</li></ul><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>扩展现实(XR)等技术的激增增加了对高质量三维(3D)图形表示的需求。工业3D应用包括计算机辅助设计(CAD)、有限元分析(FEA)、扫描和机器人技术。然而，目前用于工业3D表示的方法存在实施成本高和依赖人工输入来进行精确的3D建模的问题。为了解决这些挑战，神经辐射场(nerf)已经成为一种很有前途的方法，可以基于提供的训练2D图像来学习3D场景表示。<strong>尽管人们对nerf的兴趣日益浓厚，但它们在各种工业子领域的潜在应用仍未得到探索</strong>。<br>在本文中，我们提供了NeRF工业应用的全面检查，同时也为未来的研究工作提供了方向。我们还提出了一系列概念验证实验，证明了nerf在工业领域的潜力。这些实验包括<strong>基于nerf的视频压缩技术</strong>，以及<strong>在避免碰撞的情况下使用nerf进行3D运动估计</strong>。在视频压缩实验中，我们的结果显示，在分辨率分别为1920x1080和300x168的情况下，压缩节省高达48%和74%。运动估计实验采用机械臂三维动画对动态nerf (D-NeRF)进行训练，得到视差图的平均峰值信噪比(PSNR)为23 dB，结构相似指数(SSIM)为0.97。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>人工智能(AI)应用于计算机视觉和机器人中的3D场景理解的早期研究主要集中在简单的算法上，例如导航的边缘识别[1]。到20世纪90年代末，大多数算法缺乏从视觉数据中提取特征的能力。机器人中图像特征提取能力的增长始于尺度不变特征变换(SIFT)[2]的出现和加速鲁棒特征(SURF)[3]方法的发展。<br>机器学习的最新发展加速了深度学习在机器人领域的应用。一个值得注意的突破是基于AlexNet卷积神经网络(CNN)的深度学习的成功实现[4]，在性能方面优于传统的计算机视觉方法。从那时起，深度学习被广泛应用于各种机器人场景，实现了计算机视觉任务与端到端训练机器人控制代理的集成。<br>在[5]中进行的一项全面调查发现，深度学习已经成功地应用于机器人的各个领域，包括抓取规划、路径规划、感觉整合以及广泛的深度强化学习方法。这些应用使机器人能够学习复杂的任务，包括联合规划和控制模型，如[6]等作品所示。<br>此外，随着近年来cpu、gpu和tpu计算能力的增长，我们在数字孪生和工业元应用上观察到更多的工业活动。一个突出的例子是<strong>宝马的数字工厂</strong>，在那里，全面的3D扫描被用来准确地表示所有的生产地点和数字孪生体，使实时，虚拟导航不受地理或时间的限制[7]。这种技术允许重建各种操作要素，如生产线、机器和人员。此外，开发的虚拟工厂可以使用虚拟现实耳机headsets来捕获、分析和体验虚拟领域的工作空间，从而实现沉浸式参与。一般来说，使用3D扫描或3D建模设计的数字孪生可以在整个工业过程中节省大量的运营费用。<br>[8]中的作者详细回顾了2017年之前基于深度学习的机器人3D计算机视觉技术。本文综述了两种主要的三维计算机视觉任务类别:<strong>三维分类和三维生成</strong>。分类任务包括对象类的存在和估计其3D边界框。生成任务包括将输入转换为3D空间的2D或3D表示。3D表示的例子包括深度图[9]和体素网格[10]，[11]。该综述还概述了各种神经结构，主要是cnn，它们使用2D图像深度图通道或3D表示(如体素)进行训练。这些基于深度学习的方法在从2D图像中提取深度和3D表示方面比以前的方法表现得更好<br><strong>然而，现有的方法有一定的局限性，例如需要人工输入3D建模</strong>，这导致了高昂的管理成本和实现时间。基于扫描对象点云的替代方法具有极高的存储和计算要求。这些缺点正在推动进一步的研究以改进现有的技术。<br>神经隐式表征在[12]中有正式描述，是深度学习中最近出现的一个值得注意的范式。与获取广义知识的传统模型不同，神经隐式表示被设计为过拟合训练样本。这使他们能够在推理过程中以低误差重建样本，使他们能够在神经网络权重内存储多媒体数据，如图像，音频和视频</p><p><strong>在这项工作中，我们专注于NeRF，一个隐式神经表征的例子</strong>。NeRF通过基于描绘场景的样本图像将3D坐标映射到相应的亮度和光密度来学习3D场景表示。我们探索了几种工业应用，其中nerf已被实验性地用于解决各种问题。此外，我们还就nerf的未来潜力提供了建议和见解，以应对现有的工业挑战。<br>到目前为止，已经发表了几篇关于nerf的review文章。值得注意的贡献包括Gao等[13]、Zhu等[14]和Tewari等[80]。然而，值得注意的是，这些评论主要集中在nerf及其衍生物上，忽略了利用图像空间2D推理的神经渲染技术的讨论。此外，一项额外的工作[15]在深入探索NeRF的同时，对神经领域的更广泛主题进行了全面的讨论。<br>我们的工作有几个独特的优势，填补了该领域现有综述论文的空白。<strong>首先，它特别关注NeRF在工业和机器人领域的应用</strong>，强调了在这一特定领域扩展NeRF应用的巨大潜力。这种有针对性的方法可以更深入地探索工业和机器人环境中存在的独特挑战和机遇。<strong>其次，我们的工作不仅涵盖了已建立的NeRF应用，还引入了有前途的创新想法</strong>，这些想法有可能推动进一步的高级研究。不像现有的评论可能只是简单地触及这些想法，我们的工作更广泛地探讨了它们，揭示了它们的潜在影响和好处<br>我们的工作超越了理论讨论，包括概念验证实验。这些实验验证了这些建议的可行性和潜力，可供进一步研究。包括这样的实验证据加强了我们关于提出的想法的适用性和有效性的论点。<br>总之，这项工作填补了两个主要的研究空白。<strong>一是缺乏对nerf在工业领域的综述和应用探索论文</strong>。<strong>其次是传统的应用三维图形和表示方法在该领域的不足</strong>，现有的和新的基于nerf的方法可以解决这些问题。我们的主要贡献可以概括如下:</p><ul><li>我们全面回顾了工业应用中计算机图形学和3D表示的现有方法，并概述了它们的弱点。</li><li>我们研究了NeRF方法在工业应用中的可行性，并与主要使用的非NeRF方法的缺点进行了比较。</li><li>我们对这项工作中研究的新型NeRF应用进行了实验评估。</li></ul><p><em>本文其余部分的结构概述如下。第二节解释了基本nerf的基本原则，特别是静态场景的操作。第三节讨论了NeRF衍生变体的主要家族的分类及其与感兴趣领域的相关性。第四部分概述了工业领域中3D渲染和3D场景表示应用的现状，以及nerf在这些应用中的潜在适用性。第V节介绍了nerf的数学描述和我们的概念验证实验所需的评估指标。第六节和第七节分别介绍了基于nerf的视频压缩和深度估计的避障实验。最后，论文在第八部分结束，总结了本文的主要发现和贡献。</em></p><h1 id="NEURAL-RADIANCE-FIELDS-NERFS-FUNDAMENTALS-AND-WORKFLOW"><a href="#NEURAL-RADIANCE-FIELDS-NERFS-FUNDAMENTALS-AND-WORKFLOW" class="headerlink" title="NEURAL RADIANCE FIELDS (NERFS): FUNDAMENTALS AND WORKFLOW"></a>NEURAL RADIANCE FIELDS (NERFS): FUNDAMENTALS AND WORKFLOW</h1><p>NeRFs [16]属于内隐神经表征模型家族。这种模型通常表示单个连续实体，例如，描述单个图像或单个3D场景属性的信号。它们也可以被认为是这种实体的压缩和信号函数近似技术。<strong>nerf使用完全连接的深度神经网络将3D静态场景表示集成到神经网络权重中</strong>。其目的是以图像的形式对信号进行参数化，并对数据进行过拟合。原始的NeRF实现使用基于基本多层感知器(MLP)体系结构的模型。这与大多数过度拟合是不必要现象的任务不同，因为神经网络通常用于对一组实体的泛化，而不是对单个学习实体的过度拟合。输入是一组描绘单个3D场景及其相应相机姿势的图像。可微分渲染函数计算亮度，用于获得2D图像渲染的结果像素。优化神经场景表示，然后通过最小化地面真实和生成的图像之间的指导。nerf完成了一项被广泛称为新视图合成的任务，用于从特定姿势生成2D图像或生成3D场景模型。</p><p>图1提供了NeRF工作原理的文本描述的视觉概述，以及整篇论文中使用的一些基本符号。与大多数其他方法相比，使用nerf生成高保真3D静态场景或对象所需的工作量更少，工作流程类似于立体摄影测量[17]。该工作流程包括训练NeRF，仅使用从不同视点捕获所需3D场景的输入图像集合。原始NeRF架构的一个限制是，它需要以相机变换矩阵的形式提供关于每个训练图像的相机位置和角度的额外信息。然而，它的一些衍生变体成功地解决了这个缺点。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821142517.png" alt="image.png"></p><p>在训练过程中，NeRF通过近似3D空间中点的体积密度和亮度来增强其输出的准确性和真实感。这两个量对于定义给定场景中的对象属性至关重要。体积密度决定了物质在空间中某一点的不透明度。辐射度决定了由无限小的投影面积发射或反射的辐射功率的量，单位是每无限小的立体角的瓦数。图1a直观地表示了这些概念。为了显色，有必要分别考虑单个红、绿、蓝颜色通道的亮度。<br>当相机捕捉2D场景投影时，每个图像看起来可能完全不同。NeRF学习一个独立于任何视图转换的底层3D场景表示，可用于生成训练集中未见的新视图。多个训练图像允许NeRF学习所有训练摄像机视点一致的场景表示。<br>图像由NeRF使用传统的体绘制合成，如图1b所示。体积渲染将新视图图像的像素作为沿3D场景中穿越点的光线路径的数值集成的亮度，逐渐累积亮度并最终进入相机光圈。在渲染过程中，NeRF只查询相关光线路径的体积密度和亮度，这取决于相机的视点。<br>将训练集中摄像机视点合成的渲染结果与地面真值图像进行对比，得到梯度下降的损失信号，如图1c所示。这允许我们逐步优化MLP权重，以更好地表示场景。</p><h1 id="CLASSIFICATION-OF-DERIVATIVE-NERF-IMPLEMENTATIONS"><a href="#CLASSIFICATION-OF-DERIVATIVE-NERF-IMPLEMENTATIONS" class="headerlink" title="CLASSIFICATION OF DERIVATIVE NERF IMPLEMENTATIONS"></a>CLASSIFICATION OF DERIVATIVE NERF IMPLEMENTATIONS</h1><p>本节总结了衍生的NeRF变体，这些变体在效率或质量改进方面增强了原始NeRF架构的特定方面。已经开发了几个扩展来增强各种基本的NeRF架构属性，包括训练时间、质量、所需的样本和渲染时间。了解这些NeRF改进的知识对于解决不同科学和实际环境中固有的特定应用限制和要求至关重要。<br>每一项改进都是由衍生的NeRF变体引入的，这些变体解决了工业NeRF用例中的各种限制。例如，如果机器人在移动到未知环境时需要使用NeRF进行空间导航，直到从不同视点收集到环境中重要部分的足够多的图像，则样本效率可能是有用的。低渲染时间(以性能为导向)对于需要低反应时间的系统是有用的，例如，机器人在空间中高速移动。最后，动态NeRF变量可用于捕获运动，其应用的一个例子是学习机器人的预定义重复运动以进行时空协调。<br>我们对NeRF变体进行了分类，并根据其对原始NeRF的主要改进重点对以下子部分进行了构建。这些一般类别是NeRF变体，侧重于样本效率、性能和系统动力学。尽管这种分类有助于快速确定特定任务的最佳NeRF变体，但大多数论文同时关注多种改进。为了反映这一点，表1对这些作品进行了更细粒度的分类。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821142946.png" alt="image.png"></p><h2 id="Latent-vectors-for-sample-efficiency-and-scene-modification"><a href="#Latent-vectors-for-sample-efficiency-and-scene-modification" class="headerlink" title="Latent vectors for sample efficiency and scene modification"></a>Latent vectors for sample efficiency and scene modification</h2><p>一些实现采样效率的NeRF变体利用潜在向量对从不同训练数据中学习到的对象类别的抽象特征进行编码。虽然潜在向量不是提高样本效率的唯一方法，如RegNeRF[36]所示，但它们也可以在一些NeRF架构中实现场景修改，如CodeNeRF[41]。</p><p>RegNeRF[36]从少至三张输入图像中增强场景重建。因此，它规范了新视图渲染图像补丁的几何形状和外观。几何正则化利用了三维几何平滑的先验假设，这适用于大多数具有低空间频率几何的现实世界场景。外观正则化基于一个新的视图图像补丁，利用其正确性的可能性来优化NeRF。这种似然是使用在非结构化图像数据集上训练的归一化流模型获得的<br>pixelNeRF[37]利用CNN提取的局部特征，在训练过程中学习2D和3D特征之间的对应关系。该方法还采用聚类算法将图像像素划分为NeRF子模块，从而实现并行训练。<br>LOLNeRF[38]使用NeRF的生成潜在优化(GLO)从给定对象类的单个图像执行3D结构重建。解码器网络使用从每个图像中提取的潜在代码表示进行训练，该编码表示编码与3D场景重建相关的抽象特征。在训练过程中，对潜在代码的提取进行逐步细化<br>CodeNeRF[41]在训练过程中学习对象三维结构和纹理的解纠缠潜码。它在单个帧上使用推理时间优化来调整潜在代码，这用于调节NeRF输出。这使得在不改变MLP权重的情况下，可以在推理时修改对象形状或纹理以匹配单个图像示例。此外，与类似的方法不同，执行此任务不需要了解相机姿势。<br>用于高质量视图合成的稀疏RGB-D图像的nerf[46]表明，通过对每个场景进行预训练和微调，它可以从少至6个输入图像中进行训练。<br>ActiveNeRF[47]为NeRF提供了不确定性感知的主动学习，并通过在训练过程中选择最能降低NeRF不确定性的特定样本来提高场景表示质量。通过这种方式，它可以通过关注最重要的样本来更有效地利用资源。<br>NeRDi[39]以语言引导扩散为一般图像先验的单视图NeRF合成, 利用2D扩散模型学习的多视图先验从单个图像重建3D场景。它使用图像字幕和输入图像的文本反转来分别引导粗和细物体的外观。<br><strong>SparseNeRF</strong>[48]利用来自低成本深度传感器或深度估计模型的不完全深度信息，以少量样本训练NeRF。它在局部光场融合(LLFF)[49]和数据库事务单元(DTU)[50]数据集上优于其他最先进的方法。</p><h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2><p>现有的研究工作改进了各种NeRF性能指标，如渲染速度、训练速度和模型内存占用。有些方法可以同时对多个指标进行改进。效率的增强还可以在资源受限的硬件上实现更好的性能和新的实际应用，降低部署成本并实现对机器人有用的更小的硬件尺寸。</p><p>NSVF: Neural sparse voxel fields[18]通过在体素八树中排列的体素内训练一组隐式场，将渲染性能提高了10倍。体素结构是在训练过程中学习的，通过跳过不相关的体素来加速渲染。<br>KiloNeRF[19]是一种面向渲染性能的方法，它将3D场景体分成许多较小的子体，每个子体由一个单独的小MLP服务，而不是在整个3D场景上训练单个大MLP。与原始NeRF相比，这导致渲染速度提高了三个数量级<br>Adaptive Voronoi NeRF[20]使用与KiloNeRF类似的方法，将单个NeRF函数划分为更小的函数。然而，它使用Voronoi图，通过一组近似器将要学习的几何图形均匀地划分到单个Voronoi细胞中。该方法不需要像KiloNeRF那样单独的神经网络蒸馏步骤，并且使用256个Voronoi细胞比使用512个规则间隔细胞的KiloNeRF获得更好的质量<br>Mega-NeRF[25]是一种设计<strong>用于大型3D场景训练的架构</strong>，例如无人机(UAV)飞越，可以捕获整个城市街区大小的区域。它使用一个稀疏神经网络，其参数专门用于场景的局部部分。当在Mega-NeRF架构上使用快速渲染NeRF变体时，它可以在基本NeRF的基础上实现40倍的加速，而质量损失可以忽略不计。<br>SNeRG[21]引入了一种称为稀疏神经辐射网格(SNeRG)的数据结构，它可以将预先计算的数据存储在体素的稀疏网格中。每个体素包含由NeRF计算的光密度，漫反射颜色和学习的镜面颜色特征向量，仅在训练期间使用。在推断时，漫射颜色和光密度沿射线进行alpha合成。高光颜色特征向量分别进行alpha合成，并馈送到输出高光颜色组件的MLP。然后将漫反射和镜面分量图像合并到最终图像中。这种方法可以将NeRF在普通硬件上的渲染速度提高到每秒30帧。它还实现了空间效率，只需要90mb的存储空间。<br>PlenOctrees[22]首先训练NeRF变体NeRFSH (SH代表球面谐波)，它使用封闭形式的球面基函数代替颜色。然后，NeRF-SH被预先计算成一个基于八叉树的3D结构，称为PlenOctrees。颜色可以通过将两个视角定义的给定方向的加权球谐基相加得到。全八叉树允许直接优化进一步的质量改进和三倍的训练加速。他们也实现了3000倍的渲染速度比原来的NeRF在150 FPS。<br>Plenoxels[28]是PlenOctrees的后续作品，它使用不同的体素结构来表示场景。它没有使用八叉树，而是使用指向体素的密集3D指针数组。每个体素边缘定义一个球谐函数，通过球谐函数的三线性插值得到每个体素内的全光函数。这与PlenOctree形成对比，PlenOctree使用单个常量来表示体素体积。Plenoxels可以在不使用NeRF的情况下进行端到端优化，实现比原始NeRF快约100倍的训练速度。因此，基于我们的NeRF分类方案，我们将Plenoxels主要分类为以训练绩效为重点的方法<br>FastNeRF[23]将原始NeRF MLP分解为两个MLP:一个依赖于位置，一个依赖于视图。位置相关的MLP将3D位置坐标作为输入，并输出具有D分量的深亮度图。视相关MLP以两个视角θ和φ作为输入，输出亮度贴图分量权重。这种分解使得高效的缓存和查询亮度字段，导致速度比原来的NeRF增加了大约3000倍，平均帧率约为200 FPS。<br>SqueezeNeRF[26]通过将位置相关的MLP拆分为三个独立的MLP(每个轴对一个MLP)进一步改进了FastNeRF。与FastNeRF相比，这将缓存内存需求减少了60倍。<br>深度监督NeRF[29]使用从多视图图像中提取的稀疏点云作为一种廉价的信号来指导NeRF进行场景重建。这提高了场景重建质量，并将训练速度提高了2×to 3倍。</p><h2 id="Dynamic-scene-reconstruction-with-NeRF"><a href="#Dynamic-scene-reconstruction-with-NeRF" class="headerlink" title="Dynamic scene reconstruction with NeRF"></a>Dynamic scene reconstruction with NeRF</h2><p>NeRF可以从多个视图生成逼真的静态场景重建。它还可以扩展到处理具有时间变化的动态场景。动态NeRF模型对于需要3D捕获重复过程的工业应用非常有用。大多数现有的动态NeRF模型专注于捕捉过去的运动，而不是预测未来的场景运动。<strong>它们中的许多都依赖于变形场的概念</strong>。本节中提到的一些方法主要是由人体动作捕捉引起的，其中获得多个完美静态视图是具有挑战性的<br>D-NeRF[44]使用学习函数$\Psi_{\mathrm{x}}(\mathbf{x},\mathbf{d})\mapsto(\mathbf{c},\sigma)$的MLP对参考时刻(通常为t = 0)的静态3D场景的规范外观进行编码，将3D位置和视图方向映射到颜色和光密度对。对于其他时刻，它使用另一个MLP来学习一个变形函数，该函数将场景中的每个规范点映射到其新位置Ψt(x, t)。该函数输出位移向量∆x，对应于每个场景点的运动。这种方法比单独学习每个时间瞬间的场景外观的单个时间条件MLP更有效。<br>TiNeuVox[31]具有时间感知神经体素的快速动态辐射场，采用小变形网络和多距离插值来实现与D-NeRF相当或更好的质量，但训练时间要快得多，通常不到10分钟，而D-NeRF需要数十小时。<br>Nerfies[45]通过NeRF解决人体捕捉的挑战，即使在多次自拍拍摄期间试图保持静止，由于不可避免的人体运动(例如，呼吸，肌肉抽搐等)，这也是困难的。当从不同角度投射光线时，这种运动导致σ和c在空间上的不对准。与D-NeRF使用时间潜在向量进行变形网络训练不同，Nerfies使用基于时间和视图变换的潜在向量，因为可能有来自相同视图但不同时间戳的多个样本。<br>HyperNeRF[42]扩展了Nerfies来处理拓扑变化，由于拓扑变化引入了不连续，变形场无法很好地捕捉到拓扑变化。HyperNeRF将NeRF投影到一个更高维度的空间中，其中每个样本代表该空间的一个切片。它学习了一个高维函数在切片之间进行插值。受水平集方法的启发，HyperNeRF使用弯曲的切割面而不是直的超平面，从而产生由高维函数描述的更简单的形状。HyperNeRF在插值(4.1%)和新颖视图合成(8.6%)方面优于Nerfies。<br>NeRF-DS[43]解决了基于变形场的方法的局限性，如Nerfies和HyperNeRF用于重建动态镜面物体(例如，闪亮的茶壶或玻璃器皿)。重新制定了NeRF函数，使其依赖于曲面的位置和方向，并增加了一个运动物体遮罩来引导变形场。<br><strong>DeVRF</strong>[32]与其他动态NeRF模型(如D-NeRF、Nerfies、HyperNeRF和神经场景流场(NSFF))相比，DeVRF[32]方法在训练中实现了100倍的加速。<strong>它的目标是面向内的场景，并使用多摄像头设置场景捕捉</strong>。<br>LFNs: LFNs(光场网络)是另一种基于神经光场而不是辐射场来探索神经表征的研究。动态LFN模型DyLiN[51]向HyperNeRF学习，HyperNeRF是一位高质量但速度慢的teacher。在PSNR视觉质量指标方面，DyLiN优于当前最先进的动态模型HyperNeRF和TiNeuVox。此外，DyLiN比这两种方法都快得多，实现了一个数量级的加速。<br>最近解决动态LFN问题的其他研究包括带限辐射场(blrf)[52]、RefiNeRF[53]、时间插值(temporal interpolation is all you need)[35]、D-Tensorf[34]和DyNeRF[33]。</p><h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><p>Mip-NeRF: Mip-NeRF[27]提出了一种抗混叠技术，该技术对3D场景的每个像素使用锥体而不是射线进行采样，就像在原始NeRF中一样。这提高了不同相机距离和视图分辨率的渲染质量，在原始NeRF数据集上平均误差减少了17%。此外，与蛮力NeRF超采样相比，该模型实现了60%的误差减少和22倍的渲染加速。与最初的NeRF实现相比，该模型还将模型大小减小了一半。<br>ICARUS: ICARUS[54]是在FPGA上测试的一种节能的NeRF推理和渲染加速器硬件架构。当前设计的ICARUS实现了每帧45.75 s的NeRF渲染速度，功耗仅为282.8 mW。该设计尚未实现并行化，可以进一步提高帧生成时间。</p><h1 id="CURRENT-STATUS-OF-INDUSTRY-AND-ADVANTAGES-OFNERF-ALTERNATIVES"><a href="#CURRENT-STATUS-OF-INDUSTRY-AND-ADVANTAGES-OFNERF-ALTERNATIVES" class="headerlink" title="CURRENT STATUS OF INDUSTRY AND ADVANTAGES OFNERF ALTERNATIVES"></a>CURRENT STATUS OF INDUSTRY AND ADVANTAGES OFNERF ALTERNATIVES</h1><p>在本节中，我们将介绍3D表示和可视化应用于此类应用的商业方法的主要领域，以及应用于这些领域的特定基于nerf的最新方法。</p><h2 id="CAE-Computer-aided-engineering"><a href="#CAE-Computer-aided-engineering" class="headerlink" title="CAE: Computer-aided engineering"></a>CAE: Computer-aided engineering</h2><p>计算机辅助工程(CAE)是指使用计算机软件和工具来协助工程和设计过程。它包含各种技术和方法，利用计算机技术来分析和模拟产品和系统的行为。<br>计算机辅助设计(CAD)作为CAE的一个特殊分支，使用计算机软件来创建和修改物理对象的几何模型。使用的主要建模方法是自由曲面建模。自由曲面通常使用基于非均匀有理b样条(NURBS)的边界表示(B-Rep)[55]。生成的模型可以作为产品尺寸分析、自动创建技术图纸、物料清单生成和其他任务的输入。<br>CAD模型可以重新用于各种应用，特别是当采用特定技术来促进其重用时[56]。一个值得注意的应用是通过有限元分析(FEA)进行仿真，它允许对产品或制造设备内的特性和相互作用进行计算和交互式可视化。这些模拟可以包含诸如零件应力、温度、失效模式、流体动力学和电磁相互作用等因素，并且它们通常会发现CAD模型中的缺陷或缺点，从而需要对其进行修改。因此，CAD模型也可以基于FEA仿真中使用的修改网格模型进行更新[57]。<br>CAE的更广泛领域包括一系列类似的方法，包括有限体积法(FVM)和有限差分法(FDM)、计算机辅助制造(CAM)和计算流体动力学(CFD)。这些技术共同使工程师和设计师能够分析和优化他们的设计，考虑各种因素和相互作用，从而改进产品和系统。<br>除了产品设计之外，制造过程需要通过尽可能少的昂贵的部署和重新部署迭代来设计和优化。3D计算机可视化允许对原型制造设施进行快速和无错误的视觉评估。</p><ul><li>新兴的<strong>生成式nerf</strong>可以从零开始生成所需的3D模型，可以部分补充甚至实现CAD工具在未来的作用。生成设计的应用包括产品设计，为其他模型生成训练数据，或环境设计，允许另一个训练好的模型更好地泛化。为NeRF开发的生成方法具有不同程度的复杂性和对生成结果的可用控制。这反映了现有2D屏幕空间图像生成方法的不同控制选项的类似情况，例如生成对抗网络(gan)、扩散模型和多模态变压器模型。纯粹由自然语言引导的生成式nerf可以产生多样化和创造性的输出。<strong>然而，它们对输出属性的控制有限，这使得它们只适合装饰和娱乐导向的设计</strong><ul><li>与仅以自然语言为条件的方法相比，文本和形状引导的latent-NeRF方法[58]对生成的结果提供了更好的控制。除了文本描述外，该方法还为自动纹理生成提供形状指导或精确的3D形状。潜在的NeRF方法是基于扩散的，这与传统的在红、绿、蓝(RGB)色彩空间中训练NeRF模型不同。相反，<strong>NeRF模型是在潜在空间中训练的</strong>。这种选择消除了在训练期间将图像转换为潜在空间的需要，从而减少了计算开销。</li><li>DreamFusion[59]是另一种将文本转换为NeRF 3D场景的基于扩散的方法。由于没有足够大的3D模型训练数据集与其文本描述配对，因此作者使用现有的Imagen[60]基于扩散的生成模型，该模型仅使用2D图像空间作为先验。<strong>Imagen模型将NeRF渲染的阴影图像与随机噪声线性结合</strong>，生成去噪后的图像，指导NeRF在像素空间进行优化。利用该方法对NeRF进行了优化，实现了随机角度NeRF视图的低损耗。</li><li>Magic3D[61]通过解决两个主要限制来改进DreamFusion:缓慢的NeRF优化和降低3D模型质量的低分辨率图像引导。它采用了两阶段的方法。首先，它使用低分辨率扩散模型作为先验，使用Instant-NGP NeRF创建粗略的3D表示。在第二阶段，它从NeRF中提取多边形网格及其纹理，并使用可微分渲染器和高分辨率扩散先验进一步优化它们。</li></ul></li><li>最近的一些工作也提出了有前途的方法，可以使NeRF表示用于工程模拟，如[62]和[63]。</li></ul><h2 id="SCADA-systems"><a href="#SCADA-systems" class="headerlink" title="SCADA systems"></a>SCADA systems</h2><p>supervisory control and data acquisition</p><p>建模制造过程的图形工具不仅在生产开始前的计划阶段是必不可少的，而且对于持续的监督也是必不可少的。在许多工业环境中，人为操作员的连续监测和控制是确保最佳运行的必要条件。这种监测和控制通常通过监控和数据采集(SCADA)系统来实现，SCADA系统采用图形界面，实现系统监测和控制功能。用于SCADA接口的图形范围从常用的2D交互式原理图到三维图形。在某些情况下，为了准确地理解系统的状态，或者至少，它有助于提高操作员控制系统的能力，结合三维视觉变得势在必行。<br>尽管前面提到了3D SCADA图形界面的优点，但2D界面比3D界面更容易设计。这是因为2D界面不需要创建复杂的3D多边形模型、控制器和动画。<br>使用nerf，可以大大简化创建描述受控系统的3D场景。由于SCADA系统呈现交互式图形，可以直观地反映受控系统状态的变化，因此应该使用具有<strong>可修改场景的NeRF变体</strong>。例如，当操作员远程开关一件工业机械时，其颜色，照明或类似的指示应该改变。</p><ul><li>一些NeRF变体可以对3D场景进行控制修改，一个值得注意的例子是CodeNeRF[41]。CodeNeRF允许在3D场景中精确地改变物体的纹理和形状。</li><li>CLIP-NeRF[64]通过调节文本或图像输入来促进NeRF场景的修改。它利用来自对比语言图像预训练(CLIP)模型的预先存在的嵌入，该模型为语言和图像学习联合嵌入空间。</li><li>CLIP-NeRF的一个潜在应用是能够使用参考图像快速将真实世界的变化引入NeRF场景。例如，新产品原型的照片可以提供比单独的文本更详细的信息，以准确描述新的对象属性。通过使用CLIP-NeRF，这些参考图像可以作为修改NeRF场景的宝贵输入，以纳入所需的变化。</li><li>动态nerf提供了一种替代方法来表示各种受控机械状态。他们通过显示3D物体运动序列的不同片段或单个冻结时间实例来实现这一点。例如，考虑风车的3D表示:使用动态NeRF，当查询单个时间瞬间的动态NeRF时，叶片可以被描述为静态或运动，反映现实世界的风车状态。</li></ul><h2 id="Training-simulations-for-workforce"><a href="#Training-simulations-for-workforce" class="headerlink" title="Training simulations for workforce"></a>Training simulations for workforce</h2><p>先进的3D模拟器用于在工业环境中培训工人。这些模拟器的范围从简单的键盘和鼠标控制，普通的计算机显示器到虚拟现实(VR)、增强现实(AR)和物理控制器，这些控制器模仿了反映现实世界交互的目标应用程序的物理控制。<br>AR和VR培训在工业维护和装配任务中的有效性在[65]中进行了评估，并与现实世界的培训进行了比较。<strong>研究发现，应该鼓励针对此类任务的AR培训，但VR需要进一步评估</strong>。然而，许多任务和情况过于昂贵、危险或罕见，无法在现实世界中进行训练。例如，化工行业的操作员在虚拟环境中接受培训[66]。在这种情况下，VR培训非常有用，因为没有有效的替代方法。</p><ul><li>结合精确控制的传统渲染图形和NeRF不仅可以用于训练机器学习模型，如NeRF2Real[67]所示，还可以用于训练人类。</li><li>人的中心视觉比低敏锐度的周边视觉更敏锐。这是由于视网膜的中心区域在一个叫做中央凹的区域含有密集排列的光锥。对于人类在VR环境中实时使用的nerf，传统的渲染和亮度场表示方法会不必要地渲染图像的所有区域，包括那些用于人类视觉外围部分的区域，从而浪费资源。FoV-NeRF[68]解决了这一问题，<strong>提出了将图像的质量调整到最高的中心视觉和最低的周边视觉</strong>，从而呈现出质量不均匀的图像。这种方法降低了劳动力培训或沉浸式SCADA系统的VR成本和延迟。</li></ul><h2 id="Training-simulations-for-machine-learning"><a href="#Training-simulations-for-machine-learning" class="headerlink" title="Training simulations for machine learning"></a>Training simulations for machine learning</h2><p><strong>Transfer of skills</strong> acquired in simulation:</p><ul><li>NeRF2Real[67]方法展示了在模拟环境中使用NeRF进行渲染。第一步是创建真实机器人环境的NeRF表示，其中提取障碍物进行碰撞建模，并与合成物体(包括动态球)集成。此外，利用NeRF作为机器人的视觉输入，并与合成的动态球目标相结合。随后，一个20自由度的人形机器人在这个环境中通过强化学习学会了推球的任务。</li></ul><p>Capture of environment for training of other <strong>navigation algorithms</strong>: </p><ul><li>在导航算法的背景下，经过训练的智能体从nerf中获得视图渲染输出，而不是与真实的3D环境交互或依赖于传统的3D环境多边形渲染。这是通过向nerf提供有关其模拟动作所导致的代理位置和视角变化的信息来实现的[69]。</li><li>通过将环境表示划分为重叠的NeRF块(称为block-NeRF)，这种能力甚至可以扩展到大规模环境，例如整个城市[70]。</li><li>使用nerf，还可以修改场景属性，例如实现任意场景重照明。这极大地扩展了为给定的3D场景生成的逼真2D图像样本的种类和数量。例如，以文本描述为条件的模型可以对场景中的对象材料进行任意更改，从而实现具有成本效益的训练环境修改以增强多样性[71]。</li></ul><p>Extracting quantitative physical properties:</p><ul><li>基于NeRF的场景中物体的物理参数提取得到了有趣的结果，如[72]所示。作者证明，<strong>通过NeRF应用可以获得摩擦角、粘度和杨氏模量等物体的动态特性</strong>。这些信息可以收集起来供以后在模拟中使用，机器人可以纯粹作为数据收集代理或直接用于实时机器人预测能力和决策过程。</li></ul><h2 id="Positioning-and-navigation"><a href="#Positioning-and-navigation" class="headerlink" title="Positioning and navigation"></a>Positioning and navigation</h2><p>有几种方法可用于工业定位和导航[73]。这些包括超宽带(UWB)定位、全球定位系统(GPS)、差分GPS和simultaneous localization and mapping(SLAM)。然而，这些方法都有缺点，如由于信号飞行时间计算不精确和误差累积而导致精度有限。此外，在基于激光雷达的SLAM情况下，点云数据的设备成本和数据存储/传输要求可能很高。</p><p>基于神经辐射场的SLAM方法包括束调节神经辐射场(BARFs)[74]。BARF能够对NeRF学习到的表示和训练样本(图像)的相机姿态进行从粗到精的配准，并同时优化。这使得NeRF训练不完美或缺失的相机姿势。<br>SLAM是一种需要对环境进行3D映射的技术。因此，可以重建3D场景的方法，如NeRF，通常对SLAM有用。Sun等人的研究是SLAM神经三维表面重建方法的一个例子[75]。他们提出了一种单目时间相干3D重建技术，该技术使用时间窗口来捕捉场景的时间一致性，而不是重建和合并单个帧<br>NICE-SLAM[76]对几何图形使用单独的粗、中、细分层特征网格，对外观使用另一个特征网格。受NeRF启发的SLAM方法的一个例子是NeRF-SLAM[77]，它使用基于即时ngp的分层体NeRF地图<br>在Zhu等人[78]的另一项工作中，为城市规模的NeRF引入了一种名为LATITUDE(截断动态低通滤波器的全球定位)的定位方法。该方法采用了从粗到精的定位策略。首先，使用nerf生成的样本图像训练的回归量来获得初始估计点，以优化位置估计。为了降低陷入局部极小值的风险，引入了截断动态低通滤波器(TDLF)作为解决方案。</p><h2 id="3D-scanning-and-tomography"><a href="#3D-scanning-and-tomography" class="headerlink" title="3D scanning and tomography"></a>3D scanning and tomography</h2><p>3D扫描是一项多功能技术，在不同的行业有不同的应用。一些例子是:分析采矿或制造场所的地形，监测和管理资源，如生物质[79]，检查问题和缺陷，以及执行SLAM。3D扫描还可以实现高价值的定制和与现有对象的兼容性。例如，它可以促进传统或第三方组件的更快逆向工程，定制假肢，以及为高价值产品或设备(例如，货船)的定制修复进行精确的损坏几何测量。<strong>然而，存储和处理激光雷达数据仍然是一个挑战</strong>。典型的激光雷达扫描可以包含数百万个离散点，在大面积上扫描产生的数据集可以被认为是大数据[80]，[81]。对于大规模体积估计，从感兴趣区域的一小部分获得的统计数据通常被外推到整个区域[79]。</p><p>3D场景几何知识具有广泛的应用，例如从小型组件到大型建筑物的物体的3D扫描，并使机器人能够与其环境进行交互(例如，避免障碍物和抓地力规划)。估计几何形状的一种方法是直接查询NeRF的体积密度，这已被证明与物质的存在有很好的相关性[69]。或者，可以使用立方体推进或基于符号距离场的方法提取障碍物网格，如<a href="https://readpaper.com/paper/4734289236769914881">NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes</a> ，这可以提高提取几何形状的质量。</p><p>对于一些具有挑战性的情况，例如透明物体[83]，nerf在深度估计方面提供了显著的改进。这证明了与其他技术相比，神经网络在学习推断物体几何形状所需的微妙视觉线索方面的优势。此外，可以从模糊图像中获得3D场景，NeRF在训练过程中消除了模糊性[84]。这种方法可以为NeRF训练提供更快的数据采集，而不需要能够捕获运动中清晰图像的特殊设备，例如，允许通过在低空快速移动的无人机收集大面积图像。本小节中讨论的几何估计问题在地理空间行业中特别常见，并由NeRF研究解决(例如，[85])。</p><p>几种基于nerf的方法是专门为人体跟踪和姿态估计而设计的。与工业环境中的大多数其他物体不同，人类是高度移动的，他们的运动可能是不可预测的。这些人的流动性特点需要准确、快速的实时跟踪和理想的预测方法。[86]从单目视频中对人体3D姿势进行了细粒度跟踪，包括视频中不可见的几何部分。</p><p>机器人设计的一种具体方法是创建强调人与机器人之间协作的协作机器人(collaborative robots)[87]。这种协作机器人需要对人类运动进行更细致的跟踪和预测，而不是仅仅为了躲避或松散合作而跟踪人类。最近，一些工作集中在基于nerf的人体捕获和跟踪上，这有助于解决在协作机器人设置中实现精确人机交互的挑战。例如，[88]介绍了一种基于nerf的方法，通过从稀疏的多视图图像集重建给定的人体，来渲染新颖的人体姿势和新颖的3D场景视图。</p><p>断层扫描在工业环境中有各种应用，如探伤、失效分析、计量、装配分析和逆向工程。断层扫描也被用于新的任务，如无人机辅助扫描气体羽流的空间分布[89]。如文献[90]所示，将nerf应用于x射线扫描仪获得的sinograph数据可以获得最先进的3D重建结果，其中超过扫描仪分辨率的缩放称为超分辨率(分辨率提高8倍)，实现了从少量样本重建和从45度范围内的有限角度断层扫描重建。</p><h2 id="Other-areas"><a href="#Other-areas" class="headerlink" title="Other areas"></a>Other areas</h2><p>Dense object descriptors: 论文[91]提出了一种创建密集对象描述符的方法，该方法可用于点对应和关键点检测。这种方法对于薄物体和反射物体的6自由度拾取和放置操作特别有用，这在过去一直具有挑战性。所提出的方法明显优于最近的现成解决方案，如GLU-Net[92]、GOCor[93]和PDC-Net[94]。</p><p>Point-set registration : 在地理空间和医疗领域等不同行业中，经常需要对来自不同来源或时间的视觉数据进行对齐。这种对齐可以识别和比较不同图像中3D对象或场景上的相同点或区域。例如，多次断层扫描可用于评估随时间的生理变化。点配准是实现这种对齐的一种方法。Goli等[95]使用从nerf中提取的表面场来实现成对点配准。这些表面场不受摄像机视点变化的影响。然后，优化过程找到对齐表面场的刚性转换，确保相应视觉数据的对齐</p><p>Synthetic Perturbations for <strong>Augmenting Robot Trajectories</strong> via NeRF (SPARTN)[96]是一种机器人操作训练方法，它使用NeRF为手眼机器人操作任务的专家演示生成摄动和相应的纠正动作。这将模仿学习性能提高了2.8倍。</p><p>Synthetic dataset generation for supervised learning :Neural-Sim中提出了一种使用合成nerf生成的图像来训练视觉模型的方法[97]。合成图像的变化包括姿势、变焦和照明。实现了对用于训练的数据的精确控制，并且有目的地生成了最容易导致视觉模型损失的图像。因此，针对特定领域的图像调整的高质量视觉模型可以在不收集许多潜在冗余且变化有限的真实世界样本的情况下获得</p><p>Learned semantic relationships: 使用JacobiNeRF[98]中提出的方法的NeRF可以学习场景实体(如对象甚至单个像素)之间的底层抽象语义关系。当其权重沿场景中特定点的梯度扰动时，这样的NeRF在其输出中表现出语义关联共振。例如，如果墙壁上的单个点的亮度发生变化，则给定墙壁上其他点的亮度也会发生变化，而场景中的其他点保持不变。一个JacobiNeRF应用程序正在减少注释负担，因为注释可以传播到语义链接点。这种方法通常用于实体选择和场景修改。</p><h1 id="REQUIREMENTS-FOR-PROOF-OF-CONCEPTEXPERIMENTS"><a href="#REQUIREMENTS-FOR-PROOF-OF-CONCEPTEXPERIMENTS" class="headerlink" title="REQUIREMENTS FOR PROOF-OF-CONCEPTEXPERIMENTS"></a>REQUIREMENTS FOR PROOF-OF-CONCEPTEXPERIMENTS</h1><p>我们进行了定量实验，以测试我们的一些新颖的NeRF应用在我们感兴趣的领域的可行性，并研究实现所需的方法。<br><strong>第一个实验</strong>检查了不同设置(如分辨率和压缩质量预设)下的<strong>数据节省和压缩质量之间的权衡</strong>。我们在这个实验中使用了<strong>Instant-NGP</strong>[99]，一种快速的NeRF变体，因为它可以通过不同的实验配置进行快速迭代。<br><strong>第二个实验</strong>探讨了使用<strong>动态nerf进行预测导航</strong>。我们选择了<strong>D-NeRF</strong>[44]，这是一种动态NeRF变体，可以重建高质量的动态场景。它的显著优点是一个现有的开源实现，方便了动态场景实验的快速和可重复设置。</p><h2 id="A-Mathematical-representation"><a href="#A-Mathematical-representation" class="headerlink" title="A. Mathematical representation"></a>A. Mathematical representation</h2><p>我们提出了我们在实验中使用的两种算法——Instant-NGP and dynamic NeRF,的数学模型。我们还解释了这些NeRF变体中使用的轴对齐边界框(AABB)长方体，通过限制渲染体积来加快渲染过程。</p><p>1) 主要定义:在数学中，域是定义一些基本代数运算行为的集合。神经域是一个可以通过神经网络参数化的域。</p><p>NeRF用于从一组2D图像中建立3D场景重建模型。重建是一个神经场，表示为 $\Phi:X\rightarrow Y$，将每个$\mathbf{x}_{\mathrm{recon}}\in X$映射到对应的场量$\mathbf{y}_{\mathrm{recon}}\in Y.$。相机传感器图像观测也可以表示为一个field$\Omega:S\rightarrow T$，该field将传感器坐标S, $\mathbf{x}_{\mathrm{sens}}\in S$映射到测量值T, $\mathbf{t}_{\mathrm{sens}}\in T.$。正向映射是两个函数$F:(X\rightarrow Y)\rightarrow(S\rightarrow T).$之间的可微映射。</p><p>因此，如果forward map是可微的，我们可以求解以下优化问题[100]来寻找神经场Φ: $\operatorname{argmin}_W\int_{X\times S}||F(\Phi(\mathbf{x}_{\mathrm{recon}}))-\Omega(\mathbf{x}_{\mathrm{sens}})||.$<br>W为Φ的参数。神经域的一般定义也适用于各种非nerf表示，例如神经符号距离域</p><p>缩小到NeRF，从数学的角度来看，NeRF特定的$x_{recon}$是一个5元组(x, y, z， θ， φ)一个点的三维直角坐标x, y, z坐标沿着射线和视角θ， φ在球坐标系中采样，这决定了从相机发出的射线的方向。NeRF-specific $y_{recon}$是一个4元组(r, g, b， σ)，其中r, g和b是沿特定方向的射线采样的特定点的RGB亮度颜色分量，σ是该给定点的光密度。因此，NeRF是一个将5维xrecon向量转换为另一个4维xrecon向量的函数。这个映射可以表示为:    $F(W):(x,y,z,\theta,\phi)\implies(r,g,b,\sigma),$</p><p>在体绘制过程中，光线通过每个像素发射，以计算结果像素的颜色。只有落在AABB边界体内的射线段才用于渲染计算。在近、远射线界$t_n$和$t_f$内，由射线原点和方向d在射线上的点t处定义的相机射线r(t) = 0 + td的期望颜色C(r)定义为:$C(\mathbf{r})=\int_{t_n}^{t_f}T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t),\mathbf{d})dt,$</p><p>其中，c为射线线段上某一点的光线角度d所决定的发射颜色，参数化为参数t。函数T (t)为透射率，它决定了单个c值对总c (r)的贡献。</p><p>2) Instant Neural Graphics Primitives:</p><p>使用NeRF渲染完整图像的一个主要问题是计算时间。自从最初的NeRF文件采用NeRF方法以来，已经进行了不同的尝试来加快这一进程。即时神经图形原语(Instant neural graphics primitives, Instant NGPs)[101]试图减少层数，并以指数方式减少MLP计算中的计算次数。这背后的思想是考虑两个参数:参数的数量T和我们想要获得$N_{max}$的目标分辨率。所谓的多分辨率哈希编码不仅尝试像通常的MLP一样训练权重，还尝试训练参数。每一层都是独立的，网格顶点处的特征向量，其分辨率被选择为最差和最清晰分辨率之间的几何级数[Nmin, Nmax]<br>$N_{l}=N_{\mathrm{min}}bl,$<br>$b=\exp\left(\frac{\ln N_{\max}-\ln N_{\min}}{L-1}\right),$b取决于层数和attempted分辨率。</p><p>3) D-NeRF</p><p>动态NeRF (D-NeRF)是一种能够3D动态场景捕获的NeRF变体。由于额外的时间维度，动态NeRF变量的映射必须从Eq. 2修改为以下形式:</p><p>$F(W):(x,y,z,\theta,\phi,t)\implies(r,g,b,\sigma).$</p><p>虽然学习从6D到4D的直接映射的模型可以用于该任务，但[44]的结果表明，将映射分解为两个独立的函数可以减少计算时间。具体来说，一个MLP $Φ_x$作为动态场景的单个时间瞬间的参考静态场景表示，通常为t = 0。然后，另一个MLP $Φ_t$学习受时间限制的规范场景的动态场景变形。因此，两个mlp都在感兴趣的时间间隔内学习场景的外观。</p><p>4) AABB representation</p><p>一般来说，AABB是一个与坐标轴对齐的二维矩形边界区或三维长方体边界体。它用于简化模拟任务的计算，如碰撞检测或射线相交。在nerf中，使用长方体AABB来切断AABB边界处的射线传播。<br>AABB可以用两个参数来定义:一个中心点和一个半延伸向量。中心点具有三维笛卡尔坐标$(X_0,Y_0,Z_0)$并指定长方体的位置。半延伸向量具有分量(X,Y,Z)，并指定长方体在每个方向上从中心点延伸的距离。因此，长方体的宽度为2X单位，高度为2Y单位，深度为2Z单位。综上所述，范围向量及其中心唯一地定义了AABB，可以用于渲染加速。</p><h2 id="B-Evaluation-metrics"><a href="#B-Evaluation-metrics" class="headerlink" title="B. Evaluation metrics"></a>B. Evaluation metrics</h2><p>我们考虑完善的计算机视觉领域评估指标:PSNR和SSIM。</p><p>PSNR是最大信号功率与噪声功率之比，单位为dB。计算PSNR既需要噪声破坏前的原始图像，也需要噪声破坏后的图像。利用噪声图像的均方误差(MSE)来计算噪声功率分量。对于尺寸为m x n像素的灰度图像，MSE的计算方法如下[102]</p><p>$MSE=\frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i,j)-K(i,j)]^{2},$</p><p>其中I和K分别是描述原始图像和噪声图像的函数，通过将以i and j为索引的像素映射到信号值。对于灰度图像，信号值决定了整体像素亮度，对于彩色图像，信号值决定了RGB颜色通道亮度值。对于RGB颜色空间中的彩色图像，MSE只是在所有三个颜色通道上计算。</p><p>PSNR的计算公式如下:$PSNR=10\cdot\log_{10}\left(\frac{MAX_{I}^{2}}{MSE}\right),$<br>$MAX_{I}$表示原始无噪声图像中的峰值信号值。<br>PSNR通常以dB表示，可计算为: $PSNR(dB)=20log_{10}(MAX_I)-10log_{10}(MSE).$</p><p>SSIM是基于图像的亮度、对比度和结构来衡量两幅图像之间的结构相似性。与PSNR相反，该指标考虑了人类视觉感知i的具体情况，并提供了更接近普通人的质量评估函数。它是通过比较图像中像素的局部斑块(矩形窗口)来计算的。其计算公式如下式[102]<br>$\mathrm{SSIM}(x,y)=\frac{\left(2\mu_x\mu_y+c_1\right)\left(2\sigma_{xy}+c_2\right)}{\left(\mu_x^2+\mu_y^2+c_1\right)\left(\sigma_x^2+\sigma_y^2+c_2\right)},$</p><p>式中，x、y为两个大小为N × N的图像块，μx、μy为x、y的平均像素值，σ2x、σ2y为x、y的方差，σxy为x and y的协方差，c1、c2为分母趋近于零时避免不稳定的两个常数。SSIM的取值范围是-1 ~ 1,<strong>1表示完全相似，-1表示完全不相似</strong>。</p><h1 id="PROOF-OF-CONCEPT-I-UAV-VIDEO-COMPRESSION-USING-INSTANT-NGP"><a href="#PROOF-OF-CONCEPT-I-UAV-VIDEO-COMPRESSION-USING-INSTANT-NGP" class="headerlink" title="PROOF-OF-CONCEPT I.: UAV VIDEO COMPRESSION USING INSTANT-NGP"></a>PROOF-OF-CONCEPT I.: UAV VIDEO COMPRESSION USING INSTANT-NGP</h1><p>我们提出的第一个新颖的NeRF应用是基于NeRF的视频压缩技术。这种方法只对nerf生成的具有给定相机姿势的场景的预期2D表示与实时捕获的实际帧之间的差异进行编码。这种增强的视频压缩降低了传输所需的比特率，因为只有关于摄像机姿态的数据和真实帧与生成帧之间转换的信息会通过网络发送。我们在虚拟工业3D场景中进行了实验，并测量了在这种情况下使用我们的压缩方法传输视频流的无人机所能实现的压缩节省。</p><h2 id="A-Proposed-NeRF-based-compression"><a href="#A-Proposed-NeRF-based-compression" class="headerlink" title="A. Proposed NeRF-based compression"></a>A. Proposed NeRF-based compression</h2><p>虽然广泛使用的视频压缩算法使用光流来编码帧间的差异，但由于一些帧的变化不能用前一帧的视觉流变换来描述，因此限制增加了需要添加到每一帧的信息量[103]。另一方面，nerf可以提取通过广泛的3D转换获得的视图信息。<br>由于NeRF对完整3D场景的了解，与传统的视频压缩相比，它具有之前所有编码帧中未见的像素知识。例如，当视图挫折移动到视频中显示这些时，视觉流无法访问视图边缘以外的未见像素的信息。同样，当物体在旋转过程中被遮挡的部分变得可见时，它也没有关于这些部分的信息。<br>重要的是，经过训练的NeRF的权重比为学习的3D场景生成的单个2D视图图像占用的数据存储空间更少[16]。<br>我们所有的实验都使用了H.264压缩算法。在H.264中，压缩帧流包括所谓的I帧和P帧。I帧包含完整的图像信息，P帧仅编码I帧中编码的图像与其时间相邻图像之间的差异数据[104]。在压缩视频中，每n帧是一个I帧，在压缩开始前n通常被确定为一个常数。<br>在给定场景上对NeRF进行预训练后，压缩视频中通常存储在第一帧中的整帧信息可以被省略，NeRF根据需要仅从摄像机姿态信息中生成，所需数据少于1kb。对于网络上的视频流，发送方和接收方都使用NeRF在感兴趣的场景上训练的副本，因此两者都可以使用NeRF生成的图像而不是发送I帧。<br>适合无人机用例场景的网络架构包括在MEC网络中分配计算负载，以便将nerf相关的计算从无人机卸载到边缘服务器。我们的压缩方法和MEC网络架构的设计如图2所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821160422.png" alt="image.png"><br><em>(a)无人机摄像机捕捉环境。相机的真实帧和姿态被无线传输到附近的多址边缘计算(MEC)服务器。(b) MEC服务器采用NeRF模型进行基于相机姿态的新视图合成。H.264编解码器对real帧和NeRF帧进行编码，得到包含它们差异的P帧，并随姿态通过网络传输。(c)接收器使用H.264编解码器从P帧和从相机姿态生成的本地NeRF帧重建真实帧。</em></p><h2 id="B-Experimental-design-and-results"><a href="#B-Experimental-design-and-results" class="headerlink" title="B. Experimental design and results"></a>B. Experimental design and results</h2><p>首先，我们在表2中列出了所有重要的仿真参数。作为NeRF模型，使用的是Instant-NGP[99]，没有任何超参数改变或架构修改。我们准备了一个实验来证明新型基于NeRF的压缩算法在图2所示的工业3D场景中的性能，并通过在Blender 3D建模软件中<strong>渲染工业3D场景来生成NeRF的训练数据集</strong>。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821161057.png" alt="image.png"></p><p>实验示意图如图3所示，其中一架无人机带着相机在虚拟环境中飞行，并对沿其轨迹捕获的帧进行索引方案。该图还说明了NeRF在帧重建过程中引入的一些质量退化伪影。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821161801.png" alt="image.png"><br><em>一架无人机摄像机在虚线无人机轨迹上的点捕捉图像。图像的索引从0到158，这种索引也在图5-7中使用。索引为10的帧SSIM最差，如图5所示。从这个框架裁剪的区域，通过NeRF重建，被显示为突出SSIM退化的原因。</em></p><p>图4显示了代表PSNR指标的四条测量曲线，它评估了NeRF重建3D场景视图的质量与原始视图质量的比较。这些测量是从模拟无人机轨迹期间捕获的单个帧中获得的，每个帧分配一个index范围从0到158。PSNR曲线对应于表2中列出的不同分辨率。值得注意的是，不同指标的PSNR值不同，表明重建质量不同。有趣的是，当尝试更高的分辨率时，可实现的PSNR会降低。然而，值得一提的是，<strong>在500和720分辨率下，PSNR值是相似的，在这个分辨率范围内，PSNR和分辨率之间表现出更好的权衡</strong>。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821161912.png" alt="image.png"><br><em>用PSNR (dB)测量了无人机沿三维场景轨迹的NeRF帧重建质量。</em></p><p>图5显示了对应的一组分辨率目标的SSIM。SSIM测量值不同;然而，整体行为保持一致，所有分辨率都遵循与PSNR相似的趋势。此外，<strong>在500和720分辨率下</strong>实现的SSIM也有相似之处，这进一步支持了这样一个概念，<strong>即在SSIM中无需做出重大妥协就可以实现更高的分辨率</strong>。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821162121.png" alt="image.png"><br><em>利用SSIM测量了无人机三维场景沿轨迹的NeRF帧重构质量</em></p><p>图6显示了不同分辨率下压缩节省的百分比。我们的方法的压缩节省是相对于H.264压缩得到的一组帧对I到P帧来计算的。我们使用公式100 <em> Isize/(Isize + Psize)来计算压缩节省，其中Isize和Psize分别是I帧和P帧的大小。由于我们不通过网络传输I帧，而是使用NeRF生成它，因此<em>*Isize占Isize + Psize总数的百分比表示节省</em></em></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821162214.png" alt="image.png"><br><em>相对于H.264的压缩节省实现了单个图像沿无人机轨迹通过3D场景。</em></p><p>我们绘制了通过所提出的压缩方法沿无人机轨迹对不同帧分辨率从300 ×168到1920 × 1080(全高清)的159个连续帧所获得的压缩增益。主要结果是压缩节省与帧分辨率成反比。对于1920x1080分辨率，压缩节省百分比的值从45- 48%到300x168分辨率时的66- 74%不等。<strong>低分辨率节省的传输数据更多</strong></p><p><strong>分辨率和压缩节省百分比的反比关系到nerf生成图像的缺陷</strong>。已经观察到，原始和nerf生成的图像的高频信号成分在降尺度期间丢失。这种现象可以看作是有损压缩的一种形式，通过去除NeRF引入的高频伪影，使NeRF生成的图像更接近地面真实视图。SSIM的行为支持这些观察结果。它以一种似乎与压缩节省相关的方式随着分辨率的降低而提高。对比，分别见图5和图6对应的SSIM和压缩特性。<br>我们使用的分辨率范围低端的相对较低的值适用于多个重要场景。如果视频用于视觉机器学习下游模型，例如cnn，由于内存限制，目前最先进的预训练模型通常只能以相对较低的分辨率工作，最高可达800 × 600。</p><h2 id="C-Computational-and-energy-tradeoffs-of-NeRF-based-video-compression"><a href="#C-Computational-and-energy-tradeoffs-of-NeRF-based-video-compression" class="headerlink" title="C. Computational and energy tradeoffs of NeRF-based video compression"></a>C. Computational and energy tradeoffs of NeRF-based video compression</h2><p>我们的实验表明，与传统的压缩技术相比，<strong>使用NeRF可以实现显著的压缩节省，但代价是更高的计算和能源需求</strong>。这可以通过设计合适的网络体系结构(如MEC)来减少，该体系结构将计算资源放置在通信端点附近<br>虽然这样的配置要求无人机在没有我们的压缩方法的情况下向MEC服务器发送单个无人机视频流，但从服务器发出的流量仍然明显较低。这offloads了工厂网络节点，这些节点聚合了来自多个链接的流量，通常会经历最高的网络负载，例如，许多这样的无人机摄像头流。在网络边缘本地提供高吞吐量和低延迟通常成本更低，也更容易，在那里不存在这种流量聚合的问题。<br>根据发送方和接收方的计算能力，有多种可能的计算卸载配置。NeRF计算可以卸载到靠近压缩视频发送方、接收方或两者的边缘服务器。如果发送方或接收方不受电池或计算资源的限制，则不需要卸载。然而，由于在我们的模拟场景中，发送方是一个电池受限的无人机，<strong>因此将基于nerf的压缩工作负载卸载到边缘服务器MEC并发送高吞吐量流是有益的</strong>。</p><h1 id="PROOF-OF-CONCEPT-II-OBSTACLE-AVOIDANCE-USING-D-NERF"><a href="#PROOF-OF-CONCEPT-II-OBSTACLE-AVOIDANCE-USING-D-NERF" class="headerlink" title="PROOF-OF-CONCEPT II. : OBSTACLE AVOIDANCE USING D-NERF"></a>PROOF-OF-CONCEPT II. : OBSTACLE AVOIDANCE USING D-NERF</h1><p>关于动态NeRF概念验证实验，我们在重复机械手臂运动的3D动画图像上训练<strong>D-NeRF</strong>。由于这些动作的重复性，D-NeRF可以根据过去的手臂位置来预测未来的手臂运动。</p><h2 id="A-Proposed-D-NeRF-based-obstacle-avoidance"><a href="#A-Proposed-D-NeRF-based-obstacle-avoidance" class="headerlink" title="A. Proposed D-NeRF-based obstacle avoidance"></a>A. Proposed D-NeRF-based obstacle avoidance</h2><p>对于D-NeRF中动态场景的单个时间瞬间，有多种场景几何提取方法，可用于避障。由于诸如光线行进之类的方法相对较慢，我们对一组光线使用视差图。视差图是对一对立体图像之间的视距离或像素运动的度量，可用于估计场景的深度。<strong>视差图因此帮助我们估计与给定像素相交的光线与最近障碍物表面相交的距离</strong>。由于使用这些信息来检测或预测与无人机边界体的碰撞是微不足道的，因此我们从实验中省略了碰撞检测，并直接关注生成的视差图的效用和质量。<br>运动预测需要与实时手臂3D姿势同步，并将其与学习到的动态场景时间轴上的过去姿势相匹配。几种超出我们工作范围的方法可用于姿态确定，例如[105]。我们的目标是展示机器人手臂和移动机器人(在我们的实验中，无人机)之间避免碰撞的可能性。这个用例显然可以扩展到其他具有重复移动模式的工业过程。我们基于d - nerf的避障设计如图7所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821170702.png" alt="image.png"><br><em>a)具有特定相机姿态的无人机在t时刻捕获目标b) D-NeRF模型以RGB图、不透明度图和视差图的形式输出指定时间t相机姿态的新视图，如图8所示。c)不透明度图和视差图是无人机进行避障或路径规划的关键组件。</em></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821170743.png" alt="image.png"><br><em>从代表机器人手臂的D-NeRF模型中提取的所有地图，在t1 = 0.05和t3 = 0.95两个时间瞬间进行渲染。a) RGB地图描述了新的视图合成。b)不透明度图显示光线通过环境的传输，环境可以是透明的、部分不透明的或完全不透明的。c)处理后的视差图以热图的形式捕捉这些时刻重复运动时的机械臂表面深度。</em></p><h2 id="B-Experimental-design-and-results-1"><a href="#B-Experimental-design-and-results-1" class="headerlink" title="B. Experimental design and results"></a>B. Experimental design and results</h2><p>首先，我们在III中列出了所有重要的仿真参数。在D-NeRF训练中，利用环绕机械臂的圆形轨迹的虚拟摄像机收集绘制的机械臂动画视图，模拟无人机的飞行轨迹。为了获得可用于碰撞检测的描述机械臂表面的深度数据，考虑了D-NeRF生成的视差图。由训练D-NeRF生成的原始视差图包含一定数量的视觉噪声，可见为不规则点。它们离机械臂的轮廓既远又近。有几种可能的方法来去除这种噪声[106]，[107]，[108]，对于这个特殊的用例，我们采用了[109]中提出的图像处理技术。我们演示了训练好的D-NeRF渲染，噪声去除蒙版，并在t1 = 0.05和t3 = 0.95两个时间瞬间获得深度数据。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821171943.png" alt="image.png"></p><p>图8a显示了这些时间瞬间的D-NeRF渲染。为了得到最终的清理后的视差图，如图8c所示，我们首先生成了一个不透明度图，它比视差图提供了更高水平和质量的手臂轮廓细节，但也包含了较小的噪声。然后，我们对不透明度图应用简单的侵蚀和扩张OpenCV操作序列，以创建如图8b所示的干净的不透明度蒙版，该蒙版可用作裁剪蒙版来裁剪原始的噪声视差图。不透明度地图侵蚀去除了大部分不属于机械臂轮廓的噪声，<strong>因为机械臂是场景中最大的连续物体</strong>。在膨胀步骤中读取了机械臂轮廓上的大部分像素。我们把去噪后的不透明度贴图裁剪出来的视差贴图称为处理后的视差贴图。</p><p>D-NeRF生成的视差图的质量以PSNR衡量，相对于通过动态场景的连续帧的训练、测试和验证轨迹的地面真实视差图，如图9所示。为了可视化多模态数据分布，采用核密度估计，训练、测试和验证数据集分别用灰色、蓝色和黄色表示。可视化中较宽的部分表示存在某些值的可能性较高。蓝点表示中位数，水平线表示平均平均值，箱形图描述了四分位数范围，可以深入了解分布的中间部分的分布情况。我们可以看到，<strong>包含在D-NeRF训练集中的帧对两个测量的质量指标偏离基本事实的程度较小</strong>。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821171222.png" alt="image.png"><br><em>使用PSNR度量比较Blender和D-NeRF图像之间的视差图。</em></p><p>图10所示的结果表明，与PSNR相比，SSIM测量的深度重建平均质量明显更高。对于上下文中，重要的是要注意哪些值被认为对SSIM和PSNR有利。SSIM的取值范围是0.97到1.0，而PSNR的取值范围是30 dB到50 dB。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230821171347.png" alt="image.png"><br><em>使用SSIM度量比较Blender和D-NeRF图像之间的视差图。</em></p><p>在避免碰撞的背景下，SSIM度量对大型结构重建质量的关注变得至关重要。这是由于碰撞边界体积，它只能粗略地近似障碍物的几何形状，因此提供了一个可容忍的误差幅度，以应对较小的深度估计偏差。PSNR指标对这种小尺度偏差更为敏感;然而，避免碰撞很少需要完美的精度。此外，未来的研究可以探索PSNR和SSIM之间的差异如何影响其他特定的下游任务，从而为进一步的研究提供有价值的见解。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Review </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Review </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3D Gaussian Splatting</title>
      <link href="/3DReconstruction/Multi-view/Explicit%20Volumetric/3D%20Gaussian%20Splatting/"/>
      <url>/3DReconstruction/Multi-view/Explicit%20Volumetric/3D%20Gaussian%20Splatting/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>3D Gaussian Splatting  for Real-Time Radiance Field Rendering</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://scholar.google.at/citations?user=jeasMB0AAAAJ&amp;hl=en">Bernhard Kerbl</a><em> 1,2      <a href="https://grgkopanas.github.io/">Georgios Kopanas</a></em> 1,2      <a href="https://people.mpi-inf.mpg.de/~tleimkue/">Thomas Leimkühler</a>3      <a href="http://www-sop.inria.fr/members/George.Drettakis/">George Drettakis</a>1,2</td></tr><tr><td>Conf/Jour</td><td>SIGGRAPH</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">3D Gaussian Splatting for Real-Time Radiance Field Rendering (inria.fr)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4786887855003664385&amp;noteId=1920399556132916992">3D Gaussian Splatting for Real-Time Radiance Field Rendering (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230817194605.png" alt="image.png"></p><ul><li>将点描述成高斯体，对点云进行<strong>优化</strong>（点云模型）<ul><li>高质量、非结构化的离散表示——高斯体：均值控制位置，协方差控制高斯体形状（缩放+旋转）</li><li>针对3D高斯特性的优化方法，并同时进行自适应密度控制</li></ul></li><li>Splatting的渲染方式，区别于体渲染<ul><li>实现了使用GPU进行快速可微的渲染，允许各向异性的抛雪球(Splatting)和快速反向传播</li></ul></li></ul><span id="more"></span><h1 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h1><p>一维高斯分布$x\sim\mathcal{N}(\mu,\sigma^2)$</p><ul><li>概率密度函数$p(x)=\frac1{\sigma\sqrt{2\pi}}exp(-\frac{(x-\mu)^2}{2\sigma^2})$</li></ul><p>三维标准高斯：$\mathbf{v}=[a,b,c]^T$ </p><script type="math/tex; mode=display">\begin{aligned}p(\mathbf{v})& =p(a)p(b)p(c)  \\&=\frac1{(2\pi)^{3/2}}exp(-\frac{a^2+b^2+c^2}2) \\&=\frac1{(2\pi)^{3/2}}exp(-\frac12\mathbf{v}^T\mathbf{v})\end{aligned}</script><p>其中p(a),p(b),p(c)~N(0,1)</p><p>然后<strong>推广到一般三维高斯</strong>：$\mathbf{v}=\mathbf{A}(\mathbf{x}-\mu)$<br>$p(\mathbf{v})=\frac{1}{(2\pi)^{3/2}}exp(-\frac{1}{2}(\mathbf{x}-\mu)^T\mathbf{A}^T\mathbf{A}(\mathbf{x}-\mu))$ , 其中$\mathbf{x}=[x,y,z]^T$ ， $\mu=[E(x),E(y),E(z)]^T$ </p><ul><li>对两边求积分：$\int p(\mathbf{v}) d\mathbf{v} = 1=\iiint_{-\infty}^{+\infty}\frac{1}{(2\pi)^{3/2}}exp(-\frac{1}{2}(\mathbf{x}-\mu)^T\mathbf{A}^T\mathbf{A}(\mathbf{x}-\mu))d\mathbf{v}$ ,其中$d\mathbf{v}=d(\mathbf{A}(\mathbf{x}-\mu))=|\mathbf{A}|d\mathbf{x}$</li><li>得到$1 =\iiint_{-\infty}^{+\infty}\frac{|\mathbf{A}|}{(2\pi)^{3/2}}exp(-\frac{1}{2}(\mathbf{x}-\mu)^T\mathbf{A}^T\mathbf{A}(\mathbf{x}-\mu))d\mathbf{x}$</li><li>因此一般三维高斯概率密度函数$p(\mathbf{x})=\dfrac{|\mathbf{A}|}{(2\pi)^{3/2}}exp(-\dfrac{1}{2}(\mathbf{x}-\mu)^T\mathbf{A}^T\mathbf{A}(\mathbf{x}-\mu))$</li></ul><p>概密函数-协方差矩阵形式：$p(\mathbf{x})=\frac1{|\Sigma|^{1/2}(2\pi)^{3/2}}exp(-\frac12(\mathbf{x}-\mu)^T\Sigma^{-1}(\mathbf{x}-\mu))$<br>协方差矩阵(对称矩阵)-特征值分解：$\Sigma=U\Lambda U^T$  ，其中$U^{T}U=I$</p><ul><li>本文简化为$G(x)=e^{-\frac{1}{2}(x)^{T}\Sigma^{-1}(x)}$</li><li>协方差矩阵可以看做一个缩放$\Lambda = S S^T$(特征值)和一个旋转$U = R$(特征向量)<ul><li>$\Sigma=RSS^{T}R^{T}$,  $\Sigma=\mathbf{A}\mathbf{A}^{T}= (U\Lambda^{1/2})(U\Lambda^{1/2})^T$</li></ul></li></ul><blockquote><p><a href="https://www.zhihu.com/tardis/zm/art/37609917?source_id=1003">如何直观地理解「协方差矩阵」？ (zhihu.com)</a></p></blockquote><h1 id="L-amp-D-amp-C"><a href="#L-amp-D-amp-C" class="headerlink" title="L&amp;D&amp;C"></a>L&amp;D&amp;C</h1><p>我们的方法并非没有局限性。In regions where the scene is not well observed we have artifacts;在这些地区，其他方法也很困难(例如，图11中的Mip-NeRF360)。尽管各向异性高斯函数具有如上所述的许多优点，但我们的方法可以创建拉长的伪影或“斑点”高斯函数(见图12);同样，以前的方法在这些情况下也很困难。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230818141809.png" alt="image.png"></p><p>当我们的优化产生较大的高斯分布时，偶尔也会出现弹出现象popping artifacts;这往往发生在视觉依赖的区域。产生这些弹出伪影的一个原因是光栅化器中的保护带对高斯信号的微不足道的抑制。一种更有原则的筛选方法将减轻这些人为因素。另一个因素是我们简单的可见性算法，这可能导致高斯函数突然切换深度/混合顺序。<strong>这可以通过反锯齿来解决，我们把它留给未来的工作</strong>。此外，我们目前没有将任何正则化应用于我们的优化;这样做将有助于处理看不见的区域和弹出的工件。</p><p>虽然我们在完整的评估中使用了相同的超参数，但早期的实验表明，降低位置学习率对于在非常大的场景(例如城市数据集)中收敛是必要的。<br>尽管与以前的基于点的解决方案相比，我们非常紧凑，但我们的<strong>内存消耗</strong>明显高于基于nerf的解决方案。在大场景的训练过程中，在我们未优化的原型中，GPU内存消耗峰值可以超过20gb。然而，这个数字可以通过仔细的底层优化逻辑实现(类似于InstantNGP)而显著降低。<strong>渲染经过训练的场景需要足够的GPU内存来存储完整的模型</strong>(大型场景需要几百兆字节)，<strong>光栅化需要额外的30-500兆内存</strong>，具体取决于场景大小和图像分辨率。我们注意到有很多机会可以进一步减少我们的方法的内存消耗。点云的压缩技术是一个研究得很好的领域[De Queiroz and Chou 2016];看看这些方法如何适应我们的表现将是很有趣的。</p><h2 id="DISCUSSION-AND-CONCLUSIONS"><a href="#DISCUSSION-AND-CONCLUSIONS" class="headerlink" title="DISCUSSION AND CONCLUSIONS"></a>DISCUSSION AND CONCLUSIONS</h2><p>我们已经提出了第一种方法，真正允许实时，高质量的辐射场渲染，在各种场景和捕获风格，同时需要训练时间与最快的以前的方法竞争。<br>我们选择的3D高斯原语保留了优化体渲染的属性，同时直接允许快速基于飞溅的栅格化。我们的工作表明，与广泛接受的观点相反，<strong>连续表示并不是严格必要的，以允许快速和高质量的辐射场训练</strong>。<br>我们的大部分(~ 80%)训练时间都花在Python代码上，因为我们在PyTorch中构建了我们的解决方案，以允许其他人轻松使用我们的方法。只有栅格化例程是作为优化的CUDA内核实现的。我们期望将剩余的优化完全移植到CUDA，例如，在InstantNGP [Müller et al . 2022]，可以为性能至关重要的应用程序提供进一步的显着加速。<br>我们还展示了构建实时渲染原则的重要性，利用GPU的功能和软件光栅化管道架构的速度。这些设计选择是训练和实时渲染性能的关键，提供了比以前的体积射线marching性能的竞争优势。<br>看看我们的高斯函数是否可以用来对捕获的场景进行网格mesh重建，这将是很有趣的。除了广泛使用网格的实际意义之外，这将使我们能够更好地理解我们的方法在体积和表面表示之间的连续体中的位置。<br>总之，我们已经提出了第一个实时渲染解决方案的亮度场，与渲染质量匹配最昂贵的以前的方法，训练时间与最快的现有解决方案竞争。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>辐射场方法最近彻底改变了用多张照片或视频捕获的场景的新颖视图合成。然而，实现高视觉质量仍然需要训练和渲染成本高昂的神经网络，而最近更快的方法不可避免地要牺牲速度来换取质量。<br>对于无界和完整的场景(而不是孤立的对象)和1080p分辨率的渲染，<strong>目前没有任何方法可以实现实时显示速率</strong>。<br>我们介绍了三个关键要素，使我们能够在保持有竞争力的训练时间的同时实现最先进的视觉质量，重要的是能够<strong>在1080p分辨率下</strong>实现<strong>高质量的实时</strong>(≥30 fps)新视图合成。</p><ul><li>首先，从相机校准过程中产生的稀疏点开始，我们用3D高斯分布表示场景，该分布保留了连续体辐射场的理想属性，用于场景优化，同时避免了在空白空间中不必要的计算;</li><li>其次，我们对三维高斯函数进行交错优化/密度控制，特别是优化各向异性协方差，以实现对场景的准确表示;</li><li>第三，我们开发了一种快速的可视性感知渲染算法，该算法支持各向异性飞溅splatting，既加速了训练，又允许实时渲染。</li></ul><p>我们在几个已建立的数据集上展示了最先进的视觉质量和实时渲染。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>网格和点是最常见的3D场景表示，因为它们是显式的，非常适合基于GPU/ cuda的快速光栅化。相比之下，最近的神经辐射场(NeRF)方法建立在连续场景表示的基础上，通常使用体积射线推进优化多层感知器(MLP)，用于捕获场景的新视图合成。<br>同样，迄今为止最有效的辐射场解决方案<strong>建立在连续表示的基础上</strong>，通过插值存储的值，例如，体素Plenoxels[fridovic - keil and Yu et al. 2022]或哈希InstantNGP[Müller et al . 2022]网格或点Point-nerf:[Xu et al .2022]。虽然这些方法的连续性有助于优化，但渲染所需的随机采样成本很高，并且可能导致噪声。<br>我们引入了一种结合了两种世界最佳的新方法: </p><ul><li>我们的3D高斯表示允许优化最先进的(SOTA)视觉质量和竞争性训练时间，</li><li>而我们的tile-based splatting解决方案确保在几个先前发布的数据集上以1080p分辨率以SOTA质量实时渲染Mip-NeRF 360[Barron等人2022;海德曼等人2018;Knapitsch et al. 2017]。</li></ul><p>我们的目标是允许对多张照片拍摄的场景进行实时渲染，并以优化时间创建表示，速度与之前针对典型真实场景的最有效方法一样快。近期实现快速训练的方法Plenoxels、InstantNGP，但难以达到当前SOTA NeRF方法(即Mip-NeRF360)所获得的视觉质量[Barron et al.2022]，这需要长达48小时的培训时间。快速但质量较低的亮度场方法可以根据场景实现交互式渲染时间(每秒10-15帧)，但<strong>在高分辨率下无法实现实时渲染</strong>。</p><p>我们的解决方案基于三个主要组件：</p><ul><li>我们首先引入三维高斯函数作为灵活和富有表现力的场景表示。我们从与以前类似nerf的方法相同的输入开始，即使用运动结构(SfM)校准的相机[Snavely等人]。并使用作为SfM过程的一部分免费生成的稀疏点云初始化3D高斯集。与大多数需要多视图立体(MVS)数据的基于点的解决方案相反[Aliev等人，2020;Kopanas等人，2021;ckert et al. 2022]，我们<strong>仅使用SfM点作为输入就获得了高质量的结果</strong>。<ul><li>请注意，<strong>对于nerf合成数据集，我们的方法即使在随机初始化的情况下也能达到高质量</strong>。我们展示了3D高斯函数是一个很好的选择，因为它们是一种可微的体积表示，但它们也可以be rasterized通过将它们投影到2D并应用标准$\alpha$-混合，使用等效的图像形成模型作为NeRF。</li></ul></li><li>我们方法的第二个组成部分是优化三维高斯函数的属性-三维位置，不透明度$\alpha$，各向异性协方差和球面谐波(SH)系数-与自适应密度控制步骤交错，在<strong>优化过程中我们添加和偶尔删除3D高斯</strong>。优化过程产生了一个相当紧凑、非结构化和精确的场景表示(所有测试场景的1-5百万高斯)。</li><li>我们方法的第三个也是最后一个元素是我们的实时渲染解决方案，<strong>它使用快速GPU排序算法</strong>，并受到tile-based rasterization的启发，遵循最近的工作[Lassner和Zollhofer 2021]。</li><li>然而，由于我们的3D高斯表示，我们可以执行respects visibility ordering的各向异性splatting——这要归功于排序和𝛼-blending ，并通过跟踪尽可能多的splats的遍历实现快速准确的backward。 </li></ul><p>综上所述，我们提供了以下贡献:<br>•引入各向异性3D高斯作为高质量，非结构化的辐射场表示。<br>•3D高斯属性的优化方法，与自适应密度控制交错，为捕获的场景创建高质量的表示。<br>•GPU的快速、可微分渲染方法，具有可视性感知，允许各向异性splatting和快速反向传播，以实现高质量的新视图合成。</p><p>我们在先前发布的数据集上的结果表明，<strong>我们可以从多视图captures中优化我们的3D高斯分布</strong>，并获得与之前最佳质量的隐式辐射场方法相同或更好的质量。我们还可以实现与最快方法相似的训练速度和质量，重要的是为新视图合成提供高质量的实时渲染。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>首先简要概述了传统的重建，然后讨论了基于点的渲染和基于RF的渲染，讨论了它们的相似性; 辐射场是一个广阔的领域，所以我们只关注直接相关的工作。有关该领域的完整覆盖，请参阅最近的优秀调查[Tewari et al. 2022;谢等。2022]。</p><h3 id="Traditional-Scene-Reconstruction-and-Rendering"><a href="#Traditional-Scene-Reconstruction-and-Rendering" class="headerlink" title="Traditional Scene Reconstruction and Rendering"></a>Traditional Scene Reconstruction and Rendering</h3><p>第一种新颖的视图合成方法是基于光场，首先密集采样[Gortler等人，1996;Levoy和Hanrahan 1996]，然后允许非结构化捕获[Buehler等，2001]。结构-从运动(SfM)的出现[Snavely等。2006]开创了一个全新的领域，一组照片可以用来合成新的views。SfM在相机校准期间估计稀疏点云，最初用于简单的3D空间可视化。随后的多视图立体(MVS)多年来产生了令人印象深刻的全3D重建算法[Goesele等人，2007]，使几种视图合成算法得以发展[Chaurasia等人，2013;Eisemann et al. 2008;海德曼等人。2018;Kopanas等人。2021]。所有这些方法都是将输入图像重新投影和融合到新的视图摄像机中，并利用几何结构来指导这种重新投影。<strong>这些方法</strong>在许多情况下产生了很好的结果，但<strong>当MVS生成不存在的几何形状时，通常不能从未重建的区域或“过度重建”中完全恢复</strong>。最近的神经渲染算法[Tewari et al. 2022]大大减少了这种伪影，避免了在GPU上存储所有输入图像的巨大成本，在大多数方面都优于这些方法。</p><h3 id="Neural-Rendering-and-Radiance-Fields"><a href="#Neural-Rendering-and-Radiance-Fields" class="headerlink" title="Neural Rendering and Radiance Fields"></a>Neural Rendering and Radiance Fields</h3><p>深度学习技术很早就被用于新颖视图合成[Flynn et al. 2016;Zhou et al. 2016]; cnn用于估计混合权重[Hedman等人，2018]，或用于纹理空间解决方案[Riegler和Koltun 2020;他们等人。2019]。<strong>使用基于mvs的几何是大多数这些方法的主要缺点</strong>; 此外，使用cnn进行最终渲染经常会导致时间闪烁temporal flickering.。<br>新视角合成的体积表示由Soft3D提出[Penner and Zhang 2017]; 随后提出了与体积ray-marching相结合的深度学习技术[Henzler等人，2019;Sitzmann et al. 2019]基于连续可微密度场来表示几何。<strong>由于查询体积需要大量的样本，因此使用体积射线行进渲染具有显着的成本</strong>。神经辐射场(Neural Radiance Fields, nerf) [Mildenhall et al. 2020]引入了重要性采样和位置编码来提高质量，<strong>但使用了大型多层感知器，对速度产生了负面影响</strong>。NeRF的成功导致了后续方法的爆炸式增长，这些方法通常是通过引入规范化策略来解决质量和速度问题; 目前最先进的新视角合成图像质量是Mip-NeRF360 [Barron et al. 2022]。<strong>虽然渲染质量非常出色，但训练和渲染时间仍然非常高</strong>; 在提供快速培训和实时渲染的同时，我们能够达到或在某些情况下超过这种质量。</p><p>最近的方法主要集中在更快的训练和/或渲染上，主要是通过利用三种设计选择: <strong>使用空间数据结构来存储(神经)特征</strong>，这些特征随后在体射线行进期间被插值，<strong>不同的编码</strong>和<strong>MLP容量</strong>。这些方法包括空间离散化的不同变体different variants of space discretization[Chen et al. 2022b,a;fridovic - keil和Yu等人。2022;Garbin et al. 2021;海德曼等人。2021;Reiser等人。2021;Takikawa等人。2021;Wu等人。2022;Yu et al. 2021]，码本codebooks[Takikawa et al. 2022]，以及哈希表等编码encodings such as hash tables[m<s:1> ller et al. 2022]，允许完全使用较小的MLP或前述foregoing神经网络[friovich - keil and Yu et al. 2022;Sun et al. 2022]。<br>这些方法中最值得注意的是：<br>InstantNGP ，使用哈希网格和占用网格来加速计算，并使用较小的MLP来表示密度和外观;<br>Plenoxels [friovich - keil and Yu et al. 2022]使用稀疏体素网格来插值连续密度场，并且能够完全放弃神经网络。<br>两者都依赖于<strong>球面谐波</strong>: 前者直接表示方向效果，后者将其输入编码到颜色网络。虽然两者都提供了出色的结果，<strong>但这些方法仍然难以有效地表示空白空间</strong>，这部分取决于场景/捕获类型。此外，<strong>图像质量</strong>在很大程度上<strong>受到用于加速的结构化网格的选择的限制</strong>，而<strong>渲染速度</strong>则<strong>受到需要为给定的射线行进步骤查询许多样本的阻碍</strong>。<br>我们使用的非结构化，显式gpu友好的<strong>3D高斯函数</strong>实现了更快的渲染速度和更好的质量，而<strong>不需要神经组件</strong>。</p><h3 id="Point-Based-Rendering-and-Radiance-Fields"><a href="#Point-Based-Rendering-and-Radiance-Fields" class="headerlink" title="Point-Based Rendering and Radiance Fields"></a>Point-Based Rendering and Radiance Fields</h3><p>基于点的方法有效地渲染不连贯和非结构化的几何样本(即点云)[Gross and Pfister 2011]。在其最简单的形式中，点样本渲染[Grossman and Dally 1998]栅格化具有固定大小的非结构化点集，为此它可以利用本地支持的点类型图形api [Sainz and Pajarola 2004]或GPU上的并行软件栅格化[Laine and Karras 2011; Schütz et al . 2022]。虽然对底层数据是真实的，<strong>但点样本渲染存在漏洞，导致混叠，并且是严格不连续的</strong>。高质量的基于点的渲染开创性工作解决了这些问题，通过“Splatting”点基元的范围大于一个像素，例如圆形或椭圆形圆盘，椭球体或冲浪[Botsch等人，2005;菲斯特等人。2000;Ren et al. 2002;Zwicker et al. 2001b]。</p><p>最近人们对基于点的可微分渲染技术产生了兴趣[Wiles等人2020;Yifan et al. 2019]。用神经特征增强点并使用CNN进行渲染[Aliev等人2020;ckert et al. 2022]导致快速甚至实时的视图合成; <strong>然而，它们仍然依赖于初始几何形状的MVS</strong>，因此继承了它的artifacts，最明显的是在无特征/闪亮区域或薄结构等困难情况下的过度或重建不足。</p><p><strong>Point-based 𝛼-blending and NeRF-style volumetric rendering</strong> 图像形成模式基本相同. 具体来说，颜色c是由沿射线的体绘制给出的：<br>$C=\sum_{i=1}^{N}T_{i}(1-\exp(-\sigma_{i}\delta_{i}))\mathbf{c}_{i}\quad\mathrm{with}\quad T_{i}=\exp\left(-\sum_{j=1}^{i-1}\sigma_{j}\delta_{j}\right),$ Eq.1<br>where </p><ul><li>samples of density 𝜎, </li><li>transmittance 𝑇 , </li><li>and color c<br>are taken along the ray with intervals $𝛿_𝑖$ . </li></ul><p><strong>This can be re-written as</strong><br>$C=\sum_{i=1}^{N}T_{i}\alpha_{i}\mathbf{c}_{i},$ 其中 $\alpha_i=(1-\exp(-\sigma_i\delta_i))\text{and}T_i=\prod_{j=1}^{i-1}(1-\alpha_i).$Eq.2</p><p>一种典型的基于神经点的方法(例如，[Kopanas et al. 2022, 2021])计算颜色c通过将重叠在像素上的N个有序点混合在一起，得到一个像素;<br>$C=\sum_{i\in\mathcal{N}}c_{i}\alpha_i\prod_{j=1}^{i-1}(1-\alpha_{j}),$Eq.3<br>where </p><ul><li>$c_𝑖$ is the color of each point </li><li>$𝛼_𝑖$ is given 通过计算二维高斯函数的协方差 Σ [Yifan et al . 2019] 乘以学习到的每个点的不透明度。</li></ul><p>从Eq. 2和Eq. 3可以清楚地看到，<strong>图像的形成模型是相同的</strong>。<strong>然而，渲染算法是非常不同的</strong>。</p><ul><li>nerf是一个连续的表示，隐含地表示空/占用空间; <strong>为了找到</strong>Eq. 2中的<strong>样本，需要进行昂贵的随机抽样</strong>，由此产生噪声和计算开销。</li><li>相比之下，<strong>点是一种非结构化的离散表示，它具有足够的灵活性，可以创建、破坏和替换类似于NeRF的几何体</strong>。这是通过优化不透明度和位置来实现的，正如之前的工作[Kopanas等人，2021]所示，同时避免了完整体积表示的缺点。</li></ul><p>Pulsar[Lassner and Zollhofer 2021]实现了快速球体光栅化，这启发了我们tile-based and sorting renderer。然而，根据上面的分析，我们想要保持(近似)传统𝛼-blending on sorted splats，以具有体积表示的优势: 与顺序无关的方法相比order-independent，我们的<strong>栅格化respects visibility order</strong>。<br>此外，我们在一个像素的所有splats上反向传播梯度，并对各向异性splats进行光栅化。这些元素都有助于我们的高视觉质量的结果(见7.3节)。此外，前面提到的方法也使用cnn进行渲染，导致时间不稳定temporal instability。尽管如此，Pulsar [Lassner and Zollhofer 2021]和ADOP [Rückert et al . 2022] 的渲染速度成为我们开发快速渲染解决方案的动力。</p><p>在关注镜面效果的同时，Neural Point Catacaustics [Kopanas等人，2022]的基于漫射点的渲染轨迹通过使用MLP克服了这种时间不稳定性，<strong>但仍然需要MVS几何作为输入</strong>。<br>该类别中最新的方法[Zhang et al. 2022]不需要MVS，并且还使用SH作为方向; <strong>然而，它只能处理一个对象的场景，并且需要掩码进行初始化</strong>。虽然对于小分辨率和低点计数来说速度很快，但尚不清楚它如何扩展到典型数据集的场景[Barron et al. 2022;海德曼等人。2018;Knapitsch et al. 2017]。<br>我们使用3D高斯图像进行更灵活的场景表示，<strong>避免了对MVS几何图形的需要</strong>，<strong>并实现了实时渲染</strong>，这要归功于我们对投影高斯图像的tile-based rendering algorithm。</p><p>最近的一种方法[Xu et al .2022]采用径向基函数方法，用点来表示辐射场。他们在优化过程中采用点修剪和致密化技术，<strong>但使用体积射线推进，无法实现实时显示速率</strong>。</p><p>在人体行为捕捉领域，已经使用三维高斯函数来表示捕获的人体[Rhodin et al. 2015;Stoll et al. 2011]; 最近，它们被用于视觉任务的体积射线行进[Wang等人2023]。在类似的背景下也提出了神经体积原语Neural volumetric primitives[Lombardi等人，2021]。虽然这些方法启发了选择3D高斯作为我们的场景表示，<strong>但它们侧重于重建和渲染单个孤立对象(人体或面部)的具体情况</strong>，<strong>从而产生深度复杂性较小的场景</strong>。</p><p>相比之下，我们<strong>对各向异性协方差的优化</strong>、<strong>交错优化/密度控制</strong>以及<strong>高效的深度排序渲染</strong>使我们<strong>能够处理完整、复杂的场景，包括室内和室外的背景，以及深度复杂性很大的场景</strong>。</p><h1 id="OVERVIEW"><a href="#OVERVIEW" class="headerlink" title="OVERVIEW"></a>OVERVIEW</h1><p>我们方法的输入是一组静态场景图像，以及通过 SfM [Schönberger 和 Frahm 2016] 校准的相应摄像机，该方法会产生稀疏点云作为side-effect。<br>我们<strong>从这些点中创建了一组三维高斯</strong>（第 4 章），由位置（均值）、协方差矩阵和不透明度𝛼 定义，可以实现非常灵活的优化机制。这使得三维场景的表示相当紧凑，部分原因是高度各向异性的体积斑块可以用来紧凑地表示精细结构。辐射场的<strong>方向性外观分量</strong>（颜色）<strong>通过球面谐波（SH）表示</strong>，遵循标准做法[Fridovich-Keil 和 Yu 等人，2022；Müller 等人，2022]。<br>我们的算法<strong>通过一系列三维高斯参数的优化步骤</strong>，即位置、协方差、𝛼 和 SH 系数，以及高斯密度自适应控制的交错操作，来<strong>创建辐射场表示</strong>（第 5 节）。<br>我们的方法之所以高效，关键在于我们<strong>tile-based rasterizer</strong>（第 6 章），它可以对各向异性的splats进行𝛼 混合，并通过快速排序遵守可见性顺序。快速栅格化器还包括通过跟踪累积的𝛼值实现的fast backward，对可接收梯度的高斯数量没有限制。图 2 展示了我们的方法概览。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230817210209.png" alt="image.png"><br><em>优化从稀疏的SfM点云开始，并创建一组3D高斯点云。然后，我们优化和自适应控制这组高斯函数的密度。在优化过程中，我们使用快速的基于tile的渲染器，与SOTA快速辐射场方法相比，允许有竞争力的训练时间。一旦训练，我们的渲染器允许实时导航的各种场景。</em></p><h1 id="DIFFERENTIABLE-3D-GAUSSIAN-SPLATTING"><a href="#DIFFERENTIABLE-3D-GAUSSIAN-SPLATTING" class="headerlink" title="DIFFERENTIABLE 3D GAUSSIAN SPLATTING"></a>DIFFERENTIABLE 3D GAUSSIAN SPLATTING</h1><p>我们的目标是从没有法线的稀疏点集(SfM)开始，优化一个场景表示，允许高质量的新视图合成。要做到这一点，我们需要<strong>一个原语，它继承了可微分体积表示的属性，同时是非结构化和显式的，以允许非常快速的渲染</strong>。我们选择3D高斯分布，它是可微的，可以很容易地投影到2D分布，允许快速$\alpha$-混合渲染。</p><p>我们的表示方法与之前使用二维点的方法有相似之处[Kopanas 等人，2021 年；Yifan 等人，2019 年]，并假定每个点都是具有法线的平面小圆。鉴于 SfM 点的极度稀疏性，估计法线非常困难。同样，从这样的估计中优化噪声非常大的法线也非常具有挑战性。相反，<strong>我们将几何建模为一组无需法线的 3D 高斯</strong>。我们的高斯是由以点（平均值） 𝜇 为中心的全三维协方差矩阵 Σ 在世界空间中定义的[Zwicker 等人，2001a]：</p><p>$G(x)=e^{-\frac{1}{2}(x)^{T}\Sigma^{-1}(x)}$<br>这个高斯函数在我们的混合过程中乘以$\alpha$。<br>不过，我们需要将三维高斯投影到二维空间，以便进行渲染。Zwicker 等人 [2001a] 演示了如何将三维高斯投影到图像空间。给定一个视图变换 W，摄像机坐标中的协方差矩阵 Σ′ 如下所示：</p><p>$\Sigma’=JW\Sigma W^{T}J^{T}$<br>其中，𝐽 是投影变换仿射近似的雅各比。Zwicker 等人[2001a]的研究还表明，如果我们跳过 Σ′ 的第三行和第三列，就会得到一个 2×2 的方差矩阵，其结构和性质与以前的研究[Kopanas 等人，2021]中从有法线的平面点出发的方差矩阵相同。<br>一个明显的方法是直接优化协方差矩阵Σ来获得代表辐射场的三维高斯函数。<strong>然而，协方差矩阵只有在正半定时才有物理意义</strong>。对于我们的所有参数的优化，我们使用梯度下降，不能轻易地约束产生这样的有效矩阵，更新步骤和梯度可以很容易地创建无效的协方差矩阵。<br><strong>因此，我们选择了一种更直观，但同样具有表现力的表示来进行优化</strong>。三维高斯函数的协方差矩阵Σ类似于描述椭球体的结构。给定一个缩放矩阵S和旋转矩阵R，我们可以找到相应的Σ:<br>$\Sigma=RSS^{T}R^{T}$</p><p><strong>为了允许独立优化这两个因素</strong>，我们将它们分开存储:一个3D矢量s缩放和四元数q表示旋转。这些可以简单地转换成它们各自的矩阵并组合起来，确保归一化q得到一个有效的单位四元数。<br><strong>为了避免在训练过程中由于自动微分造成的巨大开销</strong>，我们明确地推导了所有参数的梯度。具体的导数计算细节见附录A。<br>这种各向异性协方差的表示-适合于优化-允许我们优化3D高斯函数以适应捕获场景中不同形状的几何形状，从而<strong>产生相当紧凑的表示</strong>。图3说明了这些情况。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230818133754.png" alt="image.png"></p><h1 id="OPTIMIZATION-WITH-ADAPTIVE-DENSITY-CONTROL-OF-3D-GAUSSIANS"><a href="#OPTIMIZATION-WITH-ADAPTIVE-DENSITY-CONTROL-OF-3D-GAUSSIANS" class="headerlink" title="OPTIMIZATION WITH ADAPTIVE DENSITY CONTROL OF 3D GAUSSIANS"></a>OPTIMIZATION WITH ADAPTIVE DENSITY CONTROL OF 3D GAUSSIANS</h1><p><strong>我们方法的核心</strong>是优化步骤，它创建一个密集的3D高斯集，准确地代表自由视图合成的场景。除了positions 𝑝, 𝛼, and covarianceΣ，我们还优化了代表颜色c的SH系数，每一个高斯的正确捕获视图依赖的场景外观。这些参数的优化与控制高斯密度的步骤交织在一起，以更好地表示场景。</p><h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p>优化是基于渲染的连续迭代，并将结果图像与捕获数据集中的训练视图进行比较。不可避免地，由于3D到2D投影的模糊性，几何体可能会被错误地放置。因此，我<strong>们的优化需要能够创建几何体，并在几何体位置不正确时破坏或移动它</strong>。三维高斯分布的协方差参数的质量对于表示的紧凑性至关重要，因为大的均匀区域可以用少量的大各向异性高斯分布来捕获。<br>我们使用随机梯度下降技术进行优化，充分利用标准gpu加速框架，以及为某些操作添加自定义CUDA内核的能力，遵循最近的最佳实践[fridovic - keil和Yu等人。2022;Sun et al. 2022]。特别是，我们的<strong>快速栅格化</strong>(参见第6节)对于优化的效率至关重要，因为它是优化的主要计算瓶颈。<br>我们使用s型激活函数为了将$\alpha$其约束在<code>[0−1)</code>范围内并获得平滑的梯度，与用指数激活函数表示协方差的尺度原因类似。<br>我们将初始协方差矩阵估计为各向同性高斯矩阵，其轴等于到最近的三个点的距离的平均值。我们使用类似于Plenoxels的标准指数衰减调度技术[friovich - keil and Yu et al. 2022]，但仅用于位置。损失函数为L1结合D-SSIM项:<br>$\mathcal{L}=(1-\lambda)\mathcal{L}_{1}+\lambda\mathcal{L}_{\mathrm{D-SSIM}}$<br>我们用$\lambda= 0.2$在我们所有的测试中我们在第7.1节中提供了学习时间表和其他要素的详细信息</p><h2 id="Adaptive-Control-of-Gaussians"><a href="#Adaptive-Control-of-Gaussians" class="headerlink" title="Adaptive Control of Gaussians"></a>Adaptive Control of Gaussians</h2><p>我们从SfM的初始稀疏点集开始，然后应用我们的方法<strong>自适应地控制单位体积上的高斯点的数量及其密度</strong>，使我们能够从初始的高斯稀疏集到更密集的集，更好地代表场景，并具有正确的参数。在优化预热之后(参见7.1节)，我们每100次迭代致密化一次，并删除任何本质上透明的高斯分布，即$\alpha$低于阈值$\epsilon_{\alpha}$ 。<br>我们对高斯函数的自适应控制需要填充空白区域。它侧重于缺少几何特征的区域(“欠重建”)，但也适用于场景中高斯分布覆盖大面积的区域(通常对应于“过度重建”)。我们观察到两者都具有较大的视图空间位置梯度。直观地说，这可能是因为它们对应的区域还没有很好地重建，而优化试图移动高斯函数来纠正这一点。<br>由于这两种情况都是致密化的良好候选者，因此我们对具有高于阈值的视图空间位置梯度的平均幅度的高斯函数进行致密化$\tau_{\mathrm{pos}}$，我们在测试中将其设置为0.0002。<br>We next present details of this process, illustrated in Fig. 4.<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230818134813.png" alt="image.png"><br><em>我们的自适应高斯致密化方案。顶行(重建不足):当小尺度几何(黑色轮廓)没有被充分覆盖时，我们克隆相应的高斯。下行(过度重建):如果小尺度几何由一个大的碎片表示，我们将其分成两部分。</em></p><p>对于处于重建不足区域的小高斯函数，我们需要覆盖必须创建的新几何。对于这种情况，最好是克隆高斯函数，通过简单地创建一个相同大小的副本，并沿着位置梯度的方向移动它。<br>另一方面，高方差区域中的大高斯分布需要被分割成小高斯分布。我们用两个新的高斯函数替换这些高斯函数，并将它们的尺度除以$\phi=1.6$，我们通过实验确定。我们还通过使用原始的3D高斯作为采样的PDF来初始化它们的位置。</p><p>在第一种情况下，我们检测并处理增加系统的总体积和高斯数的需要，而在第二种情况下，我们保留总体积，但增加高斯数。<br>与其他体积表示类似，我们的优化可能会遇到靠近输入摄像头的<strong>漂浮物</strong>; 在我们的例子中，这可能会导致高斯密度的不合理增加。调节高斯函数数量增加的有效方法是每迭代𝑁 = 3000 次，将𝛼 值设置为接近零。优化后，我们会在需要的地方增加高斯的 𝛼，同时允许我们的剔除方法删除 𝛼 小于 𝜖𝛼 的高斯。如上所述。高斯分布可能会缩小或增长，并与其他分布有相当大的重叠，但我们会定期删除在世界空间中非常大的高斯分布和在视图空间中占用空间很大的高斯分布。这种策略可以很好地控制高斯函数的总数。我们模型中的高斯函数在欧几里得空间中始终保持原函数;与其他方法不同[Barron et al. 2022;fridovitch - keil和Yu等人2022]，我<strong>们不需要空间压缩、翘翘或投影策略来处理远距离或大型高斯分布</strong>。</p><h1 id="FAST-DIFFERENTIABLE-RASTERIZER-FOR-GAUSSIANS"><a href="#FAST-DIFFERENTIABLE-RASTERIZER-FOR-GAUSSIANS" class="headerlink" title="FAST DIFFERENTIABLE RASTERIZER FOR GAUSSIANS"></a>FAST DIFFERENTIABLE RASTERIZER FOR GAUSSIANS</h1><p>我们的目标是有快速的整体渲染和快速排序，以允许近似𝛼-blending-包括各向异性splats-并避免对splats数量的硬限制，这些splats可以接收以前工作中存在的梯度[Lassner和Zollhofer 2021]。<br>为了实现这些目标，我们设计了一个tile-based rasterizer Gaussian splats，灵感来自于最近的软件光栅化方法[Lassner和Zollhofer 2021]，一次对整个图像进行预排序，避免了以往𝛼 混合解决方案中每个像素的排序费用[Kopanas等人，2022,2021]。我们的快速光栅化器允许在任意数量的混合高斯上进行有效的反向传播，并且额外的内存消耗很低，每像素只需要恒定的开销。我们的栅格化管道是完全可微的，并且给定2D投影(第4节)，可以栅格化各向异性splats，类似于以前的2D splats方法[Kopanas et al. 2021]。<br>我们的方法首先将屏幕分割为16×16块tiles，然后针对视锥体和每个块继续剔除3D高斯。具体来说，我们<strong>只保留与视锥体相交的具有99%置信区间的高斯函数</strong>。此外，我们使用保护带在极端位置trivially reject Gaussians(即，那些接近近平面和远离视锥台的平均值)，因为计算它们的投影二维协方差将是不稳定的。然后，我们根据重叠的贴图数量实例化每个高斯函数，并为每个实例分配一个键，该键结合了视图空间深度和贴图ID。然后，我们使用一个快速的GPU基数排序(Merrill and Grimshaw 2010)，基于这些键对高斯函数进行排序。需要注意的是，没有额外的按像素排序的点，混合是基于这种初始排序执行的。因此，在某些配置中，我们的𝛼混合可能是近似的。不过，当斑点的大小接近单个像素时，这些近似值就可以忽略不计了。我们发现这种选择大大提高了训练和渲染性能，而不会在融合场景中产生可见的工件artifacts。<br>在对高斯进行排序后，我们会通过识别第一个和最后一个深度排序的条目来为每个瓦片生成一个列表，这些条目会溅射到给定的瓦片上。在光栅化过程中，我们为每个瓦片启动一个线程块。每个线程块首先协同将高斯数据包加载到共享内存中，然后针对给定像素，通过前后遍历列表来累积颜色和𝛼 值，从而最大限度地提高数据加载/共享和处理的并行性。<strong>当某个像素达到目标饱和度𝛼 时，相应的线程就会停止</strong>。每隔一段时间，我们就会对一个平铺中的线程进行查询，当所有像素都达到饱和（即𝛼 变为 1）时，整个平铺的处理就会终止。有关排序的详细信息和整个光栅化方法的高级概述见附录 C。<br>在光栅化过程中，𝛼 的饱和度是唯一的停止标准。与之前的工作不同，我们<strong>不限制接受梯度更新的混合基元的数量。我们强制执行这一特性，是为了让我们的方法能够处理具有任意不同深度复杂度的场景</strong>，并准确地学习它们，而无需诉诸特定场景的超参数调整。因此，在后向处理过程中，我们必须恢复前向处理过程中每个像素混合点的完整序列。一种解决方案是在全局内存中存储每个像素任意长的混合点列表[Kopanas 等人，2021 年]。为了避免隐含的动态内存管理开销，我们选择再次遍历每个tile列表；我们可以重复使用前向遍历中的高斯排序数组和瓦片范围。为了方便梯度计算，我们现在从后向前遍历它们<br>遍历从影响tile中任何像素的最后一个点开始，并且再次协作地将点加载到共享内存中。此外，每个像素只有在深度低于或等于前向传递过程中对其颜色做出贡献的最后一个点的深度时，才会开始(昂贵的)重叠测试和处理。第4节中描述的梯度计算需要原始混合过程中每一步的不透明度值的累积。而不是遍历一个明确的列表逐步缩小的不透明度在向后传递，我们可以恢复这些中间的不透明度，只存储总累积的不透明度在向前传递结束。具体来说，每个点都存储了前向过程中最终累积的不透明度𝛼；我们在从后到前的遍历中将其除以每个点的𝛼，以获得梯度计算所需的系数。</p><h1 id="IMPLEMENTATION-RESULTS-AND-EVALUATION"><a href="#IMPLEMENTATION-RESULTS-AND-EVALUATION" class="headerlink" title="IMPLEMENTATION, RESULTS AND EVALUATION"></a>IMPLEMENTATION, RESULTS AND EVALUATION</h1><p>接下来，我们将讨论实现的一些细节，目前的结果以及与以前的工作和消融研究相比我们的算法的评估。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>实例镜像<br>PyTorch  2.0.0<br>Python  3.8(ubuntu20.04)<br>Cuda  11.8</p><ul><li><code>git clone https://github.com/graphdeco-inria/gaussian-splatting --recursive</code><ul><li>修改environment.yml中的环境名称：gaussian_splatting</li></ul></li><li>conda env create —file environment.yml</li><li>conda activate gaussian_splatting</li></ul><p>安装colmap用于convert.py</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt upgrade</span><br><span class="line">sudo apt-get install \</span><br><span class="line">    git \</span><br><span class="line">    cmake \</span><br><span class="line">    build-essential \</span><br><span class="line">    libboost-program-options-dev \</span><br><span class="line">    libboost-filesystem-dev \</span><br><span class="line">    libboost-graph-dev \</span><br><span class="line">    libboost-system-dev \</span><br><span class="line">    libboost-test-dev \</span><br><span class="line">    libeigen3-dev \</span><br><span class="line">    libsuitesparse-dev \</span><br><span class="line">    libfreeimage-dev \</span><br><span class="line">    libmetis-dev \</span><br><span class="line">    libgoogle-glog-dev \</span><br><span class="line">    libgflags-dev \</span><br><span class="line">    libglew-dev \</span><br><span class="line">    qtbase5-dev \</span><br><span class="line">    libqt5opengl5-dev \</span><br><span class="line">    libcgal-dev</span><br><span class="line"><span class="comment"># 如果一次安装不上，可以继续下步，缺什么装什么</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install colmap</span><br><span class="line"></span><br><span class="line">The following packages have unmet dependencies:  </span><br><span class="line">colmap : Depends: libqt5gui5 (&gt;= 5.4.0) but it is not going to be installed  </span><br><span class="line">Depends: libqt5widgets5 (&gt;= 5.4.0) but it is not going to be installed  </span><br><span class="line">E: Unable to correct problems, you have held broken packages.</span><br><span class="line">    sudo apt install libqt5gui5</span><br><span class="line">    Depends: libegl1 but it is not going to be installed  </span><br><span class="line">    Depends: libxkbcommon-x11-0 (&gt;= 0.5.0) but it is not going to be installed  </span><br><span class="line">        sudo apt install libegl1</span><br><span class="line">        Depends: libglvnd0 (= 1.0.0-2ubuntu2.3) but 1.3.2-1~ubuntu0.20.04.2 is to be installed  </span><br><span class="line">        Depends: libegl-mesa0 but it is not going to be installed </span><br><span class="line">            sudo apt install libglvnd0=1.0.0-2ubuntu2.3</span><br><span class="line">            sudo apt install libegl-mesa0</span><br><span class="line">                sudo apt install libglapi-mesa=20.0.8-0ubuntu1~18.04.1</span><br><span class="line">            sudo apt install libegl-mesa0</span><br><span class="line">        sudo apt install libxkbcommon-x11-0</span><br><span class="line">        Depends: libxkbcommon0 (= 0.8.2-1~ubuntu18.04.1) but 0.10.0-1 is to be installed</span><br><span class="line">            sudo apt install libxkbcommon0=0.8.2-1~ubuntu18.04.1</span><br><span class="line">        sudo apt install libxkbcommon-x11-0</span><br><span class="line">    sudo apt install libqt5widgets5</span><br></pre></td></tr></table></figure><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p><a href="https://github.com/graphdeco-inria/gaussian-splatting">gaussian-splatting</a></p><blockquote><p><a href="https://github.com/graphdeco-inria/gaussian-splatting/issues/35">remote viewer cannot find the model path · Issue #35 · graphdeco-inria/gaussian-splatting (github.com)</a> 服务端和本地端命令</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">- ./data/db/playroom</span><br><span class="line">- ./data/db/drjohnson</span><br><span class="line">- ./data/Miku</span><br><span class="line"># remote</span><br><span class="line">ssh -p 22938 root@connect.beijinga.seetacloud.com</span><br><span class="line">zZF/91WLl1</span><br><span class="line"></span><br><span class="line">conda activate gaussian_splatting</span><br><span class="line">python train.py -s ./data/Miku  # --port 6009 # 在服务端127.0.0.1:6009上运行程序</span><br><span class="line"></span><br><span class="line"># new terminal 将远程6009映射到本地6009</span><br><span class="line">ssh -L 6009:localhost:6009 root@connect.beijinga.seetacloud.com -p 22938</span><br><span class="line"></span><br><span class="line"># new terminal 运行本地SIBR GUI程序</span><br><span class="line">SIBR_remoteGaussian_app -s E:\Download\tandt_db\db\drjohnson --port 6009 --rendering-size 480 240 --force-aspect-ratio</span><br></pre></td></tr></table></figure><p>SIBR_remoteGaussian_app中操作：</p><ul><li>W, A, S, D, Q, E控制相机移动</li><li>I, K, J, L, U, O控制相机旋转</li></ul><h3 id="自定义数据集"><a href="#自定义数据集" class="headerlink" title="自定义数据集"></a>自定义数据集</h3><ul><li>自定义数据集- ./data/Miku<ul><li><code>python convert.py -s &lt;location&gt; [--resize] #If not resizing, ImageMagick is not needed</code></li><li>python convert.py -s ./data/Miku，如果之前使用过colmap生成了sparse/0/，则只需要undistortion</li><li><code>ssh -L 6009:localhost:6009 root@connect.beijinga.seetacloud.com -p 22938</code></li><li><code>SIBR_remoteGaussian_app -s E:\Download\Miku --port 6009 --rendering-size 480 240 --force-aspect-ratio</code></li></ul></li></ul><p>results</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230824163133.png" alt="image.png"></p><p>相比于对MLP进行优化，3D Gaussian Splatting直接对有意义的点云进行优化</p><ul><li><strong>渲染效果非常好(分辨率高)，速度快</strong><ul><li><strong>但是比较吃显存</strong></li><li>输出的点云中必定包含周围的环境，除非适用mask将训练数据只限定到感兴趣物体</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230824164154.png" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Explicit Volumetric </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PointCloud </tag>
            
            <tag> Real-time </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learn-cs148</title>
      <link href="/Learn/Learn-cs148/"/>
      <url>/Learn/Learn-cs148/</url>
      
        <content type="html"><![CDATA[<p><a href="https://web.stanford.edu/class/cs148/index.html">CS 148: Introduction to Computer Graphics and Imaging (stanford.edu)</a></p><span id="more"></span><h1 id="Introduction-amp-Geometry-and-Transformations"><a href="#Introduction-amp-Geometry-and-Transformations" class="headerlink" title="Introduction &amp; Geometry and Transformations"></a>Introduction &amp; Geometry and Transformations</h1><p>Blender操作和两个作业(Sphere&amp;Transformation)</p><p>Sphere<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> bpy</span><br><span class="line"><span class="keyword">import</span> bmesh </span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># Your job is to write a program that fills in </span></span><br><span class="line"><span class="comment"># the right vertex locations and faces to make a sphere. </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># I is the number of stacks minus 1 (Since we count from 0)</span></span><br><span class="line">I = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># J is the number of sectors</span></span><br><span class="line">J = <span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># r is the radius </span></span><br><span class="line">r = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">vertex_locations = []</span><br><span class="line">faces = []</span><br><span class="line"></span><br><span class="line"><span class="comment">############## <span class="doctag">TODO:</span> ADD VERTICES ##############</span></span><br><span class="line"><span class="comment"># 1. Add the top vertex (i = 0)</span></span><br><span class="line"><span class="comment"># 2. For each stack, and each sector in that stack, </span></span><br><span class="line"><span class="comment">#    add a vertex (i between 1 and I - 1)</span></span><br><span class="line"><span class="comment"># 3. Add the bottom vertex (i = I)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Add the top vertex (i = 0)</span></span><br><span class="line">vertex_locations.append((<span class="number">0.</span>,<span class="number">0.</span>,-<span class="number">10.</span>)) <span class="comment"># FIX THIS LINE</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> For each stack, and each sector in that stack, </span></span><br><span class="line"><span class="comment">#       add a vertex (i between 1 and I - 1)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> You could use theta and phi to find the correct (x,y,z) coordinate</span></span><br><span class="line"><span class="comment">#       You could also figure out the height of each stack to find z,</span></span><br><span class="line"><span class="comment">#       use the radius and z to find the radius of that stack,</span></span><br><span class="line"><span class="comment">#       and use the stack radius and theta to find x and y.</span></span><br><span class="line"><span class="comment">#       Or you could find some other intuition! How do you parse a sphere?</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> Just make sure to take note of the vertex order - you&#x27;ll need that!         </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,I):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(J):</span><br><span class="line">        <span class="comment"># x = 10 # FIX THESE LINES</span></span><br><span class="line">        z = (i * <span class="number">2</span> * r / I) - r</span><br><span class="line">        stack_raidus = math.sqrt(r**<span class="number">2</span> - z**<span class="number">2</span>)  </span><br><span class="line">        theta = <span class="number">2</span> * math.pi * j / J</span><br><span class="line">        x = stack_raidus * math.cos(theta)</span><br><span class="line">        y = stack_raidus * math.sin(theta)</span><br><span class="line">        vertex_locations.append((x,y,z))</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Add the bottom vertex (i = I)</span></span><br><span class="line">vertex_locations.append((<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">10.</span>)) <span class="comment"># FIX THIS LINE</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">############## <span class="doctag">TODO:</span> ADD FACES ##############</span></span><br><span class="line"><span class="comment"># 1. Add the top face ring (i = 0)</span></span><br><span class="line"><span class="comment"># 2. Between every two sectors in a stack, add two triangles </span></span><br><span class="line"><span class="comment">#    to form a quad below them. (i between 1 and I - 2)</span></span><br><span class="line"><span class="comment">#    (I - 1 is the bottom sector, so there is only the bottom ring below)</span></span><br><span class="line"><span class="comment"># 3. Add the bottom face ring (i = I)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Add the top face ring (i = 0)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> For every sector, you want the face to connect </span></span><br><span class="line"><span class="comment">#       your point with the following point and the top point</span></span><br><span class="line"><span class="comment">#       EXCEPT the last sector, which you want to wrap back around </span></span><br><span class="line"><span class="comment">#       to the first sector in the stack. You&#x27;ll need a separate case here. </span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> What index is the top point? If it&#x27;s the first point in your array, </span></span><br><span class="line"><span class="comment">#       it should be 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># FIX AND EXPAND THIS BEYOND JUST ONE LINE OF CODE</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(J):</span><br><span class="line">    <span class="keyword">if</span> j == J - <span class="number">1</span>:</span><br><span class="line">        faces.append((<span class="number">0</span>, j + <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        faces.append((<span class="number">0</span>, j + <span class="number">1</span>, j + <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Between every two sectors in a stack, add two triangles </span></span><br><span class="line"><span class="comment">#       to form a quad below them. (i between 1 and I - 2)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> Looking at two consecutive sectors in a stack: v1 and v2,</span></span><br><span class="line"><span class="comment">#       what are the indices of v1 and v2? What are the indices of the </span></span><br><span class="line"><span class="comment">#       two points below v1 and v2? </span></span><br><span class="line"><span class="comment">#       How will you compose 2 triangles out of these 4 points? </span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> As before, when you get to the last sector, you must wrap back around </span></span><br><span class="line"><span class="comment">#       to the first sector in that stack. This is a different case for </span></span><br><span class="line"><span class="comment">#       calculating the indices.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, I-<span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(J):</span><br><span class="line">        n = J * i + <span class="number">1</span> + j</span><br><span class="line">        <span class="keyword">if</span> j == J - <span class="number">1</span>:</span><br><span class="line">            faces.append((n , n + <span class="number">1</span> - J, n + J))</span><br><span class="line">            faces.append((n + <span class="number">1</span> - J, n + J, n + <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            faces.append((n , n + <span class="number">1</span>, n + J))</span><br><span class="line">            faces.append((n + <span class="number">1</span>, n + J, n + J + <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(faces)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ADD LINES OF CODE TO GENERATE TWO TRIANGLES</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Add the bottom face ring (i = I)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> For every sector, you want the face to connect </span></span><br><span class="line"><span class="comment">#       your point with the following point and the bottom point</span></span><br><span class="line"><span class="comment">#       EXCEPT the last sector, which you want to wrap back around </span></span><br><span class="line"><span class="comment">#       to the first sector in the stack. You&#x27;ll need a separate case here. </span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> What index is the bottom point? If you have &quot;x&quot; points total, it should be</span></span><br><span class="line"><span class="comment">#       indexed &quot;x - 1&quot;. How can you calculate the index? This should follow the</span></span><br><span class="line"><span class="comment">#       same pattern as finding the indices above. </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># FIX AND EXPAND THIS BEYOND JUST ONE LINE OF CODE</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(J):</span><br><span class="line">    <span class="keyword">if</span> j == J - <span class="number">1</span>:</span><br><span class="line">        faces.append((<span class="built_in">len</span>(vertex_locations) - <span class="number">1</span>, <span class="built_in">len</span>(vertex_locations) - <span class="number">2</span> - j, <span class="built_in">len</span>(vertex_locations) - <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        faces.append((<span class="built_in">len</span>(vertex_locations) - <span class="number">1</span>, <span class="built_in">len</span>(vertex_locations) - <span class="number">2</span> - j, <span class="built_in">len</span>(vertex_locations) - <span class="number">3</span> - j))</span><br><span class="line"></span><br><span class="line"><span class="comment">################ Don&#x27;t worry about anything below this! ################</span></span><br><span class="line"><span class="comment">#### Although it&#x27;s good to read if you&#x27;re interested in Blender scripting! ####</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Delete any old object - rename anything you want to keep! </span></span><br><span class="line"><span class="keyword">for</span> o <span class="keyword">in</span> bpy.context.scene.objects:</span><br><span class="line">    <span class="keyword">if</span> o.name == <span class="string">&quot;mySphere&quot;</span>:</span><br><span class="line">        o.select_set(<span class="literal">True</span>)</span><br><span class="line">        bpy.ops.<span class="built_in">object</span>.delete()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find collection, and make one if none exists</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(bpy.data.collections) &lt; <span class="number">1</span>:</span><br><span class="line">    new_collection = bpy.data.collections.new(<span class="string">&#x27;Scene Collection&#x27;</span>)</span><br><span class="line">    bpy.context.scene.collection.children.link(new_collection)</span><br><span class="line">        </span><br><span class="line">collection = bpy.data.collections[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a new mesh and object to put the data into</span></span><br><span class="line">sphere_mesh = bpy.data.meshes.new(<span class="string">&#x27;mySphere_mesh&#x27;</span>)</span><br><span class="line">sphere_object = bpy.data.objects.new(<span class="string">&#x27;mySphere&#x27;</span>, sphere_mesh)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add object to scene collection</span></span><br><span class="line">collection.objects.link(sphere_object)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bmesh is a method of editing the mesh data within an object</span></span><br><span class="line">bm = bmesh.new()</span><br><span class="line">bm.from_mesh(sphere_object.data) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Now we&#x27;ll add all the vertices into the new mesh</span></span><br><span class="line">vertices = []</span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> vertex_locations:</span><br><span class="line">    vertices.append(bm.verts.new(v))</span><br><span class="line"></span><br><span class="line"><span class="comment"># For the faces, we want the bmesh vertices, instead of the integer indices </span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> faces:</span><br><span class="line">    face = [vertices[i] <span class="keyword">for</span> i <span class="keyword">in</span> f]</span><br><span class="line">    bm.faces.new(face)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally, we&#x27;ll reload the mesh into the object! </span></span><br><span class="line">bm.to_mesh(sphere_object.data)  </span><br><span class="line">bm.free()</span><br></pre></td></tr></table></figure></p><p>Transformation<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> bpy</span><br><span class="line"><span class="keyword">import</span> bmesh </span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">cones = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reset_scene</span>():</span><br><span class="line">    <span class="keyword">for</span> o <span class="keyword">in</span> bpy.context.scene.objects:</span><br><span class="line">        o.select_set(<span class="literal">True</span>)</span><br><span class="line">        bpy.ops.<span class="built_in">object</span>.delete()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_cone</span>(<span class="params">name</span>):</span><br><span class="line">    <span class="comment"># Create a new mesh and object to put the data into</span></span><br><span class="line">    bpy.ops.mesh.primitive_cone_add(location=(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    cone_object = bpy.context.active_object</span><br><span class="line">    cone_object.name = name</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Bmesh is a method of editing the mesh data within an object</span></span><br><span class="line">    bm = bmesh.new()</span><br><span class="line">    bm.from_mesh(cone_object.data) </span><br><span class="line">    cones.append((cone_object, bm))</span><br><span class="line">    <span class="keyword">return</span> bm</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_transforms</span>():</span><br><span class="line">    <span class="keyword">for</span> cone <span class="keyword">in</span> cones:</span><br><span class="line">        bm = cone[<span class="number">1</span>]</span><br><span class="line">        cone_object = cone[<span class="number">0</span>]</span><br><span class="line">        bm.to_mesh(cone_object.data)  </span><br><span class="line">        bm.free()</span><br><span class="line">        </span><br><span class="line"><span class="comment">####################### DO NOT EDIT ABOVE THIS LINE! #######################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">translate_matrix</span> (distance, axis):</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Edit this matrix to be the correct translation matrix</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># X_AXIS = 0, Y_AXIS = 1, Z_AXIS = 2</span></span><br><span class="line">    <span class="keyword">if</span> axis == <span class="number">0</span>:</span><br><span class="line">        arr = np.array([ [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, distance],</span><br><span class="line">                            [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">    <span class="keyword">elif</span> axis == <span class="number">1</span>:</span><br><span class="line">        arr = np.array([ [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                            [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, distance],</span><br><span class="line">                            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">    <span class="keyword">elif</span> axis == <span class="number">2</span>:</span><br><span class="line">        arr = np.array([ [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, distance],</span><br><span class="line">                    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">    <span class="keyword">return</span> arr</span><br><span class="line">               </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rotate_matrix</span> (degrees, axis):</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Edit this matrix to be the correct rotation matrix</span></span><br><span class="line">    <span class="comment"># We want to keep the 4x4 version so that we can multiply it with the</span></span><br><span class="line">    <span class="comment"># translation matrix, which has to be 4x4 as discussed in lecture.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># X_AXIS = 0, Y_AXIS = 1, Z_AXIS = 2</span></span><br><span class="line">    <span class="keyword">if</span> axis == <span class="number">0</span>:</span><br><span class="line">        arr = np.array([ [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">0</span>, math.cos(math.radians(degrees)), -math.sin(math.radians(degrees)), <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">0</span>, math.sin(math.radians(degrees)), math.cos(math.radians(degrees)), <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">    <span class="keyword">elif</span> axis == <span class="number">1</span>:</span><br><span class="line">        arr = np.array([ [math.cos(math.radians(degrees)), <span class="number">0</span>, math.sin(math.radians(degrees)), <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                    [-math.sin(math.radians(degrees)), <span class="number">0</span>, math.cos(math.radians(degrees)), <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">    <span class="keyword">elif</span> axis == <span class="number">2</span>:</span><br><span class="line">        arr = np.array([ [math.cos(math.radians(degrees)), -math.sin(math.radians(degrees)), <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                    [math.sin(math.radians(degrees)), math.cos(math.radians(degrees)), <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">    <span class="keyword">return</span> arr </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transform_cone</span>(<span class="params">cone_bm, matrix</span>):</span><br><span class="line">    <span class="comment"># You do not need to edit this function, but you might want to include</span></span><br><span class="line">    <span class="comment"># more print statements to the OS console for debugging!</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> cone_bm.verts:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;----------Vertex BEFORE transformation (x, y, z)----------&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(v.co)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Apply the transformation matrix to every vertex</span></span><br><span class="line">        r = np.matmul(matrix, np.append(np.asarray(v.co), <span class="number">1</span>))</span><br><span class="line">        v.co = r[:-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;----------Vertices AFTER transformation (x, y, z)----------&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(v.co)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not touch this line. This just clears the scene.</span></span><br><span class="line">reset_scene()</span><br><span class="line"></span><br><span class="line"><span class="comment"># These add 4 cones, each with the names in quotes as you see below.</span></span><br><span class="line"><span class="comment"># The names will show up in the Scene Collection in Blender to let you</span></span><br><span class="line"><span class="comment"># differentiate amongst the cones.</span></span><br><span class="line"><span class="comment"># You should not need to edit these lines.</span></span><br><span class="line">q1 = add_cone(<span class="string">&quot;rot_45_x_rot_45_y&quot;</span>)</span><br><span class="line">q2 = add_cone(<span class="string">&quot;rot_45_y_rot_45_x&quot;</span>)</span><br><span class="line">q3 = add_cone(<span class="string">&quot;trans_10_x_rot_45_y&quot;</span>)</span><br><span class="line">q4 = add_cone(<span class="string">&quot;rot_45_y_trans_10_x&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Uncomment the lines below to apply the appropriate transformations</span></span><br><span class="line"><span class="comment"># to all 4 cones. They are commented out for now, since you&#x27;ll likely prefer</span></span><br><span class="line"><span class="comment"># to just work with 1 cone at first for debugging purposes.</span></span><br><span class="line"><span class="comment"># When debugging your matrices, you&#x27;ll want to only enable one transformation</span></span><br><span class="line"><span class="comment"># at a time, as your OS console will get flooded by print outs of all the</span></span><br><span class="line"><span class="comment"># changed cone vertices for each transformation.</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Rotate X 45&quot;</span>)</span><br><span class="line">transform_cone(q1, rotate_matrix(<span class="number">45</span>, <span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Rotate Y 45&quot;</span>)</span><br><span class="line">transform_cone(q1, rotate_matrix(<span class="number">45</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Rotate Y 45&quot;</span>)</span><br><span class="line">transform_cone(q2, rotate_matrix(<span class="number">45</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Rotate X 45&quot;</span>)</span><br><span class="line">transform_cone(q2, rotate_matrix(<span class="number">45</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Translate X 10&quot;</span>)</span><br><span class="line">transform_cone(q3, translate_matrix(<span class="number">10</span>, <span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Rotate Y 45&quot;</span>)</span><br><span class="line">transform_cone(q3, rotate_matrix(<span class="number">45</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Rotate Y 45&quot;</span>)</span><br><span class="line">transform_cone(q4, rotate_matrix(<span class="number">45</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Translate X 10&quot;</span>)</span><br><span class="line">transform_cone(q4, translate_matrix(<span class="number">10</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not touch this line. This applys the transformations above.</span></span><br><span class="line">apply_transforms()</span><br></pre></td></tr></table></figure></p><h1 id="Rasterization-and-Shading"><a href="#Rasterization-and-Shading" class="headerlink" title="Rasterization and Shading"></a>Rasterization and Shading</h1><h2 id="Phong-Reflection-Model"><a href="#Phong-Reflection-Model" class="headerlink" title="Phong Reflection Model"></a>Phong Reflection Model</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816184143.png" alt="image.png"></p><p>$c=c_{a}+c_{d}c_{l}max(0,n\cdot l)+c_{specular}$</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816184315.png" alt="image.png"></p><p>$c_{specular}=c_sc_lmax(0,e\cdot r)$<br>=&gt;+ shininess value: how shiny we want to tune the material<br>$c_{specular}=c_sc_lmax(0,e\cdot r)^\alpha$<br>=&gt;+ 反射r不好计算<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816184900.png" alt="image.png"><br>计算l和e的halfway vector h， 当e与r对齐时，h与n对齐<br>$h=\frac{e+l}{|e+l|}$<br>$c_{specular}=c_sc_lmax(0,n\cdot h)^\alpha$</p><p>最终：$c=c_{ambient}+c_{diffuse}+c_{specular}$<br>$\begin{aligned}c&amp;=c_a+c_dc_lmax(0,n\cdot l)+c_sc_lmax(0,n\cdot h)^\alpha\end{aligned}$</p><h2 id="How-do-we-make-the-teapot-image-look-smooth"><a href="#How-do-we-make-the-teapot-image-look-smooth" class="headerlink" title="How do we make the teapot image look smooth?"></a>How do we make the teapot image look smooth?</h2><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816190517.png" alt="image.png"></p><p>Gourad Shading:</p><p>a. b. c. 三个vertex的颜色值和法向量，则三角形中任意一点v</p><p>$c_{v}=\frac{A_{a}}{A_{total}}c_{a}+\frac{A_{b}}{A_{total}}c_{b}+\frac{A_{c}}{A_{total}}c_{c}$<br>$n_{v}=\frac{A_{a}}{A_{total}}n_{a}+\frac{A_{b}}{A_{total}}n_{b}+\frac{A_{c}}{A_{total}}n_{c}$</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816190549.png" alt="image.png"></p><p>对比三种不同计算颜色的方法：</p><div class="table-container"><table><thead><tr><th>name</th><th>方法描述</th><th>result</th></tr></thead><tbody><tr><td>Flat</td><td>使用每个顶点计算出的颜色平均值为三角形着色</td><td>simple, fast, but looks bad.</td></tr><tr><td>Gourad</td><td>通过对每个顶点的颜色进行内插，对三角形进行阴影处理</td><td>good balance between speed and visual result</td></tr><tr><td>Phong</td><td>内插每个顶点的法线，然后计算三角形中每个点的颜色</td><td>expensive but best look!</td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816191216.png" alt="image.png"></p><ul><li>光栅化的pipeline</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816191354.png" alt="image.png"></p><h1 id="Color-Images-and-Cameras"><a href="#Color-Images-and-Cameras" class="headerlink" title="Color, Images, and Cameras"></a>Color, Images, and Cameras</h1><p><strong>The images we create ARE NOT intended to duplicate reality, only to fool humans into believing such</strong></p><h2 id="Camera"><a href="#Camera" class="headerlink" title="Camera"></a>Camera</h2><p>相机坐标系</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816201009.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816202528.png" alt="image.png"></p><p>投影变换矩阵，将锥形空间转换到正方体空间orthographic clipping space<br><em>transform viewing frustrum to an orthographic bounding box</em></p><p>$\begin{pmatrix}\frac{1}{r_x}&amp;0&amp;0&amp;0\\0&amp;\frac{1}{r_y}&amp;0&amp;0\\0&amp;0&amp;\frac{d0+d1}{d0-d1}&amp;2\frac{d0d1}{d0-d1}\\0&amp;0&amp;-1&amp;0\end{pmatrix}\begin{pmatrix}x\\y\\z\\1\end{pmatrix}$</p><p>焦距f越小，传感器h越大==&gt;视野越宽<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816203747.png" alt="image.png"></p><p>光圈越大，景深越大还是越小？</p><ul><li>F = 焦距/入瞳直径 ==&gt; 控制曝光，影响景深</li><li>F越小 —&gt; 光圈越大 —&gt; 入瞳直径越大 —&gt; 景深越小</li></ul><blockquote><p><a href="https://www.bilibili.com/video/BV1t24y1k7Ye">https://www.bilibili.com/video/BV1t24y1k7Ye</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816204530.png" alt="image.png"></p><h1 id="Light-and-Optics"><a href="#Light-and-Optics" class="headerlink" title="Light and Optics"></a>Light and Optics</h1><p>3D ==&gt; solid angle</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816210323.png" alt="image.png"></p><p>辐射强度Radiant Intensity：$I(\omega)=\frac{dP}{d\omega}$ 光源在每个solid angle上的功率power，单位w</p><ul><li>各项异性光源：辐射强度在整个光源中都是不同的，需要function of steradians立体角的函数</li><li>各项同性点光源：对$\begin{aligned}dP=Id\omega\end{aligned}$积分 ==&gt; $P=\int_{sphere}Id\omega=4\pi I$</li></ul><p>辐照度Irradiance: 光照射物体单位表面积的功率。光的单位表面积功率$E=\frac{dP}{dA}$</p><ul><li>如果表面倾斜，则辐照度减小：$E=\frac{dP}{dA}\rightarrow E_{tilted}=\frac{\frac{A\cos\theta}{A}P}{A}=E\cos\theta$</li><li><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816210737.png" alt="image.png"></li><li>Irradiance <ul><li>varies based on tilt angle of surface with light</li><li>varies based on distance from light</li></ul></li></ul><p>现实世界中，光源不是点，而是一个单位面积，因此将辐射度定义为<strong>单位面积</strong>chunk的辐射强度<br>辐射度radiance：$L=\frac{dI}{dA\cos\theta}$<br>$L=\frac{dI}{dA\cos\theta}=\left(\frac{d(dP)}{d\omega dA\cos\theta}=\frac{dE}{d\omega\cos\theta}\right)$</p><p>现实世界中，光源来自各个可以看见的物体，光线传播到一个物体material上后，会发生：反射、吸收、传输（散射）</p><p>建模materials的方法：</p><ul><li>BRDF —&gt; Reflectance $BRDF(\omega_i,\omega_o)=\frac{dL_o(\omega_o)}{dE_i(\omega_i)}$<ul><li>BRDF 模拟在入射方向不变的情况下，每个向外方向反射多少光</li></ul></li><li>BTDF —&gt; Transmittance</li><li>BSSRDF —&gt; Surface Scattering散射 Reflectance</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816211756.png" alt="image.png"></p><p>更精确Lighting方程：根据BRDF<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230816212333.png" alt="image.png"></p><p>得出：$L_o(\omega_o)=\int_{i\in in}BRDF(\omega_i,\omega_o)L_i\cos\theta_id\omega_i$</p><p>HW2</p><ul><li>焦距和距离等比例变化，焦距越大，照片的信息越多(耳朵变大)</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/GIF%202023-8-17%2016-43-19.gif" alt="GIF 2023-8-17 16-43-19.gif"></p><h1 id="Raytracing"><a href="#Raytracing" class="headerlink" title="Raytracing"></a>Raytracing</h1>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PlankAssembly</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/PlankAssembly/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/PlankAssembly/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://github.com/Huenao">Wentao Hu</a>    <a href="https://bertjiazheng.github.io/">Jia Zheng</a>     <a href="https://github.com/Elsa-zhang">Zixin Zhang</a>     <a href="https://yuan-xiaojun.github.io/Yuan-Xiaojun/">Xiaojun Yuan</a>     <a href="https://sai.sysu.edu.cn/teacher/teacher01/1385356.htm">Jian Yin</a>     <a href="https://zihan-z.github.io/">Zihan Zhou</a></td></tr><tr><td>Conf/Jour</td><td>ICCV</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://manycore-research.github.io/PlankAssembly/">PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs (manycore-research.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4787600278115319809&amp;noteId=1912848205953901312">PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230813211026.png" alt="image.png"></p><p>贡献：</p><ul><li>基于Transform的自注意力提出模型，用于将2D的三个orthographic的line drawing转化为3D模型，可以实现从图纸不完美的输入中生成正确的3D模型</li><li>输入的三个orthographic图纸被编码，输出的是3D模型程序的编码，最后解码后即3D模型对应的程序DSL</li></ul><p>挑战：</p><ul><li>应用于无法获得甚至不存在大规模CAD数据的领域，例如建筑物或复杂的机械设备</li><li>没有考虑图纸中的符号、图层等信息</li></ul><span id="more"></span><h1 id="Discussion-amp-Conclusion"><a href="#Discussion-amp-Conclusion" class="headerlink" title="Discussion&amp;Conclusion"></a>Discussion&amp;Conclusion</h1><p>本文提出了一种基于三个正射影视图的三维CAD模型重建生成方法。从我们的实验中可以学到两个教训lessons:</p><ul><li>首先，与寻找2D线条图和3D模型之间的明确对应关系相比，<strong>注意机制在深度网络对不完美输入的鲁棒性中起着关键作用</strong>。</li><li>其次，<strong>在生成模型中加入领域知识有利于重构和下游应用</strong></li></ul><p>有人可能会说，我们的实验仅限于橱柜家具，这是一种特殊的CAD模型。然而，我们强调我们的主要思想和经验教训是通用的，可以应用于任何CAD模型。例如，DeepCAD[35]等先前的工作已经开发出能够生成适合机械零件的CAD命令序列的神经网络。与橱柜家具不同，机械部件通常具有非矩形轮廓(但块状较少)。因此，将我们的方法扩展到这些领域是相对简单的。</p><p>一个更具<strong>挑战</strong>性的场景是尝试将我们的数据驱动方法<strong>应用于无法获得甚至不存在大规模CAD数据的领域，例如建筑物或复杂的机械设备</strong>。此外，我们目前的方法<strong>没有考虑</strong>CAD图纸中可用的其他信息，如<strong>图层、文本、符号和注释</strong>。近年来，人们提出了几种用于CAD图中全光学符号识别的方法[6,39,5]。我们相信这些信息对于从复杂的CAD图纸进行3D重建也是至关重要的。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>在本文中，我们开发了一种新的方法来自动转换二维线条图从三个正射影到三维CAD模型。<br>该问题的<strong>现有方法</strong>通过将2D观测数据反向投影到3D空间来重建3D模型，同时保持输入和输出之间的显式对应。<strong>这种方法对输入中的错误和噪声很敏感，因此在实践中经常失败，因为人类设计师创建的输入图纸不完美</strong>。<br>为了克服这一困难，我们利用基于transformer的序列生成模型中的<strong>注意机制</strong>来学习输入和输出之间的灵活映射。此外，我们还设计了<strong>适合生成感兴趣对象的形状程序</strong>，以提高重建精度并促进CAD建模应用。<br>在一个新的基准数据集上的实验表明，当输入有噪声或不完整时，我们的方法明显优于现有的方法。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在本文中，我们解决了计算机辅助设计(CAD)中一个长期存在的问题，即从三个正射影视图重建三维物体。在当今的产品设计和制造行业中，设计师通常使用二维工程图纸来实现，更新和分享他们的想法，特别是在初始设计阶段。但为了进一步分析(如有限元分析)和制造，这些2D设计必须在CAD软件中手动实现为3D模型。因此，如果有一种方法可以自动将二维图纸转换为三维模型，将大大简化设计过程，提高整体效率</p><p>作为2D绘图中最流行的描述物体的方式，正射影视图是物体在垂直于三个主轴之一的平面上的投影(图1)。在过去的几十年里，从三个正射影视图进行三维重建得到了广泛的研究，在适用对象的类型和计算效率方面有了显著的改进[25,10,19,37,38,28,18,21,8,9]。然而，据我们所知，这些技术并没有在CAD软件和商业产品中得到广泛采用。</p><p>在现有的方法在实践中面临的挑战中，它们对图纸中的错误和缺失部件的敏感性可以说是最关键的。为了理解这个问题，我们注意到<strong>几乎所有现有的方法都遵循3D重建的标准程序</strong>，其中包括以下步骤:<br>(i)从2D顶点生成3D顶点;<br>(ii)从三维顶点生成三维边缘;<br>(iii)由三维边缘生成三维面;<br>(iv)从3D面构建3D模型(如图6所示)。遵循管道的一个主要好处是可以找到与输入视图匹配的所有解决方案，因为它在3D模型中的实体和绘图中的实体之间建立了显式的对应关系。<strong>但在实践中，设计师并不会花额外的精力去完善图纸，只要能传达他们的想法，他们就会认为一幅图纸足够好</strong>。因此，某些实体可能是错误的或缺失的。因此，前面提到的管道经常无法找到所需的解决方案。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230813161256.png" alt="image.png"></p><p>因此，为了克服这一困难，有必要以更全面的方式对二维图形进行推理，并在输入和输出之间实现更灵活的映射。最近，Transformer[29]已经成为许多NLP和CV任务的标准架构。<strong>它在序列到序列(seq2seq)问题中特别有效</strong>，例如机器翻译，其中关于上下文的推理以及输入和输出之间的软对齐是至关重要的。受此启发，我们将问题转化为seq2seq问题，并提出了一种基于transformer的深度学习方法。直观上，<strong>自关注模块可以让模型捕捉到产品设计师的意图，即使他们的图纸不完美，交叉关注模块可以实现2D图纸和3D模型中几何实体之间的灵活映射</strong>。</p><p>对几何实体使用学习表征和软对齐的另一个好处是，人们可以自由选择如何构建3D模型。这为我们提供了将领域知识合并到我们的方法中以提高其性能的机会。为了说明这一点，<strong>本文将重点</strong>放在一个特定类型的产品——<strong>橱柜家具上</strong>。如图2所示，一个橱柜通常是通过在3D建模软件中排列和连接一些木板(即木板)来构建的。为此，我们<strong>开发了一种简单的领域特定语言(DSL)</strong>，它基于声明木板，然后将它们彼此连接起来，这样每个柜子就可以用一个程序来表示。最后，给定输入的正交视图，我们训练基于transformer的模型来预测与机柜相关的程序。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230813161422.png" alt="image.png"></p><p>为了系统地评估这些方法，我们为此任务构建了一个新的基准数据集，该数据集由26,000多个3D橱柜模型组成。其中大部分是由专业室内设计师使用商业3D建模软件创建的。<strong>大量的实验表明，我们的方法对不完美输入具有更强的鲁棒性</strong>。例如，当输入图纸中30%的线条损坏或缺失时，传统方法的F1分数为8.20%，而我们的方法的F1分数为90.14%。</p><p>总之，这项工作的贡献是:<br>(i)据我们所知，<strong>我们是第一个在三维CAD模型重建任务中使用深度生成模型的人</strong>。与现有方法相比，我们的模型学习了更灵活的输入和输出之间的映射，从而对有噪声或不完整的输入具有更强的鲁棒性。<br>(ii)我们提出了一种<strong>新的网络设计</strong>，它学习形状程序将木板组装成3D机柜模型。这样的设计不仅提高了重建的精度，而且方便了CAD模型编辑等下游应用。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p><strong>3D reconstruction from three orthographic views</strong> 从三个正射视图中恢复三维模型的研究可以追溯到70年代和80年代[13,22,32,25,10]。关于这一主题的早期调查出现在[31]。根据[31]，为了获得边界表示(Brep)格式的3D对象，现有方法采用四阶段方案，在前几步的结果基础上逐步构建3D顶点、边、面和块。如前所述，<strong>该框架的一个关键优势是可以找到与输入视图完全匹配的所有可能的解决方案</strong>。<br>该任务的后续方法[37,38,28,18,21,8,9]也遵循相同的步骤，并侧重于扩展方法的适用范围以覆盖更多类型的对象。</p><ul><li>例如，Shin和Shin[28]开发了一种方法来重建由平面和有限二次面组成的物体，如圆柱体和环面，它们平行于一个主轴。</li><li>Liu等[21]为了消除对曲面轴线的限制，设计了一种将二次曲线的几何性质与仿射性质相结合的算法。</li><li>后来，Gong等人[9]提出通过链接关系图(Link-Relation Graph, LRG)中基于提示的模式匹配来识别二次曲面特征，将适用范围扩大到包括具有交互二次曲面的对象。</li><li><strong>然而，所有这些方法都假设输入是干净的，并且正如我们将在实验部分所示，在存在误差和噪声的情况下很容易崩溃</strong>。</li></ul><p>最近，Han等人[12]也训练了一个深度网络来从三个正射视图重建3D模型。<strong>然而，他们的方法以栅格图像作为输入，并产生非结构化点云格式的结果，这在CAD建模应用中几乎没有用处</strong>。相比之下，我们的方法直接使用矢量化的线条图作为输入，并生成结构化的CAD模型作为输出。</p><p><strong>Deep generative models for CAD</strong> 随着ABC[17]和Fusion 360 Gallery[34]等大规模CAD数据集的可用性，最近的一系列工作训练深度网络以2D草图[33,7,27]或3D模型[35,14,11,36]的形式生成结构化CAD数据。这些方法都将其视为序列生成问题，<strong>但用于产生输出的DSLs有所不同</strong>。我们通过将木板模型组装在一起来生成橱柜家具的想法受到ShapeAssembly[15]的启发，ShapeAssembly学习以分层3D零件图的形式生成对象。<strong>然而，与上述研究关注生成模型本身不同，我们提出利用生成模型从三个正射影视图构建有效的三维CAD模型重建方法</strong>。</p><h1 id="A-Simple-Assembly-Language-for-Cabinets"><a href="#A-Simple-Assembly-Language-for-Cabinets" class="headerlink" title="A Simple Assembly Language for Cabinets"></a>A Simple Assembly Language for Cabinets</h1><p>在本节中，我们的目标是为感兴趣的形状(即橱柜家具)定义特定于领域的语言domain-specific language(DSL)。使用这种语言，每个机柜模型都可以用形状程序表示，然后将其转换为标记序列，<strong>作为基于transformer的seq2seq模型的输出</strong>。</p><p>我们以类似于人类设计师在3D建模软件中构建模型的方式来定义我们的DSL。如图2所示，一个机柜通常是由一列木板模型组装而成的。实际上，大多数木板都是轴线排列的长方体。因此，我们使用长方体作为我们语言中唯一的数据类型。在第6节中，我们将讨论如何扩展我们的方法以适应更复杂的形状(例如，具有非矩形轮廓的木板)。</p><p>轴向长方体有6个自由度(DOF)，分别对应三个轴的起始和结束坐标:<br>$\operatorname{Cuboid}(x_{\min},y_{\min},z_{\min},x_{\max},y_{\max},z_{\max}).$ Eq.1</p><p>在实践中，人类设计人员经常使用附件操作attachment operation，而不是为所有坐标指定数值。作为几何约束的一种形式，使用附件的好处至少是双重的:</p><ul><li>首先，它使用户能够快速指定木板的位置，而无需显式计算(一些)其坐标;</li><li>其次，它促进了未来的编辑，因为对木板所做的任何更改都将自动传播到其他木板。</li><li>以图2为例。当添加一个木板(以蓝色突出显示)时，设计师可以将它的四个侧面附加到现有的木板上(包括不可见的边界框)，同时以数值指定到顶部和底部的距离。</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230813161422.png" alt="image.png"></p><p>我们的语言采用编程语言(如c++)中常用的union结构，支持通过数值或附加操作来指定木板坐标。如右图所示，式(1)中的六个坐标可以是数值，也可以是指向另一个长方体(它所依附的长方体)对应坐标的指针。图3显示了一个通过强制执行程序命令(程序1)逐步构建的示例机柜。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">union</span> <span class="title class_">Coord</span>&#123; <span class="type">float</span> v; <span class="type">float</span>* p; &#125;;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230813162814.png" alt="image.png"></p><p>程序1：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bbox = <span class="built_in">Cuboid</span>(<span class="number">-0.35</span>, <span class="number">-0.23</span>, <span class="number">-0.76</span>, <span class="number">0.35</span>, <span class="number">0.23</span>, <span class="number">0.76</span>)</span><br><span class="line">plank1 = <span class="built_in">Cuboid</span>(bbox1 , bbox2 , bbox3 , <span class="number">-0.34</span>, bbox5, bbox6 )</span><br><span class="line">plank2 = <span class="built_in">Cuboid</span>(<span class="number">0.34</span>, bbox2 , bbox3 , bbox4 , bbox5 , bbox6 )</span><br><span class="line">plank3 = <span class="built_in">Cuboid</span>(plank14 , bbox2 , <span class="number">-0.70</span>, plank21 , bbox5 , <span class="number">-0.69</span>)</span><br><span class="line">plank4 = <span class="built_in">Cuboid</span>(plank14 , bbox2 , <span class="number">0.75</span>, plank21 , bbox5 , bbox6 )</span><br><span class="line">plank5 = <span class="built_in">Cuboid</span>(plank14 , <span class="number">0.21</span>, plank36 , plank21 , <span class="number">0.22</span>, plank43 )</span><br><span class="line">plank6 = <span class="built_in">Cuboid</span>(plank14 , bbox2 , bbox3 , plank21, <span class="number">-0.21</span>, plank33 )</span><br><span class="line">plank7 = <span class="built_in">Cuboid</span>(plank14 , <span class="number">0.21</span>, bbox3 , plank21 , bbox5 , plank33 )</span><br></pre></td></tr></table></figure><p><strong>Shape program as a DAG</strong></p><p>或者，我们可以将形状程序解释为有向无环图(DAG)。请注意，每个平板模型由六个面组成，其中每个面正好对应于轴向长方体中的一个自由度$(i.e.,{x_\mathrm{min}},y_\mathrm{min},z_\mathrm{min},x_\mathrm{max},y_\mathrm{max},z_\mathrm{max}).$。因此，每个程序都可以用图$\mathcal{G}=\{\mathcal{F},\mathcal{E}\},$来表征，其顶点$\mathcal{F}=\{f_1,\ldots,f_{|\mathcal{F}|}\}$表示木板模型的面，其边$\mathcal{E}=\{e_{1},\ldots,e_{|\mathcal{E}|}\}$表示面之间的依恋关系。每个有向边$e_{i{\to}j}$都是一对有序的顶点$(f_i,f_j)$，表示第i个面$f_{i}$连接到第j个面$f_{j}$。我们假设每个面最多可以与另一个面相连;也就是说，任何面$f_{i}$的输出度最多为1。进一步，这些边可以用邻接矩阵$A\in\mathbb{R}^{|\mathcal{F}|\times|\mathcal{F}|}$来表示。具体来说，如果$f_{i}$指向$f_{j}$, $A_{ij}$为1，否则为0。</p><h1 id="The-PlankAssembly-Model"><a href="#The-PlankAssembly-Model" class="headerlink" title="The PlankAssembly Model"></a>The PlankAssembly Model</h1><p>如图1所示，我们假设输入由物体的三个正射影组成，即前视图、俯视图和侧视图:$\mathcal{V}=\{V_{F},V_{T},V_{S}\}$。每个视图都可以看作是一个二维边和边相交节点的平面图。我们用实线表示可见的边缘，虚线表示隐藏的边缘。我们的目标是重建由形状程序或等效DAG $\mathcal{G}$描述的三维机柜模型。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230813164320.png" alt="image.png"></p><p>在本文中，我们把三维重建视为一个seq2seq问题。在4.1节和4.2节中，我们描述了如何将输入视图$\mathcal{V}$和形状程序$\mathcal{G}$分别编码为1D序列$\mathcal{V}^{seq}$和$\mathcal{G}^{seq}$。然后，我们在4.3节中介绍了我们的浮游装配模型的设计，该模型采用基于变压器的编码器-解码器架构来学习概率分布$p(\mathcal{G}^\mathrm{seq}\mid\mathcal{V}^\mathrm{seq}),$。最后，我们将在4.4节中介绍实现细节。</p><h2 id="Input-Sequences-and-Embeddings"><a href="#Input-Sequences-and-Embeddings" class="headerlink" title="Input Sequences and Embeddings"></a>Input Sequences and Embeddings</h2><p>对于输入条件，我们首先按视图$\mathcal{V}$对2D边进行排序。每条二维边都写成$(x_{1},y_{1},x_{2},y_{2})$，其中我们按照x坐标从低到高排序它的两个端点，然后是y坐标(如果$x_1 = x_2$)。然后，我们用$x_1, x_2, y_1, y_2$来排序一组二维边。接下来，我们将所有边平铺成1D序列$\mathcal{V}^{\mathrm{seq}}=\{v_{1},\ldots,v_{N_{v}}\}.$。注意，由于每条2D边都有四个自由度(即两个端点的x坐标和y坐标)，所以$\mathcal{V}_{\mathrm{seq}}$的长度为$N_{v}=4N_{\mathrm{edge}}$，其中$N_{\mathrm{edge}}$是所有三个正射影视图中2D边的总数。<br>我们将第 i 个标记 $v_{i}$ 嵌入为</p><p>$\begin{aligned}E(v_i)=E_{\mathrm{value}}(v_i)+E_{\mathrm{view}}(v_i)+E_{\mathrm{cdge}}(v_i)\+E_{\mathrm{coord}}(v_i)+E_{\mathrm{type}}(v_i),\end{aligned}$ Eq.2</p><ul><li>值嵌入$E_{\mathrm{value}}$表示token令牌的量化坐标值，</li><li>视图嵌入$E_{\mathrm{view}}$表示2D边缘来自哪个视图(即前视图、前视图或侧视图)，</li><li>边缘嵌入$E_{\mathrm{edge}}$表示2D边缘在对应视图中的相对位置，</li><li>坐标嵌入$E_{\mathrm{coord}}$表示坐标在对应2D边缘的相对位置。</li><li>最后，我们使用类型嵌入 Etype 来表示 2D 边缘是否可见或隐藏。</li></ul><p>在本文中，我们将坐标值量化为 9 位整数，并为方程式中的每一项使用学习的 512-D 嵌入。 (2)。</p><h2 id="Output-Sequences-and-Embeddings"><a href="#Output-Sequences-and-Embeddings" class="headerlink" title="Output Sequences and Embeddings"></a>Output Sequences and Embeddings</h2><p>为了按顺序生成形状程序，我们需要将图 G 映射到序列 $\mathcal{G}^{\mathrm{seq}}$。这需要我们在 G 上定义一个顶点顺序 π：</p><ul><li>我们首先在拓扑上对顶点进行排序，确保直接后继者在其相应的直接前辈之前列出。</li><li>然后，不直接连接的顶点按坐标值排序。</li></ul><p>这给了我们一个排序图$\mathcal{G}^{\pi}$，其顶点 $\mathcal{F}^{\pi}$遵循顺序 π。</p><p>由于我们想捕获建模过程并促进未来的编辑，我们将附件关系优先于几何实体。与输入序列编码类似，我们展平 $\mathcal{G}^{\pi}$ 以获得一维序列 $\mathcal{G}^{seq}$。序列 $\mathcal{G}^{seq}$ 的第 i 个元素可以得到：</p><p>$g_i=\begin{cases}f_i^\pi,&amp;\text{if}A_{ij}^\pi=0,\forall j,\\e_{i\to j}^\pi,&amp;\text{if}A_{ij}^\pi=1.\end{cases}$ Eq.3</p><p>此外，我们使用两个特殊标记 [SOS] 和 [EOS] 分别表示输出序列的开始和结束</p><p>对于我们模型的解码器的输入，我们使用相关的面 $f_{i}^{\pi}$ 嵌入令牌 $g_{i}$，如下所示：<br>$E(g_i)=E(f_i^\pi)=E_{\text{value}}(f_i^\pi)+E_{\text{plank}}(f_i^\pi)+E_{\text{face}}(f_i^\pi).$ Eq.4</p><ul><li>值嵌入 $E_{\mathrm{value}}$ 表示量化的坐标值，该值用于输入和输出序列。</li><li>plank 嵌入 $E_{\mathrm{plank}}$表示柜模型中相应plank的位置，</li><li>面嵌入$E_{\mathrm{face}}$ 表示面在plank内的相对位置。</li></ul><p>如果令牌对应于 G 中的一条边 $e_{i\to j}^{\pi}$，我们将识别当前面$f_{i}^{\pi}$附加的人脸 $f_{j}^{\pi}$，并使用与$f_{j}^{\pi}$相同的值嵌入。</p><h2 id="Model-Design"><a href="#Model-Design" class="headerlink" title="Model Design"></a>Model Design</h2><p>为了解决这个 seq2seq 问题，我们将输出序列上的联合分布分解为一系列条件分布：<br>$p(\mathcal{G}^{\mathrm{seq}}\mid\mathcal{V}^{\mathrm{seq}})=\prod_{t}p\left(g_{t}\mid\mathcal{G}_{&lt;t}^{\mathrm{seq}},\mathcal{V}^{\mathrm{seq}}\right).$ Eq.5</p><p>在这里，由于 $g_t$ 可以采用几何实体（即$f_{i}^{\pi}$ ）或附件关系（即 $e_{i\to j}^{\pi}$）的形式，我们需要在固定长度的词汇表集（量化坐标值）加上输出序列中的可变长度标记集 $\mathcal{G}_{&lt;t}^{\mathrm{seq}}$上产生概率分布。</p><p>前者分布是一个分类分布，通常用于分类任务。设$\mathbf{h}_t$是解码器在时间 t 获得的隐藏特征，我们通过线性层将其投影到词汇表的大小，然后将其归一化以形成有效分布：</p><p>$p_{\mathrm{vocab}}(g_{t}\mid\mathcal{G}_{&lt;t}^{\mathrm{seq}},\mathcal{V}^{\mathrm{seq}})=\mathrm{softmax}\left(\mathrm{linear}\left(\mathbf{h}_{t}\right)\right).$ Eq.6</p><p>为了生成输出序列 $\mathcal{G}_{&lt;t}^{\mathrm{seq}}$ 在时间t 上的分布，我们采用指针网络 [30]。具体来说，我们首先使用线性层来预测指针。然后，通过点积将指针与所有前一步的隐藏特征进行比较。最后，输出序列上的分布是通过 softmax 层获得的：</p><p>$p_\mathrm{attach}(g_t\to g_k\mid\mathcal{G}_{&lt;t}^\mathrm{seq},\mathcal{V}^\mathrm{seq})=\mathrm{softmax}_k\left(\mathrm{linear}\left(\mathbf{h}_t\right)^T\mathbf{h}_{&lt;t}\right).$ Eq.7</p><p>我们没有直接比较这两个分布，而是遵循 Pointer-Generator Networks [26] 并引入附件概率 $w_{t}$ 来加权这两个分布。附件概率 $w_{t}$ 是通过线性层和 sigmoid 函数 σ(·) 获得的：$\sigma(\cdot) : w_{t}=\sigma(linear(\mathbf{h}_t))$。因此，最终分布是两个加权分布的串联concatenation：<br>$p(g_t\mid\mathcal{G}_{&lt;t}^{\text{seq}},\mathcal{V}^{\text{seq}})=\text{concat}\left\{(1-w_t)\cdot p_{\text{vocab}},w_t\cdot p_{\text{attach}}\right\}.$ Eq.8</p><p>最后，给定一个训练集，可以通过标准的交叉熵损失最大化条件分布Eq.(8)来学习模型的参数。</p><p><strong>Network architecture</strong></p><p>我们使用标准 Transformer 块 [29] 作为我们 PlankAssembly 模型的基本块。给定输入嵌入 $\{E(v_1),E(v_2),\ldots\},$编码器将它们编码为上下文嵌入。在解码时间 t，解码器根据上下文嵌入和解码器输入 $\{E(g_{1}),E(g_{2}),\ldots\}.$生成隐藏特征 $\mathbf{h}_{t}$。我们为编码器和解码器使用 6 个 Transformer 层。每层的前馈维度为 1024 和 8 个注意力头。网络架构如图 4 所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230813205510.png" alt="image.png"></p><h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><p>Training。我们使用 PyTorch Lightning [1] 实现我们的模型。我们为编码器和解码器使用 6 个 Transformer 层。每层的前馈维度为 1024 和 8 个注意力头。该网络在四个NVIDIA RTX 3090 GPU设备上训练了400K次迭代。我们使用学习率为 10−4 的 Adam 优化器 [16]。每个 GPU 的批量大小设置为 16。</p><p>Inference。在推理时，我们采取几个步骤来确保我们模型中的有效预测。首先，我们观察到两个附加面必须对应于同一轴上的相反自由度，以避免任何空间冲突。例如，一个 plank 的 $x_min$ 标记只能指向另一个 plank 的 $x_max$ 标记，反之亦然。因此，我们在推理过程中屏蔽所有无效位置。其次，我们过滤掉体积为零的预测计划。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h2><p>数据集。我们为此任务创建了一个大规模的基准数据集，利用来自<a href="https://www.kujiale.com/">Kujiale1</a>的大型橱柜家具模型存储库，这是一个内部设计行业的在线 3D 建模平台。存储库中的<strong>大多数模型都是由专业设计师使用商业参数建模软件创建的，并用于现实世界的生产</strong>。<br>几个规则用于过滤数据：<br>(i) 我们根据三个正字法orthographic视图的相似性删除重复的 3D 模型；<br>(ii) 我们排除了少于四个 planks 的模型，超过 20 个 planks，或总共超过 300 个边缘。<br>其余<strong>数据被随机分成三部分</strong>：24039 个用于训练，1329 个用于验证，1339 个用于测试。<br>为了合成三个正字法视图，我们使用来自pythonOCC[3]的HLRBRep Algo API，该API建立在Open CASCADE技术建模内核[2]之上。</p><p><strong>对于我们的任务，我们需要将每个参数柜模型解析为形状程序</strong>。我们首先提取橱柜模型中的几何实体来获得planks。请注意，在参数建模软件中，通常通过首先绘制 2D 轮廓然后应用挤压命令来创建 plank。因此，我们将每个 plank 的面分为侧面或端面，这取决于它们是否沿挤压方向。然后，给定来自两个不同planks的一对face，我们认为如果 (i) 两个face在 1mm 的距离阈值内，则存在附件关系，并且 (ii) 该对由一个侧面和一个结束面组成。最后，在G中添加了从端面到侧面的有向边</p><p>Evaluation metrics 为了评估 3D 重建结果的质量，我们使用了三个标准指标：精度precision、召回率recall和 F1 分数F1 score。<br>具体来说，对于橱柜模型，我们使用匈牙利匹配来匹配预测的planks和ground truth planks。如果具有一个gt的 3D intersection-over-union (IOU) 大于 0.5，则该预测被认为是真阳性。</p><h2 id="Comparison-to-Traditional-Methods"><a href="#Comparison-to-Traditional-Methods" class="headerlink" title="Comparison to Traditional Methods"></a>Comparison to Traditional Methods</h2><p>在本节中，我们系统地将我们的方法与从三个正字法视图进行 3D 重建的传统方法进行了比较。由于没有公开实现传统管道，我们通过紧跟之前的工作 [25, 28] 重新实现管道。回想一下，从输入视图开始，传统的管道逐步生成 3D 顶点、3D 边、3D 面和 3D 块。然后，通过枚举候选块的所有组合并检查它们的 2D 投影是否与输入视图匹配来找到解决方案。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
            <tag> Transformer </tag>
            
            <tag> CAD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Critical Analysis of NeRF-Based 3D Reconstruction</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Review/A%20Critical%20Analysis%20of%20NeRF-Based%203D%20Reconstruction/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Review/A%20Critical%20Analysis%20of%20NeRF-Based%203D%20Reconstruction/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>A Critical Analysis of NeRF-Based 3D Reconstruction</th></tr></thead><tbody><tr><td>Author</td><td>Fabio Remondino , Ali Karami  , Ziyang Yan, Gabriele Mazzacca , Simone Rigon and Rongjun Qin</td></tr><tr><td>Conf/Jour</td><td>MDPI remote sensing</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://www.mdpi.com/2072-4292/15/14/3585">A Critical Analysis of NeRF-Based 3D Reconstruction</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4787981282168537089&amp;noteId=1912892973790366720">A Critical Analysis of NeRF-Based 3D Reconstruction (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812171053.png" alt="image.png"></p><p>对比photogrammetry与NeRF在3D Reconstruction中的表现，提供数据集<a href="https://github.com/3DOM-FBK/NeRFBK">Github_NeRFBK</a></p><ul><li>photogrammetry(Colmap)<ul><li>对无纹理物体重建效果很差(non-collaborative surfaces的3D测量)，例如镜面反射物体<ul><li>非协作表面: 反射、无纹理等</li></ul></li></ul></li><li>NeRF(InstantNGP | NerfStudio | SDFstudio)</li></ul><p>结论：</p><ul><li>在传统摄影测量方法失败或产生嘈杂结果的情况下，例如无纹理、金属、高反射和透明物体，NeRF优于摄影测量</li><li>对于纹理良好和部分纹理的物体，摄影测量仍然表现更好</li></ul><p>该研究为NeRF在不同现实场景中的适用性提供了有价值的见解，特别是在遗产和工业场景中，这些场景的表面可能特别具有挑战性，未来的研究可以探索NeRF和摄影测量的结合，以提高具有挑战性场景下三维重建的质量和效率</p><span id="more"></span><h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>本文对神经辐射场(NeRF)方法在基于图像的三维重建中的应用进行了全面分析。与传统摄影测量进行比较，报告定量和视觉结果，以了解在处理多种类型的表面和场景时的优缺点。<br>该研究客观地评估了nerf生成的3D数据的优缺点，并深入了解了它们在不同现实场景和应用中的适用性。<br>该研究采用了一系列纹理良好的、无纹理的、金属的、半透明的和透明的物体，使用不同的比例和图像集进行成像。使用各种评估方法和指标对生成的基于nerf的3D数据的质量进行评估，包括噪声水平、表面偏差、几何精度和完整性。<br>报告的结果表明，<strong>在传统摄影测量方法失败或产生嘈杂结果的情况下，例如无纹理、金属、高反射和透明物体，NeRF优于摄影测量</strong>。相比之下，<strong>对于纹理良好和部分纹理的物体，摄影测量仍然表现更好</strong>。这是因为基于NeRF的方法能够生成与反射率和透明度相关的几何形状，这是由于NeRF模型依赖于视图的特性。</p><p><strong>该研究为NeRF在不同现实场景中的适用性提供了有价值的见解，特别是在遗产和工业场景中，这些场景的表面可能特别具有挑战性</strong>。更多的数据集正在准备中，并将很快在<a href="https://github.com/3DOM-FBK/NeRFBK">Github_NeRFBK</a>上共享[103]。研究结果突出了NeRF和摄影测量的潜力和局限性，为该领域的后续研究奠定了基础。未来的研究可以探索NeRF和摄影测量的结合，以提高具有挑战性场景下三维重建的质量和效率。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>摘要:本文对基于图像的三维重建进行了批判性分析，并将其与传统摄影测量法photogrammetry进行了定量比较。因此，目的是客观地评估NeRF的优势和劣势，并深入了解其在不同现实场景中的适用性，从小型物体到遗产和工业场景。在全面概述摄影测量和NeRF方法，突出各自的优点和缺点后，使用不同尺寸和表面特征的不同物体，包括无纹理，金属，半透明和透明表面，对各种NeRF方法进行比较。<br>我们使用多种标准评估生成的3D重建的质量，例如噪声水平、几何精度和所需图像的数量(即图像基线)。<br>结果表明，<strong>在具有无纹理、反射和折射表面的非协作对象方面</strong>，NeRF表现出优于摄影测量的性能。<br>相反，<strong>在物体表面具有协同纹理的情况下</strong>，摄影测量优于NeRF。这种互补性应在今后的工作中进一步加以利用。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在计算机视觉和摄影测量领域中，高质量的三维重建是一个重要的课题，在质量检测、逆向工程、结构监测、数字保存等方面有着广泛的应用。然而，提供高几何精度和高分辨率细节的低成本，便携式和灵活的3D测量技术多年来一直需求量很大。<br><strong>现有的三维重建方法大致可分为接触式和非接触式两大类</strong>[1]。</p><ul><li>为了确定物体的精确3D形状，<strong>基于接触的技术</strong>通常使用物理工具，如卡尺或坐标测量机。虽然精确的几何3D测量是可行的，并且非常适合于许多应用，但它们确实有一些缺点，例如获取数据和执行稀疏3D重建所需的时间长度，测量系统的局限性，和/或需要昂贵的仪器，这<strong>限制了它们在具有独特计量规范的专业实验室和项目中的使用</strong>。</li><li>另一方面，非接触式技术允许精确的3D重建，而没有相关的缺点。大多数研究人员都专注于被动图像方法，因为它们的低成本，便携性和灵活性在广泛的应用领域，包括工业检测和质量控制[2-5]以及遗产heritage 3D文档[6-9]。</li></ul><p>在基于图像的三维重建方法中，<strong>摄影测量photogrammetry</strong>是一种被广泛认可的方法，它可以从不同角度拍摄的一组图像中创建真实场景的密集和几何精确的三维点云。摄影测量可以处理从室内到室外环境的各种场景，并且在多个项目中具有许多商业和开源工具[10,11]。<strong>然而，摄影测量有其局限性</strong>，特别是当涉及到非协作表面的3D测量时，由于其对物体纹理属性的敏感性，并且它可能难以生成非常详细的3D重建。例如，图像中镜面反射的存在会导致高反射和弱纹理物体的噪声结果，而透明物体由于折射和镜面反射引起的纹理变化会带来重大挑战[12-15]。</p><p>最近，一种基于神经辐射场(Neural Radiance Fields, NERF)的图像数据集三维重建的新方法引起了研究界的极大关注[16 - 22]。该方法通过优化一组定向图像的连续场景函数，能够生成复杂场景的新视图。NeRF的工作原理是训练一个完全连接的网络，称为神经辐射场，通过使用渲染损失来复制场景的输入视图(图1)。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812154804.png" alt="image.png|666"></p><p>如图1所示，神经网络以一组由空间位置(x, y, z)和观看方向(θ， φ)组成的连续5D坐标作为输入，输出每个点在每个方向上的体积密度(σ)和与视图相关的发射亮度(RGB)。然后从某个角度渲染NeRF，并且可以导出3D几何形状，例如，通过行进相机光线以网格的形式[23]Marching cubes。</p><p><strong>尽管它们最近很受欢迎，但是，与更传统的摄影测量相比，仍然需要对基于NeRF的方法进行批判性分析，以便客观地量化所产生的3D模型的质量，并充分了解它们的优势和局限性</strong>。</p><p>NeRF方法最近在基于图像的3D重建领域成为摄影测量和计算机视觉的一种有前途的替代方法。因此，<strong>本研究旨在深入分析NeRF方法用于3D重建目的</strong>。我们评估了使用基于NeRF的技术和通过摄影测量在尺寸和表面特征(纹理良好，无纹理，金属，半透明和透明)的各种物体上生成的3D重建的准确性。我们根据表面偏差(噪声水平)和几何精度检查了每种技术产生的数据。<strong>最终目的是评估NeRF方法在现实场景中的适用性，并提供关于基于NeRF的3D重建方法的优势和局限性的客观评估指标</strong></p><p>本文组织如下:第2节介绍了以前使用基于摄影测量和基于NeRF的方法进行3D重建的研究活动的概述。第3节介绍了建议的质量评估管道和使用的数据集，而第4节报告了评估和比较结果。最后，在第5节给出结论和未来的研究计划。</p><h2 id="The-State-of-the-Art-SOTA"><a href="#The-State-of-the-Art-SOTA" class="headerlink" title="The State of the Art(SOTA)"></a>The State of the Art(SOTA)</h2><p>在本节中，全面概述了之前的3D重建研究，结合了摄影测量和基于NeRF的方法，并考虑了它们在非协作表面(反射、无纹理等)上的应用。</p><h3 id="Photogrammetric-Based-Methods"><a href="#Photogrammetric-Based-Methods" class="headerlink" title="Photogrammetric-Based Methods"></a>Photogrammetric-Based Methods</h3><p>摄影测量是一种被广泛接受的对纹理良好的物体进行三维建模的方法，它能够通过多视图立体(<strong>MVS</strong>)方法准确可靠地恢复物体的三维形状。基于摄影测量的方法[19,24 - 30]<strong>要么依靠特征匹配进行深度估计</strong>[27,28]，<strong>要么使用体素来表示形状</strong>[24,29,31,32]。<strong>也可以使用基于学习的MVS方法</strong>，但它们通常会取代经典MVS管道的某些部分，例如特征匹配[33 - 36]，深度融合[37,38]或多视图图像深度推断[39 - 41]。<strong>然而，具有无纹理、反射或折射表面的物体重建具有挑战性</strong>，因为所有摄影测量方法都需要在多个图像之间匹配对应关系[14]。<br><strong>为了解决这个问题</strong>，已经开发了各种摄影测量方法来重建这些非协作对象。对于无纹理的对象，已经<strong>提出了随机模式投影</strong>[13,42,43]或<strong>合成模式</strong>[14,44]等解决方案。<strong>然而，这些方法难以处理具有强镜面反射或互反射的高反射表面</strong>[43]。其他方法，如<strong>交叉极化</strong>[7,45]和<strong>图像预处理</strong>[46,47]已用于反射或非协同表面，<strong>但一些技术可能会平滑表面粗糙度并影响视图间的纹理一致性</strong>[48,49]。<br>摄影测量也用于混合方法[50 -53]，其中MVS方法用于生成稀疏的3D形状，可以作为使用光度立体(PS)进行高分辨率测量的基础。传统的[52,54,55]和基于学习的[56 - 58]PS方法也用于理解图像辐照度方程和检索成像物体的几何形状，<strong>但镜面仍然是所有基于图像的方法的挑战</strong>。</p><h3 id="NeRF-Based-Methods"><a href="#NeRF-Based-Methods" class="headerlink" title="NeRF-Based Methods"></a>NeRF-Based Methods</h3><p>合成逼真的图像和视频是计算机图形学的核心，也是几十年来研究的焦点[59]。神经渲染是一种基于学习的图像和视频生成方法，用于控制场景属性(例如，照明，相机参数，姿势，几何形状，外观等)。<strong>神经渲染将深度学习方法与计算机图形学的物理知识相结合，以实现可控和逼真的场景(3D)模型</strong>。其中，由Mildenhall等人于2020年首次提出的NeRF是一种使用隐式表示呈现新视图并重建3D场景的方法(图1)。在NeRF方法中，使用神经网络从2D图像中学习物体的3D形状。如式(1)所定义的亮度场，从每个可能的观看方向捕获场景中每个点的颜色和体积密度:<br>$F(X,d)====&gt;(c,\sigma)$ Eq.1</p><p>NeRF模型使用神经网络表示，其中X表示图像的三维坐标，d表示方位角和极视角，c表示颜色，σ表示场景的体积密度。为了保证多视角的一致性，σ的预测被设计成与观察方向无关，而颜色c可以根据观察方向和位置而变化。为了实现这一点，多层感知器(MLP)分两步使用。在第一步中，MLP以X作为输入，同时输出σ和高维特征向量。然后将特征向量与观察方向d结合，并通过另一个MLP，产生颜色表示c。</p><p>最初的NeRF实现以及随后的方法使用了不确定性分层抽样方法，如式(2)-(4)所示。这种方法包括将射线分成N个等间隔的箱子，并从每个箱子均匀地抽取一个样本:<br>$C(r)=\sum_{i=1}^N\alpha_iT_ic_i$ Eq.2</p><p>$T_i=e^{-\sum_{j=1}^{i-1}\sigma_j\delta_j}$ Eq.3</p><p>$\alpha_i=1-e^{\sigma_i\delta_i}$ Eq.4</p><p>其中$δ_i$表示连续样本(i和i + 1)之间的距离，$σ_i$和$c_i$表示沿样本点(i)的估计密度和颜色值。透明度或不透明度$α_i$在样本点(i)也使用式(4)计算。</p><p>连续的方法[60 - 62]也纳入了<strong>估计的深度</strong>，如式(5)所示，对密度施加限制，使它们类似于场景表面的类delta函数，或者在深度上强制平滑:</p><p>$D(r)=\sum_{i=1}^{N}\alpha_{i}t_{i}T_{i}$ Eq.5</p><p>为了优化MLP参数，对每个像素使用平方误差光度损失: $M=\sum_{r\in R}|C-C_{gt}|_{2}^{2}$</p><p>其中，变量$C_{gt}(R)$表示训练图像中与射线R对应的像素的地面真色，R表示与待合成图像相关联的射线批次。需要注意的是，NeRF的隐式3D表示被指定用于视图渲染。<strong>为了获得明确的三维几何图形，需要通过对每条光线取深度分布的最大似然来提取不同视图的深度图。然后可以将这些深度图融合以导出点云或输入Marching Cube[23]算法以导出3D网格</strong>。</p><p>尽管NeRF为3D重建提供了一种替代传统摄影测量方法的解决方案，并且可以在摄影测量可能无法提供准确结果的情况下产生有希望的结果，但正如不同作者所报道的那样，<strong>它仍然面临一些局限性</strong>[63 -68]。从3D计量的角度来看，需要考虑的一些主要问题包括:</p><ul><li>生成的神经渲染图(随后转换为3D网格)的分辨率可能受到输入数据的质量和分辨率的限制。一般来说，<strong>更高分辨率的输入数据将产生更高分辨率的3D网格，但代价是增加了计算需求</strong>。</li><li>使用NeRF生成神经渲染(然后是3D网格)可能是计算密集型的，<strong>需要大量的计算能力和内存</strong>。</li><li><strong>一般无法准确地模拟非刚性物体的三维形状</strong>。</li><li>原始NeRF模型是基于每像素RGB重建损失进行优化的，当仅使用RGB图像作为输入时，存在无限数量的与照片一致的解释，这<strong>可能导致噪声重建</strong>。</li><li>NeRF通常需要大量具有小基线的输入图像来生成精确的3D网格，特别是对于具有复杂几何或遮挡的场景。<strong>在难以获取图像或计算资源有限的情况下，这可能是一个挑战</strong>。</li></ul><p>针对上述问题，<strong>研究人员对原始NeRF方法进行了一些修改和扩展</strong>，以提高性能和3D结果。</p><ul><li>Tancik等人[69]和Sitzmann等人[70]在NeRF高频表示能力不足的情况下，为了提高神经渲染结果的分辨率，<strong>采用了与nerf不同频率的位置编码操作</strong>。</li><li>在此之后，其他方法<strong>侧重于以不同的方式提高神经渲染结果的效率和分辨率</strong>，包括模型加速[20,71]、压缩[72 -74]、重光照[75 -77]和视图依赖归一化[78]，或高分辨率2D特征平面[68]。<ul><li>Müller等人[20]InstantNGP 引入了具有多分辨率哈希编码的即时神经图形原语的概念，该概念允许快速高效地生成3D模型。</li><li>Barron等人[64,79]提出Mip-NeRF是原始NeRF的修改版本，允许在连续值尺度上表示场景, Mip-NeRF通过有效地渲染抗锯齿圆锥形截体而不是射线，大大增加了NeRF强调细节的能力。<strong>然而，该方法的局限性可能包括训练困难和计算效率问题</strong>。</li><li>Chen等人[72]提出了一种名为TensoRF的新方法，用于将场景的亮度场建模和重建为4D张量。这种方法表示具有每体素多通道特征的3D体素网格。<strong>除了提供卓越的渲染质量之外，与以前和现代的方法相比，这种方法实现了更低的内存使用</strong>。</li><li>Yang等人[80]提出了一种基于融合的方法，称为PS-NeRF，将NeRF的优势与光度立体方法相结合。<strong>该方法旨在通过利用NeRF重建场景的能力来解决传统光度立体技术的局限性，最终提高所得网格的分辨率</strong>。</li><li>Reiser等人[68]引入了Memory-Efficient Radiance Field (MERF)表示，通过利用稀疏特征网格和高分辨率2D特征平面，<strong>可以快速渲染大规模场景</strong>。</li><li>Li等人[21]介绍了Neuralangelo，这是一种利用多分辨率3D散列网格和神经表面渲染的创新方法，<strong>在从多视图图像中恢复密集的3D表面结构方面取得了优异的效果</strong>，<strong>可以从RGB视频捕获中实现非常详细的大规模场景重建</strong>。</li></ul></li><li>已经提出了一些方法[67,81 - 85]，将NeRF扩展到<strong>动态领域</strong>。这些方法使得重建和渲染物体的图像成为可能，而它们正在经历刚性和非刚性运动，从一个在场景中移动的相机。<ul><li>例如，Yan等人[84]引入了表面感知的动态NeRF (NeRF- DS)和掩模引导的变形场。通过将表面位置和方向作为神经辐射场函数的调节因素，<strong>NeRF-DS改善了高光表面复杂反射特性的表征</strong>。此外，<strong>使用掩模引导的变形场使NeRF-DS能够有效地处理物体运动过程中发生的大变形和遮挡</strong></li></ul></li><li>为了<strong>提高</strong>存在噪声的三维重建的<strong>精度</strong>，<strong>特别是对于光滑和无纹理的表面</strong>，一些研究将各种<strong>先验纳入优化过程</strong>。<ul><li>这些先验包括语义相似度[86]、深度平滑度[60]、表面平滑度[87,88]、曼哈顿世界假设[89]和单目几何先验[90]。</li><li>相比之下，Bian等人[91]提出的NoPe-NeRF方法使用单深度映射来约束帧之间的相对姿态并规范NeRF的几何形状。<strong>该方法具有较好的姿态估计效果，提高了新视图合成和几何重建的质量</strong>。</li><li>Rakotosaona等人[92]为3D表面重建引入了一种新颖且通用的架构，该架构有效地将nerf驱动方法中的体积表示提取到签名表面近似网络中。<strong>这种方法可以提取精确的3D网格和外观，同时保持跨各种设备的实时渲染能力</strong>。</li><li>Elsner等人[93]提出了自适应Voronoi nerf，这是一种通过使用Voronoi图将场景划分为单元来提高处理效率的技术。<strong>这些细胞随后被细分，以有效地捕获和表示复杂的细节，从而提高性能和准确性</strong>。</li><li>类似地，Kulhanek和Sattler[94]引入了一种名为tera-NeRF的新的亮度场表示，它成功地适应了作为稀疏点云给出的3D几何先验，以获取更多细节。<strong>然而，值得注意的是，渲染场景的质量可能会根据不同区域点云的密度而有所不同</strong>。</li></ul></li><li>一些工作旨在<strong>减少输入图像的数量</strong>[60,70,78,86,90,95]。<ul><li>Yu等人[95]提出了一种架构PixelNeRF，该架构使用全卷积方法对图像输入条件NeRF，<strong>使网络能够在对多个场景进行训练之前学习一个场景</strong>。这允许它从少量视点执行前馈视图合成，甚至少到只有一个视点。</li><li>类似地，Niemeyer等人[60]引入了一种方法RegNeRF，<strong>对未见视图进行采样，并对这些视图生成的斑块的外观和几何形状进行正则化</strong>。</li><li>Jain等人[86]提出了DietNeRF，通过辅助的语义一致性损失来增强少数镜头的质量，<strong>从而提高新位置的逼真渲染</strong>。DietNeRF从单个场景中学习，以准确地呈现来自相同位置的输入图像，并在不同的随机姿势中匹配高级语义特征。</li></ul></li><li>在文化遗产领域，只有有限数量的出版物明确调查并认识到nerf在3D重建、数字保存和保护目的方面的潜力[96,97]。</li></ul><h1 id="Analysis-and-Evaluation-Methodology"><a href="#Analysis-and-Evaluation-Methodology" class="headerlink" title="Analysis and Evaluation Methodology"></a>Analysis and Evaluation Methodology</h1><p>主要目标是通过<strong>客观测量产生的3D数据的质量</strong>，对基于nerf的传统摄影测量方法进行批判性评估。<br>为了实现这一点，我们考虑了各种不同大小和表面特征的物体和场景，包括纹理良好的、无纹理的、金属的、半透明的和透明的(第3.3节)。<br>拟议的评估策略和指标(第3.1和3.2节)应有助于研究人员了解每种方法的优势和局限性，并可用于对新提出的方法进行定量评估。<br><strong>所有实验均基于SDFStudio[98]和Nerfstudio[22]框架</strong>。值得提醒的是，NeRF输出是一个神经渲染;因此，使用marching cube方法[23]从每个视图的不同深度图创建网格模型。然后从网格顶点提取点云，使用Open3D库[78]进行定量评估。</p><h2 id="Proposed-Methodology"><a href="#Proposed-Methodology" class="headerlink" title="Proposed Methodology"></a>Proposed Methodology</h2><p>首先，将专用框架[22,98]中可用的各种NeRF方法应用于<strong>两个数据集</strong>，以了解其性能并选择性能最佳的方法(第4.1节)。然后，将该方法应用于其他数据集，对常规摄影测量和可用的地面真值(GT)数据进行评估和比较(第4.2-4.7节)。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812162140.png" alt="image.png"></p><p>图2显示了proposed procedure的总体概述，<strong>用于定量评估基于nerf的3D重建的性能</strong>。<br>所有收集的图像或视频都需要相机姿势才能生成3D重建，无论是使用传统的摄影测量还是基于nerf的方法。</p><ul><li>从可用图像开始，使用Colmap检索相机姿势。</li><li>然后，应用多视角立体(MVS)或NeRF来生成3D数据。</li><li>最后，我们提供了一个独特而强大的环境和条件，以提供客观的几何比较。<strong>为了实现这一目标，使用摄影测量和NeRF生成的3D数据在Cloud Compare</strong>(使用迭代最近点(ICP)算法[99])<strong>中相对于可用的地面真值(GT)数据进行共同注册和重新缩放，并进行质量评估</strong>。</li></ul><p>为了提供对几何精度的无偏评估，应用了不同的知名标准[13,43,100 - 102]，包括<strong>最佳平面拟合plane fitting</strong>、<strong>云对云比较</strong>、<strong>剖面分析profiling</strong>、精度和完整性completeness。<br>对于前两个标准，使用了标准偏差(STD)、平均误差(Mean_E)、均方根误差(RMSE)和平均绝对误差(MAE)等指标(第3.2节)。</p><ul><li>最佳平面拟合是通过使用最小二乘拟合(LSF)算法来完成的，该算法在物体的一个区域上定义一个最佳拟合平面，假设该区域是平面的。<strong>该标准允许我们评估由摄影测量或NeRF m生成的3D数据中的噪声水平</strong></li><li>通过从三维数据中提取截面来突出重建表面的复杂几何细节，从而进行剖面分析。<strong>对轮廓的检查使我们能够评估一种方法在保留几何细节(如边缘和角落)和避免平滑效果方面的性能</strong>。</li><li>C2C (Cloud-to-cloud)比较是指<strong>测量两个点云中对应点之间的最近邻距离</strong></li></ul><h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><p>尽管NeRF在3D重建中越来越受欢迎和广泛应用，但仍然<strong>缺乏基于特定标准或标准</strong>(例如VDI/VDE 2643 BLATT 3)的质量评估信息。根据之前提到的共同注册过程和标准，使用以下指标(特别是云对云和plane fitting过程):</p><p>$STD=\sqrt{\frac{1}{N-1}\sum_{j=1}^{N}(X_{j}-\underline{X})^{2}}$ Eq.7<br>$Mean_E=\frac{(X_1+X_2+\cdots X_j)}{N}$ Eq.8<br>$RMSE=\sqrt{\frac{\sum_{j=1}^{N}(X_{j})^{2}}{N}}$  Eq.9<br>$\text{MAE} =\sqrt{\frac{\sum_{j=1}^N|X_j|}N}$ Eq.10</p><p>式中，N为观测点云的个数，$X_j$为每个点与相应参考点或参考面最近的距离，$X$为平均观测距离。</p><p>准确度(Accuracy)和完备性(completeness)，也称为精确度(precision)和召回率(recall)[101,102]，涉及测量两个模型之间的距离。<br><strong>在评估精度时</strong>，计算从计算数据到地面真值(GT)的距离。相反，为了<strong>评估完整性</strong>，计算从GT到计算数据的距离。根据具体的评估方法，这些距离可以是有符号的，也可以是无符号的。<strong>准确性反映了重建点与地面真实值对齐的紧密程度</strong>，<strong>而完整性表示所有GT点被覆盖的程度</strong>。通常，使用<strong>阈值距离来确定落在可接受阈值内的点的分数或百分比。阈值是根据数据密度和噪声水平等因素确定的</strong>。</p><h2 id="Testing-Objects"><a href="#Testing-Objects" class="headerlink" title="Testing Objects"></a>Testing Objects</h2><p>为了实现工作目标，使用了不同的数据集(图3):它们具有不同尺寸和表面类型的物体，并且在不同的照明条件、材料、相机网络、尺度和分辨率下捕获。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812163209.png" alt="image.png"></p><p><strong>Ignatius和Truck数据集</strong>来源于Tanks和Temples基准[101]，其中GT数据(通过激光扫描获得)也是可用的</p><p>其他数据集(Stair, Synthetic, Industrial, Bottle_1和Bottle_2)在<strong>FBK</strong>中创建。</p><ul><li><strong>Stair数据集</strong>提供了一个具有锐利边缘的平坦、反射和纹理良好的表面。GT由台阶表面的理想平面提供。</li><li>使用Blender v3.2.2(用于几何模型，UV纹理和材料)和Quixel Mixer v2022(用于PBR纹理)创建的<strong>Synthetic 3D对象</strong>具有具有复杂几何形状的纹理良好的表面，包括边缘和角。具有特定参数的虚拟摄像机(焦距:50mm;传感器尺寸:36mm;图像大小:1920 × 1080像素)用于创建一系列图像，这些图像遵循对象周围的螺旋曲线路径。使用Blender中生成的3D模型作为GT进行精度评估。</li><li><strong>Industrial object</strong>具有无纹理和高反射的金属表面，这给所有被动3D方法带来了问题。其GT数据由Hexagon/AICON Primescan有源扫描仪获取，标称精度为63 μm。还包括两个瓶子，具有透明和折射表面:它们的GT数据是在表面粉末/喷涂后使用摄影测量法生成的。</li></ul><p>作者正在准备NeRF方法的具体基准，并将在[103]上提供，其中包含更多具有地面真实数据的数据集<br><a href="https://github.com/3DOM-FBK/NeRFBK"># NERFBK: A HOLISTIC DATASET FOR BENCHMARKING NERF-BASED 3D RECONSTRUCTION</a></p><h1 id="Comparisons-and-Analyses"><a href="#Comparisons-and-Analyses" class="headerlink" title="Comparisons and Analyses"></a>Comparisons and Analyses</h1><p>本节介绍了评估和比较基于nerf的技术与标准摄影测量(Colmap)性能的实验。在比较了多种最先进的方法(第4.1节)后，<strong>选择Instant-NGP作为基于nerf的方法进行全面评估，因为相对于其他方法，它提供了更好的结果</strong>。NeRF训练使用Nvidia A40 GPU执行，而<strong>3D结果的几何比较在标准PC上执行</strong>。</p><h2 id="State-of-the-Art-Comparison"><a href="#State-of-the-Art-Comparison" class="headerlink" title="State-of-the-Art Comparison"></a>State-of-the-Art Comparison</h2><p>主要目标是对多种基于nerf的方法进行综合分析。为了实现这一目标，我们使用了Yu等人[98]开发的SDFStudio统一框架，该框架将多种神经隐式表面重建方法整合到一个框架中。SDFStudio是建立在Nerfstudio框架上的[22]。在实现的方法中，选择了10种方法来比较它们的性能:</p><ul><li>来自Nerfstudio的Nerfacto and Tensorf</li><li>来自SDFStudio的Mono-Neus, Neus-Facto, MonoSDF, VolSDF, NeuS, MonoUnisurf and UniSurf</li><li>以及来自Müller等人最初实现的InstantNGP[20]。</li></ul><p>使用了<strong>两个数据集</strong>:<br>(i) 合成数据集，由200张图像(1920 × 1080像素)组成;<br>(ii) Ignatius dataset [101]，包含263张图像(从1920 × 1080像素分辨率的视频中提取)</p><p>与GT数据的对比结果如图4所示。在RMSE、MAE和STD方面的结果表明，<strong>Instant-NGP和Nerfacto方法取得了最好的结果，优于所有其他方法</strong>。在处理时间方面，instantngp需要不到一分钟的时间来训练两个数据集的模型，Nerfacto大约需要15分钟。<br>应该注意的是，对于Ignatius序列(图4b)，<strong>尽管MonoSDF、VolSDF和Neus-Facto的神经渲染在视觉上令人满意，但导出网格模型的行进立方体失败了;因此，不可能进行评估。</strong></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812164112.png" alt="image.png"></p><p>因此，根据所获得的精度和处理时间，本文选择了Instant-NGP进行后续实验。</p><h2 id="Image-Baseline’s-Evaluation"><a href="#Image-Baseline’s-Evaluation" class="headerlink" title="Image Baseline’s Evaluation"></a>Image Baseline’s Evaluation</h2><p>本节报告了当输入图像数量减少(即基线增加)时基于nerf的方法的评估。<br>本文对被认为优于其他方法(第4.1节)的<strong>Instant-NGP和一种成熟的用于稀疏图像场景的方法Mono-Neus进行了比较评估</strong>[66,90]。<br>实验利用由四个输入图像子集组成的<strong>合成数据集</strong>，从200到20张图像不等(图5)，逐步减少输入图像的数量(即大约加倍图像基线)。对于每一组输入图像，两种nerf方法都用于生成3D结果，保持相似的epoch数。对于每个子集，通过与GT数据点对点比较得出的RMSE估计如图5所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812164545.png" alt="image.png"></p><p>研究结果表明，当大量输入图像可用时，Instant-NGP表现出优于Mono-Neus的性能。然而，在图像数量较少的情况下，Mono-Neus优于Instant-NGP。然而，重要的是要注意，<strong>无论是Instant-NGP还是Mono-Neus都无法仅使用10个输入图像成功生成3D重建</strong>。</p><h2 id="Monte-Carlo-Simulation"><a href="#Monte-Carlo-Simulation" class="headerlink" title="Monte Carlo Simulation"></a>Monte Carlo Simulation</h2><p>目的是在相机姿势改变/扰动时评估基于nerf的3D结果的质量。因此，采用蒙特卡罗模拟[104]，<strong>在有限范围内随机扰动摄像机参数的旋转和平移</strong>。扰动后，使用<strong>Instant-NGP</strong>生成三维重建，并与参考数据进行比较。<br>在两种情况下共进行30次迭代(运行):<br>(A)在平移±20 mm和旋转±2度范围内随机扰动旋转和平移，<br>(B)在±40 mm和±4度范围内随机扰动旋转和平移。<br><strong>使用Ignatius数据集运行此模拟</strong>，结果如图6和表1所示。这些发现清楚地表明了拥有准确的相机参数的重要性。在情景A中，平均估计RMSE为19.72 mm，不确定性为2.95 mm。在情景B中，平均估计RMSE几乎保持不变(19.97 mm)，而由于较大的扰动范围，不确定性增加了一倍(5.87 mm)。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812165107.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812165231.png" alt="image.png"></p><h2 id="Plane-Fitting"><a href="#Plane-Fitting" class="headerlink" title="Plane Fitting"></a>Plane Fitting</h2><p>平面拟合方法可用于评估/测量重建平面上的噪声水平。在Stair数据集的第一个实验中(图7a)，采用相同数量的图像和相机姿势，<strong>导出了摄影测量点云和基于nerf的重建</strong>。根据最佳拟合过程识别和分析两个水平面和三个垂直平面(图7b)。派生的度量如表2所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812165151.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812165257.png" alt="image.png"></p><p>以类似的方式，使用了<strong>合成数据集</strong>，其中200张图像用于Instant-NGP, 24张图像用于摄影测量处理。选择5个垂直平面和5个水平面，如图8所示，通过拟合理想平面来进行重构对象表面的曲面偏差分析。派生的度量报告在表3中。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812165543.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812165643.png" alt="image.png"></p><p>从两个结果(表2和表3)可以清楚地看出，<strong>对于这两个对象，摄影测量优于NeRF，并且可以获得更少的噪声结果</strong>。一般来说，NeRF的均方根误差至少比摄影测量高2-3倍。</p><h2 id="Profiling"><a href="#Profiling" class="headerlink" title="Profiling"></a>Profiling</h2><p>截面轮廓的提取有助于证明三维重建方法检索几何细节或对三维几何应用平滑效果的能力。使用Cloud Compare处理第4.4节中提供的合成数据集的结果:<strong>在预定义的距离上提取几个横截面</strong>(图9)，<strong>并使用不同的指标与参考数据进行几何比较</strong>，如表4所示</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812165727.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812165802.png" alt="image.png"></p><p>对单个横截面剖面以及所有剖面的平均值所获得的结果表明，<strong>摄影测量优于NeRF，后者通常会产生更多的噪声结果</strong>(图9a-c)。例如，摄影测量的估计RMSE和STD的平均值约为0.09 mm和0.08 mm，而NeRF的这个值大于0.13 mm</p><h2 id="Cloud-to-Cloud-Comparison"><a href="#Cloud-to-Cloud-Comparison" class="headerlink" title="Cloud-to-Cloud Comparison"></a>Cloud-to-Cloud Comparison</h2><p>云对云比较是指评估数据集中相应3D样本相对于参考数据之间的相对欧几里得距离。<br>考虑具有不同特征的不同对象(图3):Ignatius、Truck、Industrial和Synthetic。它们是小型和大型的物体，具有无纹理，闪亮和金属表面。<br>对于每个数据集，使用摄影测量(<strong>Colmap</strong>)和<strong>Instant-NGP</strong>生成3D数据，然后共同注册到可用的GT(图10)。最后，度量的推导如表5所示。值得注意的是，在执行的测试中使用的图像数量并不总是相同的:<br>事实上，对于Synthetic、Ignatius和Truck数据集，</p><ul><li><strong>摄影测量已经用较少的图像数量提供了准确的结果，因此增加更多的图像不会导致进一步的改进</strong>。</li><li><strong>另一方面，对于NeRF，所有可用的图像都被使用，因为较少的图像(或扩大基线)不会导致良好的结果</strong>(参见4.2节)。</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812165937.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812170049.png" alt="image.png"></p><p><strong>从提供的结果可以看出，对于金属和高反射物体(工业数据集)，NeRF的表现优于摄影测量，而对于其他场景，摄影测量产生的结果更准确</strong></p><p>另外两种半透明和透明物体被考虑:Bottle_1和Bottle_2(图3)。<strong>玻璃物体不会漫反射入射光，也没有自己的纹理用于摄影测量3D重建任务。它们的外观取决于物体的形状、周围的背景和光照条件</strong>。<br>因此，在这种情况下，摄影测量很容易失败或产生非常嘈杂的结果。另一方面，正如Mildenhall等人所宣称的那样[16]，由于NeRF模型的视图依赖性质，NeRF可以学习正确地生成与透明度相关的几何形状。<br>对于这两个目标，基于摄影测量和NeRF的3D结果被共同注册到GT数据中，并计算度量(图11和表6)。<br>研究结果证明，<strong>对于透明目标，NeRF的表现优于摄影测量</strong>。例如，Bottle_1上摄影测量的估计RMSE, STD和MAE分别为6.5 mm, 7.1 mm和7.5 mm。相比之下，NeRF值分别显著降低到1.3 mm、1.7 mm和2.1 mm。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812170239.png" alt="image"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812170320.png" alt="image.png"></p><h2 id="Accuracy-and-Completeness"><a href="#Accuracy-and-Completeness" class="headerlink" title="Accuracy and Completeness"></a>Accuracy and Completeness</h2><p>三个不同的数据集用于比较摄影测量和NeRF的准确性和完整性: Ignatius, Industrial和Bottle_1。<br>对于NeRF (Instant-NGP)和摄影测量，这两个度量都是根据可用的地面真值数据计算的。结果如图12所示，揭示了以下见解:<br>(i)对于Ignatius数据集，与NeRF相比，摄影测量显示出更高的准确性和完整性;<br>(ii)对于工业和Bottle_1数据集，NeRF显示了稍好的结果。<br>这些发现在数量上证实了第4.6节，并且基于nerf的方法在处理具有非协作表面的物体时表现出色，特别是那些透明或有光泽的物体。相比之下，摄影测量学在捕捉这些表面的复杂细节方面面临挑战，使NeRF成为更合适或互补的选择</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230812170453.png" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Review </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Review </tag>
            
            <tag> 3DReconstruction </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRFactor</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/NeRFactor/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/NeRFactor/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination</th></tr></thead><tbody><tr><td>Author</td><td><a href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>    <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>    <a href="https://boyangdeng.com/">Boyang Deng</a>    <a href="http://www.pauldebevec.com/">Paul Debevec</a>    <a href="http://billf.mit.edu/">William T. Freeman</a>    <a href="https://jonbarron.info/">Jonathan T. Barron</a></td></tr><tr><td>Conf/Jour</td><td>TOG 2021 (Proc. SIGGRAPH Asia)</td></tr><tr><td>Year</td><td>2021</td></tr><tr><td>Project</td><td><a href="https://xiuming.info/projects/nerfactor/">NeRFactor (xiuming.info)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4667303505241849857&amp;noteId=1915674464034570496">NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814133227.png" alt="image.png"></p><p>贡献：</p><ul><li>NeRFactor在未知光照条件下从图像中恢复物体形状和反射率<ul><li>在没有任何监督的情况下，仅使用重渲染损失、简单的平滑先验和从真实世界BRDF测量中学习的数据驱动的BRDF先验，恢复了表面法线、光能见度、反照率和双向反射分布函数(BRDFs)的3D神经场</li></ul></li><li>通过NeRFactor的分解，我们可以用点光或光探针图像重新照亮物体，从任意视点渲染图像，甚至编辑物体的反照率和BRDF</li></ul><span id="more"></span><h1 id="LIMITATIONS"><a href="#LIMITATIONS" class="headerlink" title="LIMITATIONS"></a>LIMITATIONS</h1><p>尽管我们证明NeRFactor优于其变体和基线方法，但仍然存在一些重要的局限性。</p><ul><li>首先，为了使光可见性计算易于处理，我们将光探针图像的分辨率限制为16 × 32，该分辨率可能<strong>不足以生成非常硬的阴影或恢复非常高频的brdf</strong>。因此，当物体被非常高频的照明照亮时，如图S1(情况D)所示，其中太阳像素是完全HDR的，反照率估计中可能存在镜面或阴影残余(例如花瓶上的那些)。</li><li>其次，为了快速渲染，我们<strong>只考虑单弹直接照明，因此NeRFactor不能适当地考虑间接照明效果</strong>。</li><li>最后，NeRFactor使用NeRF或MVS初始化其几何估计。虽然它能够在一定程度上修复NeRF造成的错误，但<strong>如果NeRF以一种碰巧不影响视图合成的方式估计特别差的几何形状，它可能会失败</strong>。我们在两个真实的NeRF场景中观察到这一点，它们包含远处不正确的“浮动”几何体，从输入摄像机中看不到，但在物体上投下阴影。</li></ul><h1 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h1><p>在本文中，我们提出了神经辐射分解(NeRFactor)，一种从多视图图像及其相机姿势中恢复物体形状和反射率的方法。<strong>重要的是，NeRFactor在未知光照条件下从图像中恢复这些属性</strong>，而大多数先前的工作需要在多个已知光照条件下进行观察。为了解决这个问题的病态性质，<strong>NeRFactor依靠先验</strong>来估计一组合理的形状、反射率和照明，这些都能解释观察到的图像。<br>这些先验包括简单但有效的空间平滑约束(在多层感知器[MLPs]上下文中实现)和真实BRDFs上的数据驱动先验。我们证明NeRFactor实现了高质量的几何形状，足以进行重照明和视图合成，产生令人信服的反照率以及空间变化的brdf，并生成正确反映主光源存在或不存在的照明估计。<strong>通过NeRFactor的分解，我们可以用点光或光探针图像重新照亮物体，从任意视点渲染图像，甚至编辑物体的反照率和BRDF</strong>。我们相信，这项工作在从随意捕获的照片中恢复全功能3D图形资产的目标方面取得了重要进展。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>我们解决了<strong>从一个未知照明条件下照亮的物体的多视图图像(及其相机姿势)中恢复物体的形状和空间变化反射率</strong>的问题。这使得在任意环境、照明和编辑物体的材料属性下渲染物体的新视图成为可能。我们称之为神经辐射分解(<strong>NeRFactor</strong>)的方法的关键是<strong>将神经辐射场(NeRF)</strong> [Mildenhall et al. 2020]表示的物体的体积几何图形提取为distill表面表示，然后在求解空间变化的反射率和环境照明的同时联合优化几何图形。<br>具体来说，NeRFactor在没有任何监督的情况下，仅使用重渲染损失、简单的平滑先验和从真实世界BRDF测量中学习的数据驱动的BRDF先验，恢复了表面法线、光能见度、反照率和双向反射分布函数(BRDFs)的3D神经场。通过显式地建模光可视性，NeRFactor能够从反照率中分离阴影，并在任意光照条件下合成真实的软阴影或硬阴影。NeRFactor能够在合成和真实场景的这个具有挑战性和约束不足的捕获设置中恢复令人信服的自由视点重照明3D模型。定性和定量实验表明，NeRFactor在各种任务中的表现优于经典和基于深度学习的最新技术</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>从捕获的图像中恢复物体的几何形状和材料属性，这样它就可以在新的照明条件下从任意视点渲染，这是计算机视觉和图形学中一个长期存在的问题。这个问题的困难源于其<strong>基本的欠约束性质</strong>，以前的工作通常通过使用额外的观察(如扫描几何形状、已知的照明条件或多个不同照明条件下的物体图像)来解决这个问题，或者通过限制性假设(如假设整个物体的单一材料或忽略自阴影)来解决这个问题。在这项工作中，我们证明了从未知自然光照条件下捕获的物体图像中恢复令人信服的可照明表示是可能的，如图1所示。我们的关键见解是，我们可以首先从输入图像中优化神经辐射场(NeRF) [Mildenhall等人，2020]来初始化我们模型的表面法线和光照可见度(尽管我们表明使用多视图立体[MVS]几何也有效)，然后共同优化这些初始估计以及空间变化反射率和照明条件，以最好地解释观察到的图像。使用NeRF生成初始化的高质量几何估计有助于打破形状，反射率和照明之间固有的模糊性，从而使我们能够恢复完整的3D模型，用于令人信服的视图合成和重新照明，仅使用重新渲染损失，每个组件的简单空间平滑先验，以及新颖的数据驱动双向反射分布函数(BRDF)先验。由于NeRFactor明确而有效地模拟光能见度，因此它能够从反照率估计中去除阴影，并在任意新颖的光照条件下合成逼真的软阴影或硬阴影。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814133227.png" alt="image.png"></p><p>虽然<strong>NeRF估计的几何形状</strong>对于视图合成是有效的，但它<strong>有两个限制，使其不容易用于重新照明</strong>。</p><ul><li>首先，NeRF将形状建模为一个体积场，因此计算沿着摄像机光线的每个点的阴影和能见度对于整个半球的照明来说是昂贵的。</li><li>其次，由NeRF估计的几何形状包含无关的高频内容，虽然在视图合成结果中不明显，但将高频伪影引入到从NeRF几何形状计算的表面法线和光可见性中。</li></ul><p>我们通过使用NeRF几何图形的“硬表面”近似来解决第一个问题，其中我们只在每条射线的单个点上执行阴影计算，对应于体积的预期终止深度。我们通过将该表面上任何3D位置的表面法线和光可见性表示为由多层感知器(mlp)参数化的连续函数来解决第二个问题，并鼓励这些函数接近于从预训练的NeRF中获得的值，并且在空间上平滑。<br>因此，我们的模型，我们称之为神经辐射因子分解Factorization(NeRFactor)，将观测到的图像考虑到估计的环境照明中，以及具有表面法线、光能见度、反照率和空间变化的BRDFs的物体的3D表面表示。这使我们能够在任意的新环境照明下渲染物体的新视图。</p><p>In summary, our main technical contributions are:</p><ul><li>一种将物体在未知光照条件下的图像分解为形状、反射率和照度illumination的方法，从而支持自由视点重照明(带阴影)和材质编辑</li><li>将nerf估计的体积密度提取到表面几何形状(具有法线和光可见性)的策略，以便在改进几何形状和恢复反射率时用作初始化</li><li>一种新的数据驱动的BRDF先验，通过<strong>对实际测量的BRDF</strong>进行潜在代码模型的训练来学习。</li></ul><p><strong>输入和输出</strong>。NeRFactor的输入是一组<strong>在未知环境照明条件下</strong>的物体的多视图图像以及这些图像的相机姿势。NeRFactor联合估计表面法线、光能见度、反照率、空间变化brdf和环境照明的合理集合，这些集合共同解释了观测到的视图。然后，我们使用恢复的几何形状和反射率来<strong>合成任意光照下的新视点的物体图像</strong>。明确建模能见度，NeRFactor能够从反照率中去除阴影，并<strong>在任意照明下合成软阴影或硬阴影</strong>。</p><p><strong>假设</strong>。NeRFactor认为物体是由坚硬的表面组成的，每条光线只有一个交点，因此<strong>没有对散射、透明和半透明等体积光传输效应进行建模</strong>。此外，为了简化计算，我们<strong>只对直接光照进行建模</strong>。最后，我们的反射率模型考虑了具有消色差镜面反射率的材料(电介质)，因此我们<strong>不模拟金属材料</strong>(尽管可以通过额外预测每个表面点的镜面颜色来轻松扩展我们的金属材料模型)。</p><h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h2><p><strong>Inverse rendering</strong><br>[佐藤等。1997;马斯纳1998;Yu et al. 1999;Ramamoorthi and Hanrahan 2001]，将观察图像中物体的外观分解为潜在的几何形状、材料属性和光照条件的任务，是计算机视觉和图形学中长期存在的问题。由于众所周知，完整的一般反向渲染问题是严重缺乏约束的，因此大多数先前的方法都通过假设没有阴影，学习形状，照明和反射率的先验，或者需要额外的观察，例如扫描几何形状，测量的照明条件，或在多种(已知)照明条件下的物体的额外图像来解决这个问题。<br>单幅图像逆渲染方法Methods for single-image inverse rendering<br>[Barron和Malik, 2014;Li et al. 2018;Sengupta等人2019;Yu and Smith 2019;Sang and Chandraker 2020;Wei等人。2020;Li等人。2020;Lichy等人。2021]很大程度上依赖于从大型数据集中学习到的几何、反射率和照明的强大先验。最近的方法可以从单个图像中有效地推断出这些因素的合理设置，<strong>但不能恢复可以从任意视点查看的完整3D表示</strong>。</p><p>大多数恢复因式全3D模型的方法都依赖于额外的观测，而不是强先验。<strong>一种常见的策略是使用主动扫描获得的3D几何图形</strong>[Lensch等人。2003;郭等。2019;Park等人。2020;Schmitt et al. 2020;Zhang等。2021a]，代理模型[Sato等。2003;Dong等人。2014;Georgoulis et al. 2015;Gao等。2020;Chen等人]剪影面具[Oxholm和Nishino 2014;戈达尔等人。2015;夏等。2016]，或多视点立体(MVS;然后是曲面重建和网格划分)[Laffont et al. 2012;Nam et al. 2018;菲利普等人。2019;Goel等人]。作为恢复反射率和精细几何之前的起点。在这项工作中，我们表明，从使用最先进的神经体积表示估计的几何形状开始，使我们能够仅使用在一次照明下捕获的图像恢复完全分解的3D模型，而无需任何额外的观察。至关重要的是，使用这种方式估计的初始几何形状使我们能够恢复对传统几何形状估计方法具有挑战性的物体的因子模型，包括具有高反射表面和详细几何形状的物体。</p><p>计算机图形界的大量工作集中在材料获取的特定子问题上，其目标是<strong>从已知(通常是平面)几何形状的材料图像中估计双向反射分布函数(BRDF)属性</strong>。这些方法传统上利用基于信号处理的重建策略，并使用复杂的控制相机和照明设置来充分采样BRDF [Foo 2015;Matusik et al. 2003;尼尔森等人。2015年]，而最近的方法使得从更休闲的智能手机设置中获取材料成为可能[Aittala等人。2015;Hui et al. 2017]。<strong>然而，这一行的工作通常要求几何形状简单且完全已知</strong>，而我们关注的是一个更一般的问题，即我们唯一的观察是具有复杂形状和空间变化反射率的物体的图像。</p><p>我们的工作建立在计算机视觉和图形社区的最新趋势之上，<strong>该趋势将传统的形状表示(如多边形网格或离散体素网格)替换为将几何表示为参数函数的多层感知器(mlp)</strong>。这些mlp经过优化，可以通过将3D坐标映射到该位置的物体或场景属性(如体积密度、占用率或签名距离)来近似连续3D几何。该策略已经成功地从3D观测中恢复连续的3D形状表示[Mescheder等人。2019;Park et al. 2019;Tancik等人]。和固定照明下观测到的图像[Mildenhall等。2020;Yariv等。2020]。神经辐射场(NeRF) [Mildenhall等。2020]技术在优化观察图像的体积几何和外观方面特别成功，目的是渲染逼真的新视图</p><p>NeRF启发了后续方法，扩展其神经表征以实现重照明[Bi et al. 2020;Boss等;2021;Srinivasan等人。2021;张等。2021b]。我们列出了这些并发方法与NeRFactor之间的区别如下</p><ul><li>Bi等人。[2020]和NeRV [Srinivasan等。2021]<strong>需要多个已知的照明条件</strong>，而NeRFactor只处理一个未知的照明。</li><li>NeRD[Boss等人,2021]不模拟能见度或阴影，而NeRFactor可以成功地将阴影与反照率分开(如下所示)。NeRD使用分析BRDF，而NeRFactor使用编码先验的学习BRDF</li><li>PhySG [Zhang等。2021b]不模拟能见度或阴影，并使用分析BRDF，就像NeRD一样。此外，PhySG假设非空间变化的反射率，而NeRFactor模型是空间变化的brdf。</li></ul><h1 id="METHOD"><a href="#METHOD" class="headerlink" title="METHOD"></a>METHOD</h1><p>NeRFactor的输入被<strong>假设为一个未知照明条件下的物体的多视图图像(以及它们的相机姿势)</strong>。NeRFactor将物体的形状和空间变化反射率表示为一组3D字段，每个字段由多层感知器(mlp)参数化，其权重经过优化，以便“解释”观察到的输入图像集。优化后，NeRFactor的输出，在物体的表面上的每个3D位置x: 表面法向$n$,任何方向的能见度$v(\omega_i)$ ，反照率$a$和反射比reflectance $z_{BRDF}$共同解释观察到的现象。通过恢复物体的几何形状和反射率，NeRFactor支持诸如自由视点重照明(带阴影)和材料编辑等应用程序。</p><p>我们在图2中可视化NeRFactor模型和它产生的分解示例。有关实现细节，包括网络架构，训练范例，运行时等，请参阅附录的A部分和我们的GitHub存储库。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814141123.png" alt="image.png"><br><em>Model. NeRFactor leverages NeRF’s 𝜎-volume as an initialization to predict, for each surface location $𝒙_{surf}$, surface normal 𝒏, light visibility 𝑣, albedo 𝒂, BRDF latent code $𝒛_{BRDF}$, and the lighting condition. 𝒙 denotes 3D locations, $𝝎_i$ light direction, $𝝎_o$ viewing direction, and $𝜙_d, 𝜃_h, 𝜃_d$ Rusinkiewicz coordinates. Note that NeRFactor is an <strong>all-MLP architecture</strong> that models only surface points (unlike NeRF that models the entire volume).</em></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814140950.png" alt="image.png"><br><em>Example factorization. NeRFactor jointly solves for plausible surface normals, light visibility, albedo, BRDFs, and lighting that together explain the observed views. Here we visualize light visibility as ambient occlusion and 𝑧BRDF directly as RGBs (similar colors indicate similar materials).</em></p><p><em>NeRFactor是一种基于坐标的模型，它可以在无监督的情况下分解在一个未知光照条件下观察到的场景的外观。它通过使用重建损失、简单的平滑正则化和数据驱动的BRDF先验来解决这个严重不适定的问题。明确建模可见性，NeRFactor是一个基于物理的模型，支持任意光照下的阴影。</em></p><h2 id="Shape"><a href="#Shape" class="headerlink" title="Shape"></a>Shape</h2><p>我们模型的输入与NeRF [Mildenhall等人，2020]使用的输入相同，因此我们可以将NeRF应用于我们的输入图像以计算初始几何(尽管使用多视图立体[MVS]几何作为初始化也有效，如第4.4节所示)。NeRF优化了一个神经辐射场:一个MLP，它从任何3D空间坐标和2D观看方向映射到该3D位置的体积密度和该位置的粒子沿2D观看方向发射的颜色。NeRFactor通过将NeRF的估计几何形状“提炼distilling”成一个连续的表面表示来初始化NeRFactor的几何形状，从而利用NeRF的估计几何形状。特别是，我们使用优化的NeRF来计算沿任何相机光线的预期表面位置，物体表面上每个点的表面法线，以及从物体表面上每个点的任何方向到达的光的可见度。本小节描述了我们如何从优化的NeRF中获得这些函数，以及如何使用mlp重新参数化它们，以便在初始化步骤之后对它们进行微调，以改善完全的重新呈现损失(图3)。</p><p><strong>表面点</strong>。给定一个camera和一个trained的NeRF，我们根据NeRF的优化体积密度$\sigma$，计算从摄像机原点$\text{o}$沿着方向$\text{d}$的光线$\mathbf{r}(t)=\mathbf{o}+t\boldsymbol{d}$，预期会终止的位置：</p><p>$x_{\mathrm{surf}}=\boldsymbol{o}+\left(\int_{0}^{\infty}T(t)\sigma\big(\boldsymbol{r}(t)\big)tdt\right)\boldsymbol{d},$</p><p>在不被阻挡的情况下，射线行进距离𝑡的概率可以表示为$T(t)=\exp\left(-\int_{0}^{t}\sigma\big(\boldsymbol{r}(s)\big)ds\right)$。与维护完整的体积表示不同，我们将几何结构固定在这个从优化的NeRF提取出的表面上。这在训练和推断过程中都能够实现更高效的重新光照，<strong>因为我们只需在每个相机射线预期终止的位置计算出射辐射，而不是沿着每条相机射线的每个点都进行计算</strong></p><p><strong>表面法线</strong>。我们在任意3D位置计算解析表面法线$𝒏_{a}(𝒙)$，它是相对于𝒙的NeRF的𝜎-体积的负归一化梯度。然而，从训练过的NeRF派生出的法线往往带有噪声（图3），因此在渲染时会产生“凹凸”瑕疵（请参阅补充视频）。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814143126.png" alt="image.png"><br><em>NeRFactor恢复的高质量几何。 (A) 我们可以直接从训练过的NeRF中导出表面法线和光能见度。然而，用这种方式导出的几何结构太嘈杂，无法用于重新光照（请参阅补充视频）。 (B) 联合优化形状和反射性能改善了NeRF的几何结构，但仍然存在显著的噪声（例如，图II中的条纹伪影）。 (C) 在平滑性约束下进行联合优化，得到了平滑的表面法线和光能见度，类似于地面真实情况。在所有入射光方向上平均的能见度是环境遮挡。</em></p><p>因此，我们使用一个MLP $f_{\mathrm{n}}$对这些法线进行重新参数化，将从表面任何位置$x_{\mathrm{surf}}$映射到“去噪”的表面法线$n\colon f_{\mathrm{n}}:x_{\mathrm{surf}}\mapsto n.$在优化NeRFactor权重的联合优化过程中，我们鼓励这个MLP的输出：<br>I）保持接近预训练NeRF产生的法线，<br>II）在3D空间中平滑变化，<br>III）重现物体的观察外观。具<br>体而言，反映I）和II）的损失函数为…<br>$\ell_{\mathrm{n}}=\sum_{x_{\mathrm{surf}}}\left(\frac{\lambda_{1}}{3}|f_{\mathrm{n}}(x_{\mathrm{surf}})-n_{\mathrm{a}}(x_{\mathrm{surf}})|_{2}^{2}+\frac{\lambda_{2}}{3}|f_{\mathrm{n}}(x_{\mathrm{surf}})-f_{\mathrm{n}}(x_{\mathrm{surf}}+\epsilon)|_{1}\right)$<br>其中𝝐是从均值为零、标准差为0.01的高斯分布中采样得到的关于$x_{\mathrm{surf}}$的随机3D位移（对于实际场景，由于不同的场景尺度，𝝐的标准差为0.001或0.25），而$𝜆_1$和$𝜆_2$是分别设定为0.1和0.05的超参数。Oechsle等人在其并行工作中也使用了类似的表面法线平滑性损失，用于形状重建的目标。至关重要的是，不限制𝒙在预期表面上，增加了MLP的稳健性，为输出提供了一个“安全余地”，即使输入略微偏离表面，输出仍保持良好行为。正如图3所示，NeRFactor的法线MLP产生的法线质量明显优于NeRF产生的法线，并且足够平滑以用于重新光照（图5）。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814145305.png" alt="image.png"></p><p><strong>光能见度</strong>。我们通过从点到每个光源位置在NeRF的𝜎体积中进行行进，计算每个点到每个光源的能见度$𝑣_𝑎$，类似于Bi等人[2020]的方法。然而，与上述估计的表面法线一样，直接从NeRF的𝜎体积中得出的能见度估计过于嘈杂，无法直接使用（图3），并导致渲染产生伪影（请参见补充视频）。我们通过将能见度函数重新参数化为另一个多层感知器（MLP），该MLP从表面位置$x_{\mathrm{surf}}$和光线方向$𝝎_i$映射到光能见度$v{:}f_\mathrm{V}:(x_\mathrm{surf},\omega_\mathrm{i})\mapsto v.$。我们优化$𝑓_𝑣$的权重，以鼓励恢复的能见度场：<br>I）接近从NeRF跟踪的能见度，<br>II）空间平滑，<br>III）复现观察到的外观。<br>具体而言，实施I）和II）的损失函数如下：<br>$\ell_\mathrm{v}=\sum_{x_\mathrm{surf}}\sum_{\omega_\mathrm{i}}\left(\lambda_3\big(f_\mathrm{v}(x_\mathrm{surf},\omega_\mathrm{i})-v_\mathrm{a}(x_\mathrm{surf},\omega_\mathrm{i})\big)^2+\lambda_4\big|f_\mathrm{v}(x_\mathrm{surf},\omega_\mathrm{i})-f_\mathrm{v}(x_\mathrm{surf}+\epsilon,\omega_\mathrm{i})\big|\right)$<br>其中，𝝐 是上文定义的随机位移，$𝜆_3$ 和$𝜆_4$ 分别是设置为 0.1 和 0.05 的超参数。如等式所示，在相同的$𝝎_i$ 条件下，鼓励不同空间位置的平滑性，而不是相反。这样做的目的是为了避免某一位置的可见度在不同光照位置上变得模糊。请注意，这与 Srinivasan 等人[2021]中的可见度fields类似，但在我们的案例中，我们优化了可见度 MLP 参数，以去噪从预训练 NeRF 得出的可见度，并将重新渲染损失降至最低。为了计算 NeRF 可见度，我们使用了一组固定的 512 个光照位置，并预先定义了光照分辨率（稍后讨论）。经过优化后，$f_{\mathrm{V}}$可以生成空间上平滑且真实的光线可见度估计值，如图 3 (II) 和图 4 (C)所示，我们可以看到所有光线方向的平均可见度（即环境遮挡）。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814143126.png" alt="image.png"><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814145248.png" alt="image.png"></p><p>在实践中，<strong>在对模型进行全面优化之前，我们对可见度和法线 MLP 进行了独立预训练</strong>，使其仅再现来自 NeRF 𝜎 卷的可见度和法线值，而没有任何平滑度正则化或重新渲染损失。这为可见度maps提供了合理的初始化，从而避免反照率或双向反射率分布函数（BRDF）MLP 将阴影误解为 “涂抹在 “反射率变化上（见表 1 和图 S2 中的 “w/o geom.）</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814141123.png" alt="image.png"></p><h2 id="Reflectance"><a href="#Reflectance" class="headerlink" title="Reflectance"></a>Reflectance</h2><p>我们的完整 BRDF 模型𝑹包括一个完全由反照率𝒂决定的漫反射部分（朗伯）和一个镜面空间变化 BRDF$f_\mathrm{r}$（根据入射光方向$\omega_{i}$ 和出射方向$\omega_{o}$ ，定义为表面上的任意位置$x_{\mathrm{surf}}$ ）从真实世界的反射率中学习的：$R(x_{\mathrm{surf}},\omega_{\mathrm{i}},\omega_{\mathrm{o}})=\frac{a(x_{\mathrm{surf}})}{\pi}+f_{\mathrm{r}}\left(x_{\mathrm{surf}},\omega_{\mathrm{i}},\omega_{\mathrm{o}}\right).$</p><p>神经渲染的现有技术已经探索了参数化的使用。使用microfacet模型等分析brdf [Bi等。2020;Srinivasan等人。2021]在类似nerf的环境中。我们还将在第5.1节中探讨NeRFactor的“分析BRDF”版本。尽管这些分析模型为优化探索提供了有效的BRDF参数化，但没有对参数本身施加先验:在microfacet模型中可表示的所有材料都被认为是同等先验的。此外，使用显式分析模型限制了可以回收的材料集，这可能不足以对所有现实世界的brdf进行建模。<br>NeRFactor不是假设一个解析BRDF，而是从一个学习到的反射函数开始，该函数经过预训练，可以再现大量的经验观察到的真实世界的BRDF，同时也可以学习这些真实世界BRDF的潜在空间。通过这样做，我们可以学习真实世界brdf的数据驱动先验，从而鼓励优化恢复合理的反射函数。使用这样的先验是至关重要的:因为我们所有观察到的图像都是在一个(未知)照明下拍摄的，我们的问题是高度病态的，所以先验是必要的，可以从所有可能的分解集合中消除最可能的场景分解的歧义。</p><p><strong>Albedo</strong>. 我们将任何表面位置$x_{\mathrm{surf}}$的反照率𝒂参数化为一个 MLP $f_{\mathrm{a}}:x_{\mathrm{surf}}\mapsto a.$由于没有对反照率的直接监督，而且我们的模型只能观察到一种光照条件，因此我们依靠简单的空间平滑先验（和光照可见度）来区分 “含有阴影的白漆表面 “和 “黑白相间的白漆表面 “等情况。此外，观测视图的重建损失也是优化$\text{f}_a$ 的驱动因素。反映这种平滑先验的损失函数是</p><p>$\ell_{\mathrm{a}}=\lambda_{5}\sum_{x_{\mathrm{surf}}}\frac{1}{3}\big\Vert f_{\mathrm{a}}(x_{\mathrm{surf}})-f_{\mathrm{a}}(x_{\mathrm{surf}}+\epsilon)\big\Vert_{1},$</p><p>其中，𝝐 是与上述定义相同的随机三维扰动，$𝜆_5$ 是设置为 0.05 的超参数。$\text{f}_a$的输出在朗伯反射率中用作反照率，但在非漫反射分量中不用作反照率，<strong>我们假定非漫反射分量的镜面高光颜色为白色</strong>。根据 Ward 和 Shakespeare [1998] 的经验，我们将反照率预测值限制在 [0.03, 0.8]，方法是将网络的最终 sigmoid 输出值缩放 0.77，然后加上 0.03 的偏差。</p><p><strong>Learning priors from real-world BRDFs</strong>. 对于 BRDF 的镜面成分，我们试图学习现实世界 BRDF 的潜在空间，以及将所学空间 $𝒛_{BRDF}$ 中的每个潜在代码转换为完整 4D BRDF 的配对 “解码器”。为此，我们采用了生成式潜在优化（GLO）方法[Bojanowski 等人，2018]，其他基于坐标的模型，如 Park 等人[2019]和 Martin-Brualla 等人[2021]也曾使用过这种方法。我们模型的$\text{f}_{r}$部分是使用 MERL 数据集[Matusik 等人，2003 年]预训练的。由于 MERL 数据集假定材料各向同性，我们使用 Rusinkiewicz 坐标[Rusinkiewicz 1998]$(\phi_{\mathbf{d}},\theta_{\mathbf{h}},\theta_{\mathbf{d}})$（3 个自由度）而不是 $𝝎_i$ 和 $𝝎_o$（4 个自由度）对$𝒇_r$ 的输入和输出方向进行参数化。用 𝒈 表示这种坐标转换：$g:(n,\omega_{\mathrm{i}},\omega_{0})\mapsto(\phi_{\mathrm{d}},\theta_{\mathrm{h}},\theta_{\mathrm{d}}),$，其中𝒏 是该点的表面法线。我们训练了一个函数$f_{\mathrm{r}}^{\prime}$（$𝒇_r$ 的重新参数化），该函数将潜在代码 $𝒛_{BRDF}$（代表 BRDF 特性）和 Rusinkiewicz 坐标$(\phi_{\mathbf{d}},\theta_{\mathbf{h}},\theta_{\mathbf{d}})$的组合映射到消色差反射率 𝒓：</p><p>$f_\mathrm{r}’:(z_\mathrm{BRDF},(\phi_\mathrm{d},\theta_\mathrm{h},\theta_\mathrm{d}))\mapsto r.$</p><p>为了训练这个模型，我们优化了 MLP 的权重和潜在代码集 $𝒛_{BRDF}$，以再现一组真实世界的 BRDF。计算高动态范围（HDR）反射率值对数的简单均方误差来训练$𝒇 ′_{r}$</p><p>由于我们的反射率模型中的<strong>颜色部分假定由反照率 MLP 处理</strong>，因此我们将 MERL 数据集的 RGB 反射率值转换为消色差值†，从而舍弃了所有颜色信息。潜在 BRDF 识别代码$𝒛_{BRDF}$ 的参数为无约束三维向量，初始化为标准偏差为 0.01 的零均值各向同性高斯。在训练过程中，没有对 $𝒛_{BRDF}$ 施加稀疏性或规范惩罚。经过预训练后，BRDF MLP 的权重在整个模型的联合优化过程中被冻结，我们通过从头开始训练 BRDF 标识 MLP（图 2a），对每个$x_{\mathrm{surf}}$ 只预测${z_{\mathrm{BRDF}}}$：$f_\mathrm{z}:x_\mathrm{surf}\mapsto z_\mathrm{BRDF}.$.这可以看作是预测真实世界 BRDF 的可信空间中所有表面点的空间变化 BRDF。我们对 BRDF 特性 MLP 进行优化，以最小化重新渲染损失和与反照率相同的空间平滑先验：<br>$\ell_{z}=\lambda_{6}\sum_{x_{\mathrm{surf}}}\frac{\left|f_{z}(x_{\mathrm{surf}})-f_{z}(x_{\mathrm{surf}}+\epsilon)\right|_{1}}{\dim(z_{\mathrm{BRDF}})},$</p><p>其中，$𝜆_6$是一个超参数，设置为 0.01；$\dim(z_{\mathrm{BRDF}})$表示 BRDF 潜在代码的维数（在我们的实现中为 3，因为 MERL 数据集中只有 100 种材料）。最终的 BRDF 是朗伯分量与学习到的非漫反射之和（为简洁起见，去掉了$x_{\mathrm{surf}}$ 的下标）：</p><p>$R(x,\omega_{\mathrm{i}},\omega_{0})=\frac{f_{\mathrm{a}}(x)}{\pi}+f_{\mathrm{r}}^{\prime}\left(f_{\mathrm{z}}(x),g(f_{\mathrm{n}}(x),\omega_{\mathrm{i}},\omega_{0})\right),$</p><p>其中镜面高光颜色假定为白色。</p><h2 id="Lighting"><a href="#Lighting" class="headerlink" title="Lighting"></a>Lighting</h2><p>我们采用一种简单而直接的照明表示:一幅HDR光探测图像[Debevec 1998]，<strong>采用经纬度格式</strong>。与球面谐波或球面高斯混合相比，这种表示允许我们的模型表示详细的高频照明，因此支持硬投射阴影。也就是说，使用这种表示的挑战很明显:它包含大量参数，并且每个像素/参数都可以独立于所有其他像素而变化。这个问题可以通过使用光可见性MLP来改善，它允许我们快速评估一个表面点对光探头所有像素的可见性。从经验上讲，我们在照明环境中使用16×32分辨率，因为我们不期望恢复超出该分辨率的高频内容(照明被物体的BRDFs有效地低通过滤[Ramamoorthi和Hanrahan 2001]，而且我们的物体没有光泽或镜面一样)。</p><p>为了使光照更加平滑，我们在光探针𝑳 的像素点上沿水平和垂直两个方向应用了简单的$\ell^{2}$梯度惩罚：<br>$\ell_{\mathrm{i}}=\lambda_{7}\left(\left|\begin{bmatrix}-1&amp;&amp;1\end{bmatrix}<em>L\right|_{2}^{2}+\left|\begin{bmatrix}-1\\1\end{bmatrix}</em>L\right|_{2}^{2}\right),$</p><p>其中，∗ 表示卷积算子，$𝜆_7$ 是一个超参数，设置为 $5 × 10^{-6}$（考虑到有 512 个像素具有 HDR 值）。在联合优化过程中，这些探测像素会直接根据最终重建损失和梯度惩罚进行更新。</p><h2 id="Rendering"><a href="#Rendering" class="headerlink" title="Rendering"></a>Rendering</h2><p>鉴于每个点 $𝒙_{surf}$ 的表面法线、所有光照方向的可见度、反照率和 BRDF 以及估计的光照，最终基于物理的不可学习渲染器会渲染一幅图像，然后将其与观测图像进行比较。渲染图像中的误差会反向传播，但不包括预训练 NeRF 的𝜎-volume，从而推动表面法线、光能见度、反照率、BRDF 和光照的联合估计。</p><p>鉴于问题的不确定性（主要是由于我们只观测到一种未知光照），我们预计大部分有用信息将来自直接光照而非全局光照，因此只考虑单跳single-bounce直接光照（即从光源到物体表面再到摄像机）。这一假设也降低了评估模型的计算成本。在数学上，我们设置的渲染方程为（为简洁起见，再次去掉$𝒙_{surf}$的下标）：</p><script type="math/tex; mode=display">\begin{aligned}L_{0}(x,\omega_{0})=\int_{\Omega}R(x,\omega_{1},\omega_{0})L_{1}(x,\omega_{1})\big(\omega_{1}\cdot n(x)\big)d\omega_{1} \\=\sum_{\omega_{\mathrm{i}}}R(x,\omega_{\mathrm{i}},\omega_{\mathrm{0}})L_{\mathrm{i}}(x,\omega_{\mathrm{i}})\big(\omega_{\mathrm{i}}\cdot f_{\mathrm{n}}(x)\big)\Delta\omega_{\mathrm{i}}=\sum_{\omega_{\mathrm{i}}}\bigg(\frac{f_{\mathrm{a}}(x)}{\pi}+ \\f_{\mathrm{r}}^{\prime}\Big(f_{z}(x),g\big(f_{\mathrm{n}}(x),\omega_{\mathrm{i}},\omega_{0}\big)\Big)\Big)L_{\mathrm{i}}(x,\omega_{\mathrm{i}})\big(\omega_{\mathrm{i}}\cdot f_{\mathrm{n}}(x)\big)\Delta\omega_{\mathrm{i}},\end{aligned}</script><p>其中，$𝑳_o (𝒙, 𝝎_{o})$是从𝒙 看$𝝎_{o}$的出射辐射度，$𝑳_i (𝒙, 𝝎_{i})$是入射辐射度，被可见度 $f_{\mathbf{v}}(x,\omega_{\mathbf{i}}),$遮挡、沿$𝝎_{i}$ 直接到达𝒙 的光探测像素（因为我们只考虑单跳直接照射），$\Delta\omega_{\mathrm{i}}$是在$\omega_{\mathrm{i}}$处与照明样本相对应的实体角。</p><p>最终的重建损失 $ℓ_{recon}$ 只是渲染图像与观察图像之间的均方误差（权重为单位）。因此，我们的完整损失函数是之前定义的所有损失的总和：$\ell_\mathrm{recon}+\ell_\mathrm{n}+\ell_\mathrm{v}+\ell_\mathrm{a}+\ell_\mathrm{z}+\ell_\mathrm{i}.$</p><h1 id="RESULTS-amp-APPLICATIONS"><a href="#RESULTS-amp-APPLICATIONS" class="headerlink" title="RESULTS &amp; APPLICATIONS"></a>RESULTS &amp; APPLICATIONS</h1><p>在本节中，我们将展示<br>I) NeRFactor实现的高质量几何图形，<br>II) NeRFactor联合估计形状、反射率和照明的能力，<br>III)使用单点光或任意光探针(图5和图6)实现自由视点重照明的应用，<br>IV)使用MVS而不是NeRF进行形状初始化时NeRFactor的性能。<br>最后是材料编辑的应用(图8)。参见附录的B部分，了解在这项工作中使用的各种类型的数据是如何呈现、捕获或收集的</p><h2 id="Shape-Optimization"><a href="#Shape-Optimization" class="headerlink" title="Shape Optimization"></a>Shape Optimization</h2><p>NeRFactor以表面点及其相关表面法线的形式联合估计物体的形状，以及它们对每个光位置的可见性。图3显示了这些几何属性。为了可视化光的可见性，我们取16×32光探测器每个像素对应的512幅可见性图的逐像素平均值，并将该平均图(即环境遮挡)可视化为灰度图像。参见补充视频的电影的每光能见度地图(即，阴影地图)。如图3所示，我们的表面法线和光线可见度是光滑的，与地面真实情况相似，这要归功于联合估计过程，该过程最大限度地减少了重渲染错误，并鼓励空间平滑。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814143126.png" alt="image.png"></p><p>如果我们消除空间平滑性约束，只依赖于重新渲染损失，我们最终会得到不足以渲染的嘈杂几何。虽然这些几何引起的伪影可能不会在低频照明下出现，但恶劣的照明条件(如没有环境照明的单点光，即一次一灯[OLAT])会显示它们，如补充视频中所示。也许令人惊讶的是，即使当我们的平滑限制被禁用时，NeRFactor估计的几何体仍然明显比原始的NeRF几何体少噪声(比较图3的[A]和[B]，见表1的[I])，因为重渲染损失鼓励更平滑的几何体。参见5.1节了解更多细节。</p><h2 id="Joint-Estimation-of-Shape-Reflectance-amp-Lighting"><a href="#Joint-Estimation-of-Shape-Reflectance-amp-Lighting" class="headerlink" title="Joint Estimation of Shape, Reflectance, &amp; Lighting"></a>Joint Estimation of Shape, Reflectance, &amp; Lighting</h2><p>在这个实验中，我们演示了NeRFactor如何将外观分解为具有复杂几何和/或反射率的场景的形状，反射率和照明。</p><p>在对反照率进行视觉化时，我们采用了固有图像文献所使用的惯例，<strong>即假设反照率和阴影的绝对亮度是不可恢复的</strong>[Land and McCann 1971]，此外，我们还<strong>假设颜色恒定问题</strong>(解决光源平均颜色和反照率平均颜色之间的模糊问题的全局颜色校正[Buchsbaum 1980])也不在考虑范围之内。<br>根据这两个假设，我们将预测的反照率可视化，并测量其精度，方法是首先用一个已识别的全局标量对每个RGB通道进行缩放，以便最小化与真实反照率相比的均方误差(mean squared error)，正如Barron和Malik[2014]所做的那样。除非另有说明，否则所有合成场景的反照率预测都以这种方式进行校正，并且我们应用伽马校正($\gamma$= 2.2)在图中正确地显示它们。我们估计的光探头没有按这种方式进行缩放(因为照明估计不是这项工作的主要目标)，并且通过简单地将所有RGB通道的最大强度缩放为1，然后应用伽马校正($\gamma$= 2.2)。</p><p>如图4 (B)所示，NeRFactor预测的高质量和光滑的表面法线接近地面真相，但在具有高频细节的区域(如热狗面包的凹凸不平的表面)除外。在鼓中，我们看到NeRFactor成功地重建了精细的细节，如钹中心的螺丝和鼓侧面的金属边缘。对于榕树，NeRFactor可以恢复复杂的叶片几何形状。环境遮挡贴图也正确地描绘了场景中每个点对灯光的平均曝光。反照率的恢复干净，几乎没有任何阴影或阴影细节不准确地归因于反照率变化;请注意，在反照率预测中，鼓上的阴影是如何缺失的。此外，预测的光探头正确地反映了主光源和蓝天的位置([I]中的蓝色像素)。在这三个场景中，预测的BRDF都是空间变化的，正确反映了场景的不同部分具有不同的材料，如(E)中不同的BRDF潜码所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814145248.png" alt="image.png"></p><p>而不是用更复杂的表示(如球面谐波)来表示照明，我们选择了一个直接的表示:一个纬度-经度图，其像素是HDR强度。由于光照在适度漫射的BRDF反射时被低通滤波器有效地卷积[Ramamoorthi and Hanrahan 2001]，我们不期望以高于16 × 32的分辨率恢复光照。如图4 (I)所示，NeRFactor估计了一个光探测器，它正确地捕获了最左边的明亮光源和蓝天。同样，在图4 (II)中，主光源的位置也得到了正确的估计(左边明亮的白色斑点)。</p><h2 id="Free-Viewpoint-Relighting"><a href="#Free-Viewpoint-Relighting" class="headerlink" title="Free-Viewpoint Relighting"></a>Free-Viewpoint Relighting</h2><p>NeRFactor估计3D场的形状和反射率，从而实现同步重光照和视图合成。因此，本文显示的所有重光照结果和补充视频都是从新的角度呈现的。为了探测NeRFactor的极限，我们使用了苛刻的测试照明条件，即一次只打开一个点灯(OLAT)，没有环境照明。这些测试照明诱导硬投射阴影，这有效地暴露了由于不准确的几何或材料导致的渲染伪影。出于可视化的目的，我们将relit结果(使用NeRF的预测不透明度或MVS的网格轮廓)合成到背景上，背景的颜色是光探针上半部分的平均值</p><p>如图5 (II)所示，NeRFactor合成了在三种测试OLAT条件下热狗投射的正确硬阴影。NeRFactor还会在OLAT条件(I)下生成逼真的榕树渲染图，特别是当(D)中的点光源背光照射榕树时。注意，(D)中的ground truth看起来比NeRFactor的结果更亮，因为NeRFactor只模拟直接光照，而ground truth图像是用全局光照渲染的。当我们用两个新的光探针重新照亮物体时，在热导板上合成了逼真的柔和阴影(II)。在无花果中，花瓶上的镜面正确地反射了两个测试探针中的主光源。在(F)中，树叶也表现出接近地面真值的真实镜面高光。在鼓(III)中，钹被正确地估计为镜面高光，并表现出真实反射，尽管与地面真值各向异性反射(D)不同。这是预期的，因为所有MERL brdf都是各向同性的[Matusik等人，2003]。尽管NeRFactor无法解释这些各向异性反射，但它正确地将它们排除在反照率之外，而不是将它们解释为反照率涂料，因为这样做会违反反照率平滑约束，并与那些反射的视图依赖性相矛盾。在lego中，针对OLAT测试条件(IV)，使用NeRFactor合成了逼真的硬阴影。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814161008.png" alt="image.png"></p><p><strong>Relighting real scenes</strong><br>我们将NeRFactor应用于Mildenhall等人[2020]捕获的两个真实场景，花瓶和松果vasedeck and pinecone。这些捕获特别适合NeRFactor:每个场景有大约100个由未知环境照明的多视图图像。与NeRF一样，我们运行COLMAP Structure From Motion (SFM) [Schönberger and Frahm 2016]来获取每个视图的相机内参和外参。然后，我们训练一个vanilla NeRF来获得初始形状估计，我们将其提取到NeRFactor中，并与反射率和照度一起进行优化。如图6 (I)所示，外观被分解为表面法线、光能见度、反照率和空间变化的BRDF潜码的照明和3D场，它们共同解释了观测到的视图。通过这种分解，我们通过使用新的任意光探针替换估计的照度来重新照亮场景(图6 [II])。因为我们的分解是完全3D的，所以所有的中间缓冲都可以从任何视点渲染，并且显示的重照明结果也来自新的视点。请注意，将这些真实场景绑定在3D盒子中，以避免遥远的几何形状阻挡某些方向的光线，并在重照明时投下阴影。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814161454.png" alt="image.png"></p><h2 id="Shape-Initialization-Using-Multi-View-Stereo"><a href="#Shape-Initialization-Using-Multi-View-Stereo" class="headerlink" title="Shape Initialization Using Multi-View Stereo"></a>Shape Initialization Using Multi-View Stereo</h2><p>我们已经演示了NeRFactor如何使用从NeRF中提取的几何图形作为初始化，并在将反射率和光照联合分解的同时继续细化该几何图形。在这里，我们探讨NeRFactor是否可以与其他形状初始化(如MVS)一起工作。具体来说，我们考虑DTU-MVS数据集[Jensen等人。2014;Aanæs等。2016]为每个场景提供大约50个多视图图像(以及相应的相机姿势)。我们用泊松重构[Kazhdan等人]初始化NeRFactor的形状。Furukawa和Ponce[2009]重建的MVS。有关这些数据的更多细节，请参阅附录B部分。<strong>这个实验不仅探索了形状初始化的另一种可能性，而且还探索了NeRFactor评估的真实图像的另一个来源</strong>。</p><p>NeRFactor实现高质量的形状估计时，从MVS几何而不是NeRF几何开始。如图7 (A, B)所示，NeRFactor估算的表面法线和光可见度没有MVS所遭受的噪声，同时具有足够的几何细节。通过这些高质量的几何估计，NeRFactor实现了与最近邻输入图像相似的逼真视图合成结果(图7 [C])。scan110的发光材料确实有助于恢复高频照明条件(比较在[C]中恢复的两种照明条件)。然后，我们进一步从这个新颖的视角，用两个新颖的光探头重新照亮场景，如图(D, E)所示。除了逼真的镜面高光外，还要注意(D)中由NeRFactor合成的阴影，这要归功于它的可见性建模。请注意，NeRFactor选择用白色反照率和金色照明来解释scan110(而不是相反)，因为在4.2节中讨论了基本的模糊性，但仍然设法用这种合理的解释来真实地重新照亮场景。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814161812.png" alt="image.png"></p><h2 id="Material-Editing"><a href="#Material-Editing" class="headerlink" title="Material Editing"></a>Material Editing</h2><p>由于NeRFactor从外观上分解了漫反射反照率和镜面BRDF，因此可以编辑反照率，非漫反射BRDF，或两者兼而有之，然后在任意光照条件下从任何视点重新渲染编辑过的对象。这里我们重写了预估$z_{\mathrm{BRDF}}$F对MERL数据集中学习到的珍珠漆pearl-paint潜在代码和从涡轮颜色图线性插值的颜色的估计反照率，基于表面点$\text{x-coordinates.}$的空间变化。如图8(左)所示，通过NeRFactor进行因子分解，我们能够在两个具有挑战性的OLAT条件下真实地再现原始估计材料。此外，编辑后的材料也通过相同的测试OLAT条件(图8[右])重新渲染了逼真的高光和硬阴影。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230814162003.png" alt="image.png"></p><h1 id="EVALUATION-STUDIES"><a href="#EVALUATION-STUDIES" class="headerlink" title="EVALUATION STUDIES"></a>EVALUATION STUDIES</h1><p>In this section, we perform <strong>ablation studies</strong> to evaluate the importance of each model component and <strong>compare NeRFactor against both classic and deep learning-based state of the art in the tasks of appearance factorization and relighting</strong>.<br>For <strong>quantitative</strong> evaluations, we use as metrics <strong>Peak Signal-to-Noise Ratio (PSNR)</strong>, <strong>Structural Similarity Index Measure (SSIM)</strong> [Wang et al. 2004], and <strong>Learned Perceptual Image Patch Similarity (LPIPS)</strong> [Zhang et al. 2018]</p><p>另见附录C.1节，关于在不同输入光照条件下对同一物体的反照率估计是否保持一致。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Shadow&amp;Highlight </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shadow&amp;Highlight </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ref-NeRF</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/Ref-NeRF/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/Ref-NeRF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</th></tr></thead><tbody><tr><td>Author</td><td>Dor Verbin and Peter Hedman and Ben Mildenhall and Todd Zickler and Jonathan T. Barron and Pratul P. Srinivasan</td></tr><tr><td>Conf/Jour</td><td>CVPR 2022 (Oral Presentation, Best Student Paper Honorable Mention)</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://dorverbin.github.io/refnerf/">Ref-NeRF (dorverbin.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4566506849944215553&amp;noteId=1909890964757625088">Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810134741.png" alt="image.png"></p><p>贡献：</p><ul><li>借鉴Mip-NeRF的IPE，提出一种新的IDE来编码方向向量</li><li>表面法向通过Spatial MLP来预测，并通过$\mathcal{R}_{\mathrm{p}}=\sum_{i}w_{i}|\hat{\mathbf{n}}_{i}-\hat{\mathbf{n}}_{i}^{\prime}|^{2},$来正则化使得预测得到的法向量和进一步计算的反射更加平滑<ul><li>这些MLP预测的法线往往比梯度密度法线更平滑</li><li>$\hat{\mathbf{n}}(\mathbf{x})=-\frac{\nabla\tau(\mathbf{x})}{|\nabla\tau(\mathbf{x})|}.$Eq.3</li></ul></li><li>计算反射光的新渲染方式$\mathbf{c}=\gamma(\mathbf{c}_d+\mathbf{s}\odot\mathbf{c}_s),$<ul><li>$\hat{\mathbf{\omega}}_r=2(\hat{\mathbf{\omega}}_o\cdot\hat{\mathbf{n}})\hat{\mathbf{n}}-\hat{\mathbf{\omega}}_o,$ Eq.4</li><li>$L_{\mathrm{out}}(\hat{\mathbf{\omega}}_{o})\propto\int L_{\mathrm{in}}(\hat{\mathbf{\omega}}_{i})p(\hat{\mathbf{\omega}}_{r}\cdot\hat{\mathbf{\omega}}_{i})d\hat{\mathbf{\omega}}_{i}=F(\hat{\mathbf{\omega}}_{r}).$ 借鉴此BRDF，提出的Direction MLP 得出$c_s$</li><li>漫反射颜色$c_d$通过Spatial MLP预测得到</li><li>s是高光色调</li><li>将空间MLP输出的瓶颈向量b传递到Direction MLP中，这样反射的亮度就可以随着3D位置的变化而变化。</li></ul></li><li>$\mathcal{R}_{\mathrm{o}}=\sum_{i}w_{i}\max(0,\hat{\mathbf{n}}_{i}^{\prime}\cdot\hat{\mathbf{d}})^{2}.$ 正则化项惩罚朝向远离相机的法线</li></ul><p>局限：</p><ul><li>编码导致的速度慢，和Spatial MLP的loss反向传播速度比Mip-NeRF慢</li><li>没有明确地模拟相互反射或非远距离照明<ul><li>忽略互反射和自遮挡等现象</li></ul></li></ul><span id="more"></span><h1 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h1><ul><li>编码和反向传播时速度慢<ul><li>虽然Ref-NeRF在视图合成方面显著改善了之前表现最好的神经场景表示，但它需要增加计算量:评估<strong>我们的集成方向编码比计算标准位置编码稍微慢一些</strong>，并且通过空间MLP的梯度反向传播来计算法向量使我们的模型比mip-NeRF慢大约25%。</li></ul></li><li>我们通过反射方向对出射亮度的重新参数化并没有明确地模拟相互反射或非远距离照明，因此在这种情况下，我们对mip-NeRF的改进减少了。</li></ul><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们已经证明，先前用于视图合成的神经表示不能准确地表示和渲染具有镜面和反射的场景。<br>我们的模型Ref-NeRF引入了一种<strong>新的参数化和基于视图的外向辐射结构</strong>，<strong>以及法向量上的正则化器</strong>。<br>这些贡献使Ref-NeRF能够显著提高视图依赖外观的质量和场景合成视图中法向量的准确性。<br>我们相信这项工作在捕捉和再现物体和场景的丰富逼真外观方面取得了重要进展。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>神经辐射场(Neural Radiance Fields, NeRF)是一种流行的视图合成技术，它将场景表示为连续的体积函数，由多层感知器参数化，提供每个位置的体积密度和依赖于视图的发射辐射。<br>虽然基于nerf的技术擅长表示具有平滑变化的视图依赖外观的精细几何结构，<strong>但它们往往无法准确捕捉和再现光滑表面的外观</strong>。<br>我们通过引入Ref-NeRF来解决这一限制，它用反射亮度的表示取代了NeRF对依赖于视图的照射亮度的参数化，并使用空间变化的场景属性集合来构造该函数。<br>我们表明，加上法向量上的正则化器，我们的模型显著提高了镜面反射的真实感和准确性。此外，我们表明，我们的模型的内部表示的外向辐射是可解释的和有用的场景编辑。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>神经辐射场(Neural Radiance Fields, NeRF)[24]使用神经体积场景表示从新颖的视点呈现引人注目的逼真的3D场景图像。给定场景中的任何输入3D坐标，“空间”多层感知器(MLP)在该点输出相应的体积密度，而“directional”MLP沿着任何输入观看方向输出该点的输出辐射。虽然NeRF渲染的视图依赖外观乍一看似乎是合理的，但仔细检查镜面高光会发现虚假的光泽伪影在渲染视图之间逐渐消失(图1)，而不是以物理上合理的方式平滑地在表面上移动</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810135033.png" alt="image.png"><br><em>与之前表现最好的神经视图合成模型mip-NeRF相比，Ref-NeRF显着改善了法向量(顶部行)和视觉真实感(其余行)。RefNeRF的改进在渲染帧(第2行和第3行)中很明显，在渲染视频(底部行极平面图像和补充视频)中更是如此，其光滑的高光在视图中真实地移动，而不是像mipNeRF那样模糊和褪色。图像PSNR(越高越好)和表面法向平均角度误差(越低越好)如图所示。</em></p><p>这些伪影是由NeRF(以及性能最好的扩展，如mipNeRF[2])的两个基本问题引起的。</p><ul><li>首先，NeRF将每个点的出射亮度参数化为观看方向的函数，这很不适合插值。图2表明，即使是一个简单的玩具设置，场景的真实亮度功能也会随着视角方向的变化而迅速变化，特别是在高光周围。因此，<strong>NeRF只能从训练图像中观察到的特定观看方向准确地渲染场景点的外观，并且其对新视点的光滑外观的插值效果较差</strong>。</li><li>其次，NeRF倾向于使用物体内部的各向同性发射器来“伪造”镜面反射，而不是由表面上的点发出的与视图相关的辐射，从而导致物体具有半透明或“雾蒙蒙”的外壳。</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810135656.png" alt="image.png"><br><em>在NeRF和RefNeRF中，使用沿x参数化表面曲线的二维位角辐射切片，在彩色灯光下对光滑物体进行了显示。由于NeRF(中行)使用视角$φ_o$作为输入，当呈现有光泽的反射率(左)或空间变化的材料(右)时，它必须在高度复杂的函数之间进行插值，例如如图所示的不规则弯曲的彩色线条。相比之下，Ref-NeRF(下行)使用法向量$φ_n$和反射角$φ_r$来参数化辐射，并在其空间MLP中添加漫射颜色cd和粗糙度ρ，这使得即使对于有光泽或空间变化的材料，辐射函数也可以简单地建模。灰色棋盘格表示在x位置表面下的方向。</em></p><div class="note info">            <ul><li><p>NeRF简单的使用观察方向来作为MLP输入，插值得到颜色，函数建模起来很复杂</p></li><li><p>Ref-NeRF使用更复杂的体渲染函数，输入包括法向量，反射角以及漫反射颜色和粗糙度，可使用的输入参数很多，因此函数的建模很简单</p></li></ul>          </div><p>我们的关键见解是，构建NeRF对视图依赖外观的表示可以使底层功能更简单，更容易插值。<strong>我们提出了一个模型，我们称之为Ref-NeRF，它通过提供观看向量对局部法向量的反射作为输入，而不是观看向量本身，来重新参数化NeRF的定向MLP</strong>。图2(左栏)表明，对于一个由光滑物体组成的玩具场景，在远处照明下，这个反射辐射函数在整个场景中是恒定的(忽略光照遮挡和相互反射)，因为它不受表面方向变化的影响。因此，由于定向MLP作为插值内核，我们的模型能够更好地“共享”附近点之间的外观观察，从而在插值视图中呈现更逼真的视图依赖效果。我们还引入了一个<strong>集成的定向编码技术</strong>，我们将外向的亮度结构成明确的漫射和高光组件，以允许反射的亮度功能保持平滑，尽管在场景中材料和纹理的变化。</p><p>虽然这些改进至关重要地使Ref-NeRF能够准确地插值依赖于视图的外观，但它们依赖于反映从NeRF的体积几何估计的法向量的观看向量的能力。这就出现了一个问题，因为NeRF的几何形状是模糊的，并且不是紧密地集中在表面上，并且它的法向量too noisy，对于计算反射方向是有用的(如图1的右列所示)。<strong>我们用一种新的体积密度正则化器改善了这个问题，它显著提高了NeRF法向量的质量，并鼓励体积密度集中在表面周围</strong>。使我们的模型能够计算准确的反射向量并呈现真实的镜面反射，如图1所示。</p><p>主要贡献：</p><ul><li>基于观测向量对局部法向量的反射，NeRF的出射亮度的重新参数化(第3.1节)。</li><li>集成方向编码(第3.2节)，当与漫射和高光颜色的分离(第3.3节)相结合时，使反射的亮度函数能够在不同材质和纹理的场景中平滑地插值。</li><li>一种<strong>将体积密度集中在表面周围并改善NeRF法向量方向的正则化方法</strong>(第4节)。</li></ul><p>我们将这些变化应用于mip-NeRF[2]之上，mip-NeRF是目前表现最好的视图合成神经表示。我们的实验表明，Ref-NeRF产生了最先进的新视点渲染，并大大提高了以前高镜面或光滑物体的最佳视图合成方法的质量。此外，我们的外向辐射结构产生可解释的组件(法向量，材料粗糙度，漫射纹理和高光色调)，使令人信服的场景编辑能力。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>我们回顾了NeRF和用于真实感视图合成的相关方法，以及用于捕获和渲染镜面外观的计算机图形学技术。</p><ul><li><strong>3D scene representations for view synthesis</strong> 视图合成，即使用一个场景的观察图像来从新的未观察到的摄像机视点渲染图像的任务，是计算机视觉和图形学领域一个长期存在的研究问题。<ul><li>在可能密集捕获场景图像的情况下，<strong>简单的光场插值技术</strong>[12,18]可以呈现高保真度的新视图。<strong>然而，在大多数情况下，光场的详尽采样是不切实际的</strong>，因此从稀疏捕获的图像中进行视图合成的方法重建3D场景几何，以便将观察到的图像重新投影到新的视点中[8]。</li><li>对于具有光滑表面的场景，一些方法明确地构建虚拟几何来解释反射的运动[17,33,35]。早期的方法使用三角形网格作为几何表示，并通过启发式[7,9,42]或学习式[13,32]混合算法重新投影和混合多个捕获的图像来呈现新的视图。最近的研究使用了体积表示，如体素网格[20]或多平面图像[10,23,37,41,48]，它们比网格更适合基于梯度的优化。<strong>虽然这些离散的体积表示可以有效地用于视图合成，但它们的立方缩放限制了它们表示大型或高分辨率场景的能力</strong></li><li>最近基于坐标的神经表示范式用MLP取代了传统的离散表示，该MLP从任何连续输入的3D坐标映射到该位置的场景的几何形状和外观。<strong>NeRF</strong>[24]是一种用于真实感视图合成的有效的基于坐标的神经表示，它将场景表示为阻挡和发射与视图相关的光的粒子场。NeRF启发了许多后续的工作，将其神经体场景表示扩展到应用领域，包括动态和可变形场景[26]，化身动画[11,27]，甚至摄影旅游[21]。<strong>我们的工作重点是改进NeRF的核心组件:视图依赖外观的表示</strong>。我们相信，这里提出的改进可以用于改善上述许多NeRF应用程序的渲染质量。</li><li>我们的方法的一个关键组成部分考虑相机光线反射的NeRF的几何形状。这一想法被最近的作品所分享，这些作品扩展了NeRF，通过将外观分解为场景照明和材料来实现重照明[3 - 5,36,45,47]。<strong>至关重要的是，我们的模型将场景structures成不需要具有精确物理含义的组件，因此能够避免这些作品需要做出的强烈简化假设</strong>(例如已知照明[3,36]，无自遮挡[4,5,45]，单一材料场景[45])，以恢复照明和材料的明确参数表示。我们的工作还侧重于提高从NeRF几何中提取的法向量的平滑性和质量。最近将NeRF的神经体积表示与神经隐式表面表示相结合的研究也实现了这一目标[25,39,43]UNISURF,Neus。<strong>然而，这些方法主要关注从其表示中提取的等值面的质量，而不是呈现的新视图的质量，因此它们的视图合成性能明显低于性能最好的nerf类模型</strong>。</li></ul></li><li><strong>Efficient rendering of glossy appearance</strong> 我们的工作灵感来自计算机图形学中<strong>用于表示和渲染依赖于视图的镜面和反射外观的开创性方法，特别是基于预计算的技术</strong>[29]。<ul><li>在我们的定向MLP中编码的反射亮度函数类似于预过滤的环境地图[15,31]，它们被引入用于高光外观的实时渲染。预过滤的环境图利用了这样的洞察力，即<strong>从表面发出的光可以被视为入射光和(径向对称的)双向反射分布函数(BRDF)的球面卷积，该函数描述了表面的材料特性</strong>[30]。在存储这个卷积结果之后，通过简单地索引到预先过滤的环境地图中，通过观察向量关于法向量的反射方向，可以有效地渲染与物体相交的光线。</li><li>我们的工作不是渲染预定义的3D资产，而是利用这些计算机图形学的见解来解决计算机视觉问题，我们正在从图像中恢复场景的可渲染模型。此外，我们的定向MLP对反射辐射的表示在计算机图形学中使用的预过滤环境地图表示的基础上进行了关键的改进:<strong>我们的定向MLP可以表示由于光照和场景属性(如材料粗糙度和纹理)的空间变化而导致的反射辐射的空间变化</strong>，而前面描述的技术需要计算和存储每个可能材料的离散预过滤辐射地图。</li></ul></li><li>我们的工作也受到了计算机图形学中一系列工作的启发，这些工作重新参数化了directional函数，如BRDF[31,34]和外向辐射[42]，以改进插值和压缩。</li></ul><h2 id="NeRF-Preliminaries"><a href="#NeRF-Preliminaries" class="headerlink" title="NeRF Preliminaries"></a>NeRF Preliminaries</h2><p>NeRF[24]将场景表示为发射和吸收光的粒子的体积场。给定任何输入3D位置x, NeRF使用空间MLP输出体积粒子的密度τ (x)以及“瓶颈bottleneck”向量b(x)，该向量与视图方向d一起提供给第二个定向MLP，该方向d输出该3D位置上粒子发出的光的颜色c(x, d)(见图4的可视化)。请注意，Mildenhall等人[24]在他们的工作中使用了单层定向MLP，而之前的工作通常将NeRF的空间和定向MLP组合为单个MLP。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810134741.png" alt="image.png"></p><p>两个MLP在点$\mathbf{x}_{i}=\mathbf{o}+t_{i}\hat{\mathbf{d}}$处沿发源于o方向为d的射线进行查询，返回密度$\{\tau_{i}\}$和颜色$\{c_{i}\}$。这些密度和颜色使用数值正交[22]进行alpha合成，得到光线对应像素的颜色:$\mathbf{C}(\mathbf{o},\hat{\mathbf{d}})=\sum_iw_i\mathbf{c}_i,$ $w_i=e^{-\sum_{j&lt;i}\tau_j(t_{j+1}-t_j)}\left(1-e^{-\tau_i(t_{i+1}-t_i)}\right).$ Eq.1</p><p>对MLP参数进行了优化，以最小化每个像素的预测颜色$\mathbf{C}(\mathbf{o},\hat{\mathbf{d}})$与其从输入图像中获取的真实颜色$\mathbf{C}_{gt}(\mathbf{o},\hat{\mathbf{d}})$之间的L2差异<br>$\mathcal{L}=\sum_{\mathrm{o,\hat{d}}}|\mathbf{C}(\mathbf{o},\hat{\mathbf{d}})-\mathbf{C}_{\mathrm{gt}}(\mathbf{o},\hat{\mathbf{d}})|^{2}.$ Eq.2</p><p>在实践中，NeRF使用两组mlp，一组是粗的，一组是细的，以分层采样的方式，两者都经过训练以最小化公式2中的损失</p><p>先前基于nerf的模型通过使用空间MLP来预测任何3D位置的单位向量[3,47]，或使用相对于3D位置的体积密度梯度来定义场景中的法向量场[4,36] :$\hat{\mathbf{n}}(\mathbf{x})=-\frac{\nabla\tau(\mathbf{x})}{|\nabla\tau(\mathbf{x})|}.$Eq.3</p><h1 id="Structured-View-Dependent-Appearance"><a href="#Structured-View-Dependent-Appearance" class="headerlink" title="Structured View-Dependent Appearance"></a>Structured View-Dependent Appearance</h1><p>在本节中，我们将描述Ref-NeRF如何将每个点的出射亮度结构为(预过滤的)入射亮度、漫射颜色、材料粗糙度和镜面色调tint，这比通过视图方向参数化的出射亮度函数更适合于整个场景的平滑插值。通过在我们的定向MLP中明确地使用这些组件(图4)，Ref-NeRF可以准确地重现镜面高光和反射的外观。此外，我们的模型的外向辐射的分解使场景编辑<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810134741.png" alt="image.png"></p><h2 id="Reflection-Direction-Parameterization"><a href="#Reflection-Direction-Parameterization" class="headerlink" title="Reflection Direction Parameterization"></a>Reflection Direction Parameterization</h2><p>对比NeRF直接使用视图方向，我们将输出亮度作为视图方向关于局部法向量反射的函数重新参数化: $\hat{\mathbf{\omega}}_r=2(\hat{\mathbf{\omega}}_o\cdot\hat{\mathbf{n}})\hat{\mathbf{n}}-\hat{\mathbf{\omega}}_o,$ Eq.4</p><p>式中，$\hat{\omega}_o=-\hat{\mathbf{d}}$为空间中某点指向摄像机的单位向量，$\hat n$为该点处的法向量。如图2所示，这种重新参数化使镜面外观更适合插值。</p><p>对于反射视图方向旋转对称的BRDFs，即对于某些叶瓣函数lobe function p满足$f(\hat{\mathbf{\omega}}_i,\hat{\mathbf{\omega}}_o) = p(\hat{\mathbf{\omega}}_r\cdot\hat{\mathbf{\omega}}_i)$的BRDF(包括Phong等brdf[28])，<strong>忽略互反射和自遮挡等现象</strong>，视相关辐射度仅是反射方向$\hat{\mathbf{\omega}}_r$的函数:</p><p>$L_{\mathrm{out}}(\hat{\mathbf{\omega}}_{o})\propto\int L_{\mathrm{in}}(\hat{\mathbf{\omega}}_{i})p(\hat{\mathbf{\omega}}_{r}\cdot\hat{\mathbf{\omega}}_{i})d\hat{\mathbf{\omega}}_{i}=F(\hat{\mathbf{\omega}}_{r}).$Eq.5</p><p>因此，通过查询具有反射方向的定向MLP，我们有效地训练它作为$\hat{\omega}_{r}$的函数输出该积分。由于菲涅耳效应等现象[15]，更一般的BRDF可能会随着视图方向与法向量之间的角度而变化，因此我们还向定向MLP输入了$\hat{\mathbf{n}}\cdot\hat{\hat{\omega}}_{o}$，以允许模型调整底层BRDF的形状。</p><h2 id="Integrated-Directional-Encoding"><a href="#Integrated-Directional-Encoding" class="headerlink" title="Integrated Directional Encoding"></a>Integrated Directional Encoding</h2><p>在具有空间变化的材料的现实场景中，radiance不能单独表示为反射方向的函数。较粗糙的材料外观随反射方向变化缓慢，而较光滑或光泽的材料外观变化迅速。我们引入了一种技术，我们称之为集成定向编码(IDE)，它使定向MLP能够有效地表示具有任何连续值粗糙度的材料的辐射度函数。我们的IDE受到mip-NeRF[2]引入的集成位置编码的启发，它使空间MLP表示预过滤的抗混叠体积密度。</p><p>首先，我们不是像在NeRF中那样用一组正弦波编码方向，而是用一组球面谐波$\{Y_{\ell}^{m}\}.$编码方向。这种编码得益于在球体上的静止性，这一特性对欧几里得空间中位置编码的有效性至关重要[24,38]。<br>接下来，我们通过编码反射向量的分布而不是单个向量，使定向MLP能够推断具有不同粗糙度的材料。我们用von Mises-Fisher (vMF)分布(也称为归一化球形高斯分布)对单位球上定义的分布进行建模，以反射向量$\hat{\omega}_{r}$为中心，浓度参数κ定义为逆粗糙度$\kappa=1/\rho.$。粗糙度ρ由空间MLP(使用softplus激活)输出，并决定表面的粗糙度:较大的ρ值对应于具有更宽vMF分布的更粗糙的表面。我们的IDE使用在这个vMF分布下的一组球面谐波的期望值来编码反射方向的分布Eq.6<br>$\mathrm{IDE}(\hat{\mathbf{\omega}}_r,\kappa)=\left\{\mathbb{E}_{\hat{\mathbf{\omega}}\sim\mathrm{vMF}(\hat{\mathbf{\omega}}_r,\kappa)}[Y_\ell^m(\hat{\mathbf{\omega}})]\colon(\ell,m)\in\mathcal{M}_L\right\},$<br>$\mathcal{M}_{L}=\{(\ell,m):\ell=1,…,2^{L},m=0,…,\ell\}.$</p><p>在我们的补充中，我们证明了vMF分布下任意球谐的期望值有如下简单的封闭表达式:$\mathbb{E}_{\hat{\mathbf{\omega}}\sim\mathrm{vMF}(\hat{\mathbf{\omega}}_r,\kappa)}[Y_\ell^m(\hat{\mathbf{\omega}})]=A_\ell(\kappa)Y_\ell^m(\hat{\mathbf{\omega}}_r),$Eq.7</p><p>并且第$\ell$个衰减函数A (κ)可以用一个简单的指数函数很好地近似:$A_{\ell}(\kappa)\approx\exp\left(-\frac{\ell(\ell+1)}{2\kappa}\right).$ Eq.8</p><p>图3说明了我们的集成方向编码具有直观的行为;通过降低κ来增加材料的粗糙度对应于衰减编码的高阶球面谐波，从而产生更宽的插值核，从而<strong>限制了所表示的视相关颜色中的高频</strong>。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810152303.png" alt="image.png"><br><em>我们使用集成的方向编码使定向MLP能够表示任何连续值粗糙度的反射亮度函数。编码的每个分量都是一个球谐函数与一个具有浓度参数κ的vMF分布卷积，由我们的空间MLP输出(相当于vMF下球谐的期望)。较少粗糙的位置接收更高频率的编码(顶部)，而更多粗糙的区域接收衰减的高频编码。我们的IDE允许在不同粗糙度的位置之间共享照明信息，并允许编辑反射率。</em></p><h2 id="Diffuse-and-Specular-Colors"><a href="#Diffuse-and-Specular-Colors" class="headerlink" title="Diffuse and Specular Colors"></a>Diffuse and Specular Colors</h2><p>我们通过分离漫射和高光组件进一步简化了出射亮度的函数，使用漫射颜色(根据定义)仅是位置的函数。我们修改空间MLP以输出漫射色$c_d$和高光色调s，并将其与定向MLP提供的高光色$c_s$相结合以获得单个颜色值:$\mathbf{c}=\gamma(\mathbf{c}_d+\mathbf{s}\odot\mathbf{c}_s),$Eq.9</p><p>式中$\odot$为元素乘法，$\gamma$为固定色调映射函数，将线性颜色转换为sRGB[1]，并将输出颜色剪辑为lie[0,1]。</p><h2 id="Additional-Degrees-of-Freedom"><a href="#Additional-Degrees-of-Freedom" class="headerlink" title="Additional Degrees of Freedom"></a>Additional Degrees of Freedom</h2><p>照明的相互反射和自遮挡等效果会导致照明在场景中的空间变化。因此，我们将空间MLP输出的瓶颈向量b传递到定向MLP中，这样反射的亮度就可以随着3D位置的变化而变化。</p><h1 id="Accurate-Normal-Vectors"><a href="#Accurate-Normal-Vectors" class="headerlink" title="Accurate Normal Vectors"></a>Accurate Normal Vectors</h1><p>虽然上一节中描述的出射辐射结构为镜面插值提供了更好的参数化，但它依赖于对体积密度的良好估计，以促进准确的反射方向矢量。然而，<strong>基于nerf的模型所恢复的体积密度场存在两个局限性</strong>:<br>1) 根据方程3的体积密度梯度估计的法向量通常非常嘈杂(图1和图5);<br>2) NeRF倾向于通过在物体内部嵌入发射器并使用“雾蒙蒙的”漫射表面部分遮挡它们来“伪造”镜面高光(见图5)。这是一个次优的解释，因为它要求表面上的漫射内容是半透明的，这样嵌入的发射器就可以“发光”。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810154823.png" alt="image.png"></p><p>我们通过使用预测法线来计算反射方向来解决第一个问题:<br>对于沿着射线的每个位置$x_i$，我们从空间MLP输出一个3向量，然后我们将其归一化以获得预测法线$\hat{\mathbf{n}}_i^{\prime}.$<br>我们使用一个简单的惩罚将这些预测的法线与沿每条射线$\{\hat{\mathbf{n}}_{i}\}$的潜在密度梯度法线样本联系起来:$\mathcal{R}_{\mathrm{p}}=\sum_{i}w_{i}|\hat{\mathbf{n}}_{i}-\hat{\mathbf{n}}_{i}^{\prime}|^{2},$ Eq.10</p><p>其中，$w_i$为第i个样本沿射线的权重，定义如式1。<strong>这些MLP预测的法线往往比梯度密度法线更平滑</strong>，因为梯度算子在MLP的有效插值核上起到了高通滤波器的作用[38]。</p><p>我们通过引入一个新的正则化术语来解决第二个问题，<strong>该术语惩罚“背向”的法线</strong>，即朝向远离相机的法线，沿着有助于光线渲染颜色的光线的样本$\mathcal{R}_{\mathrm{o}}=\sum_{i}w_{i}\max(0,\hat{\mathbf{n}}_{i}^{\prime}\cdot\hat{\mathbf{d}})^{2}.$ Eq.11</p><p>这种正则化在“雾蒙蒙”的表面上起着惩罚作用:当样本“可见”(高$w_i$)并且体积密度沿着射线减小时(即，$\hat{\mathbf{n}}_i^{\prime}$与射线方向$\hat{\mathrm{d}}$之间的点积为正)，样本就会受到惩罚。这种法线方向的损失使我们的方法无法将镜面解释为隐藏在半透明表面下的发射器，并且由此产生的改进法线使Ref-NeRF能够计算精确的反射方向，用于查询定向MLP</p><p>在整篇论文中，我们使用梯度密度法线进行可视化和定量评估，因为它们直接展示了底层恢复场景几何的质量。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>我们在mip-NeRF[2]上实现了我们的模型，这是一个减少混叠的NeRF的改进版本。我们使用与mip-NeRF相同的空间MLP架构(8层，256个隐藏单元，ReLU激活)，但我们使用了比mip-NeRF更大的方向MLP(8层，256个隐藏单元，ReLU激活)，以更好地表示高频反射辐射分布。<br>我们使用与之前的视图合成工作相同的定量指标[2,24,45]PSNR, SSIM[40]和LPIPS[46]用于评估渲染质量，平均角误差(MAE)用于评估估计的法向量。</p><p>虽然NeRF[24]使用的“Blender”数据集包含了各种复杂几何形状的物体，但它在材料多样性方面受到严重限制:大多数场景主要是Lambertian的。为了探索更具挑战性的材料属性，我<strong>们创建了一个额外的“Shiny Blender”数据集，其中有6个不同的光滑物体在类似NeRF数据集的条件下在Blender中渲染(每个场景100个训练和200个测试图像)</strong>。表1中的定量结果突出了我们的模型在渲染这些高度镜面场景的新视图方面比mip-NeRF(以前表现最好的技术)具有显著优势。<br>我们还包括三个改进版本的mip-NeRF，它们都有一个8层定向MLP，分别是:<br>1)没有额外的组件;<br>2)在定向MLP中附加视点方向的法向量(如IDR[44]和VolSDF [43]);<br>3)我们的方向损失应用于mipNeRF的密度梯度法向量。</p><p>我们的方法在新颖的视图渲染质量和法向量精度方面明显优于所有这些改进的变体，都是以前表现最好的神经视图合成方法。</p><ul><li>虽然PhySG[45]恢复更准确的法线，但它需要真实的对象掩码(所有其他方法只需要RGB图像)，并且产生明显更差的渲染。图5展示了我们使用数据集中的一个对象的方法的影响:</li><li>虽然mip-NeRF[2]无法恢复具有两个粗糙度的简单金属球体的几何形状和外观，但我们的方法产生了近乎完美的重建。图9显示了该数据集的另一个可视化示例，展示了我们的模型在恢复法向量和渲染镜面方面的改进。</li></ul><p>我们还将Ref-NeRF与来自原始NeRF论文[24]的<strong>标准Blender数据集</strong>上的最新神经视图合成基线方法进行了比较。表2显示，我们的方法在所有图像质量指标上优于所有先前的工作。与mip-NeRF相比，我们的方法在法向量的MAE方面也有了很大的提高(35%)。虽然混合表面体积VolSDF表示[43]恢复的法向量略准确(MAE低15%)，但我们的PSNR比他们的高得多(6dB)。此外，VolSDF倾向于过度平滑几何，这使得我们的结果在质量上优于检查(参见图6)。</p><p>除了这两个合成数据集之外，我们还在一组3个<strong>Real Captured Scenes真实捕获的场景</strong>上评估了我们的模型。我们捕获了一个“轿车”场景，并使用了稀疏神经辐射网格论文[14]中的“花园球体”和“玩具车”捕获。图8和我们的补充表明，我们渲染的镜面反射和恢复的法向量通常在这些真实场景中更加准确。</p><p>我们的外向辐射结构使视图一致的场景编辑。虽然我们没有将外观进行完整的反向渲染分解为brdf和照明，但我们的各个组件的行为直观，并实现了从标准NeRF无法实现的视觉上合理的场景编辑结果。图7显示了场景组件的示例编辑，我们的补充视频包含了演示编辑模型的视图一致性的其他示例。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Shadow&amp;Highlight </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shadow&amp;Highlight </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ShadowNeuS</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/ShadowNeuS/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/ShadowNeuS/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision</th></tr></thead><tbody><tr><td>Author</td><td>Jingwang Ling and Zhibo Wang and Feng Xu</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://gerwang.github.io/shadowneus/">ShadowNeuS (gerwang.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4736823086481948673&amp;noteId=1908667790249771264">ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230809181405.png" alt="image.png"><br>方法：假设场景不发光，且忽略相互反射</p><ul><li>从二值阴影图像中获得可见表面的入射亮度，然后处理更复杂的RGB图像</li><li>入射光辐射$C_\mathrm{in}(x,l)=L\prod_{i=1}^N(1-\alpha_i)$， 从单视图、多光源中重建出3D shape<ul><li>$\mathcal{L}_\mathrm{shadow}=|\widehat{C}_\mathrm{in}-I_\mathrm{s}|_1.$ </li></ul></li><li>出射光辐射$C(x,-\mathbf{v})=(\rho_d+\rho_s)C_{\mathrm{in}}(x,l)(l\cdot\mathbf{n})$<ul><li>$\mathcal{L}_\mathrm{rgb}=|\widehat{C}-I_\mathrm{r}|_1$</li></ul></li></ul><p>表现：<strong>outperforms the SOTAs in single-view reconstruction</strong>, and it has the power to reconstruct scene geometries out of the camera’s line of sight.</p><span id="more"></span><h1 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h1><p>大量的实验证明了所提出的阴影射线监督在重建神经场景中的有效性。然而，作为对阴影射线建模的早期尝试，我们的方法是基于几个假设的。<strong>我们假设场景不发光，忽略相互反射来简化光建模</strong>。我们观察到一些薄结构过于复杂，在我们的重建中仍然可能缺失。这是一个普遍的限制，可以通过薄结构神经SDF的进展得到改善，正如最近的工作[10,27]。</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>与NeRF监控摄像机光线相比，我们在神经场景表示中实现了阴影光线的完全可微监控。该技术可以从单视图多光观测中进行形状重建，<strong>并支持纯阴影和RGB输入</strong>。我们的技术对点和方向光都很有效，可以用于3D重建和重照明。提出了一种多射线采样策略，以解决表面边界对阴影射线定位的挑战。实验表明，<strong>该方法在单视图重建方面优于sota，并且具有在相机视线之外重建场景几何形状的能力</strong>。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>通过监督场景和多视图图像平面之间的摄像机光线，NeRF为新视图合成任务重建神经场景表示。另一方面，光源和场景之间的阴影光线还需要考虑。因此，我们提出了一种<strong>新的阴影射线监督方案</strong>，该方案既优化了沿射线的采样，也优化了射线的位置。通过监督阴影光线，我们成功地从多个光照条件下的单视图图像中重建了场景的神经SDF。给定单视图二元阴影，我们训练一个神经网络来重建一个不受相机视线限制的完整场景。通过进一步建模图像颜色和阴影光线之间的相关性，我们的技术也可以有效地扩展到RGB输入。我们将我们的方法与以前的工作进行了比较，在挑战性的任务上：从单视图二进制阴影或RGB图像中重建形状，并观察到显著的改进</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>近年来，神经场[45]被用于三维场景的表征。由于能够使用紧凑的神经网络连续参数化场景，因此它达到了卓越的质量。神经网络的性质使其能够适应3D视觉中的各种优化任务，包括基于图像的[30,53]和基于点云的[28,33]等长期存在的问题。因此，越来越多的研究使用神经场作为三维场景的表征来完成各种相关任务。</p><p>其中，NeRF[29]是将部分基于物理的光传输[40]纳入神经场的代表性方法。光传输描述了光从光源到场景，然后从场景到相机的传播。NeRF考虑后一部分沿相机光线(从相机穿过场景的光线)建模场景和相机之间的相互作用。通过将不同视点的摄像机光线与相应的记录图像进行监督，NeRF优化了一个神经场来代表场景。然后，NeRF通过优化后的神经场，从新的视点投射相机光线，生成新的视点图像。<br>然而，<strong>NeRF并没有对从场景到光源的光线进行建模</strong>，这促使我们考虑:我们能否通过监督这些光线来优化神经场?这些光线通常被称为阴影光线，因为从光源发出的光可以被沿着光线的场景粒子吸收，从而在场景表面产生不同的光可见性(也称为阴影)。通过记录表面的入射光，我们应该能够监督阴影光线来推断场景几何。<br>鉴于这一观察，我们推导出一个新的问题，即监督阴影光线以优化表示场景的神经场，类似于对相机光线建模的 NeRF。与NeRF中的多个视点一样，<strong>我们使用不同的光方向多次照亮场景，以获得足够的观测</strong>。对于每个照明，我们使用固定相机将场景表面的光可见性记录为阴影射线的监督标签。由于通过 3D 空间连接场景和光源的光线，我们可以重建不受相机视线限制的完整 3D 形状。<br>当使用相机输入监督阴影光线时，我们解决了几个挑战。在NeRF中，每条射线的位置可以由已知的相机中心唯一确定，但阴影射线需要由场景表面确定，这是没有给出的，尚未重建。我们使用迭代更新策略来解决这个问题，其中我们从当前表面估计开始对阴影射线进行采样。更重要的是，我们将采样位置可微到几何表示，从而可以优化阴影射线的起始位置。然而，这种技术不足以在深度突然变化的表面边界处推导出正确的梯度，这与最近在可微渲染的发现相吻合[2,21,24,42,56]。因此，我们通过聚合从多个深度候选开始的阴影射线来计算表面边界。它仍然是有效的，因为边界只占少量的表面，但它显著提高了表面重建质量。<strong>此外，摄像机记录的RGB值编码了表面的出射辐射，而不是入射辐射。出射辐射是光、材料和表面取向的耦合效应</strong>。我们建议对材料和表面方向进行建模，以分解来自RGB输入的入射辐射，以实现重建，而不需要阴影分割(图1中的第1行和第2行)。由于材料建模是可选的，我们的框架还可以采<strong>用二值阴影图像</strong>[18]来<strong>实现形状重建</strong>(图1中的第3行)。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230809174633.png" alt="image.png"></p><p>我们将我们的方法与以前的单视图重建方法（包括基于阴影和基于 RGB）进行比较，并观察到形状重建的显着改进。理论上，我们的方法处理了 NeRF 的双重问题。因此，比较这两种技术的相应部分可以启发读者在一定程度上更深入地了解神经场景表示的本质，以及它们之间的关系。</p><p>贡献总结：</p><ul><li>利用光可见性从多个光照条件下从阴影或RGB图像重建神经SDF的框架。</li><li>一种阴影射线监督方案，通过模拟沿阴影射线的物理相互作用来包含可微光可见性，并有效地处理表面边界。</li><li>与之前关于 RGB 或二进制阴影输入的工作进行比较，以验证重建场景表示的准确性和完整性。</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li><strong>Neural fields for 3D reconstruction</strong>.神经场[45]通常使用多层感知器(MLP)网络参数化3D场景，该网络将场景坐标作为输入。它可以像点云[28,33]这样的3D约束来监督，以重建三维形状的隐式表示。也可以通过可微渲染从多视图图像优化神经场 [2, 30, 53]。<ul><li>NeRF[29]在具有复杂几何形状的场景中展示了显著的新视图合成质量。<strong>然而，NeRF中的密度表示对于正则化和提取场景表面并不容易</strong>。</li><li>因此，[31,43,52]提出将NeRF与表面表示相结合，重建高质量和定义良好的表面。虽然上述所有工作都<strong>需要已知的相机视点</strong>，[12,25,44]BARF等探索联合优化具有神经场的相机参数。</li><li><strong>NeRF不对光源进行建模，并假设场景发出光</strong>。这个假设适用于视图合成，但不是重新照明。一些作品将 NeRF 扩展到重新照明，其中阴影是一个重要的因素。[3, 4, 56] 需要共定位的相机光设置来避免捕获图像中的阴影。[5, 6, 57] 假设环境光平滑并忽略阴影。[11,35,39,49,51,59]采用以光方向为条件的神经网络对光相关阴影进行建模。其中，[11,49,51,58,59]首先使用多视图立体重建几何图形，并使用固定几何计算阴影。<strong>这些作品都没有细化几何图形以匹配捕获的图像中的阴影</strong>。然而，我们表明，通过利用阴影中的信息从头开始重建完整的 3D 形状。</li></ul></li><li><strong>Single-view reconstruction</strong>. [17, 47, 54] 探索了从少数或一张图像重建神经场，但它们需要在预训练网络中进行数据驱动的先验，因此与我们的范围不同。非视距成像[32,38,46]采用瞬态传感器捕获时间分辨信号，使重建相机视图截锥之外的场景。光度立体[9,23]从定向光下捕获的图像重建表面法线。法线可以集成以产生深度图，但需要非平凡的处理 [7, 8]</li><li><strong>Shape from Shadows</strong>. 阴影表示遮挡引起的不同入射辐射，提供场景几何线索。从阴影重建形状的历史悠久为一维曲线[16,19]、二维高度图[14,34,37,55]和三维体素网格[22,36,48]。这些工作通常在不同的光方向下捕获，以获得对阴影的充分观察。阴影显示了这些工作重建表面细节[55]和复杂的薄结构[48]的潜力。该领域最近的工作是 DeepShahadow [18]，它从阴影重建神经深度图。[41]也采用了具有固定照明但多个视点的不同设置，该设置集成了阴影映射来重建神经表示。同时独立，[50] 建议同时在神经场重建中使用阴影和阴影。特别是，<strong>他们</strong>在由根查找定位的不可微表面点计算阴影，<strong>使其依赖于可微阴影计算</strong>。<strong>我们提出了完全可微的阴影射线监督</strong>，优化了阴影射线样本和表面点，<strong>实现了纯阴影或RGB图像的神经场重建</strong>。</li></ul><h1 id="Ray-Supervision-in-Neural-Fields"><a href="#Ray-Supervision-in-Neural-Fields" class="headerlink" title="Ray Supervision in Neural Fields"></a>Ray Supervision in Neural Fields</h1><p>本节首先揭示NeRF[29]训练中作为监督相机射线的本质。从那里，我们发现了一个可推广到任意射线的射线监督方案。该方案使阴影射线能够监督神经场景表示的优化是可行的。</p><h2 id="Camera-ray-supervision-in-NeRF"><a href="#Camera-ray-supervision-in-NeRF" class="headerlink" title="Camera ray supervision in NeRF"></a>Camera ray supervision in NeRF</h2><p>NeRF 旨在优化神经场以适应感兴趣的场景。为了获得场景的观察，NeRF 需要在具有已知相机参数的多个相机视点记录图像。每个图像像素记录从已知方向穿过已知相机中心的相机光线的入射辐射。<strong>由于 NeRF 没有对外部光源进行建模并假设光是从场景粒子发射以简化具有固定照明的场景建模，因此入射辐射实际上归因于沿相机光线无穷小粒子的光吸收和发射的综合影响</strong>。为了拟合观察，NeRF 使用可微分体渲染来模拟神经场中的相同相机光线。NeRF使用求积来近似体渲染中的连续积分，采样N个距离$t_{1},\cdots,t_{N}$，从相机中心o沿相机射线方向v开始。利用场景密度$\sigma_{i}$和每个样本点$\mathbf{p}(t_i)=o+t_i\mathbf{v},$发射亮度$c_{i}$，摄像机处的估计亮度C可表示为:<br>$C(\mathbf{o},\mathbf{v})=\sum_{i=1}^{N}T_{i}\alpha_{i}c_{i},$方程式(1)<br>其中$\alpha_{i}=1-\exp{(-\sigma_{i}(t_{i+1}-t_{i}))}$ 是离散不透明度，$T_i=\exp(-\sum_{j=1}^{i-1}\sigma_j\cdot(t_{j+1}-t_j))$ 表示光透射率，即发射光的比例从点$\mathbf{p}(t_i)$到达相机。在像素处记录的入射辐射可用于监督模拟辐射 C。NeRF在每次迭代中对相机光线的随机子集进行训练。由于神经场接收来自许多摄像机光线在不同视点方向上行进的监督信号，因此它获得足够的场景信息来优化这些光线穿过空间中的神经场。</p><h2 id="Generalized-ray-supervision"><a href="#Generalized-ray-supervision" class="headerlink" title="Generalized ray supervision"></a>Generalized ray supervision</h2><p>NeRF 可以监督相机光线以优化神经场的原因是多视图相机将辐射记录为光线的标签。此外，由于每个相机都经过校准，每个记录的光线的 3D 位置和方向都是明确定义的。我们可以<strong>将多视角相机的每个像素视为一个“射线传感器”</strong>，记录特定光线的入射亮度，因为每个像素在训练中独立使用。这些射线传感器是NeRF技术的关键。更一般地说，如果我们让“射线传感器”记录场景中的其他类型的光线，也可以实现场景重建。这促使我们考虑我们是否可以监督其他光线并设计光线传感器来记录它们的辐射。</p><h2 id="Shadow-ray-supervision"><a href="#Shadow-ray-supervision" class="headerlink" title="Shadow ray supervision"></a>Shadow ray supervision</h2><p>由于相机光线在神经场景重建方面取得了巨大成功，作为光传输中的对应物，连接场景和从源的射线，也就是阴影光线，也应该能够用于重建神经场景。我们首先考虑一个理想的设置，其中<strong>许多假设的射线传感器被放置在不同但已知位置的场景中</strong>，如图 2 所示。为了沿着阴影光线观察场景，我们用已知的方向光来说明场景。每条射线传感器都捕获一条射线，该射线从光方向传递传感器。与 NeRF 不同，由于我们对源进行建模，<strong>我们假设场景不会发出光，这在物理上更正确并且可以简化以下过程</strong>。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230809180827.png" alt="image.png"></p><p>因此，射线传感器处的传入光是从光源发出的，并由沿射线的无穷小的粒子吸收。使用与等式1: $C(\mathbf{o},\mathbf{v})=\sum_{i=1}^{N}T_{i}\alpha_{i}c_{i},$类似的正交。 我们可以将神经场中模拟的传入辐射表示为<br>$C_\mathrm{in}(x,l)=L\prod_{i=1}^N(1-\alpha_i),$<br>其中，L为光源的强度，x为射线传感器的位置，l为光方向。为了获得足够的信息来约束优化，我们要求阴影光线在不同的方向上对场景进行分层。因此，我们逐个照亮具有多个光方向的场景，并每次记录传入的辐射。由于 NeRF 已经证明了这种光线监督方案的成功，因此在这里重建神经场景也很有希望。</p><h1 id="Shadow-ray-supervision-with-a-single-view-camera"><a href="#Shadow-ray-supervision-with-a-single-view-camera" class="headerlink" title="Shadow ray supervision with a single-view camera"></a>Shadow ray supervision with a single-view camera</h1><p>$C_\mathrm{in}(x,l)=L\prod_{i=1}^N(1-\alpha_i),$方程式(2)<br>请注意，在上述公式中，我们采用假设射线传感器来记录光方向上的入射辐射和场景中的已知位置。这些射线传感器是理想的，因为它们被放置在场景中的期望位置，总是面对光线。在这些强有力的假设下，可以对阴影射线获得足够的监督。然而，与NeRF不同的是，<strong>这些射线传感器很难在实际设置中实现，其中射线传感器只是多视角相机的像素</strong>。在本节中，我们将为真正的捕获设置提出一个更实用的设置。</p><p>一般来说，我们从单视图相机进行阴影射线监督，这可能是先前公式中射线传感器的实用替代方案。我们类似地用$l$方向的光照亮场景。假设场景是不透明的，因此相机准确地捕捉到可见表面的出射辐射。我们考虑两种类型的相机输入:<strong>二值阴影图像[18]和RGB图像</strong>，如图3所示。</p><ul><li>二值阴影图像使用输出亮度来确定一个点是否被照亮，这可以看作是二值化入射亮度的近似值。</li><li>RGB图像是一种更复杂的情况，记录了材料、表面方向和入射辐射的综合影响。</li></ul><p>我们将首先考虑更直接的情况，当我们可以从二值阴影图像中获得可见表面的入射亮度，然后处理更复杂的RGB图像。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230809181405.png" alt="image.png"><br><em>我们的方法概述。所提出的阴影射线监督可以应用于两种输入类型的单视图神经场景重建:二值阴影图像(左)和RGB图像(右)。对于二进制输入，我们首先使用体绘制计算阴影射线的入射亮度。然后，我们构造了一个光度损失来训练神经SDF以匹配阴影。对于 RGB 输入，我们进一步使用材料网络和渲染方程将传入的辐射转换为传出辐射。训练SDF和材料网络以匹配地面真实颜色。</em></p><p>然而，另一个挑战是，给定记录的像素值，我们仍然不知道可见表面点的确切深度。因此，我们将场景观察作为相机观察方向在未知深度的点处的出射亮度。这个问题由提出的技术处理，这些技术确定深度并将传出辐射与入射辐射联系起来。<br>我们将场景表示为符号距离函数 (SDF) $\mathcal{S}=\big\{u\in\mathbb{R}^{3}|f(u)=0\big\},$ 的零水平集，其中 f 是一个神经网络，它回归输入 3D 位置的符号距离。相机可见的3D点是相机光线和SDF之间的第一个交点。请注意，这里相机光线仅用于确定表面点，而不是构建监督，这是阴影光线的工作。具体来说，射线行进[53]用于计算当前SDF的交点x。然后我们可以通过体绘制计算交点处的入射辐射度$C_{\mathrm{in}}(x,l)$。由于我们正在建模SDF而不是密度场，我们将Eq.(2)$C_\mathrm{in}(x,l)=L\prod_{i=1}^N(1-\alpha_i),$中的离散不透明度$\alpha_{i}$替换为NeuS[43]中SDF得到的不透明度$\alpha_{i}$，如<br>$\alpha_i=\max\left(1-\frac{\Phi_s(f(p(t_{i+1})))}{\Phi_s(f(p(t_i)))},0\right),$方程式(3)<br>其中$\begin{aligned}\Phi_s(x)=(1+e^{-sx})^{-1}\end{aligned}$ 是 sigmoid 函数，s 是控制等式的可学习标量参数。 Eq.(2) 接近体积渲染或表面渲染。</p><p><strong>Differentiable intersection points</strong><br>为了在给定SDF的情况下定位交点x，光线行进是最直接的选择。然而，由于不可微，容易被深度不正确的表面点误导，导致结果更差。为了使用反向传播的梯度优化交点，我们使用隐式微分 [1, 53]，这使得交点可微到 SDF 网络参数为:<br>$\widehat{x}=x-\frac{v}{n\cdot v}f(x),$方程式(4)</p><ul><li>v 是相机光线方向</li><li>$n=\nabla_{\mathbf{x}}f(x)$从SDF网络导出的表面法线</li></ul><p>然后，我们使用 $C_{\mathrm{in}}(\widehat{x},\mathbf{l})$ 作为交点 x 处的可微辐射。由于x充当阴影射线的起始位置，它可以通过Eq.(2)的梯度进行优化。当计算的入射辐射度$:C_{\mathrm{in}}(\widehat{x},\mathbf{l})$与监督不一致时，SDF网络可以优化沿阴影光线的符号距离和光线的起始位置以适应观测。</p><p><strong>Multiple shadow rays at boundaries</strong><br>我们观察到，Eq.(4)中的$\hat x$只沿相机方向变化。当用记录的图像监督$C_\mathrm{in}(\widehat{x},\mathbf{l})$时，会在表面边界对应的像素处产生问题。在表面边界，像素跨越不同深度的不相连区域，其中每个区域占据像素区域的一部分。当$\hat x$垂直于相机方向v移动时，通过改变与每个区域成比例的面积，可以显著改变表面边界处的计算亮度。如果我们只从一个区域开始采样一条阴影光线，就会导致不正确的梯度，类似于可微网格渲染的情况[21,24]。<br>因此，我们首先获得一个对应于表面边界的像素子集Ω，并使用[56]中的表面行走程序为每个边界像素获取可微分的面积比$w$。然后我们在像素内的不同深度处找到两个交点$x_{\mathrm{n}}$和$x_{\mathrm{f}}$，并分别计算它们的入射亮度$\dot{C}_{\mathrm{in}}(\widehat{x}_{\mathrm{n}},l)$和$\dot{C}_{\mathrm{in}}(\widehat{x}_{\mathrm{f}},l)$。计算像素p对应的入射辐亮度时，我们将边界像素处的入射辐亮度平均为 Eq.5</p><script type="math/tex; mode=display">\widehat{C}_\text{in}=\begin{cases}C_\text{in}(\widehat{x},l)&p\notin\Omega\\wC_\text{in}(\widehat{x}_\text{n},l)+(1-w)C_\text{in}(\widehat{x}_\text{f},l)&p\in\Omega\end{cases}</script><p>然后，我们可以用二值阴影图像上的像素$I_{s}$来监督计算得到的入射辐射$\widehat{C}_\mathrm{in}$: $\mathcal{L}_\mathrm{shadow}=|\widehat{C}_\mathrm{in}-I_\mathrm{s}|_1.$  Eq.6</p><p><strong>Decomposing incoming radiance by inverse rendering</strong></p><p>为了处理RGB图像，我们结合了一个由材料、入射光和表面方向组成的逆渲染方程。我们将非朗伯BRDF建模为漫射分量$ρ_{d}$和镜面分量$ρ_{s}$。根据[23,49]，我们使用球面高斯基的加权组合将镜面分量ρs表示为:$\rho_s=y^TD(h,n)$，其中$\cdot\mathbf{h}=\frac{\mathbf{l}-\mathbf{v}}{|\mathbf{l}-\mathbf{v}|}$为光方向l与视场方向−v之间的半向量（v为观察方向），D为镜面基，y为镜面系数。我们对另一个MLP网络g进行建模，以回归表面位置x处的材料性质$(\mathbf{\rho}_{d},\mathbf{y})=g(\mathbf{x})$。</p><p>点x处的出射辐射可表示为$C(x,-\mathbf{v})=(\rho_d+\rho_s)C_{\mathrm{in}}(x,l)(l\cdot\mathbf{n})$ Eq.7</p><p>边界像素对应的出射亮度$\widehat{C}$是多个样本的加权组合，类似于Eq.(5)。现在我们可以使用RGB图像上的像素$I_r$来监督计算的亮度: $\mathcal{L}_\mathrm{rgb}=|\widehat{C}-I_\mathrm{r}|_1$  Eq.8</p><p><strong>Light source modeling</strong></p><p>我们的技术支持定向光或点光作为光源来计算式(2)中的入射辐亮度。对于定向光，所有阴影射线的光方向$l$和强度$L$都是已知的，并且是均匀的。对于点光，我们计算点x处的光方向和光强为$L=\frac{L_p}{|q-x|_2^2},l=\frac{q-x}{|q-x|_2}$  Eq.9, 式中$L_{p}$为标量点光强，q为光位置</p><p><strong>Training</strong><br>为了正则化网络以输出有效的SDF，我们在M个样本点上添加一个Eikonal损失[15]为<br>$\mathcal{L}_{\mathrm{eik}}=\frac{1}{M}\sum_{i}^{M}(|\nabla f(p_{i})|_{2}-1)^{2}.$ Eq.10<br>我们训练Eikonal损失with Eq.(6)或Eq.(8)，这取决于是否使用二进制阴影图像或RGB图像作为监督</p><p>我们的技术主要是在地面物体的有界场景上进行评估。为了约束摄像机光线，<strong>我们将不与SDF相交的摄像机光线设置为与地面相交</strong>。为了解决单视角输入的比例模糊问题，以精确的比例重建场景，我们假设地平面的位置和方向已知。更多关于地平面处理的讨论可以在补充材料中找到。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Implementation-details"><a href="#Implementation-details" class="headerlink" title="Implementation details"></a>Implementation details</h2><p>对于二进制阴影输入和RGB输入，我们采用了类似于Neus[43]的SDF MLP网络。当处理RGB输入时，SDF网络输出一个额外的256维特征向量。它将与3D位置和表面法线连接，通过另一个MLP网络回归漫反射和高光系数。在训练过程中，我们在每批中随机选择4张图像，对每张图像采样256个像素位置作为监督信号。通过射线行进来定位相机光线交叉点，并使用从这些交叉点开始的表面行走过程[56]来计算可能的表面边界。我们对网络进行了150k次迭代训练，在单个RTX 2080Ti上大约需要24小时。更多的实现细节可以在补充材料中找到。</p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>为了证明在场景重建中利用阴影光线信息的能力，我们在单视图二进制阴影图像和多个已知光方向下捕获的RGB图像上评估了我们的方法。我们首先与支持类似输入的最先进方法进行定性和定量比较。然后，通过综合消融研究来评估阴影射线监测方案的有效性。最后，我们展示了该方法的更多结果和应用。<br>上述实验在三个数据集上进行。</p><ul><li>首先，我们使用DeepShadow[18]发布的数据集，其中包含六个场景在不同点光下的二进制阴影图像。每个场景都是类似地形的，并由垂直向下的摄像机捕捉到。对于其他视点捕获的更复杂的场景，我们发现没有公开可用的数据集可以满足我们的需求。因此，我们构建新的合成和真实数据集进行全面评估。</li><li>对于合成数据，我们使用来自NeRF合成数据集[29]的对象渲染八个场景。每个测试用例都是通过添加一个水平面来建模地面，将对象放置在平面上，并使用Blender[13]渲染场景来构建的。我们渲染二进制阴影图像和分辨率800×800的RGB图像。为了测试不同的光类型，我们用100个方向光和100个点光渲染每个场景。我们选择在上半球随机采样的光，类似于NeRF中的相机位置选择。我们的合成数据集具有镜面效果的现实材料。透明度和相互反射被禁用，因为这些效果超出了我们的假设。</li><li>我们还捕获了一个真实的数据集，以研究我们的方法对真实捕获设置的适用性。对于每个场景，我们将物体放在地面上，仅用手持手机手电筒照亮场景，并用固定摄像机捕捉它。当手持手电筒在场景中移动时，我们捕获了大约40个RGB图像，并获得类似[4]的光位置。我们在地面上放置一个棋盘，并用相同的固定摄像机捕捉额外的图像来校准地面。所用数据集的摘要请参见表1。</li></ul><h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><p>由于比较的方法输出了可见区域的深度图或法线图，我们还通过L1的深度误差(depth L1)和在可见前景区域计算的平均角误差(normal MAE)的正态误差来评估单视图重建的质量。值得注意的是，由于一些比较方法输出的深度图没有特定的比例尺，因此深度L1是在使用ICP将深度图与地面真实值对齐后计算的。</p><h3 id="Comparison-on-binary-shadow-inputs"><a href="#Comparison-on-binary-shadow-inputs" class="headerlink" title="Comparison on binary shadow inputs"></a>Comparison on binary shadow inputs</h3><h3 id="Comparison-on-RGB-inputs"><a href="#Comparison-on-RGB-inputs" class="headerlink" title="Comparison on RGB inputs"></a>Comparison on RGB inputs</h3><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><h3 id="More-Results"><a href="#More-Results" class="headerlink" title="More Results"></a>More Results</h3>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Shadow&amp;Highlight </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
            <tag> Shadow&amp;Highlight </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Strivec</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Efficiency/Strivec/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Efficiency/Strivec/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Strivec: Sparse Tri-Vector Radiance Fields</th></tr></thead><tbody><tr><td>Author</td><td>Quankai Gao and Qiangeng Xu and Hao su and Ulrich Neumann and Zexiang Xu</td></tr><tr><td>Conf/Jour</td><td>ICCV</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://github.com/Zerg-Overmind/Strivec">Zerg-Overmind/Strivec (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4781773925486952449&amp;noteId=1905442275767271936">Strivec: Sparse Tri-Vector Radiance Fields (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230807134854.png" alt="image.png"></p><ul><li>局部CP分解，三向量</li><li>多尺度，占用网格方式</li></ul><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>在这项工作中，我们提出了一种高质量的神经场景重建和逼真的新视图合成的新方法。我们<strong>提出了一种新的基于张量分解的场景表示，它利用CP分解将3D场景紧凑地建模为表达局部亮度场的多尺度三向量张量的稀疏集</strong>。我们的表示利用了稀疏性和空间局部一致性，并导致复杂场景几何和外观的准确和有效的建模。我们证明了稀疏的三向量辐射场可以比以前最先进的神经表示(包括TensoRF和iNGP)获得更好的渲染质量，同时使用更少的参数。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>我们提出了Strivec，一种新的神经表示，它将3D场景建模为具有稀疏分布和紧凑分解的局部张量特征网格的辐射场。我们的方法利用张量分解，在最近的工作TensoRF[7]之后，对张量网格建模。<strong>与使用全局张量并专注于其向量矩阵分解的TensoRF相反，我们建议使用局部张量云并应用经典的CANDECOMP/PARAFAC (CP)分解</strong>[5]将每个张量分解为三个向量，这些向量表示沿空间轴的局部特征分布并紧凑地编码局部神经场。我们<strong>还应用多尺度张量网格来发现几何和外观的共性，并利用三向量分解在多个局部尺度上的空间一致性</strong>。<br>最终的亮度场属性是通过聚合神经特征从多个局部张量在所有尺度上回归。我们的三向量张量稀疏分布在实际场景表面周围，通过快速粗重建发现，利用3D场景的稀疏性。我们证明了我们的模型可以在使用比以前的方法(包括TensoRF和InstantNGP)更少的参数的情况下获得更好的渲染质量[27]。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>NeRF，将3D场景表示为亮度场[26]已经实现了逼真的渲染质量，并成为3D视觉和图形应用中流行的设计选择。<br>TensoRF[7]和Instant-NGP[27]，已经证明了在速度，紧凑性和质量方面使用共享全局特征编码进行辐射场建模的优势。<strong>然而，这些方法在场景中均匀地共享和分配神经特征</strong>(使用张量因子或哈希表)，<strong>假设场景内容在整个空间中同样复杂，这可能是低效的</strong>(需要高模型容量)，<strong>无法准确地模拟复杂的局部场景细节</strong>(见图1)。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230831143518.png" alt="image.png"></p><p>我们的目标是准确而紧凑地建模一个3D场景，并重现复杂的局部细节。为此，我们提出了Strivec，一种新的神经场景表示，它<strong>利用稀疏分布和紧凑分解的局部张量网格来模拟高质量的新视图合成的体积辐射场</strong>。如图1所示，我们的方法能够准确地模拟出复杂的场景结构，而这些结构是以前的方法无法很好地恢复的。更重要的是，<strong>我们卓越的渲染质量是用更少的模型容量实现的</strong>。</p><ul><li>TensoRF应用CP和向量矩阵(VM)分解技术将一个场分解为向量和矩阵，并将整个场景建模为一个全局分解张量<ul><li>TensoRF中的全局CP分解导致了一个高度紧凑的模型，但无法达到与VM分解相当的渲染质量。这是因为三向量CP分量是一级的，而整个3D场景的全局特征网格通常是复杂和高秩的，需要大量(不切实际的)CP分量来实现高精度。TensoRF通过在VM分解中引入矩阵因子来解决这个问题，本质上是增加每个张量分量的秩。</li></ul></li><li>我们利用分布在场景表面周围的多个小局部张量的稀疏集来代替单个全局张量，以实现更有效的场景建模。<strong>具体来说，我们的每个张量都表示其局部边界框内的局部辐射场，并基于CP分解用分解的三重向量紧凑建模</strong>。<ul><li>我们的模型由多个小张量网格组成，利用场景中的局部空间共性。与全局张量相比，我们的局部张量不那么复杂，等级也低得多，从而有效地减少了所需的CP分量(每个张量)的数量，并通过高度紧凑的三向量因子实现了实际的高质量辐射场重建。与TensoRF的VM模型相比，我们的局部三向量张量可以带来更好的渲染质量和紧凑性(见图1)。我们还观察到，我们的局部张量在空间轴方向上通常比全局张量更robust(这会影响张量的秩，从而影响质量;见图2)</li></ul></li></ul><p><strong>重要的是，采用局部张量(而不是全局张量)也使我们能够根据实际场景分布灵活地分配神经特征，从而比全局表示更有效地进行场景建模和更好地利用模型参数</strong>。为此，我们预先获取粗糙的场景几何形状——这可以通过快速RGBσ体积重建(如DVGO[36])或多视图立体(如Point-NeRF[43])轻松实现——直接在实际场景表面周围分布局部张量，从而产生稀疏的场景表示，避免不必要地对空场景空间进行建模。请注意，虽然以前的方法也利用了辐射场的稀疏表示(使用体素[22,45]Plenoxels或Point-NERF[43])，但它们的局部特征是独立建模和优化的。<br>我们的模型将局部框内的一组局部特征关联起来，并用三向量紧凑地表示它们，独特地利用了沿轴的局部空间相干性，并通过张量分解在特征编码中施加了局部低秩先验。此外，与以往仅使用单尺度特征网格或点云的稀疏表示不同，我们通过分布多尺度局部张量，以分层方式在多尺度上有效地建模场景几何和外观。特别是，对于任意的3D位置，我们在所有尺度上聚合其相邻三向量分量的神经特征，并从聚合特征中解码体积密度和视图相关颜色，用于亮度场渲染。<br>我们的方法采用了最好的以前的局部和全局的亮度场表示。与TensoRF和Instant-NGP等全局表示相比，<strong>我们的模型更直接地利用了场景的稀疏性</strong>;与Plenoxels和PointNeRF等局部表示相比，<strong>我们的模型利用了场景几何和外观的局部平滑性和一致性</strong>。正如我们在合成数据集和真实数据集上的实验结果所示，我们的模型能够在这些数据集上实现最先进的渲染质量，优于以前的方法，包括TensoRF和Instant-NGP，同时使用更少的模型参数，展示了我们模型的优越表征能力。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p><strong>Scene representations.</strong><br>为了表示三维场景，传统的和基于学习的方法研究了各种表示方法，如深度图[16,21]、网格[18,40,34]、点云[32,1,39]和隐式函数[10,25,28,44]。近年来，连续神经场表征在单视图三维重建[42,14]、曲面补全[11,30]、多视图重建[28]和新视图合成[26,24]等各种三维任务中表现突出。与传统的离散表示相比，<strong>连续场</strong>不受空间分辨率的限制，例如体积分辨率或点的数量。它也可以自然地用神经网络来表示，比如MLP，它以很好地逼近复杂函数而闻名。</p><p><strong>Neural field representations</strong>.<br>具体来说，NeRF[26]将3D场景表示为具有全局坐标MLP的亮度场，它联合建模几何、光照和纹理信息，从而在新颖的视图合成中实现了逼真的渲染质量。除了其优势外，纯粹基于mlp的NeRF模型[3,38]在建模高度复杂或大规模场景时，由于模型容量有限，优化速度慢，以及建模空白空间的成本，通常存在效率低下的问题[2]。</p><p>为了更有效地建模辐射场，最近的工作已经探索了将神经场与各种传统的3D表示相结合，包括体素[22,45,36,48]和点[43]。还研究了低秩表示，如三平面[6,13]和张量分解[7,29]。<br>特别是，DVGO[36]和Plenoxels[45]分别使用具有神经特征的密集体素和稀疏体素进行辐射场建模。虽然优化效率高，但<strong>这些基于局部特征网格的表示导致模型尺寸较大，并且在特征分辨率非常高时可能面临过拟合问题</strong>。因此，DVGO也可以与低分辨率网格一起工作，而Plenoxels需要额外的空间正则化条款。<br>另一方面，最近的研究采用全局特征编码来表达高分辨率特征网格，包括将空间特征散列成多尺度哈希表的Instant-NGP[27]和将特征网格分解成矢量和矩阵因子的TensoRF[7]。这些全局特征编码方法利用了整个场景空间的空间相关性，导致了快速和紧凑的重建，并且在渲染质量上超越了以前基于mlp或基于网格的表示。<strong>然而，与NeRF类似，当表示高度复杂或大规模的内容时，这种全局表示也会受到其模型容量的限制</strong>。</p><p>相反，我们的方法结合了局部和全局代表。我们的三向量场在场景中是稀疏分布的，类似于局部表示(如plenoxels和Point-NeRF); 同时，像TensoRF一样，每个域的特征由局部区域共享的三向量分量表示，利用了空间特征的共性。我们的模型利用了空间稀疏性和相干性，比以前的局部和全局表示具有更高的紧凑性和更好的重建质量(见表1)。<br>与我们的工作相关，以前的方法，如KiloNeRF[33]和BlockNeRF[37]也利用多个本地mlp来表示一个场景。具体来说，KiloNeRF关注和加速NeRF，牺牲了它们的渲染质量;BlockNeRF本质上使用多个nerf来增加总模型容量。我们的工作不是纯粹的mlp，而是建立在TensoRF[7]中基于张量分解的特征编码上，<strong>实际上我们在降低模型容量的同时实现了卓越的渲染质量</strong>。</p><h1 id="Sparse-Tri-Vector-Field-Representation"><a href="#Sparse-Tri-Vector-Field-Representation" class="headerlink" title="Sparse Tri-Vector Field Representation"></a>Sparse Tri-Vector Field Representation</h1><p>我们现在提出了我们的新辐射场表示。本质上，我们的模型由多尺度的小局部三向量张量组成，旨在利用稀疏性和多尺度空间相干性(见图2)。</p><p>三向量张量云 $\mathcal{T}=\{\tau_n|n=1,…,N\}$<br>每个局部张量τ位于p，覆盖一个边长为l的局部立方体空间ω。这个三向量张量云代表了3D空间的辐射场: $\Omega=\bigcup_{n=1}^N\omega_n.$ Eq.1</p><p>在这里，每个张量τ编码一个局部多通道特征网格，其中包括一个(单通道)密度网格$A_{\sigma}$和一个(多通道)外观网格$A_{c}$，类似于TensoRF中的张量网格[7]。与在TensoRF中使用单个全局张量[7]相比，我们使用多个局部张量对体积密度和视图相关颜色进行建模。特别地，对于任意位置$\chi\in\Omega$，我们选择M个覆盖$χ$的最近张量。在选择的张量中，我们将提取的密度和由它们的三向量因子恢复的外观特征进行聚合，用于辐射场属性回归，其中在聚合后直接获得体积密度σ，并通过小MLP $ψ$随观察方向回归与视图相关的颜色c。连续辐射场可表示为:<br>$\sigma_{\chi},c_{\chi}=A_{\sigma}(\{\mathcal G^{\sigma}(\chi)\}),\psi(A_{c}(\{\mathcal G^{c}(\chi)\}),\mathbf{d}).$ Eq.2</p><h2 id="Local-tri-vector-tensors"><a href="#Local-tri-vector-tensors" class="headerlink" title="Local tri-vector tensors."></a>Local tri-vector tensors.</h2><p>我们应用经典的正则多进(CP)分解[5]来模拟具有三向量分量的局部张量。</p><p><strong>CP decomposition.</strong><br>CP分解将M维张量$\tau\in\mathbb{R}^{I_1\times I_2\times\ldots\times I_M}$因式分解为R个rank-1张量的线性组合:<br>$\tau=\sum_{r=1}^{R}\lambda_{r}\mathbf{v}_{r}^{0}\otimes\mathbf{v}_{r}^{1}\otimes…,\otimes\mathbf{v}_{r}^{M},$ Eq.3<br>式中⊗表示外积;权重因子$λ_{r}$可以吸收成向量$\colon\{\mathbf{v}_{r}^{0},…,\mathbf{v}_{r}^{M}\}.$。<br><strong>Density and appearance tensors.</strong><br>在我们建模3D辐射场的例子中，我们将几何网格$\mathcal{G}^{\sigma}\in \mathbb{R}^{I\times J\times K}$设置为3D张量。多通道外观网格$\mathcal{G}^c\in\mathbb{R}^{I\times J\times K\times P}$对应一个4D张量。第四种外观mode是较低的维度(与空间模式相比)，代表发送到MLP解码器网络的特征的最终维度。<br>根据Eqn.3，我们通过CP分解对每个张量的特征网格$\mathcal{G}^{\sigma}$和$\mathcal{G}^{c}$进行分解:<br>$\mathcal{G}^\sigma=\sum_{r=1}^{R_\sigma}\mathcal{A}_{\sigma,r}=\sum_{r=1}^{R_\sigma}\mathrm{v}_{\sigma,r}^X\otimes\mathrm{v}_{\sigma,r}^Y\otimes\mathrm{v}_{\sigma,r}^Z,$ Eq.4<br>$\mathcal{G}^c=\sum_{r=1}^{R_c}\mathcal{A}_{c,r}\otimes\mathbf{b}_r=\sum_{r=1}^{R_c}\mathbf{v}_{c,r}^X\otimes\mathbf{v}_{c,r}^Y\otimes\mathbf{v}_{c,r}^Z\otimes\mathbf{b}_r,$ Eq.5</p><p>其中，$R_{\sigma}$和$R_{c}$表示组分数;$\mathcal{A}_{\sigma,r}$和$\mathcal{A}_{c,r}$是空间分解的分量张量;$\mathbf{V}_{\sigma,r}^{X},…,\mathbf{V}_{c,r}^{X},…$分别为分辨率为I、J、K的一维矢量，沿X、Y、z轴对场景几何形状和外观进行建模; $R_{\sigma}$和$R_{c}$为组分数;$\mathbf{b}_{r}$表示特征维度</p><p>正如在TensoRF[7]中所做的那样，我们将所有特征模式向量$\mathbf{b}_{r}$作为列堆叠在一起，最终得到$P\times R_{c}$外观矩阵B。该矩阵对张量的外观特征变化进行建模，并发挥外观字典的作用。注意，像TensoRF这样天真的CP分解将为每个局部张量分配不同的外观矩阵。<strong>相反，我们建议利用在整个局部张量云上共享的全局外观矩阵Bc，从而产生一个全局外观字典，解释整个场景的颜色相关性。这进一步提高了模型的计算效率和模型紧凑性</strong>。<br>因此，每个局部张量都由它们唯一的局部三向量因子$\mathbf{v}_{r}^{X},\mathbf{v}_{r}^{Y},\mathbf{v}_{r}^{Z}.$表示。</p><p><strong>Feature evaluation.</strong><br>为了获得一个连续的场，我们在评估张量网格特征时考虑了三线性插值。对于位置$χ$，我们首先计算其相对于位于$p$处的选定张量的位置$\tilde{X}$:<br>$\tilde{x},\tilde{y},\tilde{z}=x-p_x,y-p_y,z-p_z.$ Eq.6<br>然后，例如，为了得到在$(\tilde{x},\tilde{y},\tilde{z})$处的$\mathcal{A}_{\sigma,r}$ ，我们可以在角上计算并三线性插值8个$\mathcal{A}_{\sigma,r}$。<br>如[7]所述，首先对每个向量进行线性插值在数学上是等价的，可以减少计算成本。根据外积法则，有$\mathcal{A}_{r,i,j,k}=\mathbf{v}_{r,i}^{X}\mathbf{v}_{r,j}^{Y}\mathbf{v}_{r,k}^{Z},$则在$χ$位置插值的密度特征为:<br>$\mathcal{G}^{\sigma}(\chi)=\sum_{r}\mathbf{v}_{\sigma,r}^{X}(\tilde{x})\mathbf{v}_{\sigma,r}^{Y}(\tilde{y})\mathbf{v}_{\sigma,r}^{Z}(\tilde{z})=\sum_{r}\mathcal{A}_{\sigma,r}(\tilde{\chi}),$ Eq.7</p><p>其中$\mathbf{v}_{\sigma,r}^{X}(\tilde{x})$是$\mathrm{v}_{\sigma,r}^X$在x轴(x)处的线性插值值。这里，$\mathcal{G}^{\sigma}(\chi)$是一个标量。</p><p>同样，插值后的外观特征可以计算为: Eq.8,9,10</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{G}^{c}(\chi)& =\sum_{r}\mathbf{v}_{c,r}^{X}(\tilde{x})\mathbf{v}_{c,r}^{Y}(\tilde{y})\mathbf{v}_{c,r}^{Z}(\tilde{z})\mathbf{b}_{r}  \\&=\sum_r\mathcal{A}_{c,r}(\tilde{\chi})\mathbf{b}_r \\&=\mathbf{B}\cdot(\oplus[\mathcal{A}_{c,r}]_r),\end{aligned}</script><p>其中“⊕”表示串联concatenation，“·”表示点积。外观特征$\mathcal{G}^c(\chi)\in\mathbb{R}^P$是一个向量。</p><h2 id="Feature-aggregation"><a href="#Feature-aggregation" class="headerlink" title="Feature aggregation."></a>Feature aggregation.</h2><p>我们建议聚合来自M个相邻张量的特征，以联合建模每个3D位置的体积密度和外观χ。特别是，<strong>受Point-NeRF的启发，我们利用基于逆距离的加权函数来直接聚合多张量特征</strong>。具体来说，这个权重可以表示为$w_m=\frac{1}{|p_m-\chi|}.$ Eq.11<br>有了这个权函数，我们直接通过加权和得到密度特征:<br>$f^{\sigma}(\chi)=\frac{1}{\sum w_{m}}\sum_{m=1}^{M}w_{m}\mathcal{G}_{m}^{\sigma}(\chi).$ Eq.12</p><p>同样，外观特征聚合也可以用类似的方式表示，同时使用跨局部张量的共享外观矩阵(如第3.1节所述): Eq.13,14,15</p><script type="math/tex; mode=display">\begin{aligned}f^{c}(\chi)& =\frac1{\sum w_m}\sum_{m=1}^Mw_m\mathcal{G}_m^c(\chi)  \\&=\frac1{\sum w_m}\sum_{m=1}^Mw_m\mathbf{B}^c\cdot(\oplus[\mathcal{A}_{c,r}]_r) \\&=\frac{1}{\sum w_m}\mathbf{B}^c\cdot(\sum_{m=1}^Mw_m(\oplus[\mathcal{A}_{c,r}]_r)).\end{aligned}</script><p>注意，由于在张量之间共享外观矩阵，我们将计算复杂度从Eqn.14中的$O(M\cdot P\cdot R_c)$降低到Eqn.15中的$O((M+P)\cdot R_{c})$。</p><h2 id="Multi-scale-tri-vector-fields"><a href="#Multi-scale-tri-vector-fields" class="headerlink" title="Multi-scale tri-vector fields."></a>Multi-scale tri-vector fields.</h2><p>复杂的3D场景通常包含多频几何和外观细节。这促使我们构建多尺度张量云，以发现多尺度下的局部几何和外观共性。我们最终的亮度场是由多个三向量张量云在S不同的尺度。不同的云由不同分辨率的张量组成<br>为了回归位置$χ$的密度和外观，我们从覆盖$χ$的一组张量云中收集密度和外观特征，$\{\mathcal{T}_{s}|1\leq s\leq S,\chi\in\Omega_{s}\}$。请注意，某些尺度的张量云可能没有覆盖位置，因此$\mathfrak{l}|\{\mathcal{T}_{s}\}|\leq S.$。我们简单地计算这些尺度上的平均特征:<br>$f^{\sigma}(\chi)=\frac{1}{|\{\mathcal{T}_{s}\}|}\sum_{s}f_{s}^{\sigma}(\chi),$ Eq.16<br>$f^{c}(\chi)=\frac{1}{|\{\mathcal{T}_{s}\}|}\sum_{s}f_{s}^{c}(\chi).$ Eq.17<br>注意，$f^{\sigma}(\chi)$和$f^{c}(\chi)$是我们在多个尺度和多个相邻张量上聚合的最终密度和外观特征。</p><h2 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding."></a>Decoding.</h2><p>我们对密度特征$f^{\sigma}(\chi)$应用softplus激活来获得最终的体积密度，并通过将外观特征$f^{c}(\chi)$和观察方向d发送到MLP解码器$ψ$来回归与视图相关的颜色。</p><h1 id="Rendering-and-Reconstruction"><a href="#Rendering-and-Reconstruction" class="headerlink" title="Rendering and Reconstruction"></a>Rendering and Reconstruction</h1><p><strong>Volume Rendering</strong><br>我们评估每个像素的颜色与基于物理的体积渲染通过可微分射线推进。根据NeRF[26]，我们在$\{\chi_{q}\mid q=1,…,Q\}$沿射线，并按密度累积辐照度: Eq.18<br>$c=\sum_{q=1}^{Q}T_{q}(1-\exp(-\sigma_{q}\delta_{q}))c_{q},$<br>$T_{q}=\exp(-\sum_{t=1}^{q-1}\sigma_{t}\delta_{t}).$<br>$\sigma_q\mathrm{~and~}c_q$为遮光点的密度和亮度; $δ_{t}$为每一步的行进距离;T为透光率。</p><p><strong>Distributing local tensors</strong><br>首先，为了更好地利用场景的稀疏性，我们首先获得一个大致覆盖场景几何的几何先验。几何先验可以是任何常用的形式，例如点云、占用网格、八叉树或网格顶点。然后我们可以在几何的空间邻域中均匀分布张量。对于多尺度模型，每个尺度都是独立分布的。对于我们的大多数结果，我们快速地从多视图图像中优化一个粗糙的RGBA体积，并使用优化的占用网格作为先验几何，就像在DVGO中所做的那样[36]，这在几秒钟内完成。<br>为了保持训练的稳定性和速度，每个张量$τ$的位置$p$和覆盖范围$ω$一旦确定就固定。我们还初始化了每个正态分布张量的$3(Rσ + Rc)$向量$(\mathbf{v}_{\sigma,r}^X,…,\mathbf{v}_{c,r}^X,…)$。对于每个尺度s, $P\times R_{c}$外观矩阵$\mathrm{B}_{s}^{c}$由该尺度的所有三向量张量共享。具体来说，Eqn.15中的“$\mathbf{B}^{c}\cdot()$”可以高效地实现为全连接神经层。因此，每个尺度的$B^{c}$和全局外观MLP $ψ$将被实现为神经网络，并通过默认方法初始化[15]。</p><p><strong>Optimization and objectives.</strong><br>给定一组具有相机姿态的多视图RGB图像，在地面真实像素颜色的监督下，对稀疏三向量辐射场进行逐场景优化以重建辐射场。根据体渲染方程18,L2渲染损失可以回溯到全局MLP和聚合特征，然后，一直到局部张量的外观矩阵和特征向量。<br>我们使用渲染损失来监督重建，并在密度特征向量$\mathbf{V}_{\sigma,r}$上应用L1正则化损失来提高几何稀疏性，并避免像TensoRF[7]中所做的那样过拟合:<br>$\mathcal{L}_r=|C-\tilde{C}|_2^2,$ Eq.19<br>$\mathcal{L}_{L1}=\frac{1}{N}\sum_{r}^{R_{\sigma}}|\mathbf{v}_{\sigma,r}|,$ Eq.20<br>式中，$\tilde{C}$为地面真值像素颜色，$|\mathbf{v}_{\sigma,r}|$为密度向量上各元素绝对值之和，N为元素总数。总损失为:<br>$\mathcal{L}=\mathcal{L}_{r}+\alpha\mathcal{L}_{L1}.$ Eq.21<br>我们将稀疏项α的权值默认设置为1e−5。</p><h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><p>获得粗糙的场景几何：30s内获得$100^{3}$的占据体积，如果存在可用的几何数据，我们可以跳过这一步</p><p>默认尺度数为3</p><p>在[-1,1]的场景框中，我们将场景几何(被占用的体素中心或点)栅格化到3个不同体素大小的网格上，例如$0.4^{3},0.2^{3},0.1^{3}$。对于每个网格，我们在其占用体素的中心分布三向量张量。3个尺度的张量空间覆盖度分别为$0.6^{3}、0.3^{3}、0.15^{3}$。对于每个尺度，我们查询M = 4个附近张量<br>据[36]，我们的特征解码网络$ψ$是一个具有128个通道的2层MLP。对于每个尺度，其外观矩阵$B^{c}$由27个通道的单一线性层实现</p><p>我们在PyTorch[17]中使用自定义的CUDA操作来实现该框架。在优化过程中，我们采用[7]中的粗到细策略，将向量的维数(I, J, K)从29线性上采样到121，从15上采样到61，从7上采样到31。上采样发生在第2000、3000、4000、5500和7000步。我们使用Adam优化器[19]，初始学习率为向量的0.02，网络的0.001。在单个3090 RTX GPU上，我们训练每个模型30000步，而每个批次包含4096条射线。请在补充材料中找到更多的细节。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><ul><li>Evaluation on the NeRF Synthetic Dataset.</li><li>Evaluation on the real datasets.<ul><li>The ScanNet dataset.</li><li>The Tanks and Temples dataset.</li></ul></li><li>Ablation study</li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Efficiency </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Efficiency </tag>
            
            <tag> TensorDecomposition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensoRF</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Efficiency/TensoRF/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Efficiency/TensoRF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>TensoRF: Tensorial Radiance Fields</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://apchenstu.github.io/">Anpei Chen*</a>, <a href="http://cseweb.ucsd.edu/~zex014/">Zexiang Xu*</a>, <a href="http://www.cvlibs.net/">Andreas Geiger</a>, <a href="https://sist.shanghaitech.edu.cn/2020/0707/c7499a53862/page.htm">Jingyi Yu</a>, <a href="https://cseweb.ucsd.edu/~haosu/">Hao Su</a></td></tr><tr><td>Conf/Jour</td><td>ECCV</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://apchenstu.github.io/TensoRF/">TensoRF: Tensorial Radiance Fields (apchenstu.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4695564851771080705&amp;noteId=1905538040970952960">TensoRF: Tensorial Radiance Fields (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230807134443.png" alt="image.png"></p><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了一种高质量场景重建和渲染的新方法。我们提出了一种新的场景表示TensoRF，它利用张量分解技术<strong>将辐射场紧凑地建模和重建为分解的低秩张量分量</strong>。虽然我们的框架包含了经典的张量分解技术(如CP)，但<strong>我们引入了一种新的向量矩阵分解VM，从而提高了重建质量和优化速度</strong>。<br>我们的方法可以在每个场景不到<strong>30分钟的时间</strong>内实现高效的辐射场重建，与需要更长的训练时间(20多个小时)的NeRF相比，可以获得更好的渲染质量。此外，我们基于张量分解的方法<strong>实现了高紧凑性，导致内存占用小于75MB</strong>，大大小于许多其他先前和并发的基于体素网格的方法。我们希望我们在张张化低秩特征建模方面的发现可以对其他建模和重建任务有所启发。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>我们提出了一种新的建模和重建辐射场的方法——TensoRF。与纯粹使用mlp的NeRF不同，我们将场景的亮度场建模为4D张量，它代表具有每体素多通道特征的3D体素网格。我们的中心思想是将4D场景张量分解成多个紧凑的低秩张量分量。我们证明，在我们的框架中应用传统的CANDECOMP/PARAFAC (CP)分解-将张量分解为具有紧向量的秩一分量-可以改善vanilla NeRF。<br><strong>为了进一步提高性能</strong>，我们引入了一种新的向量矩阵(VM)分解，该分解放松了张量的两种模式的低秩约束，并将张量分解为紧凑的向量和矩阵因子。除了卓越的渲染质量之外，与之前和并发的直接优化每体素特征的工作相比，我们使用CP和VM分解的模型显著降低了内存占用。实验证明，与NeRF相比，具有CP分解的TensoRF实现了快速重建(&lt; 30分钟)，具有更好的渲染质量，甚至更小的模型大小(&lt; 4 MB)。此外，具有VM分解的TensoRF进一步提高了渲染质量，优于以前最先进的方法，同时减少了重建时间(&lt; 10分钟)并保留了紧凑的模型大小(&lt; 75 MB)。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><strong>建模和重建3D场景</strong>作为支持高质量图像合成的表示对于计算机视觉和图形在视觉效果，电子商务，虚拟和增强现实以及机器人技术中的各种应用至关重要。<br>最近，<strong>NeRF</strong>[37]及其许多后续工作[70,31]已经成功地将场景建模为辐射场，并实现了具有高度复杂几何和视图依赖外观效果的场景的逼真渲染。尽管(纯粹基于mlp的)NeRF模型需要很小的内存，<strong>但它们需要很长时间(数小时或数天)来训练</strong>。在这项工作中，我们追求一种新颖的方法，既有效的训练时间和紧凑的内存占用，同时达到最先进的渲染质量。</p><p>为了做到这一点，我们提出了TensoRF，这是一种新颖的亮度场表示，它非常紧凑，也可以快速重建，实现高效的场景重建和建模。与NeRF中使用的基于坐标的mlp不同，我们将亮度场表示为显式的特征体素网格。</p><p>需要注意的是，目前尚不清楚体素网格表示是否能提高重建的效率: 虽然<strong>之前的工作使用了特征网格</strong>[31,68,20]，<strong>但它们需要较大的GPU内存来存储尺寸随分辨率呈立方增长的体素，有些甚至需要预先计算NeRF进行蒸馏，导致重建时间很长</strong>。</p><p>我们的工作在一个原则框架中解决了体素网格表示的低效率问题，从而产生了一系列简单而有效的方法。我们利用了一个事实，即<strong>特征网格可以自然地被视为一个4D张量，其中三个模式对应于网格的XYZ轴，第四个模式代表特征通道维度</strong>。这开启了利用经典张量分解技术进行辐射场建模的可能性，该技术已广泛应用于各个领域的高维数据分析和压缩[27]。因此，我们建议将亮度场张量分解为多个低阶张量分量，从而获得准确而紧凑的场景表示。注意，我们的中心思想张量化辐射场是一般的，可以潜在地采用任何张量分解技术。</p><p>在这项工作中，我们首先尝试了经典的CANDECOMP/PARAFAC (CP)分解[7]。我们表明，使用CP分解的TensoRF已经可以实现逼真的渲染，并产生比纯粹基于MLP的NeRF更紧凑的模型(见图1和表1)。然而，在实验中，为了进一步提高复杂场景的重建质量，我们必须使用更多的组件因子，这增加了训练时间。</p><p>因此，<strong>我们提出了一种新的向量矩阵(VM)分解技术</strong>，该技术有效地减少了相同表达能力所需的组件数量，从而实现更快的重建和更好的渲染。特别是，受CP和分块项分解[13]的启发，我们提出将辐射场的全张量分解为每个张量分量的多个矢量和矩阵因子。与CP分解中纯向量外积的和不同，我们考虑向量-矩阵外积的和(见图2)。实质上，我们通过在一个矩阵因子中联合建模两个模态来放松每个分量的两个模态的秩。虽然与CP中纯粹的基于向量的分解相比，这增加了模型的大小，但我们使每个分量能够表达更高阶的更复杂的张量数据，从而显着减少了辐射场建模所需的分量数量。</p><p>通过CP/VM分解，我们的方法紧凑地编码了体素网格中空间变化的特征。可以从特征中解码出体积密度和依赖于视图的颜色，支持体积亮度场渲染。因为张量表示离散数据，我们还启用了高效的三线性插值来表示连续场。我们的表示支持具有不同解码函数的各种类型的每体素特征，包括神经特征(依赖于MLP从特征中回归与视图相关的颜色)和球面谐波(SH)特征(系数)——允许从固定的SH函数进行简单的颜色计算，并导致没有神经网络的表示。</p><p>我们的张量辐射场可以有效地从多视图图像中重建，并实现现实的新视图合成。与之前直接重建体素的工作相比，我们的张量因式分解将<strong>空间复杂度</strong>从$\mathcal{O}(n^3)$降低到$\mathcal{O}(n)$(CP)或$\mathcal{O}(n^2)$ (VM)，显著降低内存占用。请注意，虽然我们利用了张量分解，但我们不是在解决分解/压缩问题，而是基于梯度体面的重建问题，因为特征网格/张量是未知的。从本质上讲，我们的CP/VM分解在优化中提供了低秩正则化，从而获得了高渲染质量。我们通过各种设置对我们的方法进行了广泛的评估，包括CP和VM模型、不同数量的组件和网格分辨率。我们证明，所有模型都能够实现逼真的新颖视图合成结果，与之前最先进的方法相当或更好(见图1和表1)。更重要的是，我们的方法具有较高的计算和内存效率。所有TensoRF模型都能在30分钟内重建高质量的辐射场;我们最快的虚拟机分解模型需要不到10分钟，这比NeRF和许多其他方法要快得多(约100倍)，同时比以前的基于体素的并行方法需要更少的内存。请注意，与需要独特数据结构和定制CUDA内核的并发工作[50,38]不同，我们的模型的效率收益是使用标准PyTorch实现获得的。据我们所知，我们的工作是第一次从张量的角度来看待辐射场建模，并将辐射场重建问题作为低秩张量重建问题之一。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p><strong>Tensor decomposition</strong>.<br>张量分解[27]已经研究了几十年，在视觉、图形学、机器学习和其他领域有不同的应用[43,24,59,14,1,23]。一般来说，应用最广泛的分解是Tucker分解[58]和CP分解[7,19]，这两种分解都可以看作是矩阵奇异值分解(SVD)的推广。CP分解也可以看作是一种特殊的Tucker分解，其核心张量是对角的。通过将CP和Tucker分解相结合，提出了块项分解(block term decomposition, BTD)及其多种变体[13]，并将其用于许多视觉和学习任务[2,67,66]。在这项工作中，我们利用张量分解进行辐射场建模。我们直接应用了CP分解，并引入了一种新的向量矩阵分解，它可以看作是一种特殊的BTD。</p><p><strong>Scene representations and radiance fields</strong>.<br>各种场景表示，包括网格[18,61]、点云[47]、体[22,48]、隐式函数[35,46]，近年来得到了广泛的研究。许多神经表示[10,71,53,33,4]被提出用于高质量的渲染或自然信号表示[52,56,29]。NeRF[37]引入了辐射场来解决新的视图合成并实现照片逼真的质量。这种表示已迅速扩展并应用于各种图形和视觉应用，包括生成模型[9,40]、外观获取[3,5]、表面重建[62,42]、快速渲染[49,68,20,17]、外观编辑[64,32]、动态捕获[28,44]和生成模型[41,8]。虽然导致逼真的渲染和紧凑的模型，<strong>但NeRF的纯基于mlp的表示在缓慢的重建和渲染方面存在已知的局限性</strong>。最近的方法[68,31,20]利用了辐射场建模中的特征体素网格，实现了快速渲染。<strong>然而，这些基于网格的方法仍然需要较长的重建时间，甚至导致较高的内存成本，牺牲了NeRF的紧凑性</strong>。基于特征网格，我们提出了一种新的张量场景表示，利用张量分解技术，实现快速重建和紧凑建模。<br>其他方法设计了跨场景训练的可通用网络模块，以实现依赖图像的亮度场渲染[57,69,63,12]和快速重建[11,65]。我们的方法侧重于亮度场表示，只考虑每个场景的优化(如NeRF)。我们表明，我们的表示已经可以导致高效率的辐射场重建，而无需任何跨场景泛化。我们把扩展留给一般化的设置作为将来的工作。</p><p><strong>Concurrent work</strong><br>在过去的几个月里，辐射场建模领域的发展非常快，许多并行的作品已经出现在arXiv上作为预印本。DVGO[55]和Plenoxels[50]还优化了(神经或SH)特征的体素网格，以实现快速辐射场重建。然而，它们仍然像以前基于体素的方法一样直接优化每体素特征，因此需要大的内存。我们的方法将特征网格分解为紧凑的组件，从而显著提高了内存效率。Instant-NGP[38]使用多分辨率哈希来实现高效编码，并导致高紧凑性。该技术与我们基于因子分解的技术是正交的; 潜在地，我们的每个向量/矩阵因子都可以用这种散列技术进行编码，我们将这种组合留到未来的工作中。EG3D[8]使用三平面表示3D gan;它们的表示类似于我们的VM分解，可以看作是具有常量向量的特殊VM版本。</p><h1 id="CP-and-VM-Decomposition"><a href="#CP-and-VM-Decomposition" class="headerlink" title="CP and VM Decomposition"></a>CP and VM Decomposition</h1><p>我们将亮度场分解成紧凑的组件用于场景建模。为此，我们应用了经典的CP分解和新的向量矩阵(VM)分解; 两者如图2所示。我们现在用一个3D(3阶)张量的例子来讨论这两种分解。我们将介绍如何在辐射场建模中应用张量因式分解(与4D张量)在第4节。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230830135615.png" alt="image.png"><br><em>张量分解。左:CP分解(Eqn. 1)，它将张量分解为向量外积的和。右:我们的向量矩阵分解(Eqn. 3)，它将张量分解为向量矩阵外积的和。</em></p><p><strong>CP decomposition</strong></p><p>给定一个三维张量$\mathcal{T}\in\mathbb{R}^{I\times J\times K}$，CP分解将其分解为向量的外积和(如图2所示):<br>$\mathcal{T}=\sum_{r=1}^R\mathbf{v}_r^1\circ\mathbf{v}_r^2\circ\mathbf{v}_r^3$ Eq.1<br>其中$\mathbf{v}_r^1\circ\mathbf{v}_r^2\circ\mathbf{v}_r^3$对应一个rank-one张量分量，$\mathbf{v}_r^1\in\mathbb{R}^I$,$\mathbf{v}_r^2\in\mathbb{R}^{\boldsymbol{J}}$, $\mathbf{v}_r^3\in\mathbb{R}^K$是第r个分量三种模态的因式分解向量。上标表示每个因子的模态;◦表示外积。因此，每个张量元素$T_{ijk}$是标量积的和: $\mathcal{T}_{ijk}=\sum_{r=1}^R\mathbf{v}_{r,i}^1\mathbf{v}_{r,j}^2\mathbf{v}_{r,k}^3$ Eq.2<br>其中i j k表示三种模态的indices。</p><p>CP分解将一个张量分解为多个向量，表示多个紧化秩一分量。CP可以直接应用于我们的张量辐射场建模，得到高质量的结果(见表1)。<strong>但是，由于CP分解的紧凑性太高，需要很多组件来建模复杂场景，导致辐射场重构的计算成本很高</strong>。受块项分解(BTD)的启发，<strong>我们提出了一种新的VM分解方法</strong>，从而更有效地重建辐射场。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230830140627.png" alt="image.png"></p><p><strong>Vector-Matrix (VM) decomposition</strong></p><p>与利用纯矢量因子的CP分解不同，VM分解将一个张量分解为多个矢量和矩阵，如图2所示。它表示为<br>$\mathcal{T}=\sum_{r=1}^{R_1}\mathrm{v}_r^1\circ\mathrm{M}_r^{2,3}+\sum_{r=1}^{R_2}\mathrm{v}_r^2\circ\mathrm{M}_r^{1,3}+\sum_{r=1}^{R_3}\mathrm{v}_r^3\circ\mathrm{M}_r^{1,2}$Eq.3<br>$\begin{aligned}\mathrm{M}_{r}^{2,3}\in\mathbb{R}^{J\times K},\mathrm{M}_{r}^{1,3}\in\mathbb{R}^{I\times K},\mathrm{M}_{r}^{1,2}\in\mathbb{R}^{I\times J}\end{aligned}$是三种模态中两个(用上标表示)的矩阵因子。对于每个组件，我们将其两个模态秩放宽为任意大，同时将第三个模态限制为秩一; 例如，对于分量张量$\mathbf{v}_{r}^{1}\circ\mathbf{M}_{r}^{2,3}$，其模1秩为1，模2和模3秩可以是任意的，这取决于矩阵$\mathrm{M}_{r}^{2,3}$的秩。一般来说，在CP中，我们不是使用单独的向量，而是将每两个模态结合起来并用矩阵表示它们，允许每个模态用较少的分量充分参数化。R1、R2、R3可设置不同，应根据每种模式的复杂程度进行选择。<strong>我们的VM分解可以看作是一般BTD的一个特例</strong>。</p><p>注意，每个分量张量都比CP分解中的分量有更多的参数。<strong>虽然这会导致较低的紧凑性，但VM分量张量可以表达比CP分量更复杂的高维数据，从而减少了对相同复杂函数建模时所需的分量数量</strong>。另一方面，与密集网格表示相比，VM分解仍然具有非常高的紧凑性，将内存复杂度从$\mathcal{O}(N^3)$降低到$\mathcal{O}(N^2)$。</p><p><strong>Tensor for scene modeling</strong>.<br>在这项工作中，我们的重点是建模和重建辐射场的任务。在这种情况下，三个张量模式对应于XYZ轴，因此我们直接用XYZ表示模式以使其直观。同时，在3D场景表示中，我们对大多数场景考虑$R_{1}=R_{2}=R_{3}=R$，这<strong>反映了一个场景可以沿着它的三个轴分布和呈现同样复杂的事实</strong>。因此，Eqn. 3可以改写为<br>$\mathcal{T}=\sum_{r=1}^R\mathbf{v}_r^X\circ\mathbf{M}_r^{Y,Z}+\mathbf{v}_r^Y\circ\mathbf{M}_r^{X,Z}+\mathbf{v}_r^Z\circ\mathbf{M}_r^{X,Y}$ Eq.4</p><p>此外，为了简化符号和后面章节的讨论，我们还将三种类型的分量张量表示为$\mathcal{A}_r^X=\mathbf{v}_r^X\circ\mathbf{M}_r^{YZ},$$\mathcal{A}_r^Y=\mathrm{v}_r^Y\circ\mathrm{M}_r^{XZ},\mathrm{~and~}\mathcal{A}_r^Z=\mathrm{v}_r^Z\circ\mathrm{M}_r^{XY}$这里A的上标XYZ表示不同类型的组件。据此，张量元素$T_{ijk}$表示为<br>$\mathcal{T}_{ijk}=\sum_{r=1}^R\sum_m\mathcal{A}_{r,ijk}^m$ Eq.5</p><p>其中$m\in XYZ,\mathcal{A}_{r,ijk}^X=\mathrm{v}_{r,i}^X\mathrm{M}_{r,jk}^{YZ},\mathcal{A}_{r,ijk}^Y=\mathrm{v}_{r,j}^Y\mathrm{M}_{r,ik}^{XZ},\mathrm{~and~}\mathcal{A}_{r,ijk}^Z=\mathrm{v}_{r,k}^Z\mathrm{M}_{r,ij}^{XY}.$同样，我们也可以将CP分量表示为$\mathcal{A}^\gamma=\mathbf{v}_r^X\circ\mathbf{v}_r^Y\circ\mathbf{v}_r^Z,$并且Eqn. 5也可以通过考虑$\begin{aligned}m=\gamma,\end{aligned}$来表示CP分解，其中m的求和可以去掉。</p><h1 id="Tensorial-Radiance-Field-Representation"><a href="#Tensorial-Radiance-Field-Representation" class="headerlink" title="Tensorial Radiance Field Representation"></a>Tensorial Radiance Field Representation</h1><p>我们现在提出了张量辐射场表示(TensoRF)。为简单起见，我们专注于用我们的VM分解来表示TensoRF。CP分解较为简单，其分解方程只需稍加修改即可直接应用(如Eqn. 5)。</p><h2 id="Feature-grids-and-radiance-field"><a href="#Feature-grids-and-radiance-field" class="headerlink" title="Feature grids and radiance field."></a>Feature grids and radiance field.</h2><p>我们的目标是建模一个辐射场，它本质上是一个将任何3D位置x和观看方向d映射到其体积密度σ和视图相关颜色c的函数，支持可微分的射线行进进行体渲染。我们利用具有每体素多通道特征的常规3D网格$\mathcal{G}$来建模这样的函数。我们(通过特征通道)将其分成几何网格$\mathcal{G}_{\sigma}$和外观网格，分别对体积密度σ和视图相关颜色c进行建模。<br>我们的方法支持$\mathcal{G}_{c}$中的各种类型的外观特征，这取决于预先选择的函数S，它将外观特征向量和观看方向d转换为颜色c。例如，S可以是一个小的MLP或球面谐波(SH)函数，其中$\mathcal{G}_{c}$分别包含神经特征和SH系数。我们证明MLP和SH函数在我们的模型中都能很好地工作(见表1)。另一方面，我们考虑单通道网格$\mathcal{G}_{\sigma},$其值直接表示体积密度，不需要额外的转换函数。连续的基于网格的辐射场可以写成<br>$\sigma,c=\mathcal{G}_\sigma(\mathrm{x}),S(\mathcal{G}_c(\mathrm{x}),d)$ Eq.6<br>其中，$\mathcal{G}_{\sigma}$，$\mathcal{G}_{c}$表示来自位置x的两个网格的三线性插值特征。我们将$\mathcal{G}_{\sigma}$和$\mathcal{G}_{c}$建模为因式张量。</p><h2 id="Factorizing-radiance-fields"><a href="#Factorizing-radiance-fields" class="headerlink" title="Factorizing radiance fields"></a>Factorizing radiance fields</h2><p>$\mathcal{G}_\sigma\in\mathbb{R}^{I\times J\times K}$是三维张量，$\mathcal{G}_c\in\mathbb{R}^{I\times J\times K\times P}$是4D张量。其中I、J、K分别代表特征网格在X、Y、Z轴上的分辨率，p代表外观特征通道的个数。<br>我们将这些辐射场张量分解为紧化分量。特别是使用VM分解。将三维几何张量$\mathcal{G}_{\sigma}$分解为<br>$\mathcal{G}_\sigma=\sum_{r=1}^{R_\sigma}\mathrm{v}_{\sigma,r}^X\circ\mathrm{M}_{\sigma,r}^{YZ}+\mathrm{v}_{\sigma,r}^Y\circ\mathrm{M}_{\sigma,r}^{XZ}+\mathrm{v}_{\sigma,r}^Z\circ\mathrm{M}_{\sigma,r}^{XY}=\sum_{r=1}^{R_\sigma}\sum_{m\in XYZ}A_{\sigma,r}^m$ Eq.7<br>外观张量$\mathcal{G}_{c}$具有与特征通道维度相对应的附加模式。请注意，与XYZ模式相比，此模式通常具有较低的维度，从而导致较低的排名。因此，我们不将该模态与其他模态结合在矩阵因子中，而是在分解中仅使用向量$\mathbf{b}_{r}$表示该模态。具体地说，$\mathcal{G}_{c}$被分解为Eq.8</p><script type="math/tex; mode=display">\begin{aligned}G_{c}& =\sum_{r=1}^{R_{c}}\mathbf{v}_{c,r}^{X}\circ\mathbf{M}_{c,r}^{YZ}\circ\mathbf{b}_{3r-2}+\mathbf{v}_{c,r}^{Y}\circ\mathbf{M}_{c,r}^{XZ}\circ\mathbf{b}_{3r-1}+\mathbf{v}_{c,r}^{Z}\circ\mathbf{M}_{c,r}^{XY}\circ\mathbf{b}_{3r}  \\&=\sum_{r=1}^{R_c}\mathcal{A}_{c,r}^X\circ\mathbf{b}_{3r-2}+\mathcal{A}_{c,r}^Y\circ\mathbf{b}_{3r-1}+\mathcal{A}_{c,r}^Z\circ\mathbf{b}_{3r}\end{aligned}</script><p>注意，我们有$3R_{c}$向量$\mathbf{b}_{r}$来匹配分量的总数。</p><p>总的来说，我们将整个张量辐射场分解为$3R_{\sigma}+3R_{c}$矩阵$(\mathbf{M}_{\sigma,r}^{YZ},…,\mathbf{M}_{c,r}^{YZ},…)$和$3R_{\sigma}+6R_{c}$向量$(\mathbf{v}_{\sigma,r}^{X},…,\mathbf{v}_{c,r}^{X},…,\mathbf{b}_{r}).$。一般而言，我们采用$R_{\sigma}\ll I,J,K$, $R_{c}\ll I,J,K$，从而形成高度紧凑的表示，可以对高分辨率的密集网格进行编码。本质上，xyz模向量和矩阵因子，$\mathbf{v}_{\sigma,r}^{X},\mathbf{M}_{\sigma,r}^{YZ},\mathbf{v}_{c,r}^{X},\mathbf{M}_{c,r}^{YZ},…,$描述场景几何和外观沿其相应轴的空间分布。另一方面，外观特征模态向量$\mathbf{b}_{r}$表示全局外观相关性。通过将所有$\mathbf{b}_{r}$作为列叠加在一起，我们得到一个$P\times3R_{c}$矩阵B; 这个矩阵B也可以被视为一个全局外观字典，它抽象了整个场景的外观共性。</p><h2 id="Efficient-feature-evaluation"><a href="#Efficient-feature-evaluation" class="headerlink" title="Efficient feature evaluation."></a>Efficient feature evaluation.</h2><p>我们基于因子分解的模型可以以低成本计算每个体素的特征向量，每个xyz模式向量/矩阵因子只需要一个值。我们还为我们的模型启用了有效的三线性插值，从而导致连续场。</p><p><strong>Direct evaluation</strong>.<br>通过VM分解，单个体素在指标ijk处的密度值$\mathcal{G}_{\sigma,ijk}$可以通过公式5直接有效地求出:<br>$\mathcal{G}_{\sigma,ijk}=\sum_{r=1}^{R_\sigma}\sum_{m\in XYZ}\mathcal{A}_{\sigma,r,ijk}^{m}$ Eq.9<br>在这里，计算每个$\mathcal{A}_{\sigma,r,ijk}^m$只需要索引和乘以对应向量和矩阵因子的两个值。</p><p>对于外观网格$\mathcal{G}_{c}$，我们总是需要计算一个完整的P通道特征向量，着色函数S需要作为输入，对应于固定XYZ索引ijk的$\mathcal{G}_{c}$的1D切片:<br>$\mathcal{G}_{c,ijk}=\sum_{r=1}^{R_{c}}\mathcal{A}_{c,r,ijk}^X\mathbf{b}_{3r-2}+\mathcal{A}_{c,r,ijk}^Y\mathbf{b}_{3r-1}+\mathcal{A}_{c,r,ijk}^Z\mathbf{b}_{3r}$ Eq.10</p><p>在这里，特征模式没有额外的索引，因为我们计算了一个完整的向量。我们通过重新排序计算进一步简化Eqn. 10。为此，我们将$\oplus[\mathcal{A}_{c,ijk}^m]_{m,r}$表示为叠加$m=X,Y,Z$和 $r=1,…,R_{c}$所有$\mathcal{A}_{c,r,ijk}^{m}$值的向量，它是一个$3R_{c}$维的向量;在实践中，⊕也可以被看作是将所有标量值(1通道向量)连接成$3R_{c}$通道向量的连接运算符。使用矩阵B(在4.1节中介绍)堆叠所有的$\mathbf{b}_{r}$, Eqn. 10等价于矩阵向量积: $\mathcal{G}_{c,ijk}=\mathbf{B}(\oplus[\mathcal{A}_{c,ijk}^m]_{m,r})$ Eq.11</p><p>注意，公式11不仅在形式上更简单，而且在实践中也导致了更简单的实现。具体来说，当并行计算大量体素时，我们首先计算并连接所有体素的$\mathcal{A}_{c,r,ijk}^{m}$作为矩阵中的列向量，然后将共享矩阵B乘以一次。</p><p><strong>Trilinear interpolation.</strong><br>我们用三线性插值法对连续场进行建模。Na ively实现三线性插值是昂贵的，因为它需要评估8个张量值并对它们进行插值，与计算单个张量元素相比，计算量增加了8倍。<strong>然而，我们发现，由于三线性插值和外积的线性之美，对分量张量的三线性插值自然等同于对相应模态的向量/矩阵因子的线性/双线性插值</strong>。</p><p>例如，给定一个分量张量$\mathcal{A}_{r}^{X}=\mathrm{v}_{r}^{X}\circ\mathrm{M}_{r}^{YZ}$及其每个张量元素$\mathcal{A}_{r,ijk}=\mathrm{v}_{r,i}^{X}\mathrm{M}_{r,jk}^{YZ},$，我们可以计算其插值值为: $\mathcal{A}_r^X(\mathrm{x})=\mathrm{v}_r^X(x)\mathrm{M}_r^{YZ}(y,z)$ Eq.12<br>其中$\mathcal{A}_r^X(\mathrm{x})$是$\mathcal{A}_r$在三维空间中位置x = (x, y, z)处的三线性插值值，$\mathrm{v}_{r}^{X}(x)$是$\mathrm{v}_{r}^{X}$在x轴上在x处的线性插值值，$\mathrm{M}_{r}^{YZ}(y,z)$是$\mathrm{M}_{r}^{YZ}$在YZ平面上在(y, z)处的二线性插值值。类似地，我们有$\mathcal{A}_{r}^{Y}(\mathbf{x})=\mathbf{v}_{r}^{Y}(y)\mathbf{M}_{r}^{XZ}(x,z)$和$\mathcal{A}_{r}^{Z}(\mathbf{x})=\mathbf{v}_{r}^{Z}(z)\mathbf{M}_{r}^{XY}(x,y)$(对于CP分解，$\mathcal{A}_{r}^{\gamma}(\mathbf{x})=\mathbf{v}_{r}^{X}(x)\mathbf{v}_{r}^{Y}(y)\mathbf{v}_{r}^{Z}(z)$也是有效的)。因此，对两个网格进行三线性插值表示为<br>$\mathcal{G}_\sigma(\mathrm{x})=\sum_r\sum_m\mathcal{A}_{\sigma,r}^m(\mathrm{x})$ Eq.13<br>$\mathcal{G}_c(\mathrm{x})=\mathbf{B}(\oplus[\mathcal{A}_{c,r}^m(\mathrm{x})]_{m,r})$ Eq.14</p><p>这些方程与公式9和11非常相似，只是用插值值替换张量元素。我们避免了恢复8个单独的张量元素进行三线性插值，而是直接恢复插值值，从而降低了运行时的计算和内存成本。</p><h2 id="Rendering-and-reconstruction"><a href="#Rendering-and-reconstruction" class="headerlink" title="Rendering and reconstruction."></a>Rendering and reconstruction.</h2><p>方程6,12 - 14描述了我们模型的核心组成部分。结合公式6、13、14，分解后的张量辐射场可表示为<br>$\sigma,c=\sum_{r}\sum_{m}\mathcal{A}_{\sigma,r}^{m}(\mathrm{x}),S(\mathbf{B}(\oplus[\mathcal{A}_{c,r}^{m}(\mathrm{x})]_{m,r}),d)$ Eq.15<br>也就是说，在给定任何3D位置和观看方向的情况下，我们获得连续的体积密度和与视图相关的颜色。这允许高质量的辐射场重建和渲染。请注意，这个方程是通用的，并且描述了具有CP和VM分解的TensoRF。我们的辐射场重建和VM分解渲染的完整管道如图3所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230831142014.png" alt="image.png"></p><p><strong>Volume rendering.</strong><br>为了渲染图像，我们使用可微体渲染，遵循NeRF[37]。具体来说，对于每个像素，我们沿着一条射线前进，沿着射线采样q个阴影点并计算像素颜色<br>$C=\sum_{q=1}^Q\tau_q(1-\exp(-\sigma_q\Delta_q))c_q,\tau_q=\exp(-\sum_{p=1}^{q-1}\sigma_p\Delta_p)$ Eq.16<br>其中，$\sigma_q,c_q$是我们的模型在它们的采样位置$\mathbf{x}_{q}$计算出的相应的密度和颜色;$\Delta_{q}$为射线步长，$τ_{q}$为透射率。</p><p><strong>Reconstruction.</strong><br>给定一组具有已知相机姿势的多视图输入图像，我们的张量辐射场通过梯度下降来优化每个场景，最小化L2渲染损失，仅使用地面真实像素颜色作为监督。<br>我们的辐射场由张量分解解释，并由一组全局向量和矩阵建模，作为优化中整个场的关联和正则化的基本因素。<strong>然而，这有时会导致梯度下降中的过拟合和局部最小问题，导致观测值较少的区域出现异常值或噪声</strong>。我们利用压缩感知中常用的标准正则化术语，包括向量和矩阵因子上的L1范数损失和TV(总变化)损失，有效地解决了这些问题。我们发现仅应用L1稀疏性损失对于大多数数据集是足够的。然而，对于输入图像很少的真实数据集(如LLFF[36])或不完美的捕获条件(如坦克和寺庙[26,31]，具有不同的曝光和不一致的掩模)，TV损失比L1范数损失更有效。<br>为了进一步提高质量和避免局部极小值，我们应用粗到细重建。不像以前的粗到细技术，需要对其稀疏选择的体素集进行独特的细分，<strong>我们的粗到细重建只是通过线性和双线性上采样我们的xyz模式向量和矩阵因子来实现</strong>。</p><h1 id="Implementation-details"><a href="#Implementation-details" class="headerlink" title="Implementation details"></a>Implementation details</h1><p>特征接码函数S ： MLP或SH函数，P=27个特征</p><ul><li>SH：对应于具有RGB通道的三阶SH系数</li><li>Neural Features：使用带有两个FC层(128通道隐藏层)和ReLU激活的小型MLP</li></ul><p>Adam优化</p><ul><li>张量因子lr = 0.02</li><li>MLP解码器lr = 0.001</li></ul><p>GPU：Tesla V100 16GB</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>提出了一个广泛的评估我们的张量辐射场。我们首先分析分解技术、组件数量、网格分辨率和优化步骤。然后，我们将我们的方法与之前和并发的360度对象和面向数据集的工作进行比较。</p><ul><li>Analysis of different TensoRF models.</li><li>Optimization steps</li><li>Comparisons on 360◦ scenes</li><li>Forward-facing scenes</li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Efficiency </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Efficiency </tag>
            
            <tag> TensorDecomposition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UNISURF</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/UNISURF/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/UNISURF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://moechsle.github.io/">Michael Oechsle</a> <a href="https://pengsongyou.github.io/">Songyou Peng</a> <a href="http://cvlibs.net/">Andreas Geiger</a></td></tr><tr><td>Conf/Jour</td><td>ICCV 2021 (oral)</td></tr><tr><td>Year</td><td>2021</td></tr><tr><td>Project</td><td><a href="https://moechsle.github.io/unisurf/">UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction (moechsle.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4546355486135050241&amp;noteId=1791178045241021696">UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction (readpaper.com)</a></td></tr></tbody></table></div><p>使用$\hat{C}(\mathbf{r})=\sum_{i=1}^No(\mathbf{x}_i)\prod_{j&lt;i}\bigl(1-o(\mathbf{x}_j)\bigr)c(\mathbf{x}_i,\mathbf{d})$ 占据o来代替NeRF中的$\alpha$<br>将VR与SR结合起来，首先根据占据场获取表面的点$t_s$，然后在$t_s$的一个区间内均匀采样点来进行颜色场的优化(如果光线没有穿过物体，则使用分层采样)</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230806143334.png" alt="image.png|700"></p><span id="more"></span><h1 id="Discussion-and-Conclusion"><a href="#Discussion-and-Conclusion" class="headerlink" title="Discussion and Conclusion"></a>Discussion and Conclusion</h1><p>这项工作提出了UNISURF，一种统一的隐式表面和辐射场公式，用于在没有输入掩模的情况下从多视图图像中捕获高质量的隐式表面几何形状。我们相信神经隐式曲面和先进的可微分渲染程序在未来的3D重建方法中发挥关键作用。我们的统一公式显示了在比以前更一般的设置中优化隐式曲面的路径。<br>限制:通过设计，我们的模型仅限于<strong>表示固体</strong>，<strong>非透明表面</strong>。<strong>过度曝光和无纹理区域也是导致不准确和不光滑表面的限制因素</strong>。此外，在图像中很少可见的区域，重建的精度较低。在附录中更详细地讨论了限制。<br>在未来的工作中，为了从很少可见和无纹理的区域中解决歧义，<strong>先验是重建的必要条件</strong>。虽然我们在优化过程中加入了显式平滑先验，但学习捕获对象之间的规律性和不确定性的概率神经表面模型将有助于解决模糊性，从而实现更准确的重建。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>神经隐式三维表示已经成为从多视图图像重建表面和合成新视图的强大范例。<strong>不幸的是，现有的方法，如DVR或IDR需要精确的过像素对象掩码Mask作为监督</strong>。<br>与此同时，神经辐射场已经彻底改变了新的视图合成。<strong>然而，NeRF的估计体积密度不允许精确的表面重建</strong>。<br>我们的关键见解是隐式表面模型和亮度场可以以统一的方式制定，使表面和体渲染使用相同的模型。这种统一的视角使新颖，更有效的采样程序和重建精确表面的能力无需输入掩模。我们在DTU、BlendedMVS和合成室内数据集上比较了我们的方法。我们的实验表明，我们在重建质量方面优于NeRF，同时<strong>在不需要掩模的情况下与IDR表现相当</strong>。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>从一组图像中捕捉3D场景的几何形状和外观是计算机视觉的基础问题之一。为了实现这一目标，基于坐标的神经模型在过去几年中已经成为三维几何和外观重建的强大工具。<br>最近的许多方法使用连续隐式函数参数化神经网络作为几何图形的三维表示[3,8,12,31,32,37,41,43,47,57]或外观[34,38,39,40,47,52,61]。这些神经网络三维表示在多视图图像的几何重建和新视图合成方面显示出令人印象深刻的性能。神经隐式多视图重建除了选择三维表示形式(如占用场、无符号距离场或有符号距离场)外，渲染技术是实现多视图重建的关键。虽然其中一些作品将隐式表面表示为水平集，从而渲染表面的外观[38,52,61]，但其他作品通过沿着观察光线绘制样本来整合密度[22,34,49]。<br>在现有的工作中，表面渲染技术在三维重建中表现出了令人印象深刻的性能[38,61]。<strong>然而，它们需要逐像素对象掩码作为输入和适当的网络初始化，因为表面渲染技术只在表面与射线相交的局部提供梯度信息</strong>。直观地说，optimizing wrt. 局部梯度可以看作是应用于初始神经表面的迭代变形过程，初始神经表面通常被初始化为一个球体。为了收敛到一个有效的表面，需要额外的约束，如掩码监督，如图2所示。<br>==现有工作2021由于依赖mask，因此只能用于对象级重建，而无法重建大场景==</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230806160553.png" alt="image.png"></p><p>相反，像NeRF[34]这样的体绘制方法在新视图合成方面也显示了令人印象深刻的结果，also对于更大的场景。<strong>然而，作为底层体积密度水平集提取的表面通常是非光滑的(NeRF用密度来提取零水平集表面是非光滑的)</strong>，并且由于辐射场表示的灵活性而包含伪影，这在存在歧义的情况下不能充分约束 3D 几何，见图 3。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230807121145.png" alt="image.png"></p><p>贡献：在本文中，我们提出了 UNISURF（统一UNIfied 神经Neural 隐式Implicit SUrface 和辐射场Radiance）隐式表面和辐射场的原则性统一框架，目标是从一组 RGB 图像重建实体（即不透明）对象。<strong>我们的框架结合了表面渲染的好处和体绘制的好处，从而能够从没有掩码的多视图图像重建准确的几何图形</strong>。通过恢复隐式曲面，我们能够在优化过程中逐渐降低采样区域进行体绘制。从大采样区域开始，可以在早期迭代期间捕获粗略的几何图形并解决歧义<br><strong>在后面的阶段，我们抽取更接近表面的样本，提高了重建精度</strong>。我们表明，我们的方法能够在DTU MVS数据集[1]上在没有掩码监督的情况下捕获精确的几何图形，获得了与最先进的隐式神经重建方法(如IDR[61])竞争的结果，这些方法使用强掩码监督。此外，我们还在BlendedMVS数据集[60]的场景以及来自SceneNet[29]的合成室内场景上展示了我们的方法。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li>3D Reconstruction from Multi-View Images<ul><li>从多个图像重建三维几何结构一直是一个长期存在的计算机视觉问题[14]。在深度学习时代之前，经典的多视图立体(MVS)方法[2,4,5,7,20,20,20,48,50,51]要么<strong>关注跨视图匹配特征</strong>[4,48]，要么<strong>关注用体素网格表示形状</strong>[2,5,7,20,27,42,50,54,55]。前一种方法通常有一个复杂的管道，需要额外的步骤，如融合深度信息[9,30]和网格划分[18,19]，而后一种方法由于立方内存要求而限制在低分辨率。<strong>相比之下，用于3D重建的神经隐式表示</strong>不会受到离散伪影的影响，因为它们通过具有连续输出的神经网络的水平集表示表面。</li><li>最近基于学习的MVS方法试图取代经典MVS管道的某些部分。例如，一些作品学习匹配2D特征[15,21,26,56,62]，融合深度图[11,46]，或者从多视图图像中推断深度图[16,58,59]。与这些基于学习的MVS方法相反，<strong>我们的方法在优化过程中只需要弱2D监督</strong>。此外，我们的方法产生了高质量的3D几何图形，并合成了逼真的、一致的新视图</li></ul></li><li>Neural Implicit Representations<ul><li>最近，神经隐式函数作为3D几何[3,8,12,31,32,37,41,43,47,57]和外观[22,24,34,38,39,40,47,49,52]的有效表示出现了，因为它们连续地表示3D内容，无需离散化，同时具有较小的内存占用。这些方法大多需要三维监控。然而，最近的一些工作[23,34,38,52,61]证明了直接从图像进行训练的可微分渲染[23,34,38,52,61]。我们将这些方法分为两组:<strong>表面渲染和体渲染</strong>。</li><li>表面渲染方法，包括DVR[38]和IDR[61]，直接确定物体表面的亮度，并使用隐式梯度提供可微分的渲染公式。这允许从多视图图像优化神经隐式曲面。调节观看方向允许IDR捕捉高水平的细节，即使是非兰伯曲面的存在。<strong>然而，DVR和IDR都要求所有视图的像素精确对象掩码作为输入。相比之下，我们的方法可以在不需要掩模的情况下进行类似的重建</strong>。</li><li>NeRF[34]和后续研究[6,28,35,36,44,45,49,53,63]通过学习沿光线的辐射场的alpha合成来使用体渲染。该方法在新视图合成上取得了令人印象深刻的效果，并且不需要掩模监督。然而，恢复的三维几何形状远不能令人满意，如图3所示。后续的一些作品(Neural Body [44] D-NeRF[45]和NeRD[6])使用NeRF的体积密度提取网格，但都没有考虑直接优化表面。<strong>与这些作品不同的是，我们的目标是捕获精确的几何形状，并提出一种可以证明接近极限表面渲染的体绘制公式</strong></li></ul></li></ul><h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>从多视图图像中学习神经隐式3D表示的两个主要组成部分是3D表示和连接3D表示和2D观察的渲染技术。本节提供了关于<strong>隐式表面和体积辐射</strong>表示的相关背景，我们在本文中统一了固体(非透明)物体和场景的情况。</p><ul><li>Implicit Surface Models<ul><li>占用网络[31,38]将曲面表示为二元占用分类器的决策边界，并通过神经网络参数化<ul><li>$o_\theta(\mathbf{x}):\mathbb{R}^3\to[0,1]$<ul><li>其中$x∈ \mathbb{R}^3$为三维点，θ为模型参数。曲面定义为占据概率为二分之一的所有3D点的集合$\mathcal{S}=\{\mathbf{x}_s|o_\theta(\mathbf{x}_s)=0.5\}.$</li></ul></li><li>为了将颜色与表面上的每个3D点xs相关联，可以将颜色场$c_\theta(\mathbf{x}_s)$与占用场$o_{\theta}(\mathbf{x}).$联合学习，从而预测特定像素/射线r的颜色:$\hat{C}(\mathbf{r})=c_\theta(\mathbf{x}_s)$</li><li>其中$x_s$是通过沿着射线r进行根查找来检索的，具体参见[38]。占用场$o_θ$和颜色场$c_θ$的参数 θ由[24,38,61]中描述的通过梯度下降优化重建损失来确定。</li><li>虽然表面渲染允许准确地估计几何形状和外观，但现有的方法强烈依赖于物体掩模的监督，因为表面渲染方法只能推断与表面相交的射线。</li></ul></li></ul></li><li>Volumetric Radiance Models<ul><li>与隐式表面模型相比，NeRF[34]将场景表示为彩色体积密度，并通过alpha混合将辐射沿光线进行整合[25,34]。更具体地说，NeRF使用神经网络将3D位置$x∈R^3$和观看方向$d∈R^3$映射到体积密度$σ_{θ}(x)∈R^{+}$和颜色值$c_{θ}(x, d)∈R^{3}$</li><li>对观察方向的调节允许建模与视图相关的效果，如镜面反射[34,40]，并在违反朗伯假设的情况下提高重建质量[61]。让我们表示相机中心的位置。给定沿射线$\mathbf{r}=\mathbf{o}+t\mathbf{d},$的N个样本$\{\mathbf{x}_i\}$， NeRF使用数值正交近似像素/射线r的颜色:</li><li>$\hat{C}(\mathbf{r}) =\sum_{i=1}^NT_i\left(1-\exp\left(-\sigma_\theta(\mathbf{x}_i)\delta_i\right)\right)c_\theta(\mathbf{x}_i,\mathbf{d})  ,T_{i} =\exp\left(-\sum_{j&lt;i}\sigma_\theta(\mathbf{x}_j)\delta_j\right)$</li><li>其中$T_{i}$为沿射线的累积透过率，$\delta_i=|\mathbf{x}_{i+1}-\mathbf{x}_i|$为相邻样品之间的距离。由于Eq.(3)是可微的，因此密度场$σ_θ$和颜色场$c_θ$的参数θ可以通过优化重构损失来估计。详见[34]。</li><li>虽然由于NeRF的体积亮度表示，它不需要对象掩模进行训练，<strong>但从体积密度中提取场景几何图形需要仔细调整密度阈值，并由于密度场中存在的模糊性而导致伪影</strong>，如图3所示。</li></ul></li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>与也适用于非固体场景(如雾、烟)的NeRF相反，我们将焦点限制在<strong>可以用3D表面和依赖于视图的表面颜色表示的固体物体</strong>上。我们的方法利用了这两者，体积辐射表示能力来学习粗糙的场景结构，而不需要掩模监督，以及表面渲染，作为一个感应偏差，通过一组精确的3D表面来表示对象，从而实现精确的重建。</p><h2 id="Unifying-Surface-and-Volume-Rendering"><a href="#Unifying-Surface-and-Volume-Rendering" class="headerlink" title="Unifying Surface and Volume Rendering"></a>Unifying Surface and Volume Rendering</h2><p>$\hat{C}(\mathbf{r})=\sum_{i=1}^N\alpha_i(\mathbf{x}_i)\prod_{j&lt;i}\bigl(1-\alpha_j(\mathbf{x}_j)\bigr)c(\mathbf{x}_i,\mathbf{d})$</p><p>将$\alpha$值替换为占据值o<br>α值：$\alpha_i(\mathbf{x})=1-\exp\left(-\sigma(\mathbf{x})\delta_i\right).$。假设物体为实体，则α为离散占用指标变量$o\in\{0,1\}$，其值为自由空间0 = 0，占用空间0 = 1:</p><p>$\hat{C}(\mathbf{r})=\sum_{i=1}^No(\mathbf{x}_i)\prod_{j&lt;i}\bigl(1-o(\mathbf{x}_j)\bigr)c(\mathbf{x}_i,\mathbf{d})$</p><p>我们将此表达式识别为固体物体的成像模型[55]，其中项$\begin{aligned}o(\mathbf{x}_i)\prod_{j&lt;i}\left(1-o(\mathbf{x}_j)\right)\end{aligned}$对于沿光线 r的第一个被占用的样本$x_i$求值为1，对于所有其他样本求值为0。$\prod_{j&lt;i}(1-o(\mathbf{x}_j))$是可见性指标，如果在样本$x_i$之前不存在j&lt;i的被占用样本xj，则可见性指标为1。因此，C(r)取沿射线r的第一个被占用样本的颜色$C(x_i, d)$。</p><p>为了统一隐式表面和体积亮度模型，我们直接用连续占用场$o_{θ}(1)$来参数化o，而不是预测体积密度σ。按照[61]，我们在曲面法向量n和几何网络的特征向量h上对颜色场$c_{θ}$进行条件调整，这在经验上诱导了一个有用的偏差，这也在[61]中观察到对于隐式曲面的情况。重要的是，我们统一的配方允许体积和表面渲染</p><script type="math/tex; mode=display">\begin{aligned}\hat{C}_v(\mathbf{r})&=\sum_{i=1}^No_\theta(\mathbf{x}_i)\prod_{j<i}\bigl(1-o_\theta(\mathbf{x}_j)\bigr)c_\theta(\mathbf{x}_i,\mathbf{n}_i,\mathbf{h}_i,\mathbf{d})\\\hat{C}_s(\mathbf{r})&=c_\theta(\mathbf{x}_s,\mathbf{n}_s,\mathbf{h}_s,\mathbf{d})\end{aligned}</script><p>$x_s$是沿着射线r得到的坐标，$n_s,h_s$分别是$x_s$出的法向量和几何特征。请注意，x依赖于占位场$o_θ$，但为了清晰起见，我们在这里去掉了这个依赖关系。<br>这种统一公式的优点是，它既可以直接在表面上渲染，也可以在整个体块上渲染，从而在优化过程中逐渐消除歧义。正如我们的实验所证明的那样，<strong>将两者结合起来对于在没有mask监督的情况下获得准确的重建确实至关重要</strong>。能够通过根查找快速恢复表面S使更有效的体渲染，先后聚焦和细化对象表面，我们将在4.3节中描述。此外，表面渲染可以实现更快的新视图合成，如图5所示。</p><h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>$\mathcal{L}=\mathcal{L}_{rec}+\lambda\mathcal{L}_{reg}$</p><ul><li><strong>rec</strong>onstruction loss $\mathcal{L}_{rec}=\sum_{\mathbf{r}\in\mathcal{R}}|\hat{C}_{v}(\mathbf{r})-C(\mathbf{r})|_{1}$</li><li>Surface <strong>reg</strong>ularization $\mathcal{L}_{reg}=\sum_{\mathbf{x}_s\in\mathcal{S}}|\mathbf{n}(\mathbf{x}_s)-\mathbf{n}(\mathbf{x}_s+\boldsymbol{\epsilon})|_2$</li></ul><p>其中，R为minibatch中所有像素/射线的集合，S为对应表面点的集合，C(R)为像素/射线R的观测颜色，ε为一个小的随机均匀三维扰动。<br>x处的法线$\mathbf{n}(\mathbf{x}_s)=\frac{\nabla_{\mathbf{x}_s}o_\theta(\mathbf{x}_s)}{|\nabla_{\mathbf{x}_s}o_\theta(\mathbf{x}_s)|_2}$可以用双反向传播来计算</p><h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p><strong>隐式曲面模型的关键假设</strong>[38,61]是，<strong>只有与曲面第一个交点处的区域对渲染方程有贡献</strong>。然而，这个假设在早期迭代中是不正确的，因为在早期迭代中曲面没有很好地定义。因此，现有的方法[38,61]需要强有力的mask监管。相反，在随后的迭代中，当评估Eq.(7)中的体绘制方程时，关于近似曲面的知识对于绘制信息样本是有价值的。因此，我们在体绘制过程中<strong>使用了一个采样间隔单调递减的训练计划来绘制样本</strong>，如图4所示</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230807130717.png" alt="image.png"><br><em>首先，我们在占用场oθ中寻找表面$x_s$(绿色)。其次，我们在表面周围定义一个间隔，以采样点$\{x_i\}$(红色)进行体渲染。</em></p><p>换句话说，在早期的迭代中，样本{$x_i$}覆盖了整个优化体积，有效地引导了使用体绘制的重建过程。在以后的迭代中，样本{$x_i$}在估计的表面附近被拉近。由于可以通过寻根直接从占用场oθ估计表面[38]，这就消除了像NeRF那样需要分层两阶段采样的需要。我们的实验表明，这个过程对于估计精确的几何图形特别有效，同时它允许在早期迭代中解决歧义。</p><p>More formally, let $\mathbf{x}_s=\mathbf{o}+t_s\mathbf{d}.$ We obtain samples $\mathbf{x}_i=\mathbf{o}+t_i\mathbf{d}.$ by drawing N depth values $t_i$ using stratified sampling within the interval $[ts − ∆, ts + ∆]$centered at $t_s$:<br>$t_i\sim\mathcal{U}\left[t_s+\left(\frac{2i-2}N-1\right)\Delta,t_s+\left(\frac{2i}N-1\right)\Delta\right]$</p><p>在训练过程中，我们从较大的间隔∆max开始，并逐渐减少∆，以便使用以下衰减时间表对表面进行更准确的采样和优化$\Delta_k=\max(\Delta_{\max}\exp(-k\beta),\Delta_{\min})$<br>其中k为迭代次数，β为超参数。事实上，可以证明，当∆→0和n→∞时，体绘制(7)确实接近曲面绘制(8):$C_v (r)→C_s(r)$。补充材料中提供了这个极限的正式证明。</p><p>正如我们的实验所证明的那样，(14)中的衰减调度对于捕获详细的几何图形至关重要，因为它将训练开始时的大型和不确定体积的体绘制与训练结束时的表面渲染结合起来。为了减少自由空间伪影，<strong>我们将这些样本与相机和表面之间随机采样的点结合起来</strong>。对于没有曲面相交的光线，我们对整个光线进行分层采样。</p><h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><ul><li>Architecture与Yariv等人[61]类似，我们使用带有Softplus激活函数的8层MLP，占用场$o_θ$的隐藏维数为256。我们初始化网络，使得决策边界是一个球体[13]。相比之下，辐射场$c_θ$被参数化为4层的ReLU MLP。我们使用傅里叶特征[34]在k倍频阶对3D位置x和观看方向d进行编码。根据经验，我们发现3D位置x的k = 6和观看方向d的k = 4效果最好。<ul><li>占用场MLP：layer = 8 , neurons = 256 , activation = softplus, init_sphere</li><li>颜色场MLP：layer = 4 , neurons = 256 , activation = ReLU</li><li>编码采用频率编码，位置x的k=6，方向d的k=4</li></ul></li><li>Optimization在所有的实验中，我们的模型适合单一场景的多视图图像。在模型参数优化过程中，我们首先对一个视图进行随机采样，然后根据相机的本征和外征对该视图进行M像素/射线R的采样。接下来，我们渲染所有射线来计算Eq.(9)中的损失函数。对于寻根，我们使用256个均匀采样点并应用割线方法，共8步[31]。对于我们的渲染过程，我们在区间内使用N = 64个查询点，在相机和区间下界之间的空闲空间中使用32个查询点。区间衰减参数为β = 1.5e−5，∆min = 0.05，∆max = 1.0。我们使用Adam，学习率为0.0001，每次迭代优化M = 1024像素，在200k和400k迭代后进行两个衰减步骤。总的来说，我们训练我们的模型进行了45万次迭代。</li><li>Inference我们的方法允许推断三维形状以及合成新的视图图像。对于合成图像，我们可以用两种不同的方式渲染我们的表现，我们可以使用体渲染或表面渲染。在图5中，我们展示了两种渲染方法导致相似的结果。然而，我们观察到表面渲染比体渲染快。<ul><li>为了提取网格，我们采用了[31]中的多分辨率等值面提取(MISE)算法。我们使用$64^{3}$作为初始分辨率，分3步对网格进行上采样，没有基于梯度的细化。</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230807132130.png" alt="image.png"></p><h1 id="Experimental-Evaluation"><a href="#Experimental-Evaluation" class="headerlink" title="Experimental Evaluation"></a>Experimental Evaluation</h1><p>我们在广泛使用的<strong>DTU MVS数据集</strong>[17]上，对我们的方法与现有方法(IDR [61]， NeRF [34]， COLMAP[48])进行了定性和定量比较。<br>其次，我们展示了来自<strong>blendedmvs dataset</strong>[60]的样本和来自the <strong>SceneNet dataset</strong>[29]的场景合成渲染的定性比较。<br>第三，我们分析了消融研究中的渲染程序和损失函数。在补充中，我们提供了LLFF数据集[33]上的结果。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PAniC-3D</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Interesting/PAniC-3D/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Interesting/PAniC-3D/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>PAniC-3D: Stylized Single-view 3D Reconstruction from Portraits of Anime Characters</th></tr></thead><tbody><tr><td>Author</td><td>Chen, Shuhong and Zhang, Kevin and Shi, Yichun and Wang, Heng and Zhu, Yiheng and Song, Guoxian and An, Sizhe and Kristjansson, Janus and Yang, Xiao and Matthias Zwicker</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://github.com/ShuhongChen/panic3d-anime-reconstruction">ShuhongChen/panic3d-anime-reconstruction: CVPR 2023: PAniC-3D Stylized Single-view 3D Reconstruction from Portraits of Anime Characters (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4738337093785239553&amp;noteId=1903166679687203840">PAniC-3D: Stylized Single-view 3D Reconstruction from Portraits of Anime Characters (readpaper.com)</a></td></tr></tbody></table></div><p>基于<strong>EG3D无条件生成模型</strong><br>PAniC-3D对比PixelNeRF、EG3D(+Img2stylegan or +PTI)、Pifu</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230806132746.png" alt="image.png"></p><span id="more"></span><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>我们提出了PAniC-3D，这是一个能够直接从插画风格的（p）动漫（ani）角色（c）肖像中重建立体化的系统。与自然人头部的真实图像相比，动漫角色肖像插画具有更复杂和多样的发型、配饰几何，且呈现非照片般的轮廓线阴影，从而为单视角重建带来独特的挑战。此外，缺乏适用于训练和评估这一模糊化风格重建任务的3D模型和肖像插画数据。面对这些挑战，我们提出的PAniC-3D架构通过线条填充模型跨越了插画到3D领域的差距，并利用体积辐射场来表示复杂的几何形态。我们使用两个大型新数据集（11.2k个Vroid 3D模型，1k个Vtuber肖像插画）来训练我们的系统，并在新颖的AnimeRecon插画到3D对比基准上进行评估。PAniC-3D在很大程度上优于基线方法，并为确立从肖像插画中进行风格化重建的任务提供了数据支持。</p><p>随着AR/VR应用的兴起，除了对高保真度human avatars的需求在增加，对虚拟形象如动漫3D角色的需求也在增加。大多数角色设计师通常首先创建概念插图，允许他们表达复杂和高度多样化的特征，如头发，配饰，眼睛，皮肤，头饰等。<strong>不幸的是</strong>，将插图概念艺术开发成AR/VR就绪的3D资产的过程是昂贵的，需要专业的3D艺术家训练使用专家建模软件。虽然基于模板的创作者在一定程度上民主化了3D化身，但它们通常仅限于与特定身体模型兼容的3D资产。</p><p>我们提出了PAniC-3D，这是一个系统，可以直接从动漫角色的肖像插图自动重建风格化的3D角色头部。我们将问题分为两部分:<br>1)隐式单视图头部重建， implicit single-view head reconstruction<br>2)跨插图- 3d域间隙，from across an illustration-3D domain gap.<br><strong>主要贡献</strong>：</p><ul><li>PAniC-3D: a system to <strong>reconstruct the 3D radiance field</strong> of a stylized character head <strong>from a single linebased portrait illustration.单线基础的肖像插图</strong></li><li>The Vroid 3D dataset of 11.2k character models and renders, <strong>the first such dataset in the anime-style domain</strong> to provide 3D assets with multiview renders.</li><li>The Vtuber dataset of 1.0k reconstruction-friendly portraits (aligned, front-facing, neutral-expression) that <strong>bridges the illustration-render domain gap through the novel task of line removal from drawings</strong>.</li><li><strong>The AnimeRecon benchmark</strong> with 68 pairs of aligned 3D models and corresponding illustrations, enabling quantitative evaluation of both <strong>image and geometry metrics</strong> for stylized reconstruction.</li></ul><h2 id="Implicit-3D-Reconstruction"><a href="#Implicit-3D-Reconstruction" class="headerlink" title="Implicit 3D Reconstruction"></a>Implicit 3D Reconstruction</h2><p>虽然已经有很多基于网格的图像重建工作[23]，<strong>但这些系统的表现力不足以捕捉3D字符拓扑的极端复杂性和多样性。</strong><br>受到最近在生成高质量3D辐射场方面取得成功的启发[4,5,25,39]，我们转而使用隐式表示。然而，为了获得高质量的结果，最近的隐式重建工作，如PixelNerf[40]，<strong>由于缺乏公开可用的高质量3D数据，往往只从2D图像进行操作。</strong><br>一些使用复杂3D资产的隐式重建系统，如Pifu[31]，在使用基于点的监督方面取得了一定的成功，<strong>但需要仔细的点采样技术和损失平衡。</strong><br>还有一组工作是基于草图的建模，其中3D表示是从轮廓图像中恢复的。例如，Rui等人[24]使用多视图解码器来预测草图到深度和法线，然后将其用于表面重建。Song等人[44]还尝试通过学习重新调整输入来补偿多视图绘制差异。虽然与我们的单视图肖像重建问题有关，<strong>但这些方法需要多视图草图，这对于角色艺术家来说很难一致地绘制，并且无法处理颜色输入</strong><br>对于复杂的高质量3D资产，我们证明了可微体绘制在重建中的优越性。我们建立在最近的无条件生成工作(EG3D[4])的基础上，将重建问题定义为条件生成，提出了一些架构改进，并应用由我们的3D数据集提供的直接2.5D监督信号。</p><h2 id="Anime-style-3D-Avatars-and-Illustrations"><a href="#Anime-style-3D-Avatars-and-Illustrations" class="headerlink" title="Anime-style 3D Avatars and Illustrations"></a>Anime-style 3D Avatars and Illustrations</h2><p>对于3D角色艺术家来说，从肖像插图中制作3D模型是一项相当常见的任务;然而，从计算机图形学的角度来看，这种程式化的重建设置给已经不适定的问题增加了额外的模糊性。此外，虽然在流行的动漫/漫画领域有使用3D角色资产的工作(姿势估计[18]，重新定位[17,20]和休息[22]等)，但<strong>缺乏公开可用的多视图渲染3D角色资产</strong>，允许可扩展的训练(表1)。鉴于这些问题，我们提出AnimeRecon(图1d)通过配对的插图到3D基准来形式化风格化任务。并提供3D资产的Vroid数据集(图1c)，以实现大规模训练。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230806132746.png" alt="image.png"></p><p>在程式化重建问题中，我们解决了从插图中去除轮廓的问题。在线条提取[21,38]、草图简化[33,34]、线条重建[11,24]、艺术图像的线条利用[6,41]和划线去除[8,27,29,32,35]方面有很多工作;<strong>然而，从基于线条的插图中删除线条却很少受到关注</strong>。我们在更有利于3D重建的渲染图像中调整图纸的背景下研究了这种轮廓删除任务;我们发现朴素的图像到图像翻译[19,45]不适合这项任务，并提出了一种简单而有效的带有面部特征感知的对抗性训练设置。最后，我们提供了一个Vtuber肖像数据集(图1b)来训练和评估用于3D重建的轮廓去除。</p><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>PAniC-3D由两个主要组件组成(图1a):一个直接监督的3D重建器，用于预测给定前端渲染的辐射场，以及一个将图像转换为重建器训练分布的插图-渲染模块。<strong>这两个部分是独立训练的，但在推理时是顺序使用的</strong>。</p><h2 id="3D-Reconstruction-Module"><a href="#3D-Reconstruction-Module" class="headerlink" title="3D Reconstruction Module"></a>3D Reconstruction Module</h2><p>3D重建模块图3在直接监督下进行训练，将正面渲染转换为体积辐射场。我们以最近的无条件生成工作(EG3D[4])为基础，将重建问题表述为条件生成问题，提出了几种架构改进，并应用我们的3D数据集提供的直接2.5D监督信号。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230806135527.png" alt="image.png"></p><ul><li>Conditional inputinput条件输入:将给定的要重建的正面正交视图调整大小并附加到EG3D中使用的Stylegan2主干的中间特征图中[4]。此外，在最早的特征映射中，我们通过连接预训练的Resnet-50动画标注器的倒数第二个特征，为模型提供有关输入的高级领域特定语义信息。所述标注器提供适合调节所述生成器的高级语义特征;之前的工作[7]对1062个相关类别进行了预训练，如蓝头发、猫耳朵、双辫子等</li><li>Feature pooling特征池化:由于空间特征映射将像EG3D[4]一样被重新塑造成三维三平面，我们发现沿着图像轴将每个特征映射的一部分通道池化是有益的(见图3左)。这种简单的技术有助于沿着公共三平面轴分发信息，从而提高几何度量的性能。</li><li>Multi-layer triplane多层三平面:根据并行工作[1]中提出的，我们通过在每个平面上堆叠更多通道来改进EG3D三平面(见图3中心)。该方法可以解释为三平面和体素网格之间的混合(如果层数等于空间大小，它们是等效的)。当双线性采样时，每个平面设置三层可以更好地消除空间歧义，特别是有助于我们的模型生成更可信的头部背面(EG3D没有面临的挑战)。</li><li>Loss损失:我们充分利用了我们可用的3D资产提供给我们的真实2.5D表示。我们的重建损失包括:RGB L1、LPIPS[42]、轮廓L1和深度L2;这些应用于前、后、右和左正射影视图，如图3所示。除了保持生成方向外，还采用了判别损失来提高细节质量。我们还保留了EG3D训练中的R1和密度正则化损失。我们的2.5D表示和对抗性设置使我们能够超越类似的单视图重构器，如PixelNerf[40]，它只适用于颜色损失</li><li>Post-processing后处理:我们利用我们的假设，即正面视图作为输入，通过在推理时将给定的输入拼接到生成的亮度场。每个像素的交点在体内的坐标被用来采样作为一个uv纹理图的输入;我们从每个交叉点投射一些额外的光线来测试正面的可见性，并相应地应用纹理。这种简单而有效的方法以可忽略不计的成本提高了输入的细节保存。</li></ul><h2 id="Illustration-to-Render-Module"><a href="#Illustration-to-Render-Module" class="headerlink" title="Illustration-to-Render Module"></a>Illustration-to-Render Module</h2><p>为了去除输入插图中存在的不真实的等高线，但在漫射光照场中不存在，我们设计了一个插图-渲染模块(图4)。假设可以访问未配对的插图和渲染(分别是我们的Vtuber和Vroid数据集)，浅网络在绘制的线条附近重新生成像素颜色，以便对抗性地匹配渲染图像分布。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230806135919.png" alt="image.png"><br>与CycleGAN和UGATIT[19,45]等未配对的图像对图像模型类似，我们也施加了小的身份损失;虽然这对于我们的填充情况似乎适得其反，因为在非生成区域中保留了身份，但我们发现这可以稳定GAN训练。请注意，我们的设置也不同于其他填充模型，因为我们的着色是为了匹配与输入不同的分布。根据之前的工作，从基于线的动画中提取草图[6]，我们使用简单高斯差分(DoG)算子，以防止在每个笔画周围提取双线。<br>虽然图纸中出现的大多数线条应该被删除，但关键面部特征周围的某些线条必须保留，因为它们确实出现在效果图中(眼睛，嘴巴，鼻子等)。我们使用了一个现成的动漫面部地标检测器[16]，在关键结构周围创建凸壳，其中不允许填充。</p><p>我们展示了这个线移除模块确实实现了一个更像渲染的外观;当在我们的AnimeRecon对上进行评估时，它比基线方法更准确地执行图像平移(表4)，并从最终的亮度场渲染中去除线伪影(图6)。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Interesting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FORGE</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Sparse/FORGE/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Sparse/FORGE/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Few-View Object Reconstruction with Unknown Categories and Camera Poses</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://hwjiang1510.github.io/">Hanwen Jiang</a>    <a href="https://zhenyujiang.me/">Zhenyu Jiang</a>    <a href="https://www.cs.utexas.edu/users/grauman/">Kristen Grauman</a>    <a href="https://cs.utexas.edu/~yukez">Yuke Zhu</a></td></tr><tr><td>Conf/Jour</td><td>ArXiv</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://ut-austin-rpl.github.io/FORGE/">FORGE (ut-austin-rpl.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4698779822646624257&amp;noteId=1899765094261033216">Few-View Object Reconstruction with Unknown Categories and Camera Poses (readpaper.com)</a></td></tr></tbody></table></div><p><strong>估计视图之间的相对相机姿态</strong><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230803141338.png" alt="image.png"></p><p>贡献： </p><ul><li>2D<strong>提取voxel特征</strong> —&gt; <strong>相机姿态估计</strong> —&gt; <strong>特征共享+融合</strong> —&gt;  MLP<strong>神经隐式重建</strong> —&gt; 渲染已有相机位姿的图片，并计算与gt之间的loss</li><li>新的损失函数</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230806144402.png" alt="image.png"></p><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们研究了在物体类别信息和相机姿态都未知的情况下，从几个视图重构物体的问题。我们的关键见解是利用形状重建和姿态估计之间的协同作用来提高这两项任务的性能。<br>我们设计了一个体素编码器和一个相对相机姿态估计器，它们以级联的方式进行训练。姿态估计器联合推断所有视图的姿态，并建立明确的三维跨视图对应关系。然后利用预测的姿态将从每个视图中提取的三维体素特征转换为共享的重建空间。然后预测神经体积，融合每个视图的信息。<br>我们的模型在重建和相对姿态估计方面明显优于现有技术。烧蚀研究也显示了我们设计的每个模块的有效性。我们希望我们的工作能够激励未来的努力，使物体重建广泛适用和可扩展，以捕获现实世界中的日常物体。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>虽然物体重建近年来取得了很大的进步，但<strong>目前的方法通常需要密集捕获的图像和/或已知的相机姿势</strong>，并且在新的物体类别上泛化得很差。<br>为了在野外重建物体，这项工作探索了<strong>从一些没有已知相机姿势或物体类别的图像中重建一般的现实世界物体</strong>。我们工作的关键是用统一的方法解决两个基本的3D视觉问题-<strong>形状重建和姿态估计</strong>。我们的方法抓住了这两个问题的协同作用:可靠的相机姿态估计可以产生准确的形状重建，而准确的重建反过来又可以在不同视图之间产生鲁棒对应并促进姿态估计。<br>我们的方法FORGE预测每个视图的3D特征，并将它们与输入图像结合起来，建立跨视图对应关系，以估计相对相机姿势。然后通过估计的姿态将3D特征转换为共享空间，并融合到神经辐射场中。重建结果通过体绘制技术进行渲染，使我们能够在没有三维形状的情况下训练模型。<br>实验表明，<strong>FORGE可以可靠地从五个角度重构物体</strong>。我们的姿态估计方法在很大程度上优于现有的方法。预测姿态下的重建结果与使用地真姿态的重建结果相当。在新测试类别上的表现与训练期间看到的类别结果相匹配。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>通过RGB相机镜头重建真实世界的物体在AR/VR应用[3,36]、嵌入式AI[23,46]和机器人[11,59]中至关重要。传统方法[15,27,41]依赖于密集捕获的图像进行基于优化的重建。对密集输入的严格要求阻碍了这些方法的广泛适用性。<strong>相比之下，少视图对象重建</strong>[61,63]旨在从真实世界对象的少量图像(如在线产品快照)快速创建3D模型。然而，现有的工作<strong>要么需要3D的地面真实监督</strong>来进行训练[7]，<strong>要么需要经过良好校准的相机姿势</strong>来进行推理[13,63]，而且往往局限于看到的物体类别[61]。为了有效地捕捉现实生活中的物体，我们<strong>渴望开发一种实用的方法，在不依赖物体类别或相机姿势的情况下执行一般的少视图物体重建</strong>。</p><p>在本文中，我们介绍了FORGE (Few-view Object Reconstruction that GEneralize)，如图1所示。与在特定类别的规范空间中重构对象的类别级方法不同[12,53]，<strong>FORGE将单个输入编码为各自相机空间中的3D特征。然后，它估计输入视图之间的相对相机姿态，并将3D特征转换为具有估计姿态的共享重建空间。这种设计消除了对每个类别的规范重构空间的需要，并使FORGE能够跨对象类别进行泛化</strong>。<strong>转换后的三维特征随后被聚合到一个神经体中</strong>。我们遵循NeRF[26]，并使用可微体积渲染来预测神经体积的新视图。该模型通过渲染图像和原始输入图像之间的重建损失进行监督，而不需要对物体形状进行3D监督</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230803141338.png" alt="image.png"></p><p><strong>最重要的挑战是估计视图之间的相对相机姿态</strong>。现有的相机姿态估计工作利用了二维图像的对应关系[22,42,64]。对于少视点目标重建，相机姿态的剧烈变化阻碍了鲁棒二维对应关系的建立。为此，<strong>我们设计了一种新的以三维特征和二维图像为输入的相对姿态估计器，利用三维特征的对应性来消除重投影歧义</strong>。此外，为了避免成对相对姿态估计中的复合误差[17,39]，FORGE基于所有输入之间的对应关系来预测姿态。因此，它受益于3D对应，并具有相机配置的全局理解。</p><p><strong>FORGE利用形状重建和姿态估计之间的协同作用来提高两者的性能</strong>。</p><ul><li>我们首先用真实的相机姿势训练模型。使用地面真实相机姿势鼓励模型学习3D几何先验，以便从每个输入视图中提取一致的3D特征。</li><li>然后，我们学习了相对相机姿态估计器，该估计器在已建立的视图一致的三维特征空间中准确地建立了三维对应关系</li></ul><p>为了评估FORGE的泛化能力，我们设计了一个新的数据集，与以前的数据集相比，它具有不同的目标类别和更严格的相机设置[45,57]。我们的结果表明，FORGE在重建和相对相机姿态估计方面都大大优于现有技术。在预测姿态下的重建质量与使用地真相对相机姿态的性能相匹配。此外，FORGE在新对象类别上表现出较强的泛化能力，几乎与在已知对象上的表现相当。此外，我们可以很容易地从预测的神经体积中获得准确的体素重建，证明了FORGE在理解3D几何形状方面的强大功能</p><p>我们强调我们的贡献如下:<br>i) 我们开发了一种<strong>推广到新对象类别</strong>的<strong>少视图对象重建方法</strong>;<br>ii) 设计了一种<strong>新的相对姿态估计模型</strong>来处理较大的相机姿态变化;<br>iii) 我们的实验<strong>证明了形状重建和姿态估计之间的协同作用</strong>，可以提高这两个任务的质量。我们致力于发布我们的代码和数据集，以实现可重复性和未来的研究。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li><strong>Multi-View Reconstruction</strong> 从多视图图像中重建物体和场景一直是计算机视觉中存在已久的问题[44]。<ul><li>传统方法，如COLMAP[41]，以及基于学习的对应方法，如DeepV2D[48]，已经取得了巨大的成功。然而，这些SLAM [4,15,48,69]， SfM[41,47,48]和RGB-D配准[2]方法需要密集的视图输入和平滑的相机运动。</li><li>另一项工作旨在仅使用稀疏甚至少数视图作为输入，其中出现了两种主要流。<ul><li>第一种类型是无姿势的。例如3D- r2n2[7]和pix2vox++[58]直接聚合了所有输入的3D信息。SRT[40]利用大型变压器模型构建了一种无几何的方法。然而，无几何的方法很难推广到看不见的类别，因为缺乏意识到交叉视图对应的操作。</li><li>另一种方法是姿势感知，其中相机姿势用于将每个视图的特征对齐到一个共同的重建空间[13,30,31,33,54]，并且它们受益于使用地面真实相机姿势。<strong>在本文中，我们开发了一种姿态感知方法，该方法以统一的方式预测形状和相对相机姿态，而在推理过程中不使用地面真实相机姿态</strong>。</li></ul></li></ul></li><li><strong>Volumetric 3D and Neural Radiance Fields</strong> 三维显式体积表示，特别是体素网格，已广泛用于对象建模[7,8,13,28,38,50,60]和场景建模[16,49]。<ul><li>近年来，鉴于隐式表征在3D视觉任务上取得了令人印象深刻的成果[25,32]，NeRF[26]采用隐式神经辐射场进行3D体积表征。NeRF及其变体实现了对复杂几何和外观建模的坚实能力[24,34,35,43,66]。尽管如此，<strong>NeRF使用一个全局MLP来拟合每个场景，这很难优化，而且泛化程度有限</strong>。</li><li>针对传统NeRF方法的局限性，提出了两种方法。<ul><li>第一种是<strong>将二维图像特征集成到MLP</strong>中[37,55,63]，由于3D-2D投影，MLP对相机姿态误差很敏感。</li><li>第二种是使用半隐式辐射场，其中辐射场附加到体素网格[20]。</li></ul></li><li>此外，为了使亮度场更具通用性，一些工作[6,62,67]训练了3D编码器，这些编码器直接从图像中预测基于体素的亮度场。然而，MVSNeRF[6]被设置为固定数量的附近视图，ShelfSup[62]预测规范空间中的辐射场，NeRFusion[67]需要地面真实相机姿势。<strong>相比之下，我们的模型从原始图像中预测基于体素的亮度场，可以使用使用预测姿势的任意数量的视图进行融合</strong>。</li></ul></li><li><strong>Reconstruction from Images without Poses.</strong><ul><li>使用体积表示的姿态感知重建方法的一个缺点是需要精确的相机姿态。<strong>许多作品都假定可以获得地面真实的相机姿势</strong>[13,26,45]，这限制了它们的适用性。BARF[18]和NeRS[65]对关节的形状和位姿进行了优化。<strong>然而，它们仍然依赖于高度精确的初始姿势</strong>。FvOR[61]<strong>提出了一个使用密集点对应的姿态初始化模块，但需要三维形状监督</strong>。</li><li>另一项工作[17,28,51]利用了形状和姿势之间的协同作用。GRNN[51]训练了一个<strong>相对姿态估计器</strong>，并将其用于重建过程中的姿态预测。VideoAE[17]<strong>通过解除形状和姿态的纠缠，在完全无监督学习下学习相对相机姿态和3D表示</strong>。<ul><li><strong>然而，这两个作品从原始的2D视图中预测相对的相机姿势，这使得它们很难处理看不见的类别，无纹理的物体，以及由于2D模糊性而导致的大的姿势变化</strong>。</li><li><strong>我们的方法在世界坐标中预测相对相机姿态</strong>。我们使用预测的姿势进行交叉视图融合，将形状和姿势预测与底层3D形状的感知联系起来。</li></ul></li></ul></li></ul><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>研究了在不包含相机姿态和类别信息的少量RGB图像中重建目标的问题。<br>如图2所示，我们的模型FORGE<strong>从2D图像和从中提取的3D特征中学习相机姿态估计</strong>。为了处理少数视图之间的大姿态变化，<strong>我们的姿态估计器联合预测所有输入图像的相对相机姿态</strong>，而不是将成对姿态估计链接起来。为了处理来自新类别的对象，我们避免学习特定类别的先验。<strong>FORGE在自己的相机空间中提取每个视图的3D特征，并将其转换为具有估计的相对相机姿势的共享重建空间</strong>。因此，我们摆脱了特定于范畴的正则空间进行重构。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230806144402.png" alt="image.png"></p><p>在共享重建空间中，<strong>我们使用特征融合模块对单视图三维特征进行信息聚合</strong>。然后，解码器头根据融合的特征预测神经体积。我们使用体绘制技术来产生重建结果。<strong>在训练过程中，我们渲染输入视图的结果，并使用基于2d的渲染损失作为监督</strong>。FORGE可以在没有物体几何形状的任何3D监督的情况下进行训练。<strong>我们设计了一个新的损失函数来学习跨视图一致的3D特征</strong>。此外，我们可以通过简单的阈值从神经体中获得准确的体素重建。下面我们将详细介绍FORGE和训练协议。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>模型的输入为由相关相机$\mathcal{C}=\{C^i|i=1,…,k\}$捕捉的k个图像观测值$\begin{aligned}\mathcal{I}=\{I^i|i=1,…,k\}\end{aligned}$<br>模型预测:<br>i) 所有视图的3D特征$\mathcal{Z}_{3D}=\{z_{3D}^{i}|i=1,…,k\}$;<br>ii) 相对相机位姿$\Delta\Phi_i=\{\Delta\Phi_i^j|j=1,…,k;j\neq i\},\Delta\Phi_1^j\in\mathbb{SE}(3),$其中$\Delta\Phi_{i}^{j}$为相机$C^j$相对于相机$C^i$的位姿，设第i帧为规范帧。<br>然后将这些特征通过相机的相对姿态变换到相机$C^i$的帧中，并进行融合来预测神经辐射场V。默认情况下，<strong>我们使用第一帧作为规范帧$C^i$</strong>，如果没有另行指定的话。我们将在下面详细介绍每个组件。</p><h2 id="Voxel-Encoder"><a href="#Voxel-Encoder" class="headerlink" title="Voxel Encoder"></a>Voxel Encoder</h2><p>对于视图$I^{i},$编码器$F_{3D}$将其编码为3D体素特征$z_{3D}=F_{3D}(I)$，其中$z_{3D}^i\in\mathbb{R}^{c\times d\times h\times u}$。<br>我们使用ResNet-50[10]提取二维特征图$z_{2D}^i=F_{2D}(I)$，其中$z_{2D}^i\in\mathbb{R}^{C\times h\times w}.$ 然后我们在$\mathbb{R}^{(C/d)\times d\times h\times w}$中将$z_{2D}^{i}$重构为3D体素特征作为去投影操作。我们最后对其执行一个3D卷积以进行细化，这将3D通道大小更改为c。</p><h2 id="Relative-Camera-Pose-Estimator"><a href="#Relative-Camera-Pose-Estimator" class="headerlink" title="Relative Camera Pose Estimator"></a>Relative Camera Pose Estimator</h2><p>在姿态变化较大的情况下，估计相对相机姿态可能是一个不适定问题——两个视图之间共享的可见物体部分可能很小，这使得很难建立跨视图对应关系。此外，在2D图像或特征地图上建立2D对应关系[22,42]在3D-2D投影模糊的情况下是脆弱的。</p><p><strong>我们引入了两种互补的新型姿态特征提取器</strong>。<br>如图3所示，我们使用全局姿态特征提取器，将所有2D视图作为输入，并对所有帧的姿态进行联合推理。<br>我们还构建了一个成对姿态特征提取器，该提取器在预测的3D特征上计算显式的3D对应关系。<br>该设计在三个方面有利于姿态估计:<br>i)全局姿态特征提取器允许信息在所有帧之间传递。它利用来自其他视图的信息(这些视图可能与规范视图共享更大的可见部分)来推断查询视图的相对位置。此外，在2D中对姿态进行全局推理比在3D特征中要便宜得多;<br>ii)在预测的3D特征上找到3D对应，避免了3D- 2D模糊问题，使姿态估计更加准确;<br>iii)姿态估计器的多模态输入(2D视图和3D特征)使其更具鲁棒性。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230811150059.png" alt="image.png"></p><h3 id="Global-Pose-Feature-Extractor"><a href="#Global-Pose-Feature-Extractor" class="headerlink" title="Global Pose Feature Extractor"></a>Global Pose Feature Extractor</h3><p>全局姿态特征提取器吸收所有视图并联合预测姿态特征，记为$p_g\in\mathbb{R}^{(k-1)\times1024},$ 对应于所有k−1个查询视图$\mathcal{I}_{q}$。</p><p>我们使用另一个2D主干提取每个视图的2D特征图，记为$\mathbb{Z}_{2D}^{\prime}=\{z_{2D}^{\prime i}|i=1,…,k\},z_{2D}^{\prime i}\in \mathbb{R}^{h^{\prime}\times w^{\prime}\times1024}$。然后，我们将规范视图特征重塑为一维向量$k_g\in\mathbb{R}^{N_{2D}\times1024}$，其中$N_{2D}=h^{\prime}\cdot w^{\prime}.$。我们同样重构了k−1个查询视图特征$q_g\in\mathbb{R}^{N_q\times\tilde{1024}}$，其中${N_{q}=(k-1)\cdot N_{2D}.}$。然后使用多个全局姿态特征推理(GPR)模块来推断姿态特征。具体来说，每个GPR模块包括两个标准多头transformer[52]块。</p><ul><li>在第一个转换块中，我们执行交叉关注，其中查询是查询视图的特征$\text{q}_g$，键和值是规范视图的特征$k_{g}$。块解释每个查询视图和规范视图之间的2D对应关系。</li><li>然后对更新后的查询视图特征执行自关注。它通过相互引用信息来共同改进所有查询视图的对应线索。<br>在GPR模块之后，我们将更新后的<strong>查询视图特征</strong>重塑为2D，并使用2D卷积将2D分辨率降至1，以获得全局姿态特征pg。</li></ul><h3 id="Pairwise-Pose-Feature-Extractor"><a href="#Pairwise-Pose-Feature-Extractor" class="headerlink" title="Pairwise Pose Feature Extractor"></a>Pairwise Pose Feature Extractor</h3><p>成对姿态特征提取器构建k−1个特征对，其中每个特征对由来自查询视图和规范视图的3D特征组成。然后分别预测它们的姿势特征。以查询视图$I^{i}$和规范视图$I^{1}$为例。输入为三维体素特征$z_{3D}^{i},z_{3D}^{1}$，输出为相对姿态特征$p_{l}^{i}\in\mathbb{R}^{1\times1024}.$。</p><p>具体来说，我们在$\mathbb{R}^{N_{3D}\times c}$中将三维特征重塑为一个一维向量，其中$N_{3D}=d \cdot h \cdot w.$。然后我们计算相似张量$S^i=z_{3D}^i\cdot(z_{3D}^1)^T,$，其中$(\cdot)^T$是转置运算。得到对应关系为$Corr_{1D}^i=S^{i}\cdot PE_{1D},$，其中$PE_{1D}$为扩展后的高维三维位置嵌入[52]。位置嵌入表示每个体素在高维空间中的位置。$PE\in\mathbb{R}^{d\times h\times w\times c}$ ，$Corr_{1D}^i\in\mathbb{R}^{N_{3D}\times c}.$。我们将其重塑为一个三维体$Corr^i\in\mathbb{R}^{d\times\hat{h}\times w\times c}$。<br>对于volume的每个体素，其特征表示其相应体素位置在高维空间规范视图3D特征中的位置。最后，我们使用3D卷积将其缩小到分辨率1，得到相对姿态特征$p_{l}^{i}$ of $\{I^{i},I^{1}\}.$。我们将所有查询视图的特征连接起来作为最终输出$p_l\in\mathbb{R}^{(k-1)\times1024}$。</p><h3 id="Pose-Prediction"><a href="#Pose-Prediction" class="headerlink" title="Pose Prediction"></a>Pose Prediction</h3><p>我们将两个特征提取器预测的姿态特征连接起来，得到最终的特征p。我们使用MLP来回归相对相机姿态$\Delta\Phi_{i}=\{\Delta\Phi_{1}^{j}|j=2,…,k\}$表示k−1个查询视图。为了防止模型过度拟合任何一个提取的特征，我们在训练期间在MLP之前使用概率为0.6的Dropout层。</p><h2 id="Feature-Fusion-Module"><a href="#Feature-Fusion-Module" class="headerlink" title="Feature Fusion Module"></a>Feature Fusion Module</h2><h3 id="Feature-Transformation"><a href="#Feature-Transformation" class="headerlink" title="Feature Transformation"></a>Feature Transformation</h3><p>一般情况下，我们使用刚体变换$T_{i}^{j}=\Phi^{j}\cdot(\Phi^{i})^{-1}$将相机$C^i$帧中的一个三维点变换到相机$C^j$帧中，其中$(\cdot)^{-\ddot{1}}$为逆运算，Φ为相机姿态。给定k−1个查询视图提取的3D特征，我们使用相机姿势将它们转换为规范视图的相机帧。具体来说，查询视图的相机姿态由$\Phi^{i}=\Phi^{1}\cdot\Delta\Phi_{1}^{i}$计算，其中$Φ_1$为固定的规范姿态。将变换后的三维特征记为:$\dot{z}_{3D}=\{\dot{z}_{3D}^i|i=1,…,k\}.$。</p><h3 id="View-Fusion"><a href="#View-Fusion" class="headerlink" title="View Fusion"></a>View Fusion</h3><p>对于变换后的3D特征$\dot{\mathbb{Z}}_{3D}$，我们首先对它们进行平均池化。池化的3D特征作为初始化，使用ConvGRU将perview特征依次融合到最终的3D特征$Z_{3D}$中[1]。具体来说，融合序列由相对相机姿势决定，我们首先融合更接近规范视图的视图特征。当预测的相机姿态有噪声时，平均池化操作保留低频信号，顺序融合恢复高频细节。</p><h2 id="Neural-Volume-based-Rendering"><a href="#Neural-Volume-based-Rendering" class="headerlink" title="Neural Volume-based Rendering"></a>Neural Volume-based Rendering</h2><p>受NeRF[26]的启发，我们用辐射场重建物体。这些领域，表示为神经体积，以完全前馈的方式预测。</p><h3 id="Definition-and-Prediction"><a href="#Definition-and-Prediction" class="headerlink" title="Definition and Prediction"></a>Definition and Prediction</h3><p>我们将神经体积表示为$V:=(V_{\sigma},V_{f})$，其中$V_{\sigma}$为密度值，$V_{f}$为神经特征。它们有助于重建物体的几何形状和外观。给定融合后的三维特征volume $Z_{3D}$，我们使用由多个三维卷积层组成的两个预测头分别预测$V_{\sigma}$和$V_{f}$。</p><h3 id="Volume-Rendering"><a href="#Volume-Rendering" class="headerlink" title="Volume Rendering"></a>Volume Rendering</h3><p>我们使用体绘制在2D上读出预测的神经体积为$(\hat{I},\hat{I}_\sigma)=\pi(V,\Phi),$，其中Φ为相机姿态，$\hat{I}\mathrm{and}\hat{I}_{\sigma}$为渲染的图像和掩码。我们遵循NeRF中的体渲染技术[26]。不同的是，对于每个3D查询点p，我们通过插值相邻体素网格来获得其3D特征。此外，我们首先渲染一个特征映射，然后使用几个2D卷积来预测最终的RGB图像[29]。</p><h2 id="Training-Protocol"><a href="#Training-Protocol" class="headerlink" title="Training Protocol"></a>Training Protocol</h2><p>我们分三个阶段训练FORGE。</p><ul><li>首先，我们在ground-truth pose下使用$L_{3D}=L_{mv}+L_{corr}$训练体素编码器(第4.1节)、融合模块(第4.3节)和体渲染器(第4.4节)<ul><li>具体来说，$L_{mv}$是应用在所有输入视图上的二维光度损失。</li></ul></li></ul><script type="math/tex; mode=display">\begin{aligned}&L_{2D} =||I_\sigma^i-\hat{I}_\sigma^i||+\lambda_{img}||I^i-\hat{I}^i||,  \\&L_{mv} =\frac1k\Sigma_{i=1}^k(L_{2D}(I_{\sigma}^i,\hat{I}_{\sigma}^i,I^i,\hat{I}^i)+\lambda_pL_p(I^i,\hat{I}^i)), \end{aligned}</script><p>交叉视图一致性损失$L_{corr}$的设计是为了促使从不同视图提取的同一对象部分对应的特征在特征空间中接近，从而使重建结果更加连贯。完善的特征空间也受益于基于对应的姿态估计，因为使用我们的基于相似性的方法在特征空间中查找跨视图对应变得更加容易。我们计算渲染结果的损失。<br>我们将输入视图分成两组:我们使用一组(n个视图)来构建神经体积，并使用<strong>另一组(k−n个视图)的相机来渲染结果</strong>，反之亦然。第二组视图的重构应该是合理的。</p><p>$L_{corr}=\frac{1}{k-n}\Sigma_{i=n+1}^{k}L_{2D}(I_{\sigma}^{i},\ddot{I}_{\sigma}^{i},I^{i},\ddot{I}^{i}),$其中，$\ddot{I}\mathrm{~and~}\ddot{I}_\sigma$是使用视图子集作为输入呈现的结果。</p><ul><li>其次，我们用损失$L_{pose}=||\Phi-\hat{\Phi}||^{2}$训练相对相机姿态估计器，其中$\Phi,\hat$是真实的和预测的相对相机姿态。两个姿态特征提取器分别进行训练，然后一起进行微调。</li><li>最后，我们使用上述所有损失对模型进行端到端的微调。我们从经验上观察到，单阶段训练导致崩溃。我们推测姿态估计器依赖于来自初始化良好的体素编码器的表示。</li></ul><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>new dataset: <strong>Kubric Synthetic Dataset.</strong></p><p>实现细节。输入分辨率为256 × 256。我们使用k = 5张图像，分为2和3个视图，来计算跨视图一致性损失。我们设$λ_{img} = 5$， $λ_{p} = 0.02$， $λ_{pose} = 1$。神经体积包含$64^3$个体素。我们在每条光线上采样64个点进行渲染。有关更多培训详情，请参阅附录。</p><p>我们采用标准的新视点合成指标PSNR(以dB为单位)和SSIM[56]来评估重建结果。我们还评估了相对相机姿态误差。</p><h2 id="campare"><a href="#campare" class="headerlink" title="campare"></a>campare</h2><p>在重建方面，我们与PixelNeRF[63]和SRT[40]在地真位摆下进行比较，验证FORGE学习三维几何先验的能力。</p><p>对于相对姿态估计，我们比较了<br>i)基于2d的VideoAE[17]， 8点TF[39]，它们分别预测每个查询视图的姿态;<br>ii)基于2d的RelPose[64]，共同估计所有相对姿势，<br>iii)基于3d的Gen6D，分别预测每个查询视图的姿态。所有的姿态估计基线都是用真实姿态训练的。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Sparse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
            <tag> Sparse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRO-code</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/NeRO-code/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/NeRO-code/</url>
      
        <content type="html"><![CDATA[<p>NeRO代码<a href="https://github.com/liuyuan-pal/NeRO">liuyuan-pal/NeRO: [SIGGRAPH2023] NeRO: Neural Geometry and BRDF Reconstruction of Reflective Objects from Multiview Images (github.com)</a></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728142918.png" alt="image.png"></p><span id="more"></span><h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><h2 id="DummyDataset-Dataset"><a href="#DummyDataset-Dataset" class="headerlink" title="DummyDataset(Dataset)"></a>DummyDataset(Dataset)</h2><p><code>name2dataset[self.cfg[&#39;train_dataset_type&#39;]](self.cfg[&#39;train_dataset_cfg&#39;], True)</code><br>相当于：<code>DummyDataset(self.cfg[&#39;train_dataset_cfg&#39;], True)</code></p><ul><li>is_train: <ul><li><code>__getitem__ : return &#123;&#125;</code></li><li><code>__len__ : return 99999999</code> </li></ul></li><li>else:<ul><li><code>__getitem__ : return &#123;&#39;index&#39;: index&#125;</code></li><li><code>__len__ : return self.test_num</code> </li></ul></li></ul><p>not is_train:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> is_train:</span><br><span class="line">    <span class="comment"># return 一个实例 eg：GlossyRealDatabase(self.cfg[&#x27;database_name&#x27;])</span></span><br><span class="line">    database = parse_database_name(self.cfg[<span class="string">&#x27;database_name&#x27;</span>])</span><br><span class="line">    <span class="comment"># 使用database中的get_img_ids方法，获取train_ids和test_ids</span></span><br><span class="line">    train_ids, test_ids = get_database_split(database, <span class="string">&#x27;validation&#x27;</span>)</span><br><span class="line">    self.train_num = <span class="built_in">len</span>(train_ids)</span><br><span class="line">    self.test_num = <span class="built_in">len</span>(test_ids)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val&#x27;</span> , is_train)</span><br></pre></td></tr></table></figure></p><p><strong>database.py</strong>:</p><ul><li>parse_database_name</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse_database_name</span>(<span class="params">database_name:<span class="built_in">str</span></span>)-&gt;BaseDatabase:</span><br><span class="line">    name2database=&#123;</span><br><span class="line">        <span class="string">&#x27;syn&#x27;</span>: GlossySyntheticDatabase,</span><br><span class="line">        <span class="string">&#x27;real&#x27;</span>: GlossyRealDatabase,</span><br><span class="line">        <span class="string">&#x27;custom&#x27;</span>: CustomDatabase,</span><br><span class="line">    &#125;</span><br><span class="line">    database_type = database_name.split(<span class="string">&#x27;/&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> database_type <span class="keyword">in</span> name2database:</span><br><span class="line">        <span class="keyword">return</span> name2database[database_type](database_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><ul><li>get_database_split<ul><li>打乱self.img_ids，并split为test和train</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_database_split</span>(<span class="params">database: BaseDatabase, split_type=<span class="string">&#x27;validation&#x27;</span></span>):</span><br><span class="line">    <span class="keyword">if</span> split_type==<span class="string">&#x27;validation&#x27;</span>:</span><br><span class="line">        random.seed(<span class="number">6033</span>)</span><br><span class="line">        img_ids = database.get_img_ids()</span><br><span class="line">        random.shuffle(img_ids)</span><br><span class="line">        test_ids = img_ids[:<span class="number">1</span>]</span><br><span class="line">        train_ids = img_ids[<span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">elif</span> split_type==<span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">        test_ids, train_ids = read_pickle(<span class="string">&#x27;configs/synthetic_split_128.pkl&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    <span class="keyword">return</span> train_ids, test_ids</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">f = open(&#x27;E:\\BaiduSyncdisk\\NeRF_Proj\\NeRO\\configs\\synthetic_split_128.pkl&#x27;,&#x27;rb&#x27;)</span></span><br><span class="line"><span class="string">[array([&#x27;0&#x27;, &#x27;4&#x27;, &#x27;19&#x27;, &#x27;2&#x27;, &#x27;127&#x27;, &#x27;71&#x27;, &#x27;73&#x27;, &#x27;56&#x27;, &#x27;75&#x27;, &#x27;95&#x27;, &#x27;93&#x27;,</span></span><br><span class="line"><span class="string">       &#x27;110&#x27;, &#x27;91&#x27;, &#x27;7&#x27;, &#x27;5&#x27;, &#x27;3&#x27;, &#x27;68&#x27;, &#x27;30&#x27;, &#x27;66&#x27;, &#x27;113&#x27;, &#x27;111&#x27;, &#x27;33&#x27;,</span></span><br><span class="line"><span class="string">       &#x27;120&#x27;, &#x27;31&#x27;, &#x27;29&#x27;, &#x27;14&#x27;, &#x27;49&#x27;, &#x27;11&#x27;, &#x27;109&#x27;, &#x27;61&#x27;, &#x27;59&#x27;, &#x27;57&#x27;],</span></span><br><span class="line"><span class="string">      dtype=&#x27;&lt;U3&#x27;), </span></span><br><span class="line"><span class="string">[&#x27;1&#x27;, &#x27;6&#x27;, &#x27;8&#x27;, &#x27;9&#x27;, &#x27;10&#x27;, &#x27;12&#x27;, &#x27;13&#x27;, &#x27;15&#x27;, &#x27;16&#x27;, &#x27;17&#x27;, &#x27;18&#x27;, &#x27;20&#x27;, &#x27;21&#x27;, &#x27;22&#x27;, &#x27;23&#x27;, &#x27;24&#x27;,</span></span><br><span class="line"><span class="string">&#x27;25&#x27;, &#x27;26&#x27;, &#x27;27&#x27;, &#x27;28&#x27;, &#x27;32&#x27;, &#x27;34&#x27;, &#x27;35&#x27;, &#x27;36&#x27;, &#x27;37&#x27;, &#x27;38&#x27;, &#x27;39&#x27;, &#x27;40&#x27;, &#x27;41&#x27;, &#x27;42&#x27;, &#x27;43&#x27;,</span></span><br><span class="line"><span class="string">&#x27;44&#x27;, &#x27;45&#x27;, &#x27;46&#x27;, &#x27;47&#x27;, &#x27;48&#x27;, &#x27;50&#x27;, &#x27;51&#x27;, &#x27;52&#x27;, &#x27;53&#x27;, &#x27;54&#x27;, &#x27;55&#x27;, &#x27;58&#x27;, &#x27;60&#x27;, &#x27;62&#x27;, &#x27;63&#x27;,</span></span><br><span class="line"><span class="string">&#x27;64&#x27;, &#x27;65&#x27;, &#x27;67&#x27;, &#x27;69&#x27;, &#x27;70&#x27;, &#x27;72&#x27;, &#x27;74&#x27;, &#x27;76&#x27;, &#x27;77&#x27;, &#x27;78&#x27;, &#x27;79&#x27;, &#x27;80&#x27;, &#x27;81&#x27;, &#x27;82&#x27;, &#x27;83&#x27;,</span></span><br><span class="line"><span class="string">&#x27;84&#x27;, &#x27;85&#x27;, &#x27;86&#x27;, &#x27;87&#x27;, &#x27;88&#x27;, &#x27;89&#x27;, &#x27;90&#x27;, &#x27;92&#x27;, &#x27;94&#x27;, &#x27;96&#x27;, &#x27;97&#x27;, &#x27;98&#x27;, &#x27;99&#x27;, &#x27;100&#x27;, &#x27;101&#x27;,</span></span><br><span class="line"><span class="string">&#x27;102&#x27;, &#x27;103&#x27;, &#x27;104&#x27;, &#x27;105&#x27;, &#x27;106&#x27;, &#x27;107&#x27;, &#x27;108&#x27;, &#x27;112&#x27;, &#x27;114&#x27;, &#x27;115&#x27;, &#x27;116&#x27;, &#x27;117&#x27;,</span></span><br><span class="line"><span class="string">&#x27;118&#x27;, &#x27;119&#x27;, &#x27;121&#x27;, &#x27;122&#x27;, &#x27;123&#x27;, &#x27;124&#x27;, &#x27;125&#x27;, &#x27;126&#x27;]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h2 id="GlossyRealDatabase"><a href="#GlossyRealDatabase" class="headerlink" title="GlossyRealDatabase"></a>GlossyRealDatabase</h2><h3 id="init"><a href="#init" class="headerlink" title="init"></a>init</h3><ul><li>meta_info</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">meta_info=&#123;</span><br><span class="line">    <span class="string">&#x27;bear&#x27;</span>: &#123;<span class="string">&#x27;forward&#x27;</span>: np.asarray([<span class="number">0.539944</span>,-<span class="number">0.342791</span>,<span class="number">0.341446</span>],np.float32), <span class="string">&#x27;up&#x27;</span>: np.asarray((<span class="number">0.0512875</span>,-<span class="number">0.645326</span>,-<span class="number">0.762183</span>),np.float32),&#125;,</span><br><span class="line">    <span class="string">&#x27;coral&#x27;</span>: &#123;<span class="string">&#x27;forward&#x27;</span>: np.asarray([<span class="number">0.004226</span>,-<span class="number">0.235523</span>,<span class="number">0.267582</span>],np.float32), <span class="string">&#x27;up&#x27;</span>: np.asarray((<span class="number">0.0477973</span>,-<span class="number">0.748313</span>,-<span class="number">0.661622</span>),np.float32),&#125;,</span><br><span class="line">    <span class="string">&#x27;maneki&#x27;</span>: &#123;<span class="string">&#x27;forward&#x27;</span>: np.asarray([-<span class="number">2.336584</span>, -<span class="number">0.406351</span>, <span class="number">0.482029</span>], np.float32), <span class="string">&#x27;up&#x27;</span>: np.asarray((-<span class="number">0.0117387</span>, -<span class="number">0.738751</span>, -<span class="number">0.673876</span>), np.float32), &#125;,</span><br><span class="line">    <span class="string">&#x27;bunny&#x27;</span>: &#123;<span class="string">&#x27;forward&#x27;</span>: np.asarray([<span class="number">0.437076</span>,-<span class="number">1.672467</span>,<span class="number">1.436961</span>],np.float32), <span class="string">&#x27;up&#x27;</span>: np.asarray((-<span class="number">0.0693234</span>,-<span class="number">0.644819</span>,-<span class="number">.761185</span>),np.float32),&#125;,</span><br><span class="line">    <span class="string">&#x27;vase&#x27;</span>: &#123;<span class="string">&#x27;forward&#x27;</span>: np.asarray([-<span class="number">0.911907</span>, -<span class="number">0.132777</span>, <span class="number">0.180063</span>], np.float32), <span class="string">&#x27;up&#x27;</span>: np.asarray((-<span class="number">0.01911</span>, -<span class="number">0.738918</span>, -<span class="number">0.673524</span>), np.float32), &#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从CloudCampare中获取的数据，其中forward是手动设置的前向，up是根据手动截取的一小块平面的法向<br><a href="https://github.com/liuyuan-pal/NeRO/blob/main/custom_object.md">NeRO/custom_object.md at main · liuyuan-pal/NeRO (github.com)</a></p><div style="display:flex; justify-content:space-between;"> <img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230803160338.png" alt="Image 1" style="width:50%;"><div style="width:10px;"></div> <img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230803160313.png" alt="Image 2" style="width:50%;"> </div><ul><li><code>_parse_colmap</code> 从cache.pkl中读取数据，如果cache.pkl不存在，则从<code>/colmap/sparse/0</code>中读取并将数据写入到cache.pkl中<ul><li>self.poses (3,4), self.Ks (3,3), self.image_names(dict is img_ids : img_name), self.img_ids(list len is num_images)</li></ul></li><li><code>_normalize</code> 读取object_point_cloud.ply，获得截取出来的物体点云坐标，将该点云坐标标准化到单位bound中，并将世界基坐标系转换到手动设置的坐标系即up、forward<ul><li>即<code>_normalize</code>将点云坐标从原来的世界坐标系w转换到新的坐标系w’下</li><li>将pose的w2c数据转换为w’2c，其中w’原点在object_point_cloud.ply中心，xyz轴是自定义的轴，如上图，但是scale不变</li></ul></li><li><code>database_name: real/bear/raw_1024</code>中最后一个raw_1024<ul><li>如果以raw开头：<ul><li>将images中图片缩放并保存到images_raw_1024中</li><li>并更换self.Ks中数据（由于缩放了W、H）</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># f = open(&#x27;E:\\BaiduSyncdisk\\NeRF_Proj\\NeRO\\data\\GlossyReal\\bear\\cache.pkl&#x27;,&#x27;rb&#x27;)</span></span><br><span class="line"><span class="string">data:</span></span><br><span class="line"><span class="string">[&#123;1: array([[-0.21238244, -0.73007965,  0.6495209 , -0.28842232],</span></span><br><span class="line"><span class="string">       [ 0.4490811 ,  0.5174136 ,  0.7284294 , -2.9991536 ],</span></span><br><span class="line"><span class="string">       [-0.86788243,  0.44639313,  0.21797541,  3.0713193 ]],</span></span><br><span class="line"><span class="string">      dtype=float32), 2: array([[ 0.13014111, -0.75269127,  0.64538294, -0.3143167 ],</span></span><br><span class="line"><span class="string">       [ 0.47691151,  0.6181942 ,  0.624813  , -2.9732292 ],</span></span><br><span class="line"><span class="string">       [-0.8692633 ,  0.22647668,  0.43941963,  2.7372477 ]],</span></span><br><span class="line"><span class="string">        dtype=float32), </span></span><br><span class="line"><span class="string">       ...&#125;</span></span><br><span class="line"><span class="string">&#123;...96: array([[6.013423e+03, 0.000000e+00, 1.368000e+03],</span></span><br><span class="line"><span class="string">       [0.000000e+00, 6.013423e+03, 1.824000e+03],</span></span><br><span class="line"><span class="string">       [0.000000e+00, 0.000000e+00, 1.000000e+00]], dtype=float32), 97: array([[6.013423e+03, 0.000000e+00, 1.368000e+03],</span></span><br><span class="line"><span class="string">       [0.000000e+00, 6.013423e+03, 1.824000e+03],</span></span><br><span class="line"><span class="string">       [0.000000e+00, 0.000000e+00, 1.000000e+00]], dtype=float32)&#125;</span></span><br><span class="line"><span class="string">&#123;1: &#x27;1436.jpg&#x27;, 2: &#x27;1437.jpg&#x27;, 3: &#x27;1438.jpg&#x27;, 4: &#x27;1439.jpg&#x27;, 5: &#x27;1440.jpg&#x27;, 6: &#x27;1441.jpg&#x27;, 7: &#x27;1442.jpg&#x27;, </span></span><br><span class="line"><span class="string">8: &#x27;1443.jpg&#x27;, 9: &#x27;1444.jpg&#x27;, 10: &#x27;1445.jpg&#x27;, 11: &#x27;1446.jpg&#x27;, 12: &#x27;1447.jpg&#x27;, 13: &#x27;1448.jpg&#x27;, 14: &#x27;1449.jpg&#x27;, </span></span><br><span class="line"><span class="string">15: &#x27;1450.jpg&#x27;, 16: &#x27;1451.jpg&#x27;, 17: &#x27;1452.jpg&#x27;, 18: &#x27;1453.jpg&#x27;, 19: &#x27;1454.jpg&#x27;, 20: &#x27;1455.jpg&#x27;, 21: &#x27;1456.jpg&#x27;, </span></span><br><span class="line"><span class="string">22: &#x27;1457.jpg&#x27;, 23: &#x27;1458.jpg&#x27;, 24: &#x27;1459.jpg&#x27;, 25: &#x27;1460.jpg&#x27;, 26: &#x27;1461.jpg&#x27;, 27: &#x27;1462.jpg&#x27;, 28: &#x27;1463.jpg&#x27;, </span></span><br><span class="line"><span class="string">29: &#x27;1464.jpg&#x27;, 30: &#x27;1465.jpg&#x27;, 31: &#x27;1466.jpg&#x27;, 32: &#x27;1467.jpg&#x27;, 33: &#x27;1468.jpg&#x27;, 34: &#x27;1469.jpg&#x27;, 35: &#x27;1470.jpg&#x27;, </span></span><br><span class="line"><span class="string">36: &#x27;1471.jpg&#x27;, 37: &#x27;1472.jpg&#x27;, 38: &#x27;1473.jpg&#x27;, 39: &#x27;1474.jpg&#x27;, 40: &#x27;1475.jpg&#x27;, 41: &#x27;1476.jpg&#x27;, 42: &#x27;1477.jpg&#x27;, </span></span><br><span class="line"><span class="string">43: &#x27;1478.jpg&#x27;, 44: &#x27;1479.jpg&#x27;, 45: &#x27;1480.jpg&#x27;, 46: &#x27;1481.jpg&#x27;, 47: &#x27;1482.jpg&#x27;, 48: &#x27;1483.jpg&#x27;, 49: &#x27;1484.jpg&#x27;, </span></span><br><span class="line"><span class="string">50: &#x27;1485.jpg&#x27;, 51: &#x27;1486.jpg&#x27;, 52: &#x27;1487.jpg&#x27;, 53: &#x27;1488.jpg&#x27;, 54: &#x27;1489.jpg&#x27;, 55: &#x27;1490.jpg&#x27;, 56: &#x27;1491.jpg&#x27;, </span></span><br><span class="line"><span class="string">57: &#x27;1492.jpg&#x27;, 58: &#x27;1493.jpg&#x27;, 59: &#x27;1494.jpg&#x27;, 60: &#x27;1495.jpg&#x27;, 61: &#x27;1496.jpg&#x27;, 62: &#x27;1497.jpg&#x27;, 63: &#x27;1498.jpg&#x27;, </span></span><br><span class="line"><span class="string">64: &#x27;1499.jpg&#x27;, 65: &#x27;1500.jpg&#x27;, 66: &#x27;1501.jpg&#x27;, 67: &#x27;1502.jpg&#x27;, 68: &#x27;1503.jpg&#x27;, 69: &#x27;1504.jpg&#x27;, 70: &#x27;1505.jpg&#x27;, </span></span><br><span class="line"><span class="string">71: &#x27;1506.jpg&#x27;, 72: &#x27;1507.jpg&#x27;, 73: &#x27;1508.jpg&#x27;, 74: &#x27;1509.jpg&#x27;, 75: &#x27;1510.jpg&#x27;, 76: &#x27;1511.jpg&#x27;, 77: &#x27;1512.jpg&#x27;, </span></span><br><span class="line"><span class="string">78: &#x27;1513.jpg&#x27;, 79: &#x27;1514.jpg&#x27;, 80: &#x27;1515.jpg&#x27;, 81: &#x27;1516.jpg&#x27;, 82: &#x27;1517.jpg&#x27;, 83: &#x27;1518.jpg&#x27;, 84: &#x27;1519.jpg&#x27;, </span></span><br><span class="line"><span class="string">85: &#x27;1520.jpg&#x27;, 86: &#x27;1521.jpg&#x27;, 87: &#x27;1522.jpg&#x27;, 88: &#x27;1523.jpg&#x27;, 89: &#x27;1524.jpg&#x27;, 90: &#x27;1525.jpg&#x27;, 91: &#x27;1526.jpg&#x27;, </span></span><br><span class="line"><span class="string">92: &#x27;1527.jpg&#x27;, 93: &#x27;1528.jpg&#x27;, 94: &#x27;1529.jpg&#x27;, 95: &#x27;1530.jpg&#x27;, 96: &#x27;1531.jpg&#x27;, 97: &#x27;1532.jpg&#x27;&#125;, </span></span><br><span class="line"><span class="string">[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, </span></span><br><span class="line"><span class="string">30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, </span></span><br><span class="line"><span class="string">57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, </span></span><br><span class="line"><span class="string">84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h1 id="Network-amp-Render"><a href="#Network-amp-Render" class="headerlink" title="Network&amp;Render"></a>Network&amp;Render</h1><h2 id="NeROShapeRenderer"><a href="#NeROShapeRenderer" class="headerlink" title="NeROShapeRenderer"></a>NeROShapeRenderer</h2><h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728142918.png" alt="image.png"></p><ul><li><p>SDFNetwork</p><ul><li><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/SDFNetwork_modify.png" alt="SDFNetwork"></li><li>39—&gt;256—&gt;256—&gt;256—&gt;217—&gt;256—&gt;256—&gt;256—&gt;256—&gt;257</li></ul></li><li><p>NeRFNetwork</p><ul><li><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Pasted%20image%2020221206180113.png" alt="Pasted image 20221206180113.png|600"></li><li>84—&gt;256—&gt;256—&gt;256—&gt;256—&gt;256+84—&gt;256—&gt;256—&gt;256+27—&gt;128—&gt;3</li><li>84—&gt;256—&gt;256—&gt;256—&gt;256—&gt;256+84—&gt;256—&gt;256—&gt;256—&gt;1</li></ul></li><li><p>AppShadingNetwork</p><ul><li>metallic_predictor<ul><li>259—&gt;256—&gt;256—&gt;256—&gt;1</li><li>259 = (256 + 3) = (feature + points)</li></ul></li><li>roughness_predictor<ul><li>259—&gt;256—&gt;256—&gt;256—&gt;1</li></ul></li><li>albedo_predictor<ul><li>259—&gt;256—&gt;256—&gt;256—&gt;3</li></ul></li><li>outer_light 直接光<ul><li>72—&gt;256—&gt;256—&gt;256—&gt;3</li><li>72 = ref_roughness = self.sph_enc(reflective, roughness)</li><li>or self.sph_enc(normals, roughness) —&gt; diffuse_lights</li></ul></li><li>inner_light 间接光<ul><li>123—&gt;256—&gt;256—&gt;256—&gt;3</li><li>123 = (51 + 72) = (pts + ref_roughness) <ul><li>pts : 2x3x8 + 3 = 51 (L=8)</li></ul></li></ul></li><li>inner_weight 遮挡概率occ_prob<ul><li>90—&gt;256—&gt;256—&gt;256—&gt;1</li><li>90 = (51 + 39) = (pts + ref_)<ul><li>ref_ = self.dir_enc(reflective)  = 2x3x6 + 3 = 39(L=6)</li></ul></li></ul></li><li>human_light_predictor  $[\alpha_{\mathrm{camera}},\mathrm{c}_{\mathrm{camera}}]=g_{\mathrm{camera}}(\mathrm{p}_{\mathrm{c}}),$<ul><li>24—&gt;256—&gt;256—&gt;256—&gt;4</li><li>24 = pos_enc = IPE(mean, var, 0, 6)</li></ul></li></ul></li></ul><div class="table-container"><table><thead><tr><th>MLP</th><th>Encoding</th><th>in_dims</th><th>out_dims</th><th>layer</th><th>neurons</th></tr></thead><tbody><tr><td>SDFNetwork</td><td>VanillaFrequency(L=6)</td><td>2x3x6+3=39</td><td>257</td><td>8</td><td>256</td></tr><tr><td>SingleVarianceNetwork</td><td>None</td><td>…</td><td>…</td><td>…</td><td>…</td></tr><tr><td>NeRFNetwork</td><td>VanillaFrequency(Lp=10,Lv=4)</td><td>2x4x10+4=84(Nerf++:4) &amp; 2x3x4+3=27</td><td>4</td><td>8</td><td>256</td></tr><tr><td>AppShadingNetwork</td><td>VanillaFrequency</td><td>…</td><td>…</td><td>…</td><td>…</td></tr><tr><td>metallic_predictor</td><td>None</td><td>256 + 3 = 259</td><td>1</td><td>4</td><td>256</td></tr><tr><td>roughness_predictor</td><td>None</td><td>256 + 3 = 259</td><td>1</td><td>4</td><td>256</td></tr><tr><td>albedo_predictor</td><td>None</td><td>256 + 3 = 259</td><td>3</td><td>4</td><td>256</td></tr><tr><td>outer_light</td><td>IDE(Ref-NeRF)</td><td>72</td><td>3</td><td>4</td><td>256</td></tr><tr><td>inner_light</td><td>VanillaFrequency+IDE</td><td>51 + 72 = 123</td><td>3</td><td>4</td><td>256</td></tr><tr><td>inner_weight</td><td>VanillaFrequency</td><td>51 + 39 = 90</td><td>1</td><td>4</td><td>256</td></tr><tr><td>human_light_predictor</td><td>IPE</td><td>2 x 2 x 6 = 24</td><td>4</td><td>4</td><td>256</td></tr></tbody></table></div><ul><li>FG_LUT: <code>[1,256,256,2]</code><ul><li>from assets/bsdf_256_256.bin</li></ul></li></ul><p><strong>weight_norm:</strong> weight_v, weight_g<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230803155215.png" alt="image.png"></p><h3 id="Render"><a href="#Render" class="headerlink" title="Render"></a>Render</h3><ul><li><code>_init_dataset</code><ul><li>parse_database_name：<code>self.database = GlossyRealDatabase(self.cfg[&#39;database_name&#39;])</code></li><li>get_database_split：self.train_ids, self.test_ids is <code>img_ids[1:] , img_ids[:1]</code></li><li>build_imgs_info(self.database, self.train_ids)：根据 train_ids 加载 train_imgs_info<ul><li>train_imgs_info：{‘imgs’: images, ‘Ks’: Ks, ‘poses’: poses}</li></ul></li><li>imgs_info_to_torch(self.train_imgs_info, device = ‘cpu’)：将train_imgs_info转换为torch、如果为imgs，则permute(0,3,1,2)，然后to device</li><li>同上加载test_imgs_info并to torch and to device</li><li><code>_construct_ray_batch(self.train_imgs_info)</code> : 返回<code>train_batch = ray_batch</code>, <code>self.train_poses = poses# imn,3,4</code>, <code>tbn = rn = imn * h * w</code>, h, w<ul><li><code>&#123;&#39;dirs&#39;: dirs.float().reshape(rn, 3).to(device), &#39;rgbs&#39;: imgs.float().reshape(rn, 3).to(device),&#39;idxs&#39;: idxs.long().reshape(rn, 1).to(device)&#125;</code></li><li>pose的<code>[:3,:3]</code>为正交矩阵</li></ul></li><li><code>_shuffle_train_batch</code>将train_batch数据打乱，每个像素点dir和rgb</li></ul></li><li>forward<ul><li>is_train:<ul><li><code>outputs = self.train_step(step)</code></li></ul></li><li>else:<ul><li>index = data[‘index’]</li><li>outputs = self.test_step(index, step=step)</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">self, step</span>):</span><br><span class="line">    rn = self.cfg[<span class="string">&#x27;train_ray_num&#x27;</span>]</span><br><span class="line">    <span class="comment"># fetch to gpu</span></span><br><span class="line">    train_ray_batch = &#123;k: v[self.train_batch_i:self.train_batch_i + rn].cuda() <span class="keyword">for</span> k, v <span class="keyword">in</span> self.train_batch.items()&#125;</span><br><span class="line">    self.train_batch_i += rn</span><br><span class="line">    <span class="keyword">if</span> self.train_batch_i + rn &gt;= self.tbn: self._shuffle_train_batch() <span class="comment"># 当完成一个tbn = img_nums * h * w时，打乱一次顺序</span></span><br><span class="line">    train_poses = self.train_poses.cuda()</span><br><span class="line">    rays_o, rays_d, near, far, human_poses = self._process_ray_batch(train_ray_batch, train_poses)</span><br><span class="line"></span><br><span class="line">    outputs = self.render(rays_o, rays_d, near, far, human_poses, -<span class="number">1</span>, self.get_anneal_val(step), is_train=<span class="literal">True</span>, step=step)</span><br><span class="line">    outputs[<span class="string">&#x27;loss_rgb&#x27;</span>] = self.compute_rgb_loss(outputs[<span class="string">&#x27;ray_rgb&#x27;</span>], train_ray_batch[<span class="string">&#x27;rgbs&#x27;</span>])  <span class="comment"># ray_loss</span></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h4 id="train-step"><a href="#train-step" class="headerlink" title="train_step"></a>train_step</h4><ul><li><code>_process_ray_batch</code><ul><li>input：ray_batch, poses</li><li>output：rays_o, rays_d, near, far, human_poses[idxs]</li><li>将原世界坐标系下o点转换到手动设置的世界坐标系w’下，并将ray_batch中的dirs转换到手动设置的世界坐标系w’下</li><li>near_far_from_sphere:<ul><li>get near and far through rays_o, rays_d</li></ul></li><li>get_human_coordinate_poses<ul><li>根据pose得到human_poses（w2c）：用于判断从相机发出的光线在物体上反射是否击中human</li><li>human_poses: 在相机原点处（相机原点不动），z轴为原相机坐标系z轴在<strong>与w’的xoy平面平行的xoy’平面</strong>的投影单位向量，y轴为w’下z-方向的单位向量</li><li><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728143216.png" alt="image.png"></li></ul></li></ul></li><li>get_anneal_val(step)<ul><li>if self.cfg[‘anneal_end’] &lt; 0 : <code>1</code></li><li>else : <code>np.min([1.0, step / self.cfg[&#39;anneal_end&#39;]])</code></li></ul></li><li>render(rays_o, rays_d, near, far, human_poses, perturb_overwrite=-1, cos_anneal_ratio=0.0, is_train=True, step=None)<ul><li>input: rays_o, rays_d, near, far, human_poses, -1, self.get_anneal_val(step), is_train=True, step=step</li><li>output: ret = outputs</li><li>sample_ray(rays_o, rays_d, near, far, perturb)<ul><li>同neus，上采样+cat_z_vals堆叠z_vals</li></ul></li><li>render_core<ul><li>input: rays_o, rays_d, z_vals, human_poses, cos_anneal_ratio=cos_anneal_ratio, step=step, is_train=is_train</li><li>output: outputs</li><li>get dists , mid_z_vals , points , inner_mask , outer_mask ,dirs , human_poses_pt</li><li>if torch.sum(outer_mask) &gt; 0: 背景使用NeRF网络得到颜色和不透明度<ul><li><code>alpha[outer_mask], sampled_color[outer_mask] = self.compute_density_alpha(points[outer_mask], dists[outer_mask], -dirs[outer_mask], self.outer_nerf)</code></li></ul></li><li>if torch.sum(inner_mask) &gt; 0: 前景使用SDF和Color网络得到sdf和颜色、碰撞信息 ， else：<code>gradient_error = torch.zeros(1)</code><ul><li><code>alpha[inner_mask], gradients, feature_vector, inv_s, sdf = self.compute_sdf_alpha(points[inner_mask], dists[inner_mask], dirs[inner_mask], cos_anneal_ratio, step)</code></li><li><code>sampled_color[inner_mask], occ_info = self.color_network(points[inner_mask], gradients, -dirs[inner_mask], feature_vector, human_poses_pt[inner_mask], step=step)</code><ul><li>sampled_color : 采样点颜色</li><li>occ_info : dict {‘reflective’: reflective, ‘occ_prob’: occ_prob,}</li></ul></li><li><code>gradient_error = (torch.linalg.norm(gradients, ord=2, dim=-1) - 1.0) ** 2</code> 梯度损失</li></ul></li><li>alpha — &gt; weight </li><li>sampled_color, weight —&gt; color</li><li>outputs = {‘ray_rgb’: color,  ‘gradient_error’: gradient_error,}</li><li><code>if torch.sum(inner_mask) &gt; 0: outputs[&#39;std&#39;] = torch.mean(1 / inv_s)</code>  | <code>else:  outputs[&#39;std&#39;] = torch.zeros(1)</code></li><li>if step &lt; 1000: <ul><li>mask = torch.norm(points, dim=-1) &lt; 1.2</li><li><code>outputs[&#39;sdf_pts&#39;] = points[mask]</code></li><li><code>outputs[&#39;sdf_vals&#39;] = self.sdf_network.sdf(points[mask])[..., 0]</code></li></ul></li><li><code>if self.cfg[&#39;apply_occ_loss&#39;]:</code><ul><li>if torch.sum(inner_mask) &gt; 0: <code>outputs[&#39;loss_occ&#39;] = self.compute_occ_loss(occ_info, points[inner_mask], sdf, gradients, dirs[inner_mask], step)</code><ul><li>compute_occ_loss 碰撞损失</li></ul></li><li>else: <code>outputs[&#39;loss_occ&#39;] = torch.zeros(1)</code></li></ul></li><li>if not is_train: <code>outputs.update(self.compute_validation_info(z_vals, rays_o, rays_d, weights, human_poses, step))</code></li><li>return outputs</li></ul></li></ul></li><li><strong>rgb_loss</strong>: <code>outputs[&#39;loss_rgb&#39;] = self.compute_rgb_loss(outputs[&#39;ray_rgb&#39;], train_ray_batch[&#39;rgbs&#39;])</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cfg[<span class="string">&#x27;rgb_loss&#x27;</span>] = <span class="string">&#x27;charbonier&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_rgb_loss</span>(<span class="params">self, rgb_pr, rgb_gt</span>):</span><br><span class="line">    <span class="keyword">if</span> self.cfg[<span class="string">&#x27;rgb_loss&#x27;</span>] == <span class="string">&#x27;l2&#x27;</span>:</span><br><span class="line">        rgb_loss = torch.<span class="built_in">sum</span>((rgb_pr - rgb_gt) ** <span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">elif</span> self.cfg[<span class="string">&#x27;rgb_loss&#x27;</span>] == <span class="string">&#x27;l1&#x27;</span>:</span><br><span class="line">        rgb_loss = torch.<span class="built_in">sum</span>(F.l1_loss(rgb_pr, rgb_gt, reduction=<span class="string">&#x27;none&#x27;</span>), -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">elif</span> self.cfg[<span class="string">&#x27;rgb_loss&#x27;</span>] == <span class="string">&#x27;smooth_l1&#x27;</span>:</span><br><span class="line">        rgb_loss = torch.<span class="built_in">sum</span>(F.smooth_l1_loss(rgb_pr, rgb_gt, reduction=<span class="string">&#x27;none&#x27;</span>, beta=<span class="number">0.25</span>), -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">elif</span> self.cfg[<span class="string">&#x27;rgb_loss&#x27;</span>] == <span class="string">&#x27;charbonier&#x27;</span>:</span><br><span class="line">        epsilon = <span class="number">0.001</span></span><br><span class="line">        rgb_loss = torch.sqrt(torch.<span class="built_in">sum</span>((rgb_gt - rgb_pr) ** <span class="number">2</span>, dim=-<span class="number">1</span>) + epsilon)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    <span class="keyword">return</span> rgb_loss</span><br></pre></td></tr></table></figure><p><strong>render core</strong> : MLP output<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/render_core.png" alt="render_core.png"></p><p><strong>Occ related</strong>: Occ prob and loss_occ</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/occ.png" alt="occ.png"></p><h5 id="loss-occ"><a href="#loss-occ" class="headerlink" title="loss_occ"></a>loss_occ</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inter_dist, inter_prob, inter_sdf = get_intersection(self.sdf_inter_fun, self.deviation_network, points[mask], reflective[mask], sn0=<span class="number">64</span>, sn1=<span class="number">16</span>)  <span class="comment"># pn,sn-1</span></span><br><span class="line">occ_prob_gt = torch.<span class="built_in">sum</span>(inter_prob, -<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">return</span> F.l1_loss(occ_prob[mask], occ_prob_gt)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_intersection</span>(<span class="params">sdf_fun, inv_fun, pts, dirs, sn0=<span class="number">128</span>, sn1=<span class="number">9</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param sdf_fun:</span></span><br><span class="line"><span class="string">    :param inv_fun:</span></span><br><span class="line"><span class="string">    :param pts:    pn,3</span></span><br><span class="line"><span class="string">    :param dirs:   pn,3</span></span><br><span class="line"><span class="string">    :param sn0: # 64</span></span><br><span class="line"><span class="string">    :param sn1: # 16</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    inside_mask = torch.norm(pts, dim=-<span class="number">1</span>) &lt; <span class="number">0.999</span> <span class="comment"># left some margin</span></span><br><span class="line">    pn, _ = pts.shape</span><br><span class="line">    hit_z_vals = torch.zeros([pn, sn1-<span class="number">1</span>])</span><br><span class="line">    hit_weights = torch.zeros([pn, sn1-<span class="number">1</span>])</span><br><span class="line">    hit_sdf = -torch.ones([pn, sn1-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> torch.<span class="built_in">sum</span>(inside_mask)&gt;<span class="number">0</span>:</span><br><span class="line">        pts = pts[inside_mask]</span><br><span class="line">        dirs = dirs[inside_mask]</span><br><span class="line">        max_dist = get_sphere_intersection(pts, dirs) <span class="comment"># pn,1</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            z_vals = torch.linspace(<span class="number">0</span>, <span class="number">1</span>, sn0) <span class="comment"># sn0</span></span><br><span class="line">            z_vals = max_dist * z_vals.unsqueeze(<span class="number">0</span>) <span class="comment"># pn,sn0</span></span><br><span class="line">            weights, mid_sdf = get_weights(sdf_fun, inv_fun, z_vals, pts, dirs) <span class="comment"># pn,sn0-1</span></span><br><span class="line">            z_vals_new = sample_pdf(z_vals, weights, sn1, <span class="literal">True</span>) <span class="comment"># pn,sn1</span></span><br><span class="line">            weights, mid_sdf = get_weights(sdf_fun, inv_fun, z_vals_new, pts, dirs) <span class="comment"># pn,sn1-1</span></span><br><span class="line">            z_vals_mid = (z_vals_new[:,<span class="number">1</span>:] + z_vals_new[:,:-<span class="number">1</span>]) * <span class="number">0.5</span></span><br><span class="line">        hit_z_vals[inside_mask] = z_vals_mid</span><br><span class="line">        hit_weights[inside_mask] = weights</span><br><span class="line">        hit_sdf[inside_mask] = mid_sdf</span><br><span class="line">    <span class="keyword">return</span> hit_z_vals, hit_weights, hit_sdf</span><br></pre></td></tr></table></figure><p>反射光线与单位球交点，到pts的距离dist</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_sphere_intersection</span>(<span class="params">pts, dirs</span>):</span><br><span class="line">    dtx = torch.<span class="built_in">sum</span>(pts*dirs,dim=-<span class="number">1</span>,keepdim=<span class="literal">True</span>) <span class="comment"># rn,1</span></span><br><span class="line">    xtx = torch.<span class="built_in">sum</span>(pts**<span class="number">2</span>,dim=-<span class="number">1</span>,keepdim=<span class="literal">True</span>) <span class="comment"># rn,1</span></span><br><span class="line">    dist = dtx ** <span class="number">2</span> - xtx + <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> torch.<span class="built_in">sum</span>(dist&lt;<span class="number">0</span>)==<span class="number">0</span></span><br><span class="line">    dist = -dtx + torch.sqrt(dist+<span class="number">1e-6</span>) <span class="comment"># rn,1</span></span><br><span class="line">    <span class="keyword">return</span> dist</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230805163513.png" alt="image.png"></p><p>从pts采样点，长度dist，方向为reflective，均匀采样sn个点</p><ul><li>获取sn个采样点的sn-1个中点的坐标、sdf和invs</li><li>选取其中在物体表面上的点：<code>surface_mask = (cos_val &lt; 0)</code></li><li>计算这些点的sdf和权重</li><li>表面采样点的权重之和即为occ_prob_gt</li><li>occ_prob_gt与occ_prob进行求L1损失即为loss_occ</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_weights</span>(<span class="params">sdf_fun, inv_fun, z_vals, origins, dirs</span>):</span><br><span class="line">    points = z_vals.unsqueeze(-<span class="number">1</span>) * dirs.unsqueeze(-<span class="number">2</span>) + origins.unsqueeze(-<span class="number">2</span>) <span class="comment"># pn,sn,3</span></span><br><span class="line">    inv_s = inv_fun(points[:, :-<span class="number">1</span>, :])[..., <span class="number">0</span>]  <span class="comment"># pn,sn-1</span></span><br><span class="line">    sdf = sdf_fun(points)[..., <span class="number">0</span>]  <span class="comment"># pn,sn</span></span><br><span class="line"></span><br><span class="line">    prev_sdf, next_sdf = sdf[:, :-<span class="number">1</span>], sdf[:, <span class="number">1</span>:]  <span class="comment"># pn,sn-1</span></span><br><span class="line">    prev_z_vals, next_z_vals = z_vals[:, :-<span class="number">1</span>], z_vals[:, <span class="number">1</span>:]</span><br><span class="line">    mid_sdf = (prev_sdf + next_sdf) * <span class="number">0.5</span></span><br><span class="line">    cos_val = (next_sdf - prev_sdf) / (next_z_vals - prev_z_vals + <span class="number">1e-5</span>)  <span class="comment"># pn,sn-1</span></span><br><span class="line">    surface_mask = (cos_val &lt; <span class="number">0</span>)  <span class="comment"># pn,sn-1</span></span><br><span class="line">    cos_val = torch.clamp(cos_val, <span class="built_in">max</span>=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    dist = next_z_vals - prev_z_vals  <span class="comment"># pn,sn-1</span></span><br><span class="line">    prev_esti_sdf = mid_sdf - cos_val * dist * <span class="number">0.5</span>  <span class="comment"># pn, sn-1</span></span><br><span class="line">    next_esti_sdf = mid_sdf + cos_val * dist * <span class="number">0.5</span></span><br><span class="line">    prev_cdf = torch.sigmoid(prev_esti_sdf * inv_s)</span><br><span class="line">    next_cdf = torch.sigmoid(next_esti_sdf * inv_s)</span><br><span class="line">    alpha = (prev_cdf - next_cdf + <span class="number">1e-5</span>) / (prev_cdf + <span class="number">1e-5</span>) * surface_mask.<span class="built_in">float</span>()</span><br><span class="line">    weights = alpha * torch.cumprod(torch.cat([torch.ones([alpha.shape[<span class="number">0</span>], <span class="number">1</span>]), <span class="number">1.</span> - alpha + <span class="number">1e-7</span>], -<span class="number">1</span>), -<span class="number">1</span>)[:, :-<span class="number">1</span>]</span><br><span class="line">    mid_sdf[~surface_mask]=-<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> weights, mid_sdf</span><br></pre></td></tr></table></figure><h2 id="NeROMaterialRenderer"><a href="#NeROMaterialRenderer" class="headerlink" title="NeROMaterialRenderer"></a>NeROMaterialRenderer</h2><h3 id="MLP-1"><a href="#MLP-1" class="headerlink" title="MLP"></a>MLP</h3><p>shader_network = MCShadingNetwork</p><ul><li>feats_network = MaterialFeatsNetwork<ul><li>51—&gt;256—&gt;256—&gt;256—&gt;256+51—&gt;256—&gt;256—&gt;256—&gt;256</li><li>51: pts = get_embedder(8,3)(x) = 8 x 2 x 3 + 3 = 51</li></ul></li><li>metallic_predictor<ul><li>259—&gt;256—&gt;256—&gt;256—&gt;1</li><li>259 = 256(feature) + 3(pts)</li></ul></li><li>roughness_predictor<ul><li>259—&gt;256—&gt;256—&gt;256—&gt;1</li></ul></li><li>albedo_predictor<ul><li>259—&gt;256—&gt;256—&gt;256—&gt;3</li></ul></li><li>outer_light<ul><li>144—&gt;256—&gt;256—&gt;256—&gt;3</li><li>144 = 72 x 2 = <code>torch.cat([outer_enc, sphere_pts], -1)</code><ul><li>outer_enc = self.sph_enc(directions, 0) = 72</li><li>sphere_pts = self.sph_enc(sphere_pts, 0) = 72</li></ul></li></ul></li><li>human_light<ul><li>24—&gt;256—&gt;256—&gt;256—&gt;4</li><li>24: <code>pos_enc = IPE(mean, var, 0, 6)  # 2*2*6</code></li></ul></li><li>inner_light<ul><li>123(51 + 72)—&gt;256—&gt;256—&gt;256—&gt;3</li><li>123 = <code>torch.cat([pos_enc, dir_enc], -1)</code><ul><li>pos_enc = self.pos_enc(points)  = 51<ul><li>self.pos_enc = get_embedder(8, 3)</li></ul></li><li>dir_enc = self.sph_enc(reflections, 0) = 72</li></ul></li></ul></li></ul><div class="table-container"><table><thead><tr><th>MLP</th><th>Encoding</th><th>in_dims</th><th>out_dims</th><th>layer</th><th>neurons</th></tr></thead><tbody><tr><td>light_pts</td><td>single parameter</td><td>…</td><td>…</td><td>…</td><td>…</td></tr><tr><td>MCShadingNetwork</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr><tr><td>feats_network</td><td>VanillaFrequency</td><td>8 x 2 x 3 + 3 = 51</td><td>256</td><td>8</td><td>256</td></tr><tr><td>metallic_predictor</td><td>None</td><td>256 + 3 = 259</td><td>1</td><td>4</td><td>256</td></tr><tr><td>roughness_predictor</td><td>None</td><td>256 + 3 = 259</td><td>1</td><td>4</td><td>256</td></tr><tr><td>albedo_predictor</td><td>None</td><td>256 + 3 = 259</td><td>3</td><td>4</td><td>256</td></tr><tr><td>outer_light</td><td>IDE</td><td>72 + 72 = 144</td><td>3</td><td>4</td><td>256</td></tr><tr><td>human_light</td><td>IPE</td><td>2 x 2 x 6 = 24</td><td>4</td><td>4</td><td>256</td></tr><tr><td>inner_light</td><td>VanillaFrequency+IDE</td><td>51 + 72 = 123</td><td>3</td><td>4</td><td>256</td></tr></tbody></table></div><h3 id="Render-1"><a href="#Render-1" class="headerlink" title="Render"></a>Render</h3><ul><li><code>_init_geometry</code><ul><li>self.mesh = open3d.io.read_triangle_mesh(self.cfg[‘mesh’]) 读取 Stage1得到的mesh<ul><li><a href="http://www.open3d.org/docs/0.16.0/python_api/open3d.io.read_triangle_mesh.html#open3d.io.read_triangle_mesh">open3d.io.read_triangle_mesh — Open3D 0.16.0 documentation</a></li></ul></li><li>self.ray_tracer = raytracing.RayTracer(np.asarray(self.mesh.vertices), np.asarray(self.mesh.triangles)) 获得raytracer,用于根据rays_o和rays_d得到intersections, face_normals, depth<ul><li><a href="https://github.com/ashawkey/raytracing">ashawkey/raytracing: A CUDA Mesh RayTracer with BVH acceleration, with python bindings and a GUI. (github.com)</a></li></ul></li></ul></li><li><code>_init_dataset</code><ul><li>parse_database_name 返回<code>self.database = GlossyRealDatabase(self.cfg[&#39;database_name&#39;])</code></li><li>get_database_split :  <code>self.train_ids, self.test_ids =img_ids[1:], img_ids[:1]</code></li><li>if is_train:<ul><li>build_imgs_info : train and test <ul><li>return {‘imgs’: images, ‘Ks’: Ks, ‘poses’: poses}</li></ul></li><li>imgs_info_to_torch : train and test</li><li><code>_construct_ray_batch(train_imgs_info)</code><ul><li>train_imgs_info to ray_batch</li></ul></li><li>tbn = imn</li><li><code>_shuffle_train_batch</code></li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_init_dataset</span>(<span class="params">self, is_train</span>):</span><br><span class="line">    <span class="comment"># train/test split</span></span><br><span class="line">    self.database = parse_database_name(self.cfg[<span class="string">&#x27;database_name&#x27;</span>])</span><br><span class="line">    self.train_ids, self.test_ids = get_database_split(self.database, <span class="string">&#x27;validation&#x27;</span>)</span><br><span class="line">    self.train_ids = np.asarray(self.train_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_train:</span><br><span class="line">        self.train_imgs_info = build_imgs_info(self.database, self.train_ids)</span><br><span class="line">        self.train_imgs_info = imgs_info_to_torch(self.train_imgs_info, <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">        self.train_num = <span class="built_in">len</span>(self.train_ids)</span><br><span class="line"></span><br><span class="line">        self.test_imgs_info = build_imgs_info(self.database, self.test_ids)</span><br><span class="line">        self.test_imgs_info = imgs_info_to_torch(self.test_imgs_info, <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">        self.test_num = <span class="built_in">len</span>(self.test_ids)</span><br><span class="line"></span><br><span class="line">        self.train_batch = self._construct_ray_batch(self.train_imgs_info)</span><br><span class="line">        self.tbn = self.train_batch[<span class="string">&#x27;rays_o&#x27;</span>].shape[<span class="number">0</span>]</span><br><span class="line">        self._shuffle_train_batch()</span><br><span class="line"></span><br><span class="line">self.train_batch <span class="keyword">from</span> _construct_ray_batch<span class="string">&#x27;s return ray_batch: </span></span><br><span class="line"><span class="string">if is_train:</span></span><br><span class="line"><span class="string">    ray_batch=&#123;</span></span><br><span class="line"><span class="string">        &#x27;</span>rays_o<span class="string">&#x27;: rays_o[hit_mask].to(device),</span></span><br><span class="line"><span class="string">        &#x27;</span>rays_d<span class="string">&#x27;: rays_d[hit_mask].to(device),</span></span><br><span class="line"><span class="string">        &#x27;</span>inters<span class="string">&#x27;: inters[hit_mask].to(device),</span></span><br><span class="line"><span class="string">        &#x27;</span>normals<span class="string">&#x27;: normals[hit_mask].to(device),</span></span><br><span class="line"><span class="string">        &#x27;</span>depth<span class="string">&#x27;: depth[hit_mask].to(device),</span></span><br><span class="line"><span class="string">        &#x27;</span>human_poses<span class="string">&#x27;: human_poses[hit_mask].to(device),</span></span><br><span class="line"><span class="string">        &#x27;</span>rg<span class="string">b&#x27;: rgb[hit_mask].to(device),</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">else:</span></span><br><span class="line"><span class="string">    assert imn==1</span></span><br><span class="line"><span class="string">    ray_batch=&#123;</span></span><br><span class="line"><span class="string">        &#x27;</span>rays_o<span class="string">&#x27;: rays_o[0].to(device),</span></span><br><span class="line"><span class="string">        &#x27;</span>rays_d<span class="string">&#x27;: rays_d[0].to(device),</span></span><br><span class="line"><span class="string">        &#x27;</span>inters<span class="string">&#x27;: inters[0].to(device),</span></span><br><span class="line"><span class="string">        &#x27;</span>normals<span class="string">&#x27;: normals[0].to(device),</span></span><br><span class="line"><span class="string">        &#x27;</span>depth<span class="string">&#x27;: depth[0].to(device),</span></span><br><span class="line"><span class="string">        &#x27;</span>human_poses<span class="string">&#x27;: human_poses[0].to(device),</span></span><br><span class="line"><span class="string">        &#x27;</span>rg<span class="string">b&#x27;: rgb[0].to(device),</span></span><br><span class="line"><span class="string">        &#x27;</span>hit_mask<span class="string">&#x27;: hit_mask[0].to(device),</span></span><br><span class="line"><span class="string">    &#125;</span></span><br></pre></td></tr></table></figure><ul><li><code>_init_shader</code><ul><li><code>self.cfg[&#39;shader_cfg&#39;][&#39;is_real&#39;] = self.cfg[&#39;database_name&#39;].startswith(&#39;real&#39;)</code></li><li><code>self.shader_network = MCShadingNetwork(self.cfg[&#39;shader_cfg&#39;], lambda o,d: self.trace(o,d))</code> — MLP Network</li></ul></li><li>forward<ul><li>if is_train: self.train_step(step)</li></ul></li></ul><h1 id="Relight"><a href="#Relight" class="headerlink" title="Relight"></a>Relight</h1><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">python relight.py --blender &lt;<span class="built_in">path</span>-to-your-blender&gt; \</span><br><span class="line">                  --name bear-neon \</span><br><span class="line">                  --mesh data/meshes/bear_shape-<span class="number">300000</span>.ply \</span><br><span class="line">                  --material data/materials/bear_material-<span class="number">100000</span> \</span><br><span class="line">                  --hdr data/hdr/neon_photostudio_4k.exr</span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">eg: </span></span><br><span class="line"><span class="function"><span class="title">python</span> <span class="title">relight.py</span> --<span class="title">blender</span> <span class="title">F</span>:\<span class="title">Blender</span>\<span class="title">blender.exe</span> --<span class="title">name</span> <span class="title">bear</span>-<span class="title">neon</span> --<span class="title">mesh</span> <span class="title">data</span>/<span class="title">meshes</span>/<span class="title">bear_shape</span>-300000.<span class="title">ply</span> --<span class="title">material</span> <span class="title">data</span>/<span class="title">materials</span>/<span class="title">bear_material</span>-100000 --<span class="title">hdr</span> <span class="title">data</span>/<span class="title">hdr</span>/<span class="title">neon_photostudio_4k.exr</span></span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--blender&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--mesh&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--material&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--hdr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--name&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--trans&#x27;</span>, dest=<span class="string">&#x27;trans&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    cmds=[</span><br><span class="line">        args.blender, <span class="string">&#x27;--background&#x27;</span>, <span class="string">&#x27;--python&#x27;</span>, <span class="string">&#x27;blender_backend/relight_backend.py&#x27;</span>, <span class="string">&#x27;--&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;--output&#x27;</span>, <span class="string">f&#x27;data/relight/<span class="subst">&#123;args.name&#125;</span>&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;--mesh&#x27;</span>, args.mesh,</span><br><span class="line">        <span class="string">&#x27;--material&#x27;</span>, args.material,</span><br><span class="line">        <span class="string">&#x27;--env_fn&#x27;</span>, args.hdr,</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">if</span> args.trans:</span><br><span class="line">        cmds.append(<span class="string">&#x27;--trans&#x27;</span>)</span><br><span class="line">    subprocess.run(cmds)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>运行python relight.py后，子进程在cmd中运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">F:\Blender\blender.exe --background # 无UI界面渲染，在后台运行</span><br><span class="line">--python blender_backend/relight_backend.py # 运行给定的 Python 脚本文件</span><br><span class="line">-- # 结束option processing，后续参数保持不变。通过 Python 的 sys.argv 访问，下面的参数用于python脚本程序</span><br><span class="line">--output data/relight/bear-neon</span><br><span class="line">--mesh data/meshes/bear_shape-300000.ply</span><br><span class="line">--material data/materials/bear_material-100000</span><br><span class="line">--env_fn  data/hdr/neon_photostudio_4k.exr</span><br><span class="line"></span><br><span class="line">即先后台运行blender，在blender中运行python脚本：</span><br><span class="line">python relight_backend.py --output data/relight/bear-neon</span><br><span class="line">    --mesh data/meshes/bear_shape-300000.ply</span><br><span class="line">    --material data/materials/bear_material-100000</span><br><span class="line">    --env_fn  data/hdr/neon_photostudio_4k.exr</span><br></pre></td></tr></table></figure><h2 id="blender中运行的python脚本"><a href="#blender中运行的python脚本" class="headerlink" title="blender中运行的python脚本"></a>blender中运行的python脚本</h2><p><strong>blender_backend/relight_backend.py</strong>: </p><p>import bpy</p><p><strong>blender_backend/blender_utils.py</strong>: </p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Shadow&amp;Highlight </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Neus </tag>
            
            <tag> Code </tag>
            
            <tag> Shadow&amp;Highlight </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sampling</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF/Sampling/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/NeRF/Sampling/</url>
      
        <content type="html"><![CDATA[<p>从相机原点出发，通过像素点射出一条光线，在光线上进行采样</p><span id="more"></span><h1 id="直线光线采样"><a href="#直线光线采样" class="headerlink" title="直线光线采样"></a>直线光线采样</h1><p>将像素看成一个点，射出的光线是一条直线</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Network.png" alt="Network.png|666"><br>(NerfAcc)可大致分为：</p><ul><li>平均采样(粗采样)</li><li>空间跳跃采样(NGP中对空气跳过采样)</li><li>逆变换采样(根据粗采样得到的w分布进行精采样)</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230711125031.png" alt="image.png"></p><h2 id="平均采样"><a href="#平均采样" class="headerlink" title="平均采样"></a>平均采样</h2><p>i.e. 粗采样，在光线上平均采样n个点</p><h2 id="占据采样"><a href="#占据采样" class="headerlink" title="占据采样"></a>占据采样</h2><p>Occupancy Grids<br>通过在某分辨率占用网格中进行更新占用网格的权重，来确定哪些网格需要采样</p><h2 id="逆变换采样"><a href="#逆变换采样" class="headerlink" title="逆变换采样"></a>逆变换采样</h2><h3 id="NeRF"><a href="#NeRF" class="headerlink" title="NeRF"></a>NeRF</h3><p>简单的逆变换采样方法：根据粗采样得到的权重进行逆变换采样，获取精采样点<br><a href="https://zhuanlan.zhihu.com/p/80726483">逆变换采样 - 知乎 (zhihu.com)</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sample_pdf</span>(<span class="params">bins, weights, N_samples, det=<span class="literal">False</span>, pytest=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># Get pdf</span></span><br><span class="line">    weights = weights + <span class="number">1e-5</span> <span class="comment"># prevent nans # weights : chunk * 62</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 归一化weights</span></span><br><span class="line">    pdf = weights / torch.<span class="built_in">sum</span>(weights, -<span class="number">1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># pdf : chunk * 62</span></span><br><span class="line">    cdf = torch.cumsum(pdf, -<span class="number">1</span>) <span class="comment"># cdf : chunk * 62</span></span><br><span class="line">    cdf = torch.cat([torch.zeros_like(cdf[...,:<span class="number">1</span>]), cdf], -<span class="number">1</span>)  <span class="comment"># (batch, len(bins))  = (chunk, 63)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take uniform samples</span></span><br><span class="line">    <span class="keyword">if</span> det:</span><br><span class="line">        u = torch.linspace(<span class="number">0.</span>, <span class="number">1.</span>, steps=N_samples)</span><br><span class="line">        u = u.expand(<span class="built_in">list</span>(cdf.shape[:-<span class="number">1</span>]) + [N_samples]) <span class="comment"># u : chunk * N_samples</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        u = torch.rand(<span class="built_in">list</span>(cdf.shape[:-<span class="number">1</span>]) + [N_samples])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pytest, overwrite u with numpy&#x27;s fixed random numbers</span></span><br><span class="line">    <span class="keyword">if</span> pytest:</span><br><span class="line">        np.random.seed(<span class="number">0</span>)</span><br><span class="line">        new_shape = <span class="built_in">list</span>(cdf.shape[:-<span class="number">1</span>]) + [N_samples] <span class="comment"># new_shape : chunk * N_samples</span></span><br><span class="line">        <span class="keyword">if</span> det:</span><br><span class="line">            u = np.linspace(<span class="number">0.</span>, <span class="number">1.</span>, N_samples)</span><br><span class="line">            u = np.broadcast_to(u, new_shape) <span class="comment"># u : chunk * N_samples</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            u = np.random.rand(*new_shape)</span><br><span class="line">        u = torch.Tensor(u)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Invert CDF</span></span><br><span class="line">    u = u.contiguous() <span class="comment"># 确保张量在内存中是连续存储的</span></span><br><span class="line">    <span class="comment"># inds : chunk * N_samples</span></span><br><span class="line">    inds = torch.searchsorted(cdf, u, right=<span class="literal">True</span>) <span class="comment"># 将u中的元素在cdf中进行二分查找，返回其索引</span></span><br><span class="line">    below = torch.<span class="built_in">max</span>(torch.zeros_like(inds-<span class="number">1</span>), inds-<span class="number">1</span>) <span class="comment"># below : chunk * N_samples</span></span><br><span class="line">    above = torch.<span class="built_in">min</span>((cdf.shape[-<span class="number">1</span>]-<span class="number">1</span>) * torch.ones_like(inds), inds) <span class="comment"># above : chunk * N_samples</span></span><br><span class="line">    inds_g = torch.stack([below, above], -<span class="number">1</span>)  <span class="comment"># (batch, N_samples, 2)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># cdf_g = tf.gather(cdf, inds_g, axis=-1, batch_dims=len(inds_g.shape)-2)</span></span><br><span class="line">    <span class="comment"># bins_g = tf.gather(bins, inds_g, axis=-1, batch_dims=len(inds_g.shape)-2)</span></span><br><span class="line">    matched_shape = [inds_g.shape[<span class="number">0</span>], inds_g.shape[<span class="number">1</span>], cdf.shape[-<span class="number">1</span>]]  <span class="comment"># (batch, N_samples, 63)</span></span><br><span class="line">    cdf_g = torch.gather(cdf.unsqueeze(<span class="number">1</span>).expand(matched_shape), <span class="number">2</span>, inds_g) </span><br><span class="line">    <span class="comment"># unsqueeze(1) : (batch, 1, 63)</span></span><br><span class="line">    <span class="comment"># expand : (batch, N_samples, 63)</span></span><br><span class="line">    <span class="comment"># cdf_g : (batch, N_samples, 2)    </span></span><br><span class="line">    bins_g = torch.gather(bins.unsqueeze(<span class="number">1</span>).expand(matched_shape), <span class="number">2</span>, inds_g)</span><br><span class="line"></span><br><span class="line">    denom = (cdf_g[...,<span class="number">1</span>]-cdf_g[...,<span class="number">0</span>]) <span class="comment"># denom : chunk * N_samples</span></span><br><span class="line">    <span class="comment"># 如果denom小于1e-5，就用1代替</span></span><br><span class="line">    denom = torch.where(denom&lt;<span class="number">1e-5</span>, torch.ones_like(denom), denom)</span><br><span class="line">    t = (u-cdf_g[...,<span class="number">0</span>])/denom</span><br><span class="line">    samples = bins_g[...,<span class="number">0</span>] + t * (bins_g[...,<span class="number">1</span>]-bins_g[...,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> samples <span class="comment"># samples : chunk * N_samples</span></span><br></pre></td></tr></table></figure><h3 id="Mip-NeRF360"><a href="#Mip-NeRF360" class="headerlink" title="Mip-NeRF360"></a>Mip-NeRF360</h3><p>构建了一个提议网格获取权重来进行精采样（下）</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230722152752.png" alt="image.png"></p><h1 id="锥形光线采样"><a href="#锥形光线采样" class="headerlink" title="锥形光线采样"></a>锥形光线采样</h1><h2 id="Mip-NeRF"><a href="#Mip-NeRF" class="headerlink" title="Mip-NeRF"></a>Mip-NeRF</h2><p>将像素看成有面积的圆盘，射出的光线为一个圆锥体</p><ul><li>使用多元高斯分布来近似截锥体</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230721125154.png" alt="image.png"></p><h2 id="Tri-MipRF"><a href="#Tri-MipRF" class="headerlink" title="Tri-MipRF"></a>Tri-MipRF</h2><ul><li>使用一个各项同性的球来近似截锥体</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230726164225.png" alt="image.png"></p><h2 id="Zip-NeRF"><a href="#Zip-NeRF" class="headerlink" title="Zip-NeRF"></a>Zip-NeRF</h2><p>Multisampling</p><p>多采样：在一个截锥体中沿着光线采样6个点，每个点之间旋转一个角度</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230729141223.png" alt="image.png"></p><h1 id="混合采样"><a href="#混合采样" class="headerlink" title="混合采样"></a>混合采样</h1><p>NerfAcc：占据+逆变换采样<br>先使用占据网格确定哪些区域需要采样，再通过粗采样得到的权重使用逆变换采样进行精采样得到采样点</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/NeRF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Sampling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zip-NeRF</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Efficiency/Zip-NeRF/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Efficiency/Zip-NeRF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://jonbarron.info/">Jonathan T. Barron</a><a href="http://bmild.github.io/">Ben Mildenhall</a><a href="http://dorverbin.github.io/">Dor Verbin</a><a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a><a href="https://phogzone.com/">Peter Hedman</a></td></tr><tr><td>Conf/Jour</td><td>ICCV</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://jonbarron.info/zipnerf/">Zip-NeRF (jonbarron.info)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4762187901551247361&amp;noteId=1892446988470107392">Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields (readpaper.com)</a></td></tr></tbody></table></div><p>Zip-NeRF在抗混叠(包括NeRF从空间坐标到颜色和密度的学习映射的空间混叠，以及沿每条射线在线蒸馏过程中使用的损失函数的z-混叠)方面都取得了很好的效果，并且速度相比前作Mip-NeRF 360 提高了24X</p><p>mipNeRF 360+基于网格的模型(如Instant NGP)的技术</p><ul><li>错误率下降8~77%，并且比Mip-NeRF360提速了24X</li><li>主要贡献：<ul><li>多采样</li><li>预滤波</li></ul></li></ul><p>多采样：train左，test右</p><div style="display:flex; justify-content:space-between;"> <img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/hexify_train.gif" alt="Image 1" style="width:30%;"><div style="width:10px;"></div> <img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/hexify_test.gif" alt="Image 2" style="width:30%;"> </div><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>We have presented Zip-NeRF, a model that integrates the progress made in the formerly divergent areas of <strong>scaleaware anti-aliased NeRFs and fast grid-based NeRF training</strong>. </p><p>By leveraging ideas about <strong>multisampling</strong> and <strong>prefiltering</strong>, our model is able to achieve error rates that are 8% – 77% lower than prior techniques, while also training 24×faster than mip-NeRF 360 (the previous state-of-the-art on our benchmarks). </p><p>We hope that the tools and analysis presented here concerning aliasing (both the spatial aliasing of NeRF’s learned mapping from spatial coordinate to color and density, and z-aliasing of the loss function used during online distillation along each ray) enable further progress towards improving the <strong>quality, speed, and sample-efficiency</strong> of NeRF-like inverse rendering techniques.</p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>神经辐射场训练可以通过在NeRF的学习映射中使用基于网格的表示来加速，从空间坐标到颜色和体积密度。然而，<strong>这些基于网格的方法</strong>缺乏对尺度的明确理解，因此<strong>经常引入混叠，通常以锯齿或缺失场景内容的形式出现</strong>。<br>mip-NeRF 360之前已经解决了抗锯齿问题，它可以沿锥体而不是沿射线的点来计算子体积，但这种方法<strong>与当前基于网格的技术不兼容</strong>。</p><p>我们展示了如何使用渲染和信号处理的想法来构建一种结合mipNeRF 360和基于网格的模型(如Instant NGP)的技术，其错误率比之前的技术低8% - 77%，并且训练速度比mip-NeRF 360快24倍。</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>NeRF：原始的NeRF模型使用多层感知器(MLP)来参数化从空间坐标到颜色和密度的映射。尽管mlp结构紧凑且富有表现力，但<strong>训练速度较慢</strong>。</li><li>最近的研究已经通过用类似体格voxel-grid-like的数据结构替换或增强mlp来加速训练[7,15,27,35]。一个例子是<strong>Instant-NGP</strong> (iNGP)，它使用粗网格和细网格的pyramid(其中最细的网格使用hash map存储)来构建由微小MLP处理的学习特征，从而大大加速了训练[21]。</li><li>除了速度慢之外，最初的NeRF模型也被混淆了: <strong>NeRF对光线中的单个点进行推理</strong>，这会导致渲染图像中的“锯齿”，并限制NeRF对比例的推理能力。</li><li><strong>Mip-NeRF</strong>[2]通过投射锥体而不是射线解决了这个问题，并将整个体积置于锥形锥内，用作MLP的输入。Mip-NeRF及其后续产品mipNeRF 360[3]表明，这种方法可以在具有挑战性的现实场景中实现高精度渲染</li><li>令人遗憾的是，在快速训练和抗混叠这两个问题上取得的进展，乍一看是互不相容的。这是因为mip-NeRF的抗锯齿策略主要依赖于使用位置编码[28,30]将锥形锥台特征化为离散特征向量，但<strong>当前基于网格的方法不使用位置编码</strong>，<strong>而是使用通过插值到单个3D坐标的网格层次中获得的学习特征</strong>。尽管在渲染中抗锯齿是一个研究得很好的问题[8,9,26,31]，但大多数方法并不能自然地推广到像iNGP这样基于网格的NeRF模型中的反渲染。</li><li>在这项工作中，我们利用多采样、统计和信号处理的想法，将iNGP的网格pyramid集成到mip-NeRF 360的框架中。我们称我们的模型为“<strong>Zip-NeRF</strong>”，因为它的速度，它与mipNeRF的相似性，以及它修复类似拉链的混叠工件的能力。在mip-NeRF 360基准测试[3]上，Zip-NeRF将错误率降低了19%，训练速度比以前的最先进技术快24倍。在该基准的多尺度变体中，Zip-NeRF更彻底地测量了混联和尺度，错误率降低了77%。</li></ul><h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><p>Mip-NeRF 360和Instant NGP (iNGP)都是类似nerf的:通过投射3D光线和沿着光线距离t的特征位置来渲染像素，并将这些特征馈送到神经网络，神经网络的输出是$\alpha-composited$合成来渲染颜色。训练包括重复投射与训练图像中像素对应的光线，并最小化(通过梯度下降)每个像素的渲染和观察到的颜色之间的差异。<br>Mip-NeRF 360和iNGP在如何参数化射线坐标方面有很大的不同。在mip-NeRF 360中，射线被细分为一组区间$[t_i, t_{i+1})$，每个区间代表一个圆锥形截锥体，其形状由多元高斯近似表示，<strong>与该高斯相关的期望位置编码</strong>被用作大型MLP的输入[2]。相比之下，iNGP三线性插值到不同大小的3D网格层次中，为小型MLP生成特征向量[21]。我们的模型结合了mip-NeRF 360的整体框架和iNGP的特征化方法，但将这两种方法天真地结合起来会引入两种形式的混联:</p><ul><li>InstantNGP的特征网格方法与mip- nerf360的感知尺度的集成位置编码技术不兼容，因此iNGP生成的特征相对于空间坐标是混叠aliased的，从而产生aliased的渲染图。在第2节中，我们通过引入一种用于计算预滤波的iNGP特征的类似多采样的解决方案来解决这个问题</li><li>使用iNGP极大地加速了训练，但这揭示了mip-NeRF 360的在线蒸馏方法的问题，导致高度可见的“z-混叠”(沿着光线混叠)，其中场景内容随着相机移动而不规律地消失。在第3节中，我们用一个新的损失函数来解决这个问题，该损失函数在计算用于监督在线蒸馏的损失函数时沿每条射线进行预过滤。</li></ul><h1 id="Spatial-Anti-Aliasing"><a href="#Spatial-Anti-Aliasing" class="headerlink" title="Spatial Anti-Aliasing"></a>Spatial Anti-Aliasing</h1><p>Mip-NeRF使用的特征近似于子体中坐标位置编码的积分，这是一个锥形截锥体。这<strong>导致当特征正弦波的周期大于高斯的标准差时</strong>，其傅立叶特征振幅较小-这些特征仅在大于子体尺寸的波长处表示子体的空间位置。由于该特征对位置和比例进行编码，因此使用它的MLP能够学习呈现抗混叠图像的3D场景的多尺度表示。像iNGP这样基于网格的表示<strong>本身不允许查询sub-volumes</strong>，而是在单点使用三线性插值来构建用于MLP的特征，<strong>这导致学习模型无法对规模或混叠进行推理</strong>。<br>我们通过将每个锥形截锥体转换为<strong>一组各向同性高斯体</strong>来解决这个问题，使用多重采样和特征降权的组合: <strong>各向异性子体首先转换为一组近似其形状的点，然后假设每个点是具有一定尺度的各向同性高斯体</strong>。这种各向同性假设使我们能够利用网格中的值为零均值的事实来近似特征网格在子体积上的真实积分。通过平均这些降权特征，我们从iNGP网格中获得了尺度感知的预滤波特征。如图2所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230729140301.png" alt="image.png"><br>这里我们展示了一个toy的一维iNGP[21]，每个尺度有一个特征。每个子图表示沿x轴在所有坐标上查询iNGP的不同策略——想象一个从左向右移动的高斯函数，其中每条线是每个坐标的iNGP特征，其中每种颜色是iNGP中的不同比例。(a)查询高斯均值的朴素解决方案导致具有分段线性扭曲的特征，其中超过高斯带宽的高频很大且不准确。(b)通过将iNGP特征与高斯函数卷积得到的真实解——在实践中是一个棘手的解——得到光滑但信息丰富的粗糙特征和接近0的精细特征。(c)我们可以通过基于高斯尺度的降权来抑制不可靠的高频(每个特征后面的颜色带表示降权)，但这会导致粗糙特征中不自然的尖锐不连续。(d)或者，超采样产生合理的粗尺度特征，但产生不稳定的细尺度特征。(e)因此，我们多样本各向同性亚高斯分布(如图5所示)，并使用每个亚高斯分布的尺度来降低频率的权重。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230729140618.png" alt="image.png"></p><p>抗锯齿在图形学文献中得到了很好的探讨。Mip mapping(Mip - nerf的同名)预先计算了一种能够快速抗锯齿的数据结构，但目前尚不清楚这种方法如何应用于iNGP的基于哈希的数据结构。超采样技术[8]采用蛮力方法抗混叠，使用大量样本;我们将证明，这种方法比我们的方法效率更低，成本更高。多采样技术[11]构建一个小样本集，然后将这些多样本中的信息汇集到一个聚合表示中，并提供给一个昂贵的渲染过程——这是一种类似于我们的方法的策略。</p><h2 id="Multisampling"><a href="#Multisampling" class="headerlink" title="Multisampling"></a>Multisampling</h2><p>根据mip-NeRF[2]，我们假设每个像素对应一个半径为$\dot r t$的锥，其中沿射线的距离为t。给定沿射线$[t_0, t_1]$的一个区间，我们想要构造一组近似于该锥形截锥体形状的多样本。我们使用一个六点六边形图案，其角$θ_j$为:$\theta=\left[\begin{matrix}0,2\pi/3,4\pi/3,3\pi/3,5\pi/3,\pi/3\end{matrix}\right],$<br>它们是线性间隔的角，围绕一个圆旋转一次，排列成一对位移60度的三角形。沿着射线$t_{j}$的距离是:</p><script type="math/tex; mode=display">\begin{aligned}t_{j}&=t_{0}+\frac{t_{\delta}\left(t_{1}^{2}+2t_{\mu}^{2}+\frac{3}{\sqrt{7}}\left(\frac{2j}{5}-1\right)\sqrt{\left(t_{\delta}^{2}-t_{\mu}^{2}\right)^{2}+4t_{\mu}^{4}}\right)}{t_{\delta}^{2}+3t_{\mu}^{2}}\\\mathrm{where~}t_{\mu}&=(t_{0}+t_{1})/2,t_{\delta}=(t_{1}-t_{0})/2\end{aligned}</script><p>它们是$[t_0, t_1)$中的线性间隔值，经过位移和缩放以将质量集中在离截体远端的附近。我们的多样本坐标相对于射线是:</p><script type="math/tex; mode=display">\left\{\begin{bmatrix}\dot{r}t_j\cos(\theta_j)/\sqrt{2}\\\dot{r}t_j\sin(\theta_j)/\sqrt{2}\\t_j\end{bmatrix}\Bigg|j=0,1,\ldots,5\right\}.</script><p>通过将这些3D坐标乘以一个标准正交基，将它们旋转成世界坐标，该标准正交基的第三个向量是射线方向(其前两个向量是垂直于射线的任意帧)，然后由射线原点移动。通过构造，这些多样本的样本均值和方差(沿射线方向和垂直于射线方向)与锥形截锥体的均值和方差完全匹配，类似于mip-NeRF高斯分布。<strong>在训练期间，我们随机旋转和翻转每个图案</strong>，在渲染期间，我们确定地翻转和旋转每个其他图案30度。图3显示了这两种策略的可视化。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230729141223.png" alt="image.png"></p><p>我们使用这6个多样本$\{x_{j}\}$作为各向同性高斯函数的平均值，每个高斯函数的标准差为$σ_j$。我们设置$σ_{j} = (\dot r t_{j} /√2)$，用一个超参数进行缩放，在所有实验中都是0.5。由于iNGP网格要求输入坐标位于有界域中，因此我们应用了mip-NeRF 360[3]中的收缩函数。由于这些高斯函数是各向同性的，我们可以使用mip-NeRF 360使用的卡尔曼滤波方法的有效替代方法来计算由这种收缩引起的比例因子;详见附录。</p><h2 id="Downweighting"><a href="#Downweighting" class="headerlink" title="Downweighting"></a>Downweighting</h2><p>虽然多采样是减少混叠的有效工具，但对每个多采样使用朴素三线性插值仍然会导致高频混叠，如图2(d)所示。对于每个单独的多样本的抗混叠插值，我们以一种与每个多样本的各向同性高斯在每个网格单元中的拟合程度成反比的方式重新加权每个尺度的特征: <strong>如果高斯比被插值到的单元大得多，则插值的特征可能不可靠，应该降低权重</strong>。Mip-NeRF的IPE特征也有类似的解释。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230729140301.png" alt="image.png"><br>在iNGP中，对坐标x的每个$\{V_{\ell}\}$的插值是通过将x按网格的线性大小$n_{\ell}$缩放，并对$V_{\ell}$执行三线性插值以获得$c_{\ell}$长度的向量来完成的。我们用均值${x_j}$和标准差${σ_j}$插值一组多采样各向同性高斯函数。通过对高斯CDFs的推理，我们可以计算出在$V_{\ell}$中的$[−1/2n, 1/2n]^3$立方内的每个高斯PDF的分数，并将其插值为与比例相关的降权因子$ω_{j,\ell} = erf(1/\sqrt{8σ^{2}_{j} n^{2}_{\ell}})$，其中$ω_{j,\ell}∈[0,1]$。如第4节所述，我们对$\{V_{\ell}\}$施加权衰减，这促使$V_{\ell}$中的值呈正态分布且为零均值。这个零均值假设让我们将期望的网格特征相对于每个多样本的高斯近似为$\omega_{j}\cdot\mathbf{f}_{j,\ell}+(1-\omega_{j})\cdot\mathbf{0}=\omega_{j}\cdot\mathbf{f}_{j,\ell}.$。这样，我们就可以通过对每个多样本的插值特征进行加权平均，近似得到所要表征的圆锥截锥体所对应的期望特征:$\mathbf{f}_{\ell}=\max_{j}(\omega_{j,\ell}\cdot\mathrm{trilerp}(n_{\ell}\cdot\mathbf{x}_{j};V_{\ell})).$<br>这组特性$\{\mathbf{f}_{\ell}\}$被连接起来并作为输入提供给MLP，就像在iNGP中一样。我们还连接了$\{ω_{j,\ell}\}$的特征版本，详细信息请参见附录。</p><h1 id="Z-Aliasing-and-Proposal-Supervision"><a href="#Z-Aliasing-and-Proposal-Supervision" class="headerlink" title="Z-Aliasing and Proposal Supervision"></a>Z-Aliasing and Proposal Supervision</h1><p>虽然前面详细介绍的多采样和降权方法是减少空间混叠的有效方法，但是我们必须考虑沿着射线的另一个混叠源，我们将其称为z-aliasing。这种z-aliasing是由于mip-NeRF 360使用了一个学习产生场景几何上界的提议MLP: 在训练和渲染期间，沿着射线的间隔被这个提议MLP反复评估，以生成直方图，由下一轮采样重新采样，只有最后一组样本由NeRF MLP渲染。Mip-NeRF 360表明，与之前使用图像重建来监督学习一个[2]或多个[19]nerf的策略相比，这种方法显著提高了速度和渲染质量。我们观察到mip-NeRF 360中的MLP倾向于学习从输入坐标到输出体积密度的非光滑映射。这导致了射线“跳过”场景内容的伪影，如图4所示。虽然这个工件在mip-NeRF 360中是微妙的，但如果我们为我们的提议网络使用iNGP后端而不是MLP(从而增加我们模型快速优化的能力)，它就会变得普遍和视觉上显著，特别是当相机沿着其z轴平移时。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230729145358.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/z_alias_pdf_labeled.gif" alt="z_alias_pdf_labeled.gif"></p><p>mip-NeRF 360中z-混叠的根本原因是<strong>用于监督提案网络的“层间损失”</strong>，它将等效的损失分配给NeRF和提案直方图bins，无论它们的重叠是部分的还是完全的-提案直方图箱只有在它们完全没有重叠时才会受到惩罚。为了解决这个问题，我们提出了一种替代损耗，与mip-NeRF 360的层间损耗不同，它是沿射线距离连续且平滑的。图6是两种损失函数的比较。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230729150649.png" alt="image.png"><br>在这里，我们可视化了一个toy设置的提案监督，其中窄NeRF直方图(蓝色)沿着相对于粗提案直方图(橙色)的射线转换。(a) mip-NeRF 360使用的损失是分段常数，但(b)我们的损失是平滑的，因为我们将NeRF直方图模糊成分段线性样条(绿色)。我们的预过滤损失让我们学习反混叠提案分布。</p><p>Blurring a Step Function:<br>为了设计一个<strong>对沿射线的距离光滑的损失函数</strong>，我们必须首先构造一种将分段常数阶跃函数转化为连续分段线性函数的技术。平滑离散1D信号是微不足道的，只需要用盒滤波器(或高斯滤波器等)进行离散卷积。但是当处理端点连续的分段常数函数时，这个问题就变得困难了，因为阶跃函数中每个区间的端点可能在任意位置。栅格化阶跃函数并执行卷积并不是一个可行的解决方案，因为这在非常窄的直方图箱的常见情况下是失败的。</p><p>因此需要一种新的算法。考虑一个阶跃函数(x, y)，其中$(x_i, x_{i+1})$是区间i的端点，$y_i$是区间i内阶跃函数的值。我们想要将这个阶跃函数与半径为r的矩形脉冲进行卷积，该脉冲积分为1$:[|x| &lt; r]/(2r)$，其中[]是艾弗森括号。观察到阶跃函数的单个区间i与该矩形脉冲的卷积是一个分段线性梯形，其结点为$(x_i−r, 0)， (x_i+ r, yi)， (x_i+1−r, y_i)， (x_i+1 + r, 0)$，而分段线性样条是位于每个样条结处的缩放三角函数的二重积分[12]。,和这一事实总和通勤与整合,我们可以有效地缠绕一个矩形脉冲的阶跃函数如下:我们把每个端点$x_i$的阶跃函数分成两个signed delta functions位于$x_i−r$和$x_i+r$值正比于相邻的y值之间的变化,我们交错(通过排序)δ函数,然后我们把这些排序δ函数两次(参见算法1补充的伪代码)。有了这个，我们可以构造我们的抗混叠损失函数。</p><p>Anti-Aliased Interlevel Loss:<br>我们继承的mip-NeRF 360中的提议监督方法需要一个损失函数，该损失函数以NeRF (s, w)产生的阶跃函数和提议模型$(\hat s, \hat w)$产生的类似阶跃函数作为输入。这两个阶跃函数都是直方图，其中s和$\hat s$是端点位置的向量，w和$\hat w$是权值之和≤1的向量，其中$w_i$表示场景内容在阶跃函数区间的可见程度。每个$s_i$是真度量距离$t_i$的某个归一化函数，根据某个归一化函数g(·)，我们将在后面讨论。请注意，s和$\hat s$是不相同的-每个直方图的端点是不同的。<br>为了训练一个提议网络，在不引入混联的情况下约束NeRF预测的场景几何，我们需要一个损失函数，可以测量(s, w)和$(\hat s, \hat w)$之间的距离，该距离相对于沿着光线的平移是平滑的，尽管这两个阶跃函数的端点是不同的。为此，我们将使用我们之前构建的算法模糊NeRF直方图(s, w)，然后将模糊的分布重新采样到提议直方图的区间集中，以产生一组新的直方图权重$w^{\hat s}$。图5描述了这个过程。在将模糊的NeRF权重重新采样到提案直方图的空间后，我们的损失函数是$w^{\hat s}$和$\hat w$的元素函数:$\mathcal{L}_{\mathrm{prop}}(\mathbf{s},\mathbf{w},\hat{\mathbf{s}},\hat{\mathbf{w}})=\sum_{i}\frac{1}{\hat{w}_{i}}\max(0,\not\nabla(w_{i}^{\hat{\mathbf{s}}})-\hat{w}_{i})^{2}.$<br>虽然这种损失类似于mip-NeRF 360的损失(带有停止梯度的半二次卡方损失)，但用于生成$w^{\hat s}$的模糊和重采样可以防止混叠</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230729140618.png" alt="image.png"><br>计算我们的抗锯齿损失需要我们平滑和重新采样NeRF直方图(s, w)到与提议直方图$(\hat s, \hat w)$相同的端点集，我们在这里概述。(1)我们用w除以s内每个区间的大小，得到积分≤1的分段常数PDF。(2)我们将该PDF与矩形脉冲进行卷积，以获得分段线性PDF。(3)集成此PDF以生成分段二次CDF，该CDF通过分段二次插值在三元关系式的每个位置$\hat s$进行查询。(4)通过取相邻插值值之间的差值，我们得到$w^{\hat s}$，这是重新采样到提议直方图$\hat s$端点的NeRF直方图权值w。(5)重新采样后，我们将loss $\mathcal{L}_{prop}$评估为$w^{\hat s}$和$\hat w$的元素函数，因为它们共享一个公共的坐标空间。</p><p>Normalizing Metric Distance:<br>在mip-NeRF 360中，我们用归一化距离$s∈[0,1]$来参数化沿着射线$t∈[t_{near}, t_{far}]$的度量距离(其中tnear和tfar是手动定义的近和远平面距离)。渲染使用度量距离t，但<strong>重新采样和提案监督使用归一化距离s</strong>，其中一些函数g(·)定义了两者之间的双射。mip-NeRF 360中使用的层间损耗对距离的单调变换是不变的，因此它不受g(·)的选择的影响。然而，我们的抗混叠损失预滤波消除了这种不变性，并且在我们的模型中使用mip-NeRF 360’s g(·)会导致灾难性的失败，因此我们必须构建一个新的归一化。为此，我们构建了一个新的power转换:$\mathcal{P}(x,\lambda)=\frac{|\lambda-1|}{\lambda}\left(\left(\frac{x}{|\lambda-1|}+1\right)^{\lambda}-1\right).$<br>该函数在原点处的斜率为1，因此射线原点附近的归一化距离与度量距离成正比(无需调整非零的近平面距离曲线)，但远离原点的度量距离变得弯曲，类似于对数距离(λ = 0)或逆距离(λ = - 1)。这让我们可以在不同的归一化之间平滑地插值，而不是在不同的离散函数中交换。如图7所示为P(x， λ)的可视化，以及与我们的归一化方法的比较，即$g(x) = \mathcal{P}(2x， - 1.5)$ , 当$s∈[0,1 /2]$时，该曲线大致为线性，但当$s∈[1/2,1]$时，该曲线介于逆和反平方之间</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230729154650.png" alt="image.png"></p><h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>我们的模型是在JAX[6]中实现的，基于mip-NeRF 360代码库[20]，重新实现了iNGP的体素网格金字塔和哈希值，取代了mip-NeRF 360使用的大型MLP。我们的<strong>整体模型架构与mip-NeRF 360相同</strong>，除了第2节和第3节中引入的抗锯齿调整，以及我们在这里描述的一些额外修改。</p><p>像mip-NeRF 360一样，我们使用两轮提议采样，每次64个样本，然后在最后的NeRF采样轮中使用32个样本。我们的抗混叠级间损失被施加在两轮提议采样上，第一轮的矩形脉冲宽度为r = 0.03，第二轮为r = 0.003，损耗乘数为0.01。我们对每一轮提议采样使用单独的提议iNGP和MLP，并且我们的NeRF MLP使用比iNGP使用的更大的依赖于视图的分支。详见附录。</p><p>我们对iNGP做的一个小而重要的修改是对存储在其网格和哈希金字塔中的特征代码施加标准化的权重衰减:$\sum_\ell\operatorname{mean}(V_\ell^2).$。通过惩罚每个金字塔级别$V_{\ell}$的方格网格/哈希值的平均值的和，我们诱导出与惩罚所有值的和的naive解决方案截然不同的行为，因为粗尺度比细尺度受到的惩罚要多几个数量级。这个简单的技巧非常有效——与没有权重衰减相比，它大大提高了性能，并且显著优于朴素权重衰减。在所有实验中，我们对这个归一化权重衰减使用0.1的损失乘数。</p><ul><li>360 Dataset</li><li>Multiscale 360 Dataset</li></ul><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="非论文code"><a href="#非论文code" class="headerlink" title="非论文code"></a>非论文code</h2><p><a href="https://github.com/SuLvXiangXin/zipnerf-pytorch">SuLvXiangXin/zipnerf-pytorch: Unofficial implementation of ZipNeRF (github.com)</a></p><h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Clone the repo.</span><br><span class="line">git clone https://github.com/SuLvXiangXin/zipnerf-pytorch.git</span><br><span class="line">cd zipnerf-pytorch</span><br><span class="line"></span><br><span class="line"># Make a conda environment.</span><br><span class="line">conda create --name zipnerf python=3.9</span><br><span class="line">conda activate zipnerf</span><br><span class="line"></span><br><span class="line"># Install requirements.</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line"># Install other extensions</span><br><span class="line">pip install ./gridencoder</span><br><span class="line"></span><br><span class="line"># Install nvdiffrast (optional, for textured mesh)</span><br><span class="line">git clone https://github.com/NVlabs/nvdiffrast</span><br><span class="line">pip install ./nvdiffrast</span><br><span class="line"></span><br><span class="line"># Install a specific cuda version of torch_scatter </span><br><span class="line"># see more detail at https://github.com/rusty1s/pytorch_scatter</span><br><span class="line">CUDA=cu117</span><br><span class="line">pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+$&#123;CUDA&#125;.html</span><br></pre></td></tr></table></figure><h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><ul><li>Dataset</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir data</span><br><span class="line">cd data</span><br><span class="line"></span><br><span class="line"># e.g. mipnerf360 data</span><br><span class="line">wget http://storage.googleapis.com/gresearch/refraw360/360_v2.zip</span><br><span class="line">unzip 360_v2.zip</span><br></pre></td></tr></table></figure><ul><li>Train</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># Configure your training (DDP? fp16? ...)</span><br><span class="line"># see https://huggingface.co/docs/accelerate/index for details</span><br><span class="line">accelerate config</span><br><span class="line"></span><br><span class="line"># Where your data is </span><br><span class="line">DATA_DIR=data/360_v2/bicycle</span><br><span class="line">EXP_NAME=360_v2/bicycle</span><br><span class="line"></span><br><span class="line"># Experiment will be conducted under &quot;exp/$&#123;EXP_NAME&#125;&quot; folder</span><br><span class="line"># &quot;--gin_configs=configs/360.gin&quot; can be seen as a default config </span><br><span class="line"># and you can add specific config useing --gin_bindings=&quot;...&quot; </span><br><span class="line">accelerate launch train.py \</span><br><span class="line">    --gin_configs=configs/360.gin \</span><br><span class="line">    --gin_bindings=&quot;Config.data_dir = &#x27;$&#123;DATA_DIR&#125;&#x27;&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.exp_name = &#x27;$&#123;EXP_NAME&#125;&#x27;&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.factor = 4&quot;</span><br><span class="line"></span><br><span class="line"># or you can also run without accelerate (without DDP)</span><br><span class="line">CUDA_VISIBLE_DEVICES=0 python train.py \</span><br><span class="line">    --gin_configs=configs/360.gin \</span><br><span class="line">    --gin_bindings=&quot;Config.data_dir = &#x27;$&#123;DATA_DIR&#125;&#x27;&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.exp_name = &#x27;$&#123;EXP_NAME&#125;&#x27;&quot; \</span><br><span class="line">      --gin_bindings=&quot;Config.factor = 4&quot; </span><br><span class="line"></span><br><span class="line"># alternatively you can use an example training script </span><br><span class="line">bash scripts/train_360.sh</span><br><span class="line"></span><br><span class="line"># blender dataset</span><br><span class="line">bash scripts/train_blender.sh</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># metric, render image, etc can be viewed through tensorboard</span><br><span class="line">tensorboard --logdir &quot;exp/$&#123;EXP_NAME&#125;&quot; </span><br><span class="line"></span><br><span class="line">autoDl:</span><br><span class="line">tensorboard --logdir &quot;exp/$&#123;EXP_NAME&#125;&quot; --port 6007</span><br><span class="line">lsof -i:6007</span><br><span class="line">    kill -7 PID</span><br><span class="line">ps -ef | grep tensorboard | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9</span><br></pre></td></tr></table></figure><ul><li>Render video</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch render.py \</span><br><span class="line">    --gin_configs=configs/360.gin \</span><br><span class="line">    --gin_bindings=&quot;Config.data_dir = &#x27;$&#123;DATA_DIR&#125;&#x27;&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.exp_name = &#x27;$&#123;EXP_NAME&#125;&#x27;&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.render_path = True&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.render_path_frames = 480&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.render_video_fps = 60&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.factor = 4&quot;  </span><br><span class="line"></span><br><span class="line"># alternatively you can use an example rendering script </span><br><span class="line">bash scripts/render_360.sh</span><br></pre></td></tr></table></figure><ul><li>Evaluate</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># using the same exp_name as in training</span><br><span class="line">accelerate launch eval.py \</span><br><span class="line">    --gin_configs=configs/360.gin \</span><br><span class="line">    --gin_bindings=&quot;Config.data_dir = &#x27;$&#123;DATA_DIR&#125;&#x27;&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.exp_name = &#x27;$&#123;EXP_NAME&#125;&#x27;&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.factor = 4&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># alternatively you can use an example evaluating script </span><br><span class="line">bash scripts/eval_360.sh</span><br></pre></td></tr></table></figure><ul><li>Extract mesh</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># more configuration can be found in internal/configs.py</span><br><span class="line">accelerate launch extract.py \</span><br><span class="line">    --gin_configs=configs/360.gin \</span><br><span class="line">    --gin_bindings=&quot;Config.data_dir = &#x27;$&#123;DATA_DIR&#125;&#x27;&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.exp_name = &#x27;$&#123;EXP_NAME&#125;&#x27;&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.factor = 4&quot;</span><br><span class="line">#    --gin_bindings=&quot;Config.mesh_radius = 1&quot;  # (optional) smaller for more details e.g. 0.2 in bicycle scene</span><br><span class="line">#    --gin_bindings=&quot;Config.isosurface_threshold = 20&quot;  # (optional) empirical value</span><br><span class="line">#    --gin_bindings=&quot;Config.mesh_voxels=134217728&quot;  # (optional) number of voxels used to extract mesh, e.g. 134217728 equals to 512**3 . Smaller values may solve OutoFMemoryError</span><br><span class="line">#    --gin_bindings=&quot;Config.vertex_color = True&quot;  # (optional) saving mesh with vertex color instead of atlas which is much slower but with more details.</span><br><span class="line">#    --gin_bindings=&quot;Config.vertex_projection = True&quot;  # (optional) use projection for vertex color</span><br><span class="line"></span><br><span class="line"># or extracting mesh using tsdf method</span><br><span class="line">accelerate launch tsdf.py \</span><br><span class="line">    --gin_configs=configs/360.gin \</span><br><span class="line">    --gin_bindings=&quot;Config.data_dir = &#x27;$&#123;DATA_DIR&#125;&#x27;&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.exp_name = &#x27;$&#123;EXP_NAME&#125;&#x27;&quot; \</span><br><span class="line">    --gin_bindings=&quot;Config.factor = 4&quot;</span><br><span class="line"></span><br><span class="line"># alternatively you can use an example script </span><br><span class="line">bash scripts/extract_360.sh</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Efficiency </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Efficiency </tag>
            
            <tag> Encoding </tag>
            
            <tag> Sampling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRO</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/NeRO/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Shadow&amp;Highlight/NeRO/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>NeRO: Neural Geometry and BRDF Reconstruction of Reflective Objects from Multiview Images</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://liuyuan-pal.github.io/">Yuan Liu</a>, <a href="https://totoro97.github.io/">Peng Wang</a>, <a href="https://clinplayer.github.io/">Cheng Lin</a>, <a href="https://www.xxlong.site/">Xiaoxiao Long</a>, <a href="https://jiepengwang.github.io/">Jiepeng Wang</a>, <a href="https://lingjie0206.github.io/">Lingjie Liu</a>, <a href="https://homepages.inf.ed.ac.uk/tkomura/">Taku Komura</a>, <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html">Wenping Wang</a></td></tr><tr><td>Conf/Jour</td><td>SIGGRAPH 2023</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://liuyuan-pal.github.io/NeRO/">NeRO: Neural Geometry and BRDF Reconstruction of Reflective Objects from Multiview Images (liuyuan-pal.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4761535311519940609&amp;noteId=1889502311513975040">NeRO: Neural Geometry and BRDF Reconstruction of Reflective Objects from Multiview Images (readpaper.com)</a></td></tr></tbody></table></div><p>Reference</p><blockquote><p><a href="https://readpaper.com/paper/3204455502">[PDF] NeRD: Neural Reflectance Decomposition From Image Collections</a><br><a href="https://readpaper.com/paper/692131090958098432">[PDF] SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections</a><br><a href="https://readpaper.com/paper/4645908786821742593">[PDF] Relighting4D: Neural Relightable Human from Videos</a><br><a href="https://readpaper.com/paper/682591079116292096">[PDF] Neural 3D Scene Reconstruction with the Manhattan-world Assumption</a><br><a href="https://readpaper.com/paper/640484809354805248">[PDF] NeROIC: Neural Rendering of Objects from Online Image Collections</a></p></blockquote><p>对金属反光材质的物体重建效果很好</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728162856.png" alt="imgae"></p><p>提出了一种新的光表示方法，颜色由漫反射和镜面反射两部分组成，通过两个阶段的方法来实现</p><ul><li>Stage1：使用集成方向编码来近似光积分，使用shadow MLP对直接光和间接光进行model，学习到了表面几何形状</li><li>Stage2：蒙特卡罗采样固定几何形状，重建更精确的表面BRDF和环境光<ul><li>$\mathbf{c}_{\mathrm{diffuse}}=\frac{1}{N_{d}}\sum_{i}^{N_{d}}(1-m)\mathrm{a}L(\omega_{i}),$</li><li>$\mathbf{c}_{\mathrm{specular}}=\frac{1}{N_{s}}\sum_{i}^{N_{s}}\frac{FG(\omega_{0}\cdot\mathbf{h})}{(\mathbf{n}\cdot\mathbf{h})(\mathbf{n}\cdot\omega_{\mathbf{0}})}L(\omega_{i}),$</li></ul></li></ul><span id="more"></span><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728142918.png" alt="image.png"></p><p>Stage1 MLP：</p><ul><li>SDF&amp;Material：<ul><li>input: p2PE</li><li>output: Albedo , Metallic , Roughness, SDF</li></ul></li><li>Refection计算：SDF2n法向量 , v观察方向 —&gt; t反射方向</li><li>Direct Light：<ul><li>input: Roughness&amp;t to IDE</li><li>output: shading</li></ul></li><li>Indirect Light: 间接光与球空间中的位置有关<ul><li>input: Roughness&amp;t to IDE , p2PE</li><li>output: shading</li></ul></li><li>Occlusion Prob: 来确定在渲染中将使用直接灯光还是间接灯光<ul><li>input: t2DE, p2PE</li><li>output: shading</li></ul></li><li>Opaque Density计算：SDF —&gt; w权重</li><li>Shading计算： Albedo , Metallic , shading —&gt; c颜色<ul><li>Light integral approximation ， 由 $g_{direct}$输出、$g_{indirect}$输出和遮挡概率s(t)计算出光积分</li><li>由漫射光积分、镜面反射光积分、反照率a和金属度m计算出最终该点的颜色</li></ul></li></ul><script type="math/tex; mode=display">\begin{gathered}\mathbf{c}(\omega_{0})=\mathbf{c}_{\mathrm{diffuse}}+\mathbf{c}_{\mathrm{specular}}, \\\mathbf{c}_{\mathrm{diffuse}}=\int_{\Omega}(1-m)\frac{\mathbf{a}}{\pi}L(\omega_{i})(\omega_{i}\cdot\mathbf{n})d\omega_{i}, \\\mathbf{c}_{\mathrm{specular}}=\int_{\Omega}\frac{DFG}{4(\omega_{i}\cdot\mathbf{n})(\omega_{0}\cdot\mathbf{n})}L(\omega_{i})(\omega_{i}\cdot\mathbf{n})d\omega_{i}. \end{gathered}</script><p>光近似：<br>$\mathbf{c}_{\mathrm{diffuse}}=\text{a}(1-m)\underbrace{\int_{\Omega}L(\omega_{i})\frac{\omega_{i}\cdot\mathbf{n}}{\pi}d\omega_{i},}_{L_{\mathrm{diffuse}}}$<br>$\mathbf{c}_{\mathrm{specular}}\approx\underbrace{\int_{\Omega}L(\omega_{i})D(\rho,\mathbf{t})d\omega_{i}}_{L_{\mathrm{specular}}}\cdot\underbrace{\int_{\Omega}\frac{DFG}{4(\omega_{0}\cdot\mathbf{n})}d\omega_{i},}_{M_{\mathrm{specular}}}$<br>其中亮度可以分为直接光(outer sphere)和间接光(inner sphere)</p><script type="math/tex; mode=display">\begin{aligned}L_{\mathrm{specular}}&\approx[1-s(\mathrm{t})]\int_{\Omega}g_{\mathrm{direct}}(SH(\omega_l))D(\rho,\mathrm{t})d\omega_l+\\&s(\mathrm{t})\int_{\Omega}g_{\mathrm{indirect}}(SH(\omega_l),\mathrm{p})D(\rho,\mathrm{t})d\omega_l\\&\approx[1-s(\mathrm{t})]g_{\mathrm{direct}}(\int_{\Omega}SH(\omega_l)D(\rho,\mathrm{t})d\omega_l)+\\&s(\mathrm{t})g_{\mathrm{indirect}}(\int_{\Omega}SH(\omega_i)D(\rho,\mathrm{t})d\omega_l,\mathrm{p}).\end{aligned}</script><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728135846.png" alt="image.png"></p><ul><li>优点<ul><li>不需mask，主要目标是重建物体的几何形状和BRDF的颜色</li></ul></li><li>不足<ul><li>几何中的细节无法重建出来（太光滑）</li><li>由于颜色依赖法向量估计，表面法线的错误会导致难以拟合正确的颜色</li><li>依赖于准确的输入相机姿势，并且估计反射物体上的相机姿势通常需要稳定的纹理，如用于图像匹配的校准板。</li><li>很慢，在3090(24G)上，Stage1的隐式重建需要大概10个小时左右，Stage2的BRDF色彩重建需要3个半小时左右</li></ul></li></ul><p>纹理校准板</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728161948.png" alt="image.png"></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了一种神经重建方法NeRO，它可以在<strong>不知道环境光照条件和物体掩模的情况下</strong>精确地重建反射物体的几何形状和BRDF。NeRO的关键思想是明确地将渲染方程合并到神经重构框架中。<br>NeRO通过提出一种<strong>新颖的光表示</strong>和采用两阶段方法来实现这一具有挑战性的目标。</p><ul><li>在第一阶段，通过应用易于处理的近似，我们用阴影mlp对直接和间接光进行建模，并忠实地学习表面几何形状。</li><li>在第二阶段，我们通过蒙特卡罗采样固定几何形状，重建更精确的表面BRDF和环境光。</li></ul><p>实验表明，与最先进的技术相比，NeRO可以实现更好的表面重建质量和反射物体的BRDF估计。</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>我们提出了一种基于神经渲染的方法，称为NeRO，用于从未知环境中捕获的多视图图像中重建反射物体的几何形状和BRDF</p><blockquote><p>BRDF: 双向反射分布函数 The <strong>bidirectional reflectance distribution function</strong><br><a href="https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function">Bidirectional reflectance distribution function - Wikipedia</a></p></blockquote><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230727122713.png" alt="image.png"></p><p>${\displaystyle f_{\text{r}}(\omega _{\text{i}},\,\omega _{\text{r}})\,=\,{\frac {\mathrm {d} L_{\text{r}}(\omega _{\text{r}})}{\mathrm {d} E_{\text{i}}(\omega _{\text{i}})}}\,=\,{\frac {1}{L_{\text{i}}(\omega _{\text{i}})\cos \theta _{\text{i}}}}{\frac {\mathrm {d} L_{\text{r}}(\omega _{\text{r}})}{\mathrm {d} \omega _{\text{i}}}}}$</p><p>反射物体的多视图重建非常具有挑战性，因为镜面反射依赖于视图，从而违反了多视图一致性，而多视图一致性是大多数多视图重建方法的基础。<br>最近的神经渲染技术可以模拟环境光和物体表面之间的相互作用，以适应与视图相关的反射，从而使从多视图图像中重建反射物体成为可能。然而，在神经渲染中，环境光的精确建模是一个棘手的问题，特别是在几何形状未知的情况下。<strong>现有的神经渲染方法对环境光进行建模时，大多只考虑直射光，依靠物体蒙版来重建反射较弱的物体</strong>。因此，这些方法无法重建反射物体，特别是在没有物体掩模和物体被间接光照射的情况下。</p><p>我们建议采取两步走的办法来解决这个问题。</p><ul><li>首先，通过应用<strong>分割和近似split-sum approximation</strong>和<strong>集成方向编码</strong>来近似直接和间接光的阴影效果，我们能够准确地重建反射物体的几何形状，而不需要任何物体遮罩。</li><li>然后，在物体几何形状固定的情况下，我们使用更精确的采样来恢复物体的环境光和BRDF。大量的实验表明，我们的方法能够在不知道环境光和物体掩模的情况下，仅从RGB图像中准确地重建反射物体的几何形状和BRDF。</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>Multiview 3D reconstructionreconstruction，a fundamental task in computer graphics and vision近年来取得了巨大进步[Oechsle et al. 2021;Schönberger等。2016;Wang等。2021 a, b;姚等人。2018;Yariv等。2021,2020]。<ul><li>尽管取得了令人信服的成果，但在现实环境中经常看到的反射物体的重建仍然是一个具有挑战性和突出的问题。反光物体通常有光滑的表面，部分或全部照射在物体上的光被反射。当从不同的角度观察物体时，反射导致颜色不一致。然而，大多数多视图重建方法严重依赖于视图一致性来进行立体匹配。这对现有技术的重建质量构成了重大障碍。图2 (b)显示了广泛使用的COLMAP [Schönberger et al. 2016]在反射物体上的重建</li></ul></li><li>作为多视图重建的新兴趋势，基于神经渲染的曲面建模显示出处理复杂物体的强大能力[Oechsle等人。2021;Wang等。2021 b;Yariv等人。2021年,2020]。在这些所谓的神经重建方法中，底层表面几何被表示为隐式函数，例如，由多层感知(MLP)编码的符号距离函数(SDF)。为了重建几何图形，这些方法<strong>通过建模与视图相关的颜色并最小化渲染图像与输入图像之间的差异来优化神经隐式函数</strong>。<ul><li>然而，神经重建方法仍然难以重建反射物体。图2 (c)给出了示例。原因是这些方法中使用的颜色函数只将颜色与视图方向和表面几何形状关联起来，而<strong>没有明确考虑反射的底层遮阳机制</strong>。因此，拟合表面上不同视角方向的镜面颜色变化会导致错误的几何形状，即使在位置编码中频率更高，或更深更宽的MLP网络。</li></ul></li><li>为了解决具有挑战性的表面反射，我们建议明确地将渲染方程的公式[Kajiya 1986]纳入神经重建框架。渲染方程使我们能够考虑表面双向反射分布函数(BRDF) [Nicodemus 1965]与环境光之间的相互作用。由于反射物体的外观受到环境光线的强烈影响，因此依赖于视图的镜面反射可以用渲染方程很好地解释。<strong>通过显式渲染函数，大大增强了现有神经重构框架的表征能力</strong>，以捕获高频镜面颜色变化，从而显著有利于反射物体的几何重建。</li><li>显式地将渲染方程合并到神经重建框架中并不是微不足道not trivial.的。在未知的表面位置和未知的环境光下，计算环境光的积分是一个棘手的问题。<ul><li>为了可跟踪地评估渲染方程，现有的材料估计方法[Boss等]。[2021 a, b;Hasselgren et al. 2022;Munkberg等人。2022;Verbin et al. 2022;Zhang等。2021a,b, 2022b]强烈依赖物体掩模来获得正确的表面重建，主要用于无强镜面反射的物体的材料估计，在反射物体上的效果要差得多，如图2 (d,e)所示。此外，这些方法大多进一步简化了渲染过程，只考虑来自遥远区域的光(直射光)[Boss等]。2021 a, b;Munkberg et al. 2022;Verbin等。2022;Zhang等。[2021a]，因此很难重建被物体本身或附近区域(间接光)的反射光照射的表面。虽然有方法[Hasselgren et al. 2022;Zhang et al. 2021b, 2022b]考虑到渲染中的间接光，它们要么需要具有已知几何形状的重建辐射场[Zhang et al. 2021b];2021b, 2022b]或只使用很少的射线样本来计算光[Hasselgren等。2022]，这会导致对反射对象的不稳定收敛或对对象掩模的额外依赖。因此，同时考虑直接光和间接光来正确重建反射物体的未知表面仍然是一个挑战。</li></ul></li><li>通过将渲染方程整合到神经重建框架中，我们提出了一种称为NeRO的方法，用于仅从RGB图像中重建反射物体的几何形状和BRDF。NeRO的关键组成部分是一种新颖的光表示。<strong>在这种光表示中，我们使用两个单独的mlp分别编码直接光和间接光的亮度，并计算遮挡概率以确定在渲染中应该使用直接光还是间接光</strong>。这样的光表示有效地适应了直射光和间接光，以精确地重建反射物体的表面。基于提出的光表示，NeRO<strong>采用两阶段策略对神经重建中的渲染方程进行易于处理的评估</strong>。<ul><li>NeRO的第一阶段采用分割和近似和集成方向编码[Verbin等人。2022]来评估渲染方程，该方程可以在<strong>折衷compromised的环境光和表面BRDF估计的情况下产生精确的几何重建</strong>。</li><li>然后，在重建几何固定的情况下，NeRO的第二阶段通过蒙特卡罗采样更准确地评估渲染方程来改进估计的BRDF。</li><li>通过光表示和两阶段设计，该方法从本质上扩展了神经渲染方法对反射物体的表示能力，使其充分发挥了学习几何表面的潜力。</li></ul></li><li>为了评估NeRO的性能，我们引入了一个合成数据集和一个真实数据集，这两个数据集都包含被复杂环境光照射的反射物体。在这两个数据集上，NeRO都成功地重建了反射物体的几何和表面BRDF，而基线MVS方法和神经重建方法都失败了。我们的方法的输出是一个带有估计BRDF参数的三角形网格，可以很容易地用于下游应用，如重照明。</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230727123733.png" alt="image.png"></p><h2 id="RELATED-WORKS"><a href="#RELATED-WORKS" class="headerlink" title="RELATED WORKS"></a>RELATED WORKS</h2><ul><li>Multiview 3D reconstruction（MVS）<ul><li>多视角三维重建或多视角立体(MVS)已经研究了几十年。传统的多视图重建方法主要依靠<strong>三维点的多视图一致性</strong>来建立对应关系并估计不同视图上的深度值。随着深度学习技术的进步，最近的许多研究[Cheng et al .2020;Wang等。2021年;Yan等。2020;Yang et al. 2020;姚等人。2018]尝试引入神经网络来估计MVS任务的对应关系，这在广泛使用的基准测试中展示了令人印象深刻的重建质量[Geiger等人。2013;Jensen等人。2014;Scharstein and Szeliski 2002]。</li><li>在本文中，我们的目标是重建具有<strong>强镜面反射的反射物体</strong>。强烈的镜面反射违背了多视图一致性，因此这些基于对应的方法在反射对象上表现不佳。</li><li>Neural surface reconstruction神经渲染和神经表示因其强大的表征能力和对新视图合成任务的显著改进而备受关注。<ul><li>DVR [Niemeyer et al. 2020]首次在多视图重建中引入了神经渲染和神经表面表示。</li><li>IDR [Yariv et al. 2020]通过可微球追踪和Eikonal正则化[Gropp et al. 2020]提高了重建质量。</li><li>UNISURF [Oechsle等]。VolSDF [Yariv et al. 2021]和Neus [Wang et al. 2021 2021b]在多视图表面重建中引入了可微体绘制，提高了鲁棒性和质量。随后的工作在各个方面改进了基于体渲染的多视图重建框架，例如引入曼哈顿或正常先验[Guo等. 2022b;Wang等。2022c]，利用对称性[Insafutdinov et al. 2022;Zhang等。2021c]，提取图像特征[damon et al. 2022;Long et al. 2022]，提高保真度[Fu et al. 2022;Wang等。2022b]和效率[Li et . 2022;Sun et al. 2022;Wang等。2022年;Wu等人。2022;赵等。2022a]。</li><li>与这些工作类似，我们也遵循体绘制框架进行表面重建，但我们的重点是重建具有强镜面反射的反射物体，这是现有神经重建方法尚未探索的突出问题。</li></ul></li></ul></li><li>Reflective object reconstruction<ul><li>只有少数作品试图通过使用额外的物体遮罩来重建多视图立体环境中的反射物体[Godard等人。2015]或去除反射[Wu等。2018]。</li><li>除了不受控制的多视图重建，一些作品[Han et al. 2016;Roth and Black 2006]采用已知镜面流的约束设置[Roth and Black 2006]或已知环境[Han等。2016]用于重建理想的镜面物体。</li><li>其他一些工作通过编码射线来利用额外的射线信息[Tin等]或利用偏振图像[Dave et al. 2022;Kadambi et al. 2015;Rahmann and Canterakis 2001]用镜面反射来重建物体。</li><li>[Whelan et al. 2018]利用扫描仪的反射图像重建场景中的镜像平面。</li><li>这些方法被限制在一个相对严格的设置与特殊设计的捕获设备。相比之下，我们的目标是直接从多视图图像中重建反射物体，这些图像可以很容易地用手机相机捕捉到。</li><li>一些基于图像的渲染方法[Rodriguez et al .2020;Sinha et al. 2012]是专门为NVS任务设计的光滑或反射物体。</li><li>NeRFRen [Guo等 2022a]重构了存在镜像平面的场景的神经密度场。</li><li>神经点溃散学[Kopanas et al .2022]应用翘曲场来提高反射物体的渲染质量。</li><li>Ref-Nerf [Verbin et al .][2022]提出了集成方向编码(IDE)来提高反射材料的NVS质量。</li><li>我们的方法<strong>结合了IDE来重建反射物体，并使用神经SDF进行表面重建</strong>。一个并行的工作ORCA [Tiwary et al. 2022]扩展到从光滑物体上的反射重建场景的辐射场，这也重建了管道中的物体。由于ORCA的目标主要是重建场景的亮度场，因此它依赖于物体蒙版来重建反射物体。相比之下，<strong>我们的方法不需要物体遮罩</strong>，我们的主要目标是重建物体的几何形状和BRDF。</li></ul></li><li>BRDF estimation<ul><li>从图像中估计地表BRDF主要基于逆渲染技术[Barron and Malik 2014;Nimier-David等人。2019]。</li><li>一些方法[Gao et . 2019;郭等。2020;Li等人。2020年,2018年;温鲍尔等人，2022;叶等人。2022]在直接估计BRDF和照明之前依赖于物体或场景。</li><li>可微分渲染器Differentiable renderers[Chen et al. 2019,2021;Kato et al. 2018;Liu et al. 2019;Nimier-David等人。2019]允许从图像损失中直接优化BRDF。为了获得更准确的BRDF估计，大多数方法[Bi et al .2020年,(无日期);Cheng等。2021;Kuang et al. 2022;李和李2022a,b;Nam等人。2018;Schmitt et al. 2020;Yang等。2022a,b;Zhang等人2022a]要求物体的多个图像由不同的组合手电筒照射。</li><li>在本文中，我们估计了带有移动摄像机的静态场景中的BRDF，这也是Boss等人采用的设置[2021a, 2022, 2021b;Deng et al. 2022;Hasselgren等人。2022;Munkberg et al. 2022;张等。2021a,b, 2022b]。</li><li>其中，PhySG [Zhang等 2021a]， NeRD [Boss等 2021a]，神经网络- pil [Boss等 2021b]和NDR [Munkberg et al. 2022]在BRDF估计中考虑了直接环境光与表面之间的相互作用。后续工作MII[张等 2022b]， NDRMC [Hasselgren等。2022]，DIP [Deng等 2022]和NeILF [Yao等 2022]增加间接照明，这提高了估计BRDF的质量</li><li>这些方法的主要目的是重建普通物体的BRDF，避免太多的镜面反射，从而在反射物体上产生低质量的BRDF。其他一些方法[Chen and Liu 2022;Duchêne et al. 2015;Gao等。2020;Liu等人。2021;Lyu等人。2022;Nestmeyer等人。2020;菲利普等人。2019年,2021年;Rudnev等人。2022;Shih et al. 2013;你等人。2020;Yu and Smith 2019;赵等，2022b;Zheng等人2021]主要针对重光照任务，而不是为重建表面几何或BRDF而设计的。</li><li>NeILF [Yao et al. 2022]与我们方法的第二阶段最相似，两者都固定了几何形状，以通过MC采样优化BRDF。然而，NeILF没有对镜面瓣进行重要采样，只是从一个位置和一个方向预测光线，而不考虑遮挡。相比之下，<strong>我们的方法明确区分直接和间接光，并在漫反射和镜面上使用重要采样，以便更好地估计反射物体的BRDF。</strong></li></ul></li></ul><h1 id="METHOD"><a href="#METHOD" class="headerlink" title="METHOD"></a>METHOD</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>给定一组已知相机姿势的RGB图像作为输入，我们的目标是重建图像中反射物体的表面和BRDF。注意，<strong>我们的方法不需要知道物体遮罩或环境光</strong>。<br>NeRO的pipeline由两个阶段组成</p><ul><li>在第一阶段(3.3节)，我们通过优化<strong>带有体渲染的神经SDF</strong>来重建反射物体的几何形状，其中估计近似的直接和间接光来模拟依赖于视图的镜面颜色。</li><li>在第二阶段(第3.4节)，我们固定了物体的几何形状，并微调了直接和间接光，以<strong>计算反射物体的精确BRDF</strong>。</li></ul><p>我们首先简要回顾一下Neus [Wang et al. 2021b]和微观面BRDF模型[Cook and Torrance 1982;托伦斯和斯派洛1967]</p><p>我们遵循Neus，用MLP网络编码的SDF来表示物体表面。$g_{sdf}(x)$，曲面为${x∈R^{3} |\mathcal{g}_{sdf} (x) = 0}$。然后，将体绘制[Mildenhall et al. 2020]应用于从神经SDF中渲染图像。给定相机光线$o+tv$从相机中心沿方向发射到空间，我们采样射线上的点$\{\mathbf{p}_{j}=\mathbf{o}+t_{j}\mathbf{v}|t_{j}&gt;0,t_{j-1}&lt;t_{j}\}.$。然后，计算这个相机光线的渲染颜色</p><p>$\hat{\mathbf{c}}=\sum_{n}w_{j}\mathbf{c}_{j},$</p><p>权重w通过[Wang et al. 2021b]中提出的不透明密度从SDF值导出的。c是这个点的颜色，由MLP网络输出得到$\mathbf{c}_{j}=g_{\mathrm{color}}(\mathbf{p}_{j},\mathbf{v})$。然后，通过最小化渲染颜色c与gt的c之间的差异，两个MLP网络的参数是train得到的。$g_{sdf}$的零水平集提取重构曲面。为了使颜色函数能够正确地表示反射表面上的高光颜色，NeRO使用<strong>Micro-facet BRDF</strong>将NeuS的<strong>颜色函数替换为阴影函数</strong></p><p>Micro-facet BRDF: 点$p_j$的输出颜色<br>$\mathbf{c}(\omega_{0})=\int_{\Omega}L(\omega_{i})f(\omega_{i},\omega_{0})(\omega_{i}\cdot\mathbf{n})d\omega_{i},$</p><ul><li>$\omega_{o}= -v$ 是外观察方向，$c(\omega_o)$是外观察方向上电$p_j$的颜色</li><li>$\mathbf{n}$是表面法向量</li><li>$\omega_i$是输入光方向on the upper half sphere Ω,</li><li>BRDF function：$f(\omega_{i},\omega_{0}) \in [0,1]^{3}$</li><li>$L(\omega_{i}) \in [0,+\infty)^3$ 是入射光的亮度</li><li>在NeRO中，法向n是从SDF的梯度计算的。BRDF函数由<strong>漫反射部分和镜面部分组成</strong></li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230727122713.png" alt="image.png"></p><script type="math/tex; mode=display">f(\omega_{i},\omega_{0})=\underbrace{(1-m)\frac{a}{\pi}}_{\mathrm{diffuse}}+\underbrace{\frac{DFG}{4(\omega_{i}\cdot\mathbf{n})(\omega_{0}\cdot\mathbf{n})}}_{\mathrm{specular}},</script><ul><li>$m \in [0,1]$: the metalness of the point</li><li>1-m为漫反射部分的权重</li><li>$a \in [0,1]^3$:点的反照率颜色the albedo color of the point</li><li>𝐷 is the normal distribution function,</li><li>𝐹 is the Fresnel term</li><li>𝐺 is the geometry term<ul><li>𝐷, 𝐹 and 𝐺 are all determined by the metalness 𝑚 , the roughness $𝜌 ∈ [0, 1]$ and the albedo a</li></ul></li><li>该点的BRDF由金属度、粗糙度和反照率决定，all of which are predicted by a material MLP $𝑔_{material}$ in NeRO, i.e., $[𝑚, 𝜌, a] = 𝑔_{material} (p).$</li></ul><p>i.e: <strong>颜色值</strong></p><script type="math/tex; mode=display">\begin{gathered}\mathbf{c}(\omega_{0})=\mathbf{c}_{\mathrm{diffuse}}+\mathbf{c}_{\mathrm{specular}}, \\\mathbf{c}_{\mathrm{diffuse}}=\int_{\Omega}(1-m)\frac{\mathbf{a}}{\pi}L(\omega_{i})(\omega_{i}\cdot\mathbf{n})d\omega_{i}, \\\mathbf{c}_{\mathrm{specular}}=\int_{\Omega}\frac{DFG}{4(\omega_{i}\cdot\mathbf{n})(\omega_{0}\cdot\mathbf{n})}L(\omega_{i})(\omega_{i}\cdot\mathbf{n})d\omega_{i}. \end{gathered}</script><p>如前所述，准确评估体绘制中每个样本点的漫反射和镜面反射颜色的积分是棘手的。因此，我们提出了一个两步框架来近似计算这两个积分。在第一阶段，我们的首要任务是忠实地重建几何表面。</p><h2 id="Stage-I-Geometry-reconstruction"><a href="#Stage-I-Geometry-reconstruction" class="headerlink" title="Stage I: Geometry reconstruction"></a>Stage I: Geometry reconstruction</h2><p>为了重建反射物体的表面，我们采用了与Neus[Wang et al. 2021b]相同的神经SDF表示和体绘制算法(Eq. 1)，但使用了不同的颜色函数。在NeRO中，我们预测金属度、粗糙度和反照率使用微面BRDF来计算颜色。为了在Neus的体绘制中使计算易于处理，我们采用了分割和近似[Karis and Games 2013]，它将灯光和BRDF积的积分分离为两个单独的积分。</p><h3 id="镜面反射颜色"><a href="#镜面反射颜色" class="headerlink" title="镜面反射颜色"></a>镜面反射颜色</h3><p>$\mathbf{c}_{\mathrm{specular}}\approx\underbrace{\int_{\Omega}L(\omega_{i})D(\rho,\mathbf{t})d\omega_{i}}_{L_{\mathrm{specular}}}\cdot\underbrace{\int_{\Omega}\frac{DFG}{4(\omega_{0}\cdot\mathbf{n})}d\omega_{i},}_{M_{\mathrm{specular}}}$</p><ul><li>$L_{specular}$是光在正态分布函数上的积分$D(\rho,\mathbf{t}) \in [0,1]$, specular lobe<ul><li>t : is the reflective direction</li></ul></li><li>$M_{specular}$为BRDF的积分</li></ul><p>请注意，粗糙的表面有较大的镜面瓣，而光滑的表面有较小的镜面瓣。<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728135126.png" alt="image.png"></p><p>BRDF的积分可以由$\begin{aligned}M_{\mathrm{specular}}=((1-m)<em>0.04+m</em>\mathrm{a})*F_1+F_2,\end{aligned}$直接计算</p><ul><li>where 𝐹1 and 𝐹2 are two pre-computed scalars depending on the roughness 𝜌</li></ul><h3 id="漫反射颜色"><a href="#漫反射颜色" class="headerlink" title="漫反射颜色"></a>漫反射颜色</h3><p>The diffuse color：<br>$\mathbf{c}_{\mathrm{diffuse}}=\text{a}(1-m)\underbrace{\int_{\Omega}L(\omega_{i})\frac{\omega_{i}\cdot\mathbf{n}}{\pi}d\omega_{i},}_{L_{\mathrm{diffuse}}}$</p><p>$L_{diffuse}$为漫射光积分</p><p><strong>由材料MLP预测的m,$\rho$,a。唯二未知的量为两个光积分</strong><br>然而，为了计算光积分，我们不像以前的方法那样对环境光进行预滤波[Boss等。2021 b;Munkberg等人。2022]但使用<strong>集成定向编码</strong>[Verbin等人。2022]。</p><h3 id="光表示Light-representation"><a href="#光表示Light-representation" class="headerlink" title="光表示Light representation"></a>光表示Light representation</h3><p>在NeRO中，我们在对象周围定义一个边界球来构建神经SDF。由于我们只重建边界球内的表面，所以我们将所有<strong>来自边界球外的光称为直接光</strong>，而<strong>将边界球内表面反射的光称为间接光</strong>，如图4所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728135846.png" alt="image.png"></p><p>$L(\omega_i)=[1-s(\omega_i)]g_\text{direct}(SH(\omega_i))+s(\omega_i)g_\text{indirect}(SH(\omega_i),\text{p}),$</p><ul><li>两个mlp分别用于直接光和间接光：$g_{direct}, g_{indirect}$.这样所有的点都被相同的直接环境光照亮。这在解释反射对象的依赖于视图的颜色之前提供了一个强大的global</li><li>由于间接光在空间中变化，因此附加一个点位置p作为输入</li></ul><h3 id="Light-integral-approximation"><a href="#Light-integral-approximation" class="headerlink" title="Light integral approximation"></a>Light integral approximation</h3><p>我们使用集成方向编码来近似光积分</p><script type="math/tex; mode=display">\begin{aligned}L_{\mathrm{specular}}&\approx[1-s(\mathrm{t})]\int_{\Omega}g_{\mathrm{direct}}(SH(\omega_l))D(\rho,\mathrm{t})d\omega_l+\\&s(\mathrm{t})\int_{\Omega}g_{\mathrm{indirect}}(SH(\omega_l),\mathrm{p})D(\rho,\mathrm{t})d\omega_l\\&\approx[1-s(\mathrm{t})]g_{\mathrm{direct}}(\int_{\Omega}SH(\omega_l)D(\rho,\mathrm{t})d\omega_l)+\\&s(\mathrm{t})g_{\mathrm{indirect}}(\int_{\Omega}SH(\omega_i)D(\rho,\mathrm{t})d\omega_l,\mathrm{p}).\end{aligned}</script><p>在第一个近似中，我们使用遮挡概率𝑠(t)以替换不同光线的遮挡概率𝑠 (𝜔𝑖 )。在第二近似中，我们交换MLP的阶数和积分<br>我们只需要评估MLP网络$𝑔_{direct}$和$𝑔_{indirect}$积分方向编码$\int_{\Omega}SH(\omega_i)D(\rho,\mathrm{t})d\omega_l$一次</p><p>通过选择正态分布函数𝐷 是von Mises–Fisher（vMF）分布（球面上的高斯分布）, Ref-NeRF已经展示了$\int_{\Omega}SH(\omega_i)D(\rho,\mathrm{t})d\omega_l$有一个近似闭合形式的解，在这种情况下，我们在这里使用这个闭合形式的解来近似光的积分。</p><p>类似地，对于漫反射光积分，<br>$\frac{\omega_i\cdot\mathbf{n}}\pi\approx D(1.0,\mathbf{n}).$</p><p>请注意，分裂和近似和光积分近似仅在第一阶段使用，以实现易于处理的计算，并将在第二阶段被更准确的蒙特卡罗采样所取代。</p><h3 id="Occlusion-loss"><a href="#Occlusion-loss" class="headerlink" title="Occlusion loss"></a>Occlusion loss</h3><p>在灯光表示中，我们使用的遮挡概率𝑠 由MLP$𝑔_{occ}$预测来确定在渲染中将使用直接灯光还是间接灯光。然而，如图5所示，如果我们不对遮挡概率𝑠施加约束，并且让MLP网络从渲染loss中学习𝑠，预测的遮挡概率将与重建的几何结构完全不一致，并导致不稳定的收敛。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728141150.png" alt="image.png"></p><p>因此，我们使用神经SDF来约束预测的遮挡概率。给定从采样点p发射到其反射方向t的光线，我们计算其遮挡概率$s_{march}$在神经SDF中by ray-marching，并执行计算概率$s_{march}$和the predicted probability 𝑠之间的一致性$\ell_{occ}=|s_{\mathrm{march}}-s|_{1},$为遮挡概率正则化的损失</p><h3 id="Training-Losses"><a href="#Training-Losses" class="headerlink" title="Training Losses"></a>Training Losses</h3><p>我们计算相机光线的颜色并计算Charbonier损失，在渲染颜色和输入地真颜色之间作为渲染损失(渲染损失)</p><p>同时，我们观察到SDF的前几个训练步骤是不稳定的，要么是极大地扩大了表面，要么是把表面压得太小。在前1k步中应用稳定化正则化损失。$\ell=\ell_\text{render}+\lambda_\text{eikonal}\ell_\text{eikonal}+\lambda_\text{occ}\ell_\text{occ}+1\text{(step}&lt;1000)\ell_\text{stable},$</p><p>我们也采用Eikonal损失[Gropp等人。2020]将SDF梯度的范数正则化为1</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728142918.png" alt="image.png"></p><h3 id="Reflection-of-the-capturer"><a href="#Reflection-of-the-capturer" class="headerlink" title="Reflection of the capturer"></a>Reflection of the capturer</h3><p>在我们的模型中，我们假设一个静态照明环境。然而，在现实中，总是有一个人拿着相机捕捉周围反射物体的图像。移动的人会在物体的反射中可见，这就违反了静态照明的假设，如图7 (a)的红圈所示。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728143216.png" alt="image.png"></p><p>由于照片捕捉器相对于相机是静态的，我们在XoY平面建立了一个2D NeRF在摄像机坐标系中<br>在计算直射光时，我们还检查光线是否击中，如果hit点$p_{c}$存在，则使用$g_{camera}$，$[\alpha_{\mathrm{camera}},\mathrm{c}_{\mathrm{camera}}]=g_{\mathrm{camera}}(\mathrm{p}_{\mathrm{c}}),$来计算$\alpha_{camera}$和颜色</p><ul><li>$\alpha_{camera}$指示光线是否被捕获器遮挡</li><li>$\mathrm{c}_{\mathrm{camera}}$表示该点上捕获器的颜色</li><li>然后，直射光是$(1-\alpha_{\mathrm{camera}})g_{\mathrm{direct}}(\omega_{i})+ \alpha_{camera} c_{camera}$</li></ul><h2 id="Stage-II-BRDF-estimation"><a href="#Stage-II-BRDF-estimation" class="headerlink" title="Stage II: BRDF estimation"></a>Stage II: BRDF estimation</h2><p>到目前为止，在第一阶段之后，我们已经忠实地重建了反射物体的几何形状，但只得到了一个粗略的BRDF估计，需要进一步细化。在第二阶段，我们的目标是<strong>准确地评估渲染方程</strong>，<strong>从而精确地估计表面BRDF</strong>，即金属度、反照率和粗糙度。有了第一阶段的固定几何体，我们只需要在表面点上计算渲染方程。因此，现在可以应用<strong>蒙特卡罗采样</strong>来计算公式5中的漫射颜色和公式6中的镜面颜色。在MC采样中，我们对漫反射瓣和反射瓣都进行了重要采样。</p><ul><li><p>Importance sampling</p><ul><li>在蒙特卡洛采样中，漫反射颜色c漫反射是通过用<strong>余弦加权半球概率</strong>对射线进行采样来计算的$\mathbf{c}_{\mathrm{diffuse}}=\frac{1}{N_{d}}\sum_{i}^{N_{d}}(1-m)\mathrm{a}L(\omega_{i}),$</li><li>𝑖 是第i个样本射线和$𝜔_{𝑖}$是此采样光线的方向。</li><li>对于镜面反射颜色C，我们将GGX分布应用为<strong>正态分布𝐷</strong>. 然后，通过DDX分布的射线采样$𝑁_{𝑠}$条光线来计算镜面颜色$c_{specular}$ [Cook和Torrance1982]$\mathbf{c}_{\mathrm{specular}}=\frac{1}{N_{s}}\sum_{i}^{N_{s}}\frac{FG(\omega_{0}\cdot\mathbf{h})}{(\mathbf{n}\cdot\mathbf{h})(\mathbf{n}\cdot\omega_{\mathbf{0}})}L(\omega_{i}),$</li><li>其中 h 是 𝜔𝑖 和 𝜔𝑜 之间的半向向量。为了评估上述两个式子我们仍然使用与第一阶段相同的材料 MLP [𝑚, 𝜌, a] = $g_{material}$来计算金属度 𝑚、粗糙度和反照率 a。第二阶段的灯表示$𝐿(𝜔_𝑖)$与第一阶段相同。由于几何是固定的，我们通过跟踪给定几何中的光线而不是从 MLP 中预测它来直接计算遮挡概率。同时，对于真实数据，我们在沿方向𝜔从p发出沿p的射线的边界球体$q_{p,𝜔}$上添加交点，如图4所示，作为直接轻MLP$𝑔_{direct}$的附加输入。</li></ul></li><li><p>Regularization terms</p><ul><li>$\ell_{\mathrm{smooth}}=|g_{\mathrm{material}}(\mathrm{p})-g_{\mathrm{material}}(\mathrm{p}+\epsilon)|_{2},$</li><li>$\ell_{\mathrm{light}}=\sum_{c}^{3}([L_{\mathrm{diffuse}}]_{C}-\frac{1}{3}\sum_{c}^{3}[L_{\mathrm{diffuse}}]_{C}),$</li><li>$\ell=\ell_{\mathrm{render}}+\lambda_{\mathrm{smooth}}\ell_{\mathrm{smooth}}+\lambda_{\mathrm{light}}\ell_{\mathrm{light}},$</li></ul></li></ul><h1 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h1><p>几何。虽然我们成功地重建了反射物体的形状，但我们的方法仍然无法捕获一些细微的细节，如图19所示。主要原因是渲染函数强烈依赖于神经SDF估计的表面法线，但神经SDF往往会产生平滑的表面法线。因此，神经 SDF 很难产生突然的正常变化来重建细微的细节，例如“Angel”的布料纹理、“Cat”的胡子和“Maneki”的纹理。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728150142.png" alt="image.png"></p><p>BRDF。在实验中，我们观察到我们的<strong>BRDF估计主要存在不正确的几何形状</strong>，特别是在“Angel”上，如图20所示。由于反射物体的外观强烈依赖于表面法线来计算反射方向，<strong>表面法线的错误会使我们的方法难以拟合正确的颜色</strong>，从而导致BRDF估计不准确。同时，NeRO中的BRDF不支持各向异性反射等高级反射。</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230728150221.png" alt="image.png"></p><p>姿态估计。另一个限制是我们的方法依赖于准确的输入相机姿势，并且估计反射物体上的相机姿势通常需要稳定的纹理，如用于图像匹配的校准板。没有校准板，我们可以从其他共同可见的非反射物体或在IMU等设备的帮助下恢复姿势。</p><h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><p><a href="https://connecthkuhk-my.sharepoint.com/:f:/g/personal/yuanly_connect_hku_hk/EvNz_o6SuE1MsXeVyB0VoQ0B9zL8NZXjQQg0KknIh6RKjQ?e=jCLH0W">https://connecthkuhk-my.sharepoint.com/:f:/g/personal/yuanly_connect_hku_hk/EvNz_o6SuE1MsXeVyB0VoQ0B9zL8NZXjQQg0KknIh6RKjQ?e=jCLH0W</a></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>AutoDL:</p><ul><li>pytorch 1.11.0 </li><li>Python  3.8(ubuntu20.04)</li><li>Cuda  11.3</li></ul><p>pip install</p><ul><li><a href="https://nvlabs.github.io/nvdiffrast/#installation">nvdiffrast</a>.</li><li><a href="https://github.com/ashawkey/raytracing">raytracing</a></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/liuyuan-pal/NeRO.git</span><br><span class="line">cd NeRO</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">-i https://pypi.tuna._tsinghua_.edu.cn/simple</span><br><span class="line"></span><br><span class="line"># nvdiffrast</span><br><span class="line">git clone https://github.com/NVlabs/nvdiffrast</span><br><span class="line">pip install .</span><br><span class="line"></span><br><span class="line"># raytracing</span><br><span class="line">git clone https://github.com/ashawkey/raytracing</span><br><span class="line">cd raytracing</span><br><span class="line">pip install .</span><br><span class="line"></span><br><span class="line">pip install --upgrade protobuf</span><br><span class="line">pip install trimesh</span><br></pre></td></tr></table></figure><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p><a href="https://github.com/liuyuan-pal/NeRO">liuyuan-pal/NeRO: [SIGGRAPH2023] NeRO: Neural Geometry and BRDF Reconstruction of Reflective Objects from Multiview Images (github.com)</a></p><p>data:  Models and datasets all can be found <a href="https://connecthkuhk-my.sharepoint.com/:f:/g/personal/yuanly_connect_hku_hk/EvNz_o6SuE1MsXeVyB0VoQ0B9zL8NZXjQQg0KknIh6RKjQ?e=MaonKe">here</a>.<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">NeRO</span><br><span class="line">|-- data</span><br><span class="line">    |-- GlossyReal</span><br><span class="line">        |-- bear </span><br><span class="line">            ...</span><br><span class="line">    |-- GlossySynthetic</span><br><span class="line">        |-- bell</span><br><span class="line">            ...</span><br></pre></td></tr></table></figure></p><h3 id="Stage-1-重建shape"><a href="#Stage-1-重建shape" class="headerlink" title="Stage 1 重建shape"></a>Stage 1 重建shape</h3><p><strong>训练程序获取隐式模型：</strong><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># reconstructing the &quot;bell&quot; of the Glossy Synthetic dataset</span><br><span class="line">python run_training.py --cfg configs/shape/syn/bell.yaml</span><br><span class="line"></span><br><span class="line"># reconstructing the &quot;bear&quot; of the Glossy Real dataset</span><br><span class="line">python run_training.py --cfg configs/shape/real/bear.yaml</span><br></pre></td></tr></table></figure></p><p>Intermediate results will be saved at <code>data/train_vis</code>. Models will be saved at <code>data/model</code>.</p><p>data/model/bear_shape</p><ul><li>(tensorboard logs_dir)logs: events.out.tfevents.1690871015.autodl-container-6a4811bc52-8879d78f</li><li>model_best.pth —&gt; model_dir = /data/model/bear_shape</li><li>model.pth  —&gt; model_dir = /data/model/bear_shape</li><li>train.txt —&gt; logs_dir = (/data/model/bear_shape —&gt; /root/tf-logs)</li><li>val.txt —&gt; logs_dir</li></ul><p>tensorboard —&gt; train/loss 40k step左，240k step右</p><div style="display:flex; justify-content:space-between;"> <img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230801155011.png" alt="Image 1" style="width:50%;"><div style="width:10px;"></div> <img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230806154947.png" alt="Image 2" style="width:50%;"> </div><p>data/train_vis/bear_shape-val —&gt; 14999-index-0.jpg左，244999-index-0.jpg右</p><div style="display:flex; justify-content:space-between;"> <img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230801152018.png" alt="Image 1" style="width:50%;"><div style="width:10px;"></div> <img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230806153943.png" alt="Image 2" style="width:50%;"> </div><p><strong>Extract mesh from the model.</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python extract_mesh.py --cfg configs/shape/syn/bell.yaml</span><br><span class="line">python extract_mesh.py --cfg configs/shape/real/bear.yaml</span><br></pre></td></tr></table></figure><p>The extracted meshes will be saved at <code>data/meshes</code>.</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230807144740.png" alt="image.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">bug: </span><br><span class="line">(nero) root@autodl-container-6a4811bc52-8879d78f:~/autodl-tmp/NeRO# python extract_mesh.py --cfg confi  </span><br><span class="line">gs/shape/real/bear.yaml  </span><br><span class="line">successfully load bear_shape step 300000!  </span><br><span class="line">/root/miniconda3/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in  </span><br><span class="line">an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../a  </span><br><span class="line">ten/src/ATen/native/TensorShape.cpp:2228.)  </span><br><span class="line">return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined]</span><br><span class="line"></span><br><span class="line">将return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined]</span><br><span class="line">修改为return _VF.meshgrid(tensors, **kwargs, indexing = ‘ij’) # type: ignore[attr-defined]，警告解除</span><br><span class="line">————————————————</span><br><span class="line"></span><br><span class="line">应该是torch版本不匹配，亲测有效，不再出现UserWarning</span><br><span class="line">————————————————</span><br><span class="line">版权声明：本文为CSDN博主「余幼时即嗜学^」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。</span><br><span class="line">原文链接：https://blog.csdn.net/weixin_45103604/article/details/124717008</span><br></pre></td></tr></table></figure><h3 id="Stage-2-Material-estimation-or-texture"><a href="#Stage-2-Material-estimation-or-texture" class="headerlink" title="Stage 2  Material estimation or texture"></a>Stage 2  Material estimation or texture</h3><p>ply mesh data<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">NeRO</span><br><span class="line">|-- data</span><br><span class="line">    |-- GlossyReal</span><br><span class="line">        |-- bear </span><br><span class="line">            ...</span><br><span class="line">    |-- GlossySynthetic</span><br><span class="line">        |-- bell</span><br><span class="line">            ...</span><br><span class="line">    |-- meshes</span><br><span class="line">        | -- bell_shape-300000.ply</span><br><span class="line">        | -- bear_shape-300000.ply</span><br><span class="line">             ...</span><br></pre></td></tr></table></figure></p><p><strong>训练BRDF色彩：</strong><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># estimate BRDF of the &quot;bell&quot; of the Glossy Synthetic dataset</span><br><span class="line">python run_training.py --cfg configs/material/syn/bell.yaml</span><br><span class="line"></span><br><span class="line"># estimate BRDF of the &quot;bear&quot; of the Glossy Real dataset</span><br><span class="line">python run_training.py --cfg configs/material/real/bear.yaml</span><br></pre></td></tr></table></figure><br>Intermediate results will be saved at <code>data/train_vis</code>. Models will be saved at <code>data/model</code>.</p><p>tensorboard —&gt; train/loss 7k step左，100K step右</p><div style="display:flex; justify-content:space-between;"> <img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230807145209.png" alt="Image 1" style="width:50%;"><div style="width:10px;"></div> <img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230807182741.png" alt="Image 2" style="width:50%;"> </div><p>data/train_vis/bear_material-val/99999-index-0.jpg<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230807181830.png" alt="image.png"></p><p><strong>Extract materials from the model</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python extract_materials.py --cfg configs/material/syn/bell.yaml</span><br><span class="line">python extract_materials.py --cfg configs/material/real/bear.yaml</span><br></pre></td></tr></table></figure><p>The extracted materials will be saved at <code>data/materials</code>.</p><p>data/materials/bear_material-100000/</p><ul><li>albedo.npy</li><li>metallic.npy</li><li>roughness.npy</li></ul><h3 id="Relighting"><a href="#Relighting" class="headerlink" title="Relighting"></a>Relighting</h3><p>使用blender进行relighting，渲染在hdr场景下的镜面反射物体</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">NeRO</span><br><span class="line">|-- data</span><br><span class="line">    |-- GlossyReal</span><br><span class="line">        |-- bear </span><br><span class="line">            ...</span><br><span class="line">    |-- GlossySynthetic</span><br><span class="line">        |-- bell</span><br><span class="line">            ...</span><br><span class="line">    |-- meshes</span><br><span class="line">        | -- bell_shape-300000.ply</span><br><span class="line">        | -- bear_shape-300000.ply</span><br><span class="line">             ...</span><br><span class="line">    |-- materials</span><br><span class="line">        | -- bell_material-100000</span><br><span class="line">            | -- albedo.npy</span><br><span class="line">            | -- metallic.npy</span><br><span class="line">            | -- roughness.npy</span><br><span class="line">        | -- bear_material-100000</span><br><span class="line">            | -- albedo.npy</span><br><span class="line">            | -- metallic.npy</span><br><span class="line">            | -- roughness.npy</span><br><span class="line">    |-- hdr</span><br><span class="line">        | -- neon_photostudio_4k.exr</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">python relight.py --blender &lt;path-to-your-blender&gt; \</span><br><span class="line">                  --name bell-neon \</span><br><span class="line">                  --mesh data/meshes/bell_shape-300000.ply \</span><br><span class="line">                  --material data/materials/bell_material-100000 \</span><br><span class="line">                  --hdr data/hdr/neon_photostudio_4k.exr \</span><br><span class="line">                  --trans</span><br><span class="line">                  </span><br><span class="line">python relight.py --blender &lt;path-to-your-blender&gt; \</span><br><span class="line">                  --name bear-neon \</span><br><span class="line">                  --mesh data/meshes/bear_shape-300000.ply \</span><br><span class="line">                  --material data/materials/bear_material-100000 \</span><br><span class="line">                  --hdr data/hdr/neon_photostudio_4k.exr</span><br><span class="line"></span><br><span class="line">eg: </span><br><span class="line">python relight.py --blender F:\Blender\blender.exe --name bear-neon --mesh data/meshes/bear_shape-300000.ply --material data/materials/bear_material-100000 --hdr data/hdr/neon_photostudio_4k.exr</span><br><span class="line"></span><br><span class="line">KeyError: &#x27;bpy_prop_collection[key]: key &quot;Principled BSDF&quot; not found&#x27;</span><br><span class="line">--&gt; 需要将blender界面设置成英文</span><br></pre></td></tr></table></figure><blockquote><p><a href="https://github.com/carson-katri/dream-textures/issues/601">KeyError: ‘bpy_prop_collection[key]: key “Principled BSDF” not found’ · Issue #601 · carson-katri/dream-textures (github.com)</a></p></blockquote><p>The relighting results will be saved at <code>data/relight</code> with the directory name of <code>bell-neon</code> or <code>bear-neon</code>. This command means that we use <code>neon_photostudio_4k.exr</code> to relight the object.</p><iframe title="nero relightNeRO reproduce: relight bear of Glossy Real dataset in neon_photostudio_4k scene" src="https://www.youtube.com/embed/Npva_2r9tWk?feature=oembed" height="113" width="200" allowfullscreen="" allow="fullscreen" style="aspect-ratio: 16 / 9; width: 100%; height: 100%;"></iframe><h2 id="eg"><a href="#eg" class="headerlink" title="eg"></a>eg</h2><p>syn/bell: </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># stage1</span><br><span class="line">python run_training.py --cfg configs/shape/syn/bell.yaml</span><br><span class="line">python extract_mesh.py --cfg configs/shape/syn/bell.yaml</span><br><span class="line"># stage2</span><br><span class="line">python run_training.py --cfg configs/material/syn/bell.yaml</span><br><span class="line">python extract_materials.py --cfg configs/material/syn/bell.yaml</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Shadow&amp;Highlight </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
            <tag> Shadow&amp;Highlight </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tri-MipRF</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Efficiency/Tri-MipRF/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Efficiency/Tri-MipRF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://wbhu.github.io/">Wenbo Hu</a>     Yuling Wang</td></tr><tr><td>Conf/Jour</td><td>ICCV</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://wbhu.github.io/projects/Tri-MipRF/">Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields (wbhu.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4781040535062183937&amp;noteId=1888227766799606528">Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields (readpaper.com)</a></td></tr></tbody></table></div><p>2023.7.26 SOTA<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230726151828.png" alt="image.png|600"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230726151646.png" alt="image.png"></p><p>like <a href="https://apchenstu.github.io/TensoRF/">TensoRF: Tensorial Radiance Fields (apchenstu.github.io)</a>+ Mip-NeRF + NGP</p><ul><li><p>类似Mip-NeRF中的采样锥形方法，但是Tri-MipRF使用了与圆锥相切的采样球S=(x,r)，代替Mip-NeRF中的多元高斯圆锥体</p><ul><li>采样球的半径通过像素圆盘半径$\dot r$（由像素大小in world Coordinate），焦距f和$t_i$确定</li></ul></li><li><p>类似TensoRF中的分解方法，将空间采样球分解到三个平面上，编码类似NGP中的HashGrid 使用2D平面来存取特征值，构建一个base level:$M^{L_{0}}$，通过downscaling来获得其他level的2D grid平面。</p><ul><li>通过base level中interest space的AABB求出$\ddot r$，并联合采样球半径r得到采样球在平面投影的level，根据此level和投影到平面上的二维坐标，在相邻两level $\mathcal{M}_{XY}^{\lfloor l\rfloor}$和$\mathcal{M}_{XY}^{\lceil l\rceil}$的2D grid中采用3线性插值得到采样球的特征值，最后三个分解平面的特征值cat起来作为MLP的一个输入</li></ul></li><li><p>一种更好的渲染视图方法：Hybrid Volume-Surface Rendering</p><ul><li>通过在密度场中marching cubes和网格抽取来获得代理网格，粗略确定相机原点到物体表面的距离</li><li>对代理网格进行有效栅格化，以获得圆锥体中轴线表面上的命中点，然后我们在距圆锥体中轴线命中点∆t距离内均匀采样球体，这产生2∆t采样间隔。</li><li>优点：可以减少需要采样点的数量，且不会影响渲染出来图片的质量</li></ul></li><li><p>优点：</p><ul><li>fine-grained details in close-up views</li><li>and free of aliasing in distant views</li><li>5 minute and smaller model parameters</li></ul></li><li><p>缺点：</p><ul><li>需要使用multi-view segmentation methods将In-the-wildIn数据集中感兴趣的物体提取出来<ul><li>即需要mask</li></ul></li></ul></li></ul><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>In this work, we propose a Tri-Mip radiance fields, <strong>TriMipRF,</strong> to make the renderings contain</p><ul><li>fine-grained details in close-up views</li><li>and free of aliasing in distant views while maintaining efficient reconstruction, i.e. within <strong>five minutes</strong>, and <strong>compact representation</strong>, i.e. 25% smaller model size than Instant-ngp.</li><li>This is realized by our <strong>novel Tri-Mip encoding</strong> and <strong>cone casting</strong>.</li><li>The Tri-Mip encoding featurizes the 3D space by <strong>three mipmaps</strong> to model the pre-filtered 3D feature space, such that the sample spheres from the cone casting can be encoded in an area-sampling manner.</li></ul><p>We also develop a <strong>hybrid volume-surface rendering strategy</strong> to enable real-time rendering (&gt; 60 FPS) on consumer-level devices.</p><p>Extensive quantitative and qualitative experiments demonstrate our Tri-MipRF achieves <strong>state-of-the-art</strong> rendering quality while having a super-fast reconstruction speed. Also, the reconstruction results on the in-the-wild captures demonstrate the applicability of our Tri-MipRF.</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><ul><li>MipNeRF[3]呈现了精细的和抗锯齿渲染，但需要几天的训练</li><li>Instant-ngp[37]可以在几分钟内完成重建，但由于忽略采样区域，在不同距离或分辨率下渲染时存在模糊或混像</li></ul><p>为此，我们提出了一种新颖的Tri-Mip encoding(“mipmap”)，可以实现神经辐射场的即时重建和抗混叠高保真渲染</p><ul><li>将预滤波的三维特征空间分解成三个正交的mipmap，通过这种方式，我们可以利用2D预滤波特征图高效地进行3D区域采样，在不牺牲效率的情况下显著提高了渲染质量。为了处理新颖的Tri-Mip表示，我们提出了一种锥形渲染技术，在考虑像素成像和观察距离的情况下，使用Tri-Mip编码有效地采样抗锯齿3D特征。</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>NeRF: MLP进行训练隐式模型</li><li>MipNeRF通过集成位置编码对预过滤的亮度场进行建模，进一步突破了渲染质量的界限。然而，如此令人印象深刻的视觉质量，在重建和渲染阶段都需要昂贵的计算，例如，MipNeRF[3]需要三天以上的重建时间，渲染一帧需要几分钟。</li><li>InstantNGP另一方面，最近的研究提出了显式或混合表示来实现高效渲染[43,17,56,20,10,6]或重构[14,47,9,37]，例如哈希编码[37]将重构时间从几天大大缩短到几分钟，实现了实时渲染。但是他们的渲染模型在基于点的采样上都存在缺陷，导致近景渲染过于模糊，远景渲染过度混叠。</li><li>由于缺乏支持有效区域采样的表示，我们面临着质量和效率之间权衡的困境。</li></ul><p>在本文中，我们的目标是设计一个既支持<strong>高保真抗混叠渲染</strong>又支持<strong>高效重建</strong>的RF(radiance field)表示。为了解决混叠和模糊问题，<br>超采样</p><ul><li>离线</li><li>通过对每个像素的足迹进行多重光线的超采样会大大增加计算成本</li></ul><p>预滤波(又称区域采样), NGP中的占据网格，只对感兴趣的区域进行采样</p><ul><li>实时渲染</li><li>直接对3D体积进行预滤波也会占用大量内存和计算量,这与效率的目标相冲突</li><li>由于哈希冲突，预过滤用哈希编码表示的亮度字段也不是很简单</li></ul><p>我们通过新颖的Tri-Mip辐射场(Tri-MipRF)实现了这一具有挑战性的目标。如图1所示，我们的Tri-MipRF实现了最先进的渲染质量，在特写视图中呈现高保真细节，并且在远处视图中没有混叠。同时，它可以超快地重建，即在单个GPU上5分钟内，而哈希编码的超采样变体Instant-ngp  5x需要大约10分钟的重建时间，并且渲染质量要低得多。</p><p>通过三个2D mip (multum in parvoto)地图来呈现3D空间。<strong>Tri-Mip编码首先将3D空间分解为三个平面</strong>(planeXY, planeXZ和planeY Z)，灵感来自于<a href="https://readpaper.com/paper/4569381233369292801">[PDF] Efficient Geometry-aware 3D Generative Adversarial Networks-论文阅读讨论-ReadPaper</a>中3D内容生成的分解，然后用一个mipmap表示每个平面。<br>它巧妙地利用二维mipmaps的不同层次对预滤波的三维特征空间进行建模。我们的Tri-MipRF属于混合表示，因为它通过Tri-Mip编码和一个微小的MLP来建模辐射场，这使得它在重建过程中收敛得很快。我们的方法的模型大小相对紧凑，因为MLP非常浅，而<strong>Tri-Mip编码只需要三个2D地图来存储mipmap的base level</strong>。<br>为了处理Tri-Mip编码，我们提出了一种有效的锥形投射渲染技术，该技术将像素作为一个圆盘，并为每个像素发出一个锥形。<strong>与MipNeRF用多元高斯对圆锥体进行采样不同，我们采用圆锥体内切的球体。球体根据其占用的面积进一步以Tri-Mip编码为特征</strong>。这样做的原因是mipmaps中的特征是各向同性预过滤的。Tri-MipRF编码对预滤波后的三维特征空间进行建模，锥形投射自适应渲染距离和分辨率，<strong>它们通过采样球的占用面积有效地连接在一起，使我们的Tri-MipRF渲染结果在近距离视图中没有模糊，在远距离视图中没有混叠</strong>。此外，我们还开发了一种混合体面渲染策略，以便在消费级gpu上实现实时渲染，例如在Nvidia RTX 3060显卡上实现60 FPS。<br>我们在公共基准和野外拍摄的图像上广泛评估了我们的Tri-MipRF。定量和定性结果均证明了该方法在高保真渲染和快速重建方面的有效性。我们的贡献总结如下</p><ul><li>我们提出了一种新颖的<strong>Tri-Mip编码</strong>，通过利用多级2D mipmaps对预滤波的3D特征空间进行建模，从而通过有效的区域采样实现抗混叠体渲染。</li><li>我们提出了一种新的<strong>锥体投射渲染技术</strong>，该技术可以有效地为每个像素发出一个锥体，同时在Tri-Mip编码的3D空间上优雅地用球体对锥体进行采样</li><li>我们的方法实现了最先进的渲染质量和重建速度(在单个GPU上5分钟内)，同时仍然保持了紧凑的表示(模型尺寸比Instantngp小25%)。由于<strong>混合体面渲染策略</strong>，我们的方法在部署在消费者级设备上时也实现了实时渲染。</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li><p>Anti-aliasing in rendering.抗混叠是计算机图形学和图像处理中的一个基本问题，在渲染界得到了广泛的研究。从数学上讲，混叠是采样率不足导致频率分量重叠的结果。<strong>超采样和预滤波(区域采样)分别是离线和实时渲染算法中减少混叠的两种典型方法</strong>。超采样抗混叠(Super-sampling anti-aliasing, SSAA)方法[15,11,32,19,51]直接提高采样率以接近奈奎斯特频率，而多采样抗混叠(multi-sampling anti-aliasing, MSAA)[1]是现代图形处理器和api事实上支持的方法。基于预过滤的方法[25,39,22,2,52,23]通过在渲染前预先计算内容的过滤版本，减轻了这一负担，因此，这种方法流更适合实时渲染。</p><ul><li>在NeRF的背景下，超级采样可以通过每像素投射多个光线并聚合渲染结果来产生最终颜色来实现。这种简单的策略很有用，但代价很高，因为计算成本随着采样率的增加而显著增加。另一方面，最近的研究(MipNeRF/360, BACON)通过提出的<strong>集成位置编码</strong>或带限坐标网络将预滤波思想引入神经辐射场，以学习场景的预滤波表示，<strong>从而使其渲染在近距离视图中不模糊，在远距离视图中不混联</strong>。但是，它们的渲染和重建速度非常慢，例如MipNeRF[3]重建一个场景大约需要三天，渲染一帧需要几分钟，这阻碍了它们的适用性。相比之下，<strong>我们的Tri-MipRF可以在5分钟内重建，并在相同的硬件上实现实时渲染，同时我们的方法在近景和远景上都比MipNeRF具有更好的渲染质量。</strong></li></ul></li><li><p>Accelerating NeRF NeRF[35]隐式地表示MLP中的场景，这导致了非常紧凑的存储，但它的重建和渲染非常慢。<strong>一些研究致力于加速渲染</strong>，通过将场景分成许多单元[42,43]来降低推理复杂性，学习减少每条射线的样本[27,38]，或缓存训练好的字段值[20,17,56,6]来减少渲染中的计算。<strong>另一项工作侧重于通过直接优化显式表示或利用混合表示</strong>，如低秩张量[9]和哈希表[37]来加快收敛速度，从而减少重构时间。特别是，哈希编码可以在5分钟左右的时间内实现即时重建和实时渲染。</p><ul><li>但是，以上几种方法的渲染模型都存在<strong>将像素作为单个点进行采样而忽略对应区域的缺陷，这会导致近景渲染过于模糊，远景渲染过度混叠</strong>。上面提到的超级采样技术可以缓解这个问题，但需要每像素投射多次光线，这大大增加了重建和渲染成本。由于哈希冲突，将预过滤与哈希编码结合起来[37]是non-trivial的。我们的方法通过提出的Tri-Mip编码来解决这个问题，有效地对预过滤的3D特征空间进行建模，这与哈希编码一样有效，但能够产生抗混叠的高保真渲染。</li></ul></li><li><p>Compact 2D representation for 3D content 直接在volumes中表示3D内容是内存和计算密集型的，而且由于3D内容总是稀疏的，因此是冗余的。Peng等[41]提出<strong>将点云的特征投影到多个平面进行三维几何重建</strong>。最近的研究[21,54,55]表明，<strong>3D内容可以在2D图像中紧凑地表示，并忠实地还原</strong>。在生成模型的背景下，EG3D[8]<strong>提出了一种三平面表示，将3D体积分解为三个二维平面进行3D内容生成</strong>，并在后续的许多生成方法中采用了这种表示[16,44,45,48,5,53,12]。此外，将这种表示进一步推广到四维空间中，以模拟动态场景[7,13]。我们的Tri-Mip编码就是受到这一行作品的启发，<strong>但是以上的表示都不能实现我们的目标，即对预滤波的3D特征空间进行建模，以实现有效的面积采样</strong>。</p></li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230726151646.png" alt="image.png"></p><ul><li>从相机原点出发，向成像平面上的像素圆盘发射一个圆锥C，并用一组与该圆锥内嵌的球体S对该圆锥进行采样</li><li>通过Tri-Mip Encoding(由三个minmaps M参数化)将球体S特征化为特征向量f，这是使我们的渲染图在近处视图中包含细粒度细节和在远处视图中没有混化的关键，因为Tri-Mip编码通过利用mipmap中的不同level有效地建模预过滤的3D特征空间。</li><li>然后，我们使用一个由权重Θ参数化的微小MLP将<strong>球体S的特征向量f</strong>和视<strong>图方向d</strong>非线性映射到球体的密度τ和颜色c ：$[\tau,c]=\mathrm{MLP}(\mathbf{f},\mathbf{d};\Theta).$</li><li>最后，利用估算的圆锥体内球体的密度和颜色，通过[33]中的数值正交近似体绘制积分，绘制圆锥体对应像素的最终颜色:</li></ul><p>$\begin{aligned}\mathbf{C}(\mathbf{t},\mathbf{d},\Theta,\mathcal{M})&amp;=\sum_iT_i(1-\exp(-\tau_i(t_{i+1}-t_i)))c_i,\\\mathrm{with}\quad T_i&amp;=\exp\left(-\sum_{k&lt;i}\tau_k(t_{k+1}-t_k)\right)\end{aligned}$</p><ul><li>通过计算loss，并反向传播优化MLP的权重参数$\Theta$和Tri-Mip编码中mipmaps的参数M</li></ul><h2 id="Cone-Casting"><a href="#Cone-Casting" class="headerlink" title="Cone Casting"></a>Cone Casting</h2><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230726164225.png" alt="image.png"></p><p>PE/IPE: 位置编码，与显式或混合体积特征编码不兼容<br>HashGrid Encoding: 显式或混合体积特征编码</p><ul><li><strong>NeRF</strong>：将像素看成一个点，从相机原点向像素点发出光线，沿着光线采样点的坐标，并利用位置频率编码$\gamma(\cdot)$对其进行特征化，得到点的特征向量<ul><li>该公式将像素建模为单个点，而忽略像素的面积，这与现实世界的成像传感器有很大不同。大多数NeRF作品[47,56,14,9,10]，包括instant-ngp[37]，都遵循了这个公式。当捕获/渲染的视图处于大致恒定的距离时，它可以近似于现实世界的情况，但当在非常不同的距离上观看时，它会导致明显的伪影，例如，在特写视图中模糊，在远距离视图中混化，因为采样与距离无关。<strong>（近处的采样与远处的采样都是一个点）</strong></li></ul></li><li><strong>MipNeRF</strong>：为每个像素发出一个锥体，并对锥体进行<strong>多元高斯采样</strong>，再通过集成位置编码(IPE)对其进行特征化。IPE由E[γ(x)]对高斯范围内点的PE积分推导而来，如图3 (b)所示。<ul><li>然而，该策略对于扩展到显式或混合表示以实现高效重建和渲染并非易事，例如，哈希编码[37]，因为IPE是基于坐标的位置编码的积分，这与显式或混合体积特征编码不兼容。</li></ul></li><li>相比之下，我们的<strong>高效锥形投射策略</strong>可以有效地与我们的Tri-Mip编码一起在体绘制期间进行区域采样。如图3 (c)所示，我们将像素表示为图像平面上的一个圆盘，而不是忽略像素面积的单个点。圆盘的半径可通过$\dot r = \sqrt{(∆x·∆y)/π}$计算，其中∆x和∆y为像素在世界坐标下的宽度和高度，可由标定后的相机参数导出。对于每个像素，我们从相机的投影中心沿方向$\mathbf{d} = \mathbf{p_{o}} - \mathbf{o}$发射一个圆锥C，其中$p_{o}$是像素的中心。圆锥体的顶点位于相机的光学中心，圆锥体与成像平面的交点是像素对应的圆盘。我们可以推导出圆锥的中轴线为$\mathbf{a(t)} = \mathbf{o} + t\mathbf{d}$。为了对锥体进行采样，我们不能遵循MipNeRF[3]使用多元高斯，因为<strong>多元高斯是各向异性的</strong>，而<strong>我们的Tri-Mip编码中的预滤波是各向同性的</strong>。</li><li>因此，我们用一组球体(x, r)对圆锥体进行采样，这些球体的圆心为x，半径为r。圆心x位于圆锥体的中心轴，半径为r，使<strong>球体与圆锥体相切</strong>，可以写成:</li></ul><p>$\begin{aligned}\mathrm{x}&amp;=\mathrm{o}+t\mathrm{d},\\\mathrm{r}&amp;=\frac{|\mathrm{x}-\mathrm{o}|_2\cdot f\dot{r}}{|\mathrm{d}|_2\cdot\sqrt{\left(\sqrt{|\mathrm{d}|_2^2-f^2}-\dot{r}\right)^2+f^2},}\end{aligned}$</p><ul><li>焦距f</li><li>$\dot r = \sqrt{(∆x·∆y)/π}$</li></ul><p>可得：采样球体S(x,r) 可以由$t_{i} \in t$确定</p><p>我们在摄像机预定义的近平面$t_{n}$和远平面$t_{f}$之间，或者在感兴趣的3D空间的圆锥体中心轴和轴向边界盒(AABB)之间的两个交点之间均匀地采样$t_{i} \in t$。为了进一步利用3D空间的稀疏性来加速cone casting，我们采用了一种二元占用网格，它粗略地标记了空与非空的空间，类似于(NGP or NerfAcc)，这样我们可以便宜地跳过空区域的样本，并将样本集中在表面附近，以避免浪费计算。</p><h2 id="Tri-Mip-Encoding"><a href="#Tri-Mip-Encoding" class="headerlink" title="Tri-Mip Encoding"></a>Tri-Mip Encoding</h2><p>为了实现我们的目标，即在近距离视图中呈现细粒度细节，在保持重建和渲染效率的同时避免在远距离视图中混叠，我们应该根据采样球体的占用面积对其S(x, r)进行constructively 特征化，这与计算机图形学中的区域采样(即预滤波)的动机相似。在instant-ngp中提出的哈希编码[37]可以通过查找哈希表和三线性插值来有效地表征采样点，然而，它不能轻易地扩展到表征球体S(x, r)。一个可行的解决方案是将超采样策略与哈希编码结合起来，以近似球体的特征。然而，超采样极大地增加了计算成本，出乎意料地牺牲了高效重构和渲染的能力。</p><p>为此，我们提出了一种新颖的Tri-Mip编码，由三个可训练的mipmap M参数化，以表征采样球体S(x, r):</p><p>$\begin{aligned}\mathbf{f}&amp;=\text{Tri-Mip}(\mathbf{x},\mathbf{r};\mathcal{M}),\\\mathcal{M}&amp;=\{\mathcal{M}_{XY},\mathcal{M}_{XZ},\mathcal{M}_{YZ}\}.\end{aligned}$</p><p>如图2所示，Tri-Mip编码使用正交投影将三维空间分解为三个平面(planeXY、planeXZ和planeYZ)，然后用mipmap ($\mathcal{M}_{XY},\mathcal{M}_{XZ},\mathcal{M}_{YZ}$)表示每个平面，对预滤波的特征空间进行建模。对于每个mipmap，基层$M^{L_{0}}$是一个形状为H × W × C的特征图，其中H、W、C分别为通道的高度、宽度和数量。基层$M^{L_{0}}$随机初始化，可在重构过程中进行训练，其他层次$(M^{L_{i}}, i = 1,2，…， N)$是通过先前水平$M^{L_{i−1}}$沿高度和宽度降2倍而得到。该预滤波策略保持了mipmap各层之间的一致性，这是实现重建目标在不同距离上的相干性的关键。</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230726151646.png" alt="image.png"></p><p>为了查询球体S(x, r)对应的特征向量f，我们首先将S正交投影到三个平面上，得到三个discs圆盘$\mathcal{D} = \{\mathcal{D}_{XY}, \mathcal{D}_{XZ}, \mathcal{D}_{YZ}\}$，如图2所示。对于每个圆盘，我们从相应的mipmap中查询一个特征向量。<br>以磁盘$\mathcal{D}_{XY}$为例，我们从mipmap $\mathcal{M}_{XY}$中查询其特征$f_{XY}$。基于正交投影的性质，圆盘$\mathcal{D_{XY}}$与被采样球体具有相同的半径r，并且DXY的中心xDXY的二维坐标为被采样球体中心x(x, y, z)的偏坐标(x, y)。对于mipmap $\mathcal{M}_{XY}$的query level $l$，我们将其赋值为:</p><script type="math/tex; mode=display">\begin{aligned}&l=log_{2}\left(\frac{\mathbf{r}}{\ddot{r}}\right), \\&\ddot{r}=\sqrt{\frac{(\mathcal{B}_{max}-\mathcal{B}_{min})_X\cdot(\mathcal{B}_{max}-\mathcal{B}_{min})_Y}{HW\cdot\pi}},\end{aligned}</script><ul><li>$\ddot{r}$为基层$\mathcal{M}^{L_{0}}$特征元素的半径</li><li>$\mathcal{B}_max$ 和$\mathcal{B}_{min}$分别是interested-3D space的AABB(Axis Aligned Bounding Box)最大和最小角</li><li>目的是将球体的半径r与mipmap $\mathcal{M}^{l}_{XY}$的某level 的特征元素的半径进行匹配。</li></ul><p>在获得查询坐标(x, y, l)后，我们可以通过三线性插值从mipmap $\mathcal{M}_{XY}$得到特征向量$f_{XY}$。如图2所示，我们首先找到mipmap的两个最近的层$\mathcal{M}_{XY}^{\lfloor l\rfloor}$和$\mathcal{M}_{XY}^{\lceil l\rceil}$然后我们将圆盘$\mathcal{D}_{XY}$的中心坐标(x, y)投影到mipmap的两层(以红点表示);接下来，我们分别找到它们的四个邻居(用橙色点表示);最后，我们根据八个相邻点到光盘$\mathcal{D}_{XY}$中心的距离进行插值，得到特征向量$f_{XY}$。三线性插值不仅提高了层次和空间分辨率的有效精度，而且产生了连续的编码空间，有利于提高训练效率。同样，我们可以分别得到圆盘$\mathcal{D}_{XZ}$和$\mathcal{D}_{YZ}$的特征向量$f_{XZ}$和$f_{YZ}$。采样球体S的最终查询特征向量f是三个圆盘特征向量$\{f_{XY}, f_{XZ}, f_{YZ}\}$的连接。</p><p>我们的 Tri-Mip 编码以预过滤的方式有效地对 3D 空间进行特征化，以便我们可以对体积渲染执行区域采样以产生没有混叠的高质量渲染。特征查询过程也很有效，即在现代 GPU 中查询 mipmap是已经高度优化的，这促进了快速重建。<br>此外，我们的 Tri-Mip 编码的存储是三个 2D 特征图，即三个 mipmap $M^{l_{0}}$ 的基本级别，因为<strong>其他级别由基础级别通过downscaling导出</strong>，这使得我们的模型足够紧凑以便于分布。请注意，Tri-Mip 编码还促进了训练的收敛速度比隐式表示 MLP 中的场景更快，例如，我<strong>们的方法只需要 25K 次迭代收敛</strong>，而 MipNeRF [3] 需要 1M 次迭代，<strong>因为 mipmap M 中的特征可以直接优化，而不是通过 MLP 的可优化权重从 IPE 映射</strong>。</p><h2 id="Hybrid-Volume-Surface-Rendering"><a href="#Hybrid-Volume-Surface-Rendering" class="headerlink" title="Hybrid Volume-Surface Rendering"></a>Hybrid Volume-Surface Rendering</h2><p>虽然我们的方法可以有效地重建辐射场，但直接在消费级gpu上进行渲染，例如。Nvidia RTX 3060显卡只能达到30 FPS左右。这是因为体积渲染固有地为每个像素在锥体内采样多个球体，尽管我们可以通过二进制占用网格跳过一些采样。<br>观察到多边形网格的高效栅格化对实时表面渲染的好处，我们开发了一种<strong>混合体面渲染策略</strong>来进一步提高渲染速度除了重建的辐射场外，我们的混合体面渲染策略还需要一个代理网格来有效地确定从相机光学中心到物体的粗略距离。幸运的是，我们可以通过在重建的密度场上marching cubes<a href="https://readpaper.com/paper/2229412420">[PDF] Marching cubes: A high resolution 3D surface construction algorithm-论文阅读讨论-ReadPaper</a>，然后mesh decimation网格抽取来获得代理网格。我们的Tri-MipRF生成的代理网格即使在复杂的结构细节中也具有高保真的质量，如图4 (a)的左侧所示，而Instant-ngp[37]和neus[49]生成的结果作为参考显示在右侧。</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230726190259.png" alt="image.png"></p><p>一旦代理网格可用，我们首先对其进行有效栅格化，以获得圆锥体中轴线表面上的命中点(如图4 (b)所示)，然后我们在距圆锥体中轴线命中点∆t距离内均匀采样球体，这产生2∆t采样间隔。这种混合体面渲染策略显著减少了样本数量，从而在消费者级gpu上实现实时渲染(&gt;60 FPS)。请参阅补充材料中的视频，以获得实时交互式渲染演示</p><h1 id="Experimental-Evaluation"><a href="#Experimental-Evaluation" class="headerlink" title="Experimental Evaluation"></a>Experimental Evaluation</h1><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><ul><li>我们将每个像素的损失按其在图像平面上的足迹面积进行缩放，称为“面积损失”：$\mathcal{L}_{area}$</li><li>使用tiny-cuda-nn扩展来实现</li><li>使用nvdiffrast库实现Tri-Mip编码<a href="https://www.zhihu.com/">[PDF] Modular Primitives for High-Performance Differentiable Rendering-论文阅读讨论-ReadPaper (zhihu.com)</a></li><li>mipmap基层形状$M^{L_{0}}$被经验地设置为H = 512, W = 512, C = 16</li><li>MLP:<ul><li>使用AdamW优化器：25K iteration<ul><li>weight decay set to $1 × 10^{−5}$</li><li>learning ratelearning ：$2 × 10^{-3}$</li><li>MultiStepLR scheduled</li></ul></li></ul></li><li>Encoding:<ul><li>lr : $2 × 10^{-2}$</li></ul></li></ul><h2 id="Evaluation-on-the-Multi-scale-Blender-Dataset"><a href="#Evaluation-on-the-Multi-scale-Blender-Dataset" class="headerlink" title="Evaluation on the Multi-scale Blender Dataset"></a>Evaluation on the Multi-scale Blender Dataset</h2><p>比较指标：</p><ul><li>PSNR,</li><li>SSIM <a href="https://readpaper.com/paper/2133665775">[PDF] Image Quality Assessment: From Error Visibility to Structural Similarity-论文阅读讨论-ReadPaper</a></li><li>VGG LPIPS<a href="https://readpaper.com/paper/2783879794">[PDF] The Unreasonable Effectiveness of Deep Features as a Perceptual Metric-论文阅读讨论-ReadPaper</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Efficiency </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Efficiency </tag>
            
            <tag> TensorDecomposition </tag>
            
            <tag> Sampling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mip-NeRF 360</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/LargeScaleScene/Mip-NeRF%20360/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/LargeScaleScene/Mip-NeRF%20360/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://jonbarron.info/">Jonathan T. Barron</a>  <a href="http://bmild.github.io/">Ben Mildenhall</a>  <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a> <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>  <a href="https://phogzone.com/">Peter Hedman</a></td></tr><tr><td>Conf/Jour</td><td>CVPR 2022 (Oral Presentation)</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://jonbarron.info/mipnerf360/">mip-NeRF 360 (jonbarron.info)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4606723627252981761&amp;noteId=1881063981903029504">Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields (readpaper.com)</a></td></tr></tbody></table></div><ul><li>novel Kalman-like scene parameterization：将场景参数化，将单位球外背景采样的截头锥参数化到r=2的球体内，单位球内的截头锥不受影响</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230722144216.png" alt="image.png"></p><ul><li>efficient proposal-based coarse-to-fine distillation framework：一个提议网络用来获取权重，用来进行精采样，再通过精采样的点根据NeRF 的MLP得到密度和颜色值</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230722152752.png" alt="image.png"></p><ul><li>regularizer designed for mipNeRF ray intervals：可以有效消除floaters(体积密集空间中不相连的小区域)和背景塌陷(远处的表面被错误地模拟成靠近相机的密集内容的半透明云)</li></ul><p>$\begin{gathered}\mathcal{L}_{\mathrm{dist}}(\mathbf{s},\mathbf{w}) =\sum_{i,j}w_iw_j\left|\frac{s_i+s_{i+1}}{2}-\frac{s_j+s_{j+1}}{2}\right| \+\frac13\sum_iw_i^2(s_{i+1}-s_i) \end{gathered}$</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230722154733.png" alt="image.png"></p><span id="more"></span><h1 id="DC"><a href="#DC" class="headerlink" title="DC"></a>DC</h1><p>Limitations. Though mip-NeRF 360 significantly outperforms mip-NeRF and other prior work, it is not perfect. </p><ul><li>Some <strong>thin structures and fine details</strong> may be missed, such as the tire spokes轮胎辐条 in the bicycle scene (Figure 5), or the 树叶上纹理veins on the leaves in the stump scene (Figure 7).</li><li>View synthesis <strong>quality will likely degrade if the camera is moved far from the center of the scene</strong>. And, like most NeRF-like models, recovering a scene <strong>requires several hours of training</strong> on an accelerator, precluding on-device training.</li></ul><p>Conclusion<br>We have presented mip-NeRF 360, a mip-NeRF extension designed for real-world scenes with unconstrained camera orientations. Using a <strong>novel Kalman-like scene parameterization</strong>, an <strong>efficient proposal-based coarse-to-fine distillation framework</strong>, and a <strong>regularizer designed for mipNeRF ray intervals</strong>, we are able to synthesize realistic novel views and complex depth maps for challenging unbounded real-world scenes, with a 57% reduction in mean-squared error compared to mip-NeRF.</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>尽管神经辐射场(NeRF)在物体和小范围空间区域上展示了令人印象深刻的视图合成结果，但在“无界”场景上，它们很难实现，因为摄像机可能指向任何方向，内容可能存在于任何距离</p><p>在这种情况下，现有的类似nerf的模型经常产生模糊或低分辨率的渲染图(由于近处和远处物体的细节和比例不平衡)，训练速度慢，并且由于从一小组图像重建大场景的任务固有的模糊性，可能会exhibit artifacts</p><p>我们提出了mip-NeRF(一种解决采样和混叠的NeRF变体)的扩展，它使用<strong>非线性场景参数化</strong>、<strong>在线蒸馏</strong>和一种新的<strong>基于扭曲的正则化</strong>来克服无界场景带来的挑战</p><p>我们的模型，我们称之为“mip-NeRF 360”，因为我们的目标场景中，摄像机围绕一个点旋转360度，与mip-NeRF相比，减少了57%的平均误差，并且能够为高度复杂的、无限的现实场景生成逼真的合成视图和详细的深度图</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>神经辐射场(NeRF)通过在基于坐标的多层感知器(MLP)的权重内编码场景的体积密度和颜色来合成高度逼真的场景渲染。这种方法在真实感视图合成方面取得了重大进展。然而，NeRF使用沿着光线的无限小的3D点对MLP的输入进行建模，这会导致在呈现不同分辨率的视图时产生混淆。Mip-NeRF解决了这个问题，将NeRF扩展到沿锥体[3]的体积截锥。虽然这提高了质量，<strong>但在处理无界场景时，NeRF和mip-NeRF都会遇到困难，因为相机可能面对任何方向，场景内容可能存在于任何距离</strong>。在这项工作中，我们提出了一个扩展到mip-NeRF，我们称之为“mip-NeRF 360”，能够产生这些无界场景的逼真渲染，如图1所示。</p><p>将类似nerf的模型应用于大型无界场景会引发三个关键问题:</p><ul><li>Parameterization.参数化：对于前景和背景的参数化处理。无界360度场景可以占据任意大的欧几里得空间区域，但mip-NeRF要求3D场景坐标位于有界域内。<ul><li>参数化。由于透视投影的原因，一个物体放置在远离相机的地方会占据图像平面的一小部分，但如果放置在离相机较近的地方，则会占据更多的图像，并且可以看到细节。因此，理想的3D场景参数化应该为附近的内容分配更多的容量，而为远处的内容分配更少的容量。</li><li>在NeRF之外，传统的视图合成方法通过在<strong>投影全景空间中参数化场景</strong>来解决这个问题[2,4,9,16,23,27,36,46,54]，或者通过将场景内容嵌入到一些使用多视图立体恢复的代理几何中[17,26,41]。NeRF成功的一个方面是它将特定场景类型与适当的3D参数化相结合。最初的NeRF论文[33]专注于360度捕捉具有遮罩背景的物体，以及所有图像朝向大致相同方向的正面场景。对于被遮挡的物体，NeRF直接在三维欧几里德空间中参数化场景，但对于面向前方的场景（LLFF），NeRF使用在射影空间中定义的坐标(归一化设备坐标，或“NDC”[5])。通过将一个无限深的相机截锥体扭曲成一个有界的立方体，其中沿z轴的距离对应于视差(逆距离)，NDC有效地重新分配了NeRF MLP的容量，这种方式与透视投影的几何形状一致。</li><li>然而，在所有方向上都是无界的场景，而不仅仅是在一个方向上，需要不同的参数化。nerf++[51]和DONeRF[34]探索了这一想法，nerf++[51]使用了一个额外的网络来模拟远处的物体，DONeRF[34]提出了一个空间扭曲过程，以缩小距离原点的点。这两种方法的行为在某种程度上类似于NDC，但在每个方向上，而不仅仅是沿着z轴。在这项工作中，我们将这一思想扩展到mip-NeRF，并提出了一种将任何平滑参数化应用于volumes(而不是点)的方法，也提出了我们自己的无界场景参数化。</li></ul></li><li>Efficiency.效率。大型和详细的场景需要更多的网络容量，但是在训练期间沿着每条射线密集地查询大型MLP是昂贵的。<ul><li>处理无界场景的一个基本挑战是，这样的场景通常很大，而且细节很详细。虽然类似NeRF的模型可以使用令人惊讶的少量权重准确地再现场景的对象或区域，但当面对日益复杂的场景内容时，NeRF MLP的容量会饱和。此外，更大的场景需要沿着每条射线进行更多的样本来精确地定位表面。例如，当将NeRF从物体缩放到建筑物时，MartinBrualla et al.[30]将MLP隐藏单元的数量增加了一倍，并将MLP评估的数量增加了8倍。这种模型容量的增加是昂贵的- NeRF已经需要几个小时来训练，并且将这个时间乘以额外的~ 40×对于大多数用途来说是非常缓慢的。</li><li>NeRF和mip-NeRF使用的从粗到细的重采样策略加剧了这种训练成本:mlp使用“粗”和“细”射线间隔多次评估，并使用两次的图像重建损失进行监督。这种方法是浪费的，因为场景的“粗糙”渲染对最终图像没有贡献。我们将训练两个MLP:一个“提案MLP”和一个“NeRF MLP”，而不是训练一个在多个尺度上监督的NeRF MLP。The proposal MLP预测体积密度(但不是颜色)，这些密度用于重新采样提供给NeRF MLP的新间隔，然后渲染图像。至关重要的是，<strong>提议的MLP产生的权重不是使用输入图像进行监督，而是使用NeRF MLP生成的直方图权重进行监督</strong>。这允许我们使用评估次数相对较少的大型NeRF MLP，以及评估次数较多的小型提案MLP。因此，我们的整个模型的总容量明显大于mip-NeRF的(~ 15倍)，从而大大提高了渲染质量，但我们的训练时间仅略微增加(~ 2倍)。</li><li>我们可以把这种方法看作是一种“在线蒸馏”:“蒸馏”通常指的是训练一个小网络来匹配一个已经训练好的大网络的输出[19]，在这里，我们通过同时训练两个网络，将NeRF MLP预测的输出结构提炼成“在线”的提议MLP。NeRV[47]对一个完全不同的任务执行类似的在线蒸馏:训练mlp近似渲染积分，以建模可见性和间接照明。我们的在线蒸馏方法在精神上类似于DONeRF中使用的“抽样oracle网络”，尽管该方法使用ground-truth深度进行监督[34]。在TermiNeRF[39]中使用了一个相关的想法，尽管这种方法只会加速推理，实际上会减慢训练速度(NeRF被训练到收敛，然后再训练一个额外的模型)。在NeRF中详细探讨了一个学习的“提议者”网络[1]，但只实现了25%的加速，而我们的方法将训练加速了300%。</li><li>一些作品试图将训练好的NeRF提取或“烘烤”成可以快速呈现的格式[18,40,50]，但这些技术并不能加速训练。通过八阶数据结构(如八阶树[43]或包围体层次[42])加速光线跟踪的想法在渲染文献中得到了很好的探索，尽管这些方法假设了场景几何的先验知识，因此不能自然地推广到逆向渲染上下文，其中场景几何是未知的，必须恢复。事实上，尽管在优化类nerf模型的同时构建了一个八叉树加速结构，神经稀疏体素场方法并没有显著减少训练时间[28]。(只是提高了推理效率，而没有提高训练效率)</li></ul></li><li>Ambiguity.歧义。无界场景的内容可能位于任何距离，只会被少量光线观察到，这加剧了从2D图像重建3D内容的固有模糊性。<ul><li>虽然NeRF传统上是使用场景的许多输入图像进行优化的，但从新的相机角度恢复NeRF以产生逼真的合成视图的问题仍然从根本上受到限制-an infinite family of NeRFs可以解释输入图像，但只有一小部分可以产生可接受的新视图结果。例如，NeRF可以通过简单地在其各自的相机前将每个图像重建为纹理平面来重建所有输入图像。原始的NeRF论文通过在整流器之前向NeRF MLP的密度头注入高斯噪声来正则化模糊场景[33]，这促使密度向零或无穷大方向倾斜。虽然这通过阻止半透明密度减少了一些“漂浮物”，但我们将表明，这对于我们更具挑战性的任务是不够的。NeRF的其他正则化器已经被提出，例如密度上的鲁棒损失[18]或表面上的平滑惩罚[35,53]，但这些解决方案解决的问题与我们的不同(分别是渲染缓慢和表面不光滑)。此外，这些正则化器是为NeRF使用的点样本设计的，而我们的方法是为了处理沿着每个mip-NeRF射线定义的连续权重。</li><li>我们将使用一个由具有挑战性的室内和室外场景组成的新数据集来展示我们对先前工作的改进。我们敦促读者观看我们的补充视频，因为我们的结果是最好的欣赏动画。</li></ul></li></ul><h1 id="Preliminaries-mip-NeRF"><a href="#Preliminaries-mip-NeRF" class="headerlink" title="Preliminaries: mip-NeRF"></a>Preliminaries: mip-NeRF</h1><p>mip-NeRF：通过将光线分成一组间隔$T_{i} = [t_{i}, t_{i+1}]$，对于每个区间，计算对应于该区间的圆锥截体的<strong>均值和协方差</strong>（将圆锥截体近似为高阶高斯分布函数），圆锥截体的半径由射线的焦距和图像平面上的像素大小决定<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230722140543.png" alt="image.png"></p><p>使用集成位置编码来表征这些值:<br>$\gamma(\mathbf{\mu},\mathbf{\Sigma})=\left\{\begin{bmatrix}\sin(2^\ell\mathbf{\mu})\exp\left(-2^{2\ell-1}\operatorname{diag}(\mathbf{\Sigma})\right)\\\cos(2^\ell\mathbf{\mu})\exp\left(-2^{2\ell-1}\operatorname{diag}(\mathbf{\Sigma})\right)\end{bmatrix}\right\}_{\ell=0}^{L-1}$</p><p>经过MLP得到密度和颜色：$\forall T_i\in\mathrm{t},\quad(\tau_i,\mathbf{c}_i)=\mathrm{MLP}(\gamma(\mathbf{r}(T_i));\Theta_\mathrm{NeRF}).$</p><p>使用体渲染函数，根据权重计算像素的颜色值：</p><p>$\begin{gathered}\mathbf{C}(\mathbf{r},\mathbf{t})=\sum_iw_i\mathbf{c}_i,\\w_i=\left(1-e^{-\tau_i(t_{i+1}-t_i)}\right)e^{-\sum_{i’&lt;i}\tau_{i’}\left(t_{i’+1}-t_{i’}\right)}\end{gathered}$</p><ul><li><p>采样方式：</p><ul><li>粗采样：均匀采样$t^c\sim\mathcal{U}[t_n,t_f],\quad\mathbf{t}^c=\operatorname{sort}(\{t^c\}).$</li><li>精采样：$t^f\sim\operatorname{hist}(\mathbf{t}^c,\mathbf{w}^c),\quad\mathbf{t}^f=\operatorname{sort}(\{t^f\}).$</li></ul></li><li><p>loss：$\sum_{\mathrm{r}\in\mathcal{R}}\frac{1}{10}\mathcal{L}_{\mathrm{recon}}(\mathbf{C}(\mathbf{r},\mathbf{t}^{c}),\mathbf{C}^{<em>}(\mathbf{r}))+\mathcal{L}_{\mathrm{recon}}(\mathbf{C}(\mathbf{r},\mathbf{t}^{f}),\mathbf{C}^{</em>}(\mathbf{r}))$</p><ul><li>recon: mean squared error.</li><li>R为光线几何</li><li>$C^{*}$为ground truth</li></ul></li></ul><h1 id="Scene-and-Ray-Parameterization"><a href="#Scene-and-Ray-Parameterization" class="headerlink" title="Scene and Ray Parameterization"></a>Scene and Ray Parameterization</h1><h2 id="重新参数化高斯函数"><a href="#重新参数化高斯函数" class="headerlink" title="重新参数化高斯函数"></a>重新参数化高斯函数</h2><p>定义一个f(x): $\mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ 映射的平滑坐标变换(n=3)</p><p>compute the linear approximation: $f(\mathbf{x})\approx f(\mathbf\mu)+\mathbf{J}_f(\mathbf\mu)(\mathbf{x}-\mathbf\mu)$</p><p>$\mathbf{J}_f(\boldsymbol\mu)$为f at $\mu$的雅克比矩阵，即对均值和方差：$f(\mu,\Sigma)=\begin{pmatrix}f(\mu),\mathbf{J}_f(\mu)\Sigma\mathbf{J}_f(\mu)^\mathrm{T}\end{pmatrix}$</p><p>这在功能上等价于经典的扩展卡尔曼滤波器[21]，其中f是状态转移模型。我们对f的选择是如下的缩略式:<br>$\operatorname{contract}(\mathbf{x})=\begin{cases}\mathbf{x}&amp;|\mathbf{x}|\leq1\\\left(2-\frac{1}{|\mathbf{x}|}\right)\left(\frac{\mathbf{x}}{|\mathbf{x}|}\right)&amp;|\mathbf{x}|&gt;1\end{cases}$<br>这种设计与NDC的动机相同:距离点应该按视差(逆距离)而不是距离成比例分布</p><p>在我们的模型中，我们不是按照公式1在欧几里得空间中使用mip-NeRF的IPE特征，而是在这个收缩空间中使用类似的特征(见附录):γ(contract(μ， Σ))。图2显示了该参数化。</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230722144216.png" alt="image.png"><br>我们的场景参数化的2D可视化。我们定义了一个contract(·)操作符(公式10，如箭头所示)，它将坐标映射到一个半径为2(橙色)的球上，其中半径为1(蓝色)内的点不受影响。我们将这种收缩应用于欧几里得3D空间中的mip-NeRF高斯函数(灰色椭圆)，类似于卡尔曼滤波器，以产生我们的收缩高斯函数(红色椭圆)，其中心保证位于半径为2的球内。contract(·)的设计结合我们根据视差线性选择空间射线间隔，意味着从位于场景原点的摄像机投射的射线在橙色区域具有等距间隔，如图所示。</p><h2 id="如何选择ray距离"><a href="#如何选择ray距离" class="headerlink" title="如何选择ray距离"></a>如何选择ray距离</h2><p>除了如何参数化3D坐标的问题，还有如何选择射线距离的问题。在NeRF中，这通常是通过从近平面和远平面均匀采样来完成的，如公式5所示。然而，如果使用NDC参数化，这个等间距的样本系列实际上是均匀间隔的反深度(视差)。这种设计决策非常适合于当相机只面向一个方向时的无界场景，但不适用于所有方向都无界的场景。因此，我们将显式地以视差线性采样距离t(参见<a href="https://readpaper.com/paper/2943054120">[PDF] Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines-论文阅读讨论-ReadPaper</a>了解此间距的详细动机)。</p><p>为了根据视差参数化射线，我们定义了欧氏射线距离t和“标准化”射线距离之间的可逆映射$s\triangleq\frac{g(t)-g(t_n)}{g(t_f)-g(t_n)},t\triangleq g^{-1}(s\cdot g(t_f)+(1-s)\cdot g(t_n)),$</p><ul><li>其中g(·)是可逆标量函数。这给了我们“标准化”的射线距离s∈[0,1]，映射到[tn, tf]。在本文中，我们将参考沿t空间或s空间的射线的距离，这取决于哪个更方便或直观。</li><li>通过设置g(x) = 1/x并构造均匀分布在空间中的射线样本，我们得到的射线样本的t-距离以视差线性分布(另外，设置g(x) = log(x)产生DONeRF的对数间距[34])。</li><li>在我们的模型中，不是使用t个距离来执行粗精采样，而是使用s个距离来执行。这意味着，不仅我们的初始样本在视差上是线性间隔的，而且从权重w的各个间隔进行的后续重采样也将以类似的方式分布。从图2中心的摄像机可以看出，射线样本的线性视差间距抵消了contract(·)。实际上，我们已经用反向深度间距共同设计了我们的场景坐标空间，这给了我们无界场景的参数化，这与原始NeRF论文的高效设置非常相似:有界空间中均匀间隔的射线间隔。</li></ul><h1 id="Coarse-to-Fine-Online-Distillation"><a href="#Coarse-to-Fine-Online-Distillation" class="headerlink" title="Coarse-to-Fine Online Distillation"></a>Coarse-to-Fine Online Distillation</h1><p>如前所述，mip-NeRF使用了一种从粗到细的重采样策略(图3)，其中使用“粗”射线间隔和“细”射线间隔对MLP进行一次评估，并使用两个级别的图像重建损失进行监督。</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230722152752.png" alt="image.png"></p><p>我们转而训练两个MLP，一个“NeRF MLP”ΘNeRF(其行为类似于NeRF和mip-NeRF使用的MLP)和一个“提议MLP”Θprop。提案MLP预测体积密度，根据公式4将其转换为提案权重向量，但不预测颜色。这些建议的权值用于采样s区间，然后提供给NeRF MLP, NeRF MLP预测自己的权值向量w(和颜色估计，用于渲染图像)。<br>至关重要的是，提议 MLP 没有经过训练来重现输入图像，而是经过训练以绑定 NeRF MLP 产生的权重 w。这两个 MLP 都是随机初始化和联合训练的，因此这种监督可以被认为是 <strong>NeRF MLP 知识的一种“在线蒸馏”到提议 MLP</strong>。我们使用大型 NeRF MLP 和小型提案 MLP，并从具有许多样本的提案 MLP 中反复评估和重新采样（为清楚起见，一些图形和讨论仅说明了单个重采样），但仅使用较小的样本集评估 NeRF MLP一次。这为我们提供了一种表现的模型，尽管它的容量比 mip-NeRF 高得多，但训练成本仅略高。正如我们将展示的那样，<strong>使用一个小的 MLP 来模拟提议分布不会降低准确性</strong>，这表明提取 NeRF MLP 比视图合成更容易的任务。</p><p>这种在线蒸馏需要一个损失函数，该函数鼓励提议的$MLP(\hat t, \hat w)$和NeRF MLP(t, w)发出的直方图保持一致。起初，这个问题可能看起来微不足道，因为最小化两个直方图之间的不相似性是一个很好的任务，但是回想一下，这些直方图t和t的“箱”不需要相似——事实上，如果提议的MLP成功地剔除了场景内容存在的距离集，t和$\hat t$将是高度不相似的。尽管文献中包含了许多测量具有相同箱位的两个直方图之间差异的方法[12,29,38]，但我们的研究相对较少。这个问题是具有挑战性的，因为我们不能假设内容在一个直方图bin内的分布:一个非零权重的区间可能表明权重在整个区间内的均匀分布，一个位于该区间内任何位置的函数，或者无数其他分布。因此，<strong>我们在以下假设下构建损失:</strong></p><ul><li>如果两个直方图都可以用任何单一的质量分布来解释，那么损失一定为零。只有当两个直方图不可能反映相同的“真实的”连续底层质量分布时，才会发生非零损失。请参阅附录以获得该概念的可视化。</li><li>要做到这一点，我们首先定义一个函数，计算与区间T重叠的所有建议权重的和:$\mathrm{bound}\big(\hat{\mathbf{t}},\hat{\mathbf{w}},T\big)=\sum_{j:T\cap\hat{T}_j\neq\emptyset}\hat{w}_j.$</li><li>如果两个直方图相互一致，则对于(t, w)中的所有区间$(T_{i}, w_{i})$，必须成立$w_{i}≤bound(\hat{\mathbf{t}},\hat{\mathbf{w}}, T_{i})$。这一性质类似于测度论[14]中外测度的可加性性质。我们的损失会惩罚任何违反这个不等式并超过这个界限的剩余直方图质量:</li><li>$\mathcal{L}_{\mathrm{prop}}\big(\mathbf{t},\mathbf{w},\hat{\mathbf{t}},\hat{\mathbf{w}}\big)=\sum_i\frac{1}{w_i}\max\big(0,w_i-\mathrm{bound}\big(\hat{\mathbf{t}},\hat{\mathbf{w}},T_i\big)\big)^2$</li><li>这种损失类似于统计学和计算机视觉中经常使用的方形直方图距离的半二次型。这种损失是不对称的，因为我们只想惩罚那些低估了NeRF MLP所隐含的分布的提案权重——高估是意料之中的，因为提案权重可能比NeRF权重更粗糙，因此会在其上形成一个上限包络。除以$w_{i}$保证了当边界为零时，这个损失相对于边界的梯度是一个常数值，这导致了性能良好的优化。由于t和$\hat t$已排序，因此可以通过使用求和面积表[11]高效地计算公式13$\mathcal{L}_{prop}$。请注意，这种损失对于距离t的单调变换是不变的(假设w和$\hat w$已经在t空间中计算过)，所以无论应用于欧几里得射线距离还是标准化射线s距离，它的行为都是相同的。</li><li>我们在NeRF直方图(t, w)和所有提案直方图$(\hat t^{k},\hat w^{k})$之间施加这种损失。与mip-NeRF一样，NeRF MLP使用带有输入图像$\mathcal{L}_{recon}$的重建损失进行监督。在计算$\mathcal{L}_{prop}$时，我们在NeRF MLP的输出t和w上放置一个停止梯度，使NeRF MLP“领先”，而提案MLP“跟随”-否则NeRF可能会产生更差的场景重建，从而使提案MLP的工作变得不那么困难。这种提议监督的效果可以从图4中看到，其中NeRF MLP逐渐将其权重w定位在场景中的一个表面周围，<strong>而提案MLP“catches up”并预测包含NeRF权重的粗提案直方图</strong>。</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230722154306.png" alt="image.png"></p><h1 id="Regularization-for-Interval-Based-Models"><a href="#Regularization-for-Interval-Based-Models" class="headerlink" title="Regularization for Interval-Based Models"></a>Regularization for Interval-Based Models</h1><p>由于姿态不佳，训练有素的nerf经常表现出两种特征伪影，我们称之为“floaters”和“背景塌陷”，如图5(a)所示。我们所说的“floaters”指的是<strong>体积密集空间中不相连的小区域</strong>，它们可以解释输入视图子集的某些方面，但从另一个角度来看，它们看起来就像模糊的云。所谓“背景塌缩”，我们指的是一种现象，在这种现象中，<strong>远处的表面被错误地模拟成靠近相机的密集内容的半透明云</strong>。在这里，我们提出了一种正则化器，如图5所示，它比NeRF使用的向体积密度[33]注入噪声的方法更有效地防止浮动和背景崩溃。</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230722154733.png" alt="image.png"></p><p>我们的正则化器有一个简单的定义，它是由一组(标准化的)射线距离s和参数化每条射线的权重w定义的阶跃函数:<br>$\mathcal{L}_{\mathrm{dist}}(\mathbf{s},\mathbf{w})=\iint\limits_{-\infty}^\infty\mathbf{w}_\mathbf{s}(u)\mathbf{w}_\mathbf{s}(v)|u-v|d_ud_v,$<br>$\mathbf{w}_\mathbf{s}(u)=\sum_iw_i\mathbb{1}_{[s_i,s_{i+1})}(u).$</p><ul><li>我们使用归一化射线距离s，因为使用t会显著提高远间隔的权重，并导致近间隔被有效忽略。这个损失是沿着这个1D阶跃函数的所有点对之间的距离的积分，由NeRF MLP分配给每个点的权重w缩放。我们称之为“失真”，因为它类似于k-means最小化失真的连续版本(尽管它也可以被认为是最大化一种自相关)。</li><li>这种损失可以通过设置w = 0来最小化(回想一下，w的总和不大于1，而不完全是1)。如果这是不可能的(即，如果射线是非空的)，则可以通过将权重合并到尽可能小的区域来最小化。图6通过在toy直方图上显示这种损失的梯度来说明这种行为。</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230722155249.png" alt="image.png"></p><p>我们的损失鼓励每条射线尽可能紧凑:<br>1)最小化每个间隔的宽度，<br>2)将距离较远的间隔相互拉近，<br>3)将权重整合到单个间隔或少量邻近的间隔中，<br>4)在可能的情况下(例如当整个射线未被占用时)使所有权重趋近于零。</p><p>由于ws(·)在每个区间内都有一个常数值，我们可以将式14重写为:</p><script type="math/tex; mode=display">\begin{gathered}\mathcal{L}_{\mathrm{dist}}(\mathbf{s},\mathbf{w}) =\sum_{i,j}w_iw_j\left|\frac{s_i+s_{i+1}}{2}-\frac{s_j+s_{j+1}}{2}\right| \\+\frac13\sum_iw_i^2(s_{i+1}-s_i) \end{gathered}</script><p>在这种形式下，我们的失真损失是微不足道的计算。这种重新表述也为这种损失的行为提供了一些直观的理解:第一项最小化所有区间中点对之间的加权距离，第二项最小化每个单独区间的加权大小。</p><h1 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h1><p>我们使用具有4层和256个隐藏单元的提议MLP和具有8层和1024个隐藏单元的NeRF MLP，两者都使用ReLU内部激活和密度τ的softplus激活。</p><p>我们对提议的MLP进行两个阶段的评估和重新采样，每个阶段使用64个样本来生产$(\hat s^{0}, \hat w^{0})$ 和$(\hat s^{1}, \hat w^{1})$, 然后使用32个样本对NeRF MLP进行一个阶段的评估，以产生(s, w)。我们将以下损失最小化: $\mathcal{L}_{\text{recon}}(\mathbf{C}(\mathbf{t}),\mathbf{C}^*)+\lambda\mathcal{L}_{\text{dist}}(\mathbf{s},\mathbf{w})+\sum_{k=0}^1\mathcal{L}_{\text{prop}}(\mathbf{s},\mathbf{w},\hat{\mathbf{s}}^k,\hat{\mathbf{w}}^k)$ 每批中所有光线的平均值(光线不包括在我们的符号中)</p><p>λ超参数平衡了我们的数据项$\mathcal{L}_{recon}$和正则器$\mathcal{L}_{dist}$;所有实验均设λ = 0.01。$\mathcal{L}_{prop}$中使用的停止梯度使Θprop的优化独立于ΘNeRF的优化，因此不需要超参数来缩放$\mathcal{L}_{prop}$的效果。对于$\mathcal{L}_{recon}$，我们使用Charbonnier loss[10] :$\sqrt{(x−x^{∗})^{2} + \epsilon^{2}} ,\epsilon= 0.001$，这比mip-NeRF中使用的均方误差实现了稍微稳定的优化。我们使用稍微修改过的mip-NeRF学习时间表来训练我们的模型(以及所有报告的类似nerf的基线):使用Adam[24]，使用超参数$β_{1} = 0.9， β_{2} = 0.999， \epsilon = 10^{−6}$，进行250k次优化迭代，批大小为214，学习率从$2 × 10^{−3}$到$2 × 10^{−5}$对数线性退火，预热阶段为512次迭代，梯度裁剪为$10^{−3}$范数。</p><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><p>我们在一个新的数据集上评估我们的模型:9个场景(5个室外和4个室内)，每个场景都包含一个复杂的中心物体或区域和详细的背景。在拍摄过程中，我们试图通过固定相机曝光设置，最小化照明变化，避免移动物体来防止光度变化-我们不打算探索“在野外”照片收集[30]所带来的所有挑战，只有比例。相机姿势估计使用COLMAP[45]</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230722162716.png" alt="image.png"></p><p>消融研究</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230722162649.png" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/LargeScaleScene </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Sampling </tag>
            
            <tag> LargeScaleScene </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mip-NeRF</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Sampling/Mip-NeRF/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Sampling/Mip-NeRF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</th></tr></thead><tbody><tr><td>Author</td><td>Jonathan T. Barron and Ben Mildenhall and Matthew Tancik and Peter Hedman and Ricardo Martin-Brualla and Pratul P. Srinivasan</td></tr><tr><td>Conf/Jour</td><td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</td></tr><tr><td>Year</td><td>2021</td></tr><tr><td>Project</td><td><a href="https://github.com/google/mipnerf">google/mipnerf (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4557703956856315905&amp;noteId=1753570799523020032">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230721125154.png" alt="image.png"></p><ul><li>一种新的采样方式：锥体采样<strong>conical frustums截头圆锥体</strong></li><li>基于PE提出了IPE，可以平滑地编码空间体积的大小和形状</li><li>将NeRF的粗精采样MLP合并为一个MLP</li></ul><p>IPE：当锥体区域较宽（正态分布很宽）时，会将高频的信息积分为0；当区域较窄时，保持原来的PEncoding<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/ipe_anim_horiz.gif" alt="ipe_anim_horiz.gif"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230721153610.png" alt="image.png"></p><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>We have presented mip-NeRF, a multiscale NeRF-like model that addresses the inherent <strong>aliasingh混叠</strong> of NeRF. NeRF works by casting <strong>rays</strong>, encoding the positions of points along those rays, and training separate neural networks at distinct scales.<br>In contrast, mip-NeRF casts <strong>cones</strong>, encodes the positions and sizes of conical frustums, and trains a single neural network that models the scene at multiple scales. By reasoning explicitly about sampling and scale, mip-NeRF is able to reduce error rates relative to NeRF by60% on our own multiscale dataset, and by 17% on NeRF’s single-scale dataset, while also being 7% faster than NeRF.Mip-NeRF is also able to match the accuracy of a bruteforce supersampled NeRF variant, while being 22× faster.<br>We hope that the general techniques presented here will be valuable to other researchers working to improve the performance of raytracing-based neural rendering models.</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>NeRF：使用一个像素对一条光线进行采样，当在不同分辨率下进行train和test渲染时，会产生excessively blurred or aliased</p><ul><li>如果很直接地对每个像素采用多条光线，计算量将倍增</li></ul><p>Mip-NeRF：tends NeRF to represent the scene at a continuously-valued scale</p><ul><li>By efficiently rendering anti-aliased conical frustums锥形截锥，减少了锯齿伪影</li><li>相比NeRF加速7%，且大小减半，mip-NeRF在NeRF数据集上降低了17%的平均错误率，在我们提出的数据集的具有挑战性的多尺度变体上降低了60%</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>Neural volumetric representations，eg:NeRF</li><li>NeRF采样策略导致过度模糊和混叠excessive blurring and aliasing</li><li>NeRF用连续的体积函数取代了传统的离散采样几何，参数化为多层感知器。为了渲染像素的颜色，NeRF通过该像素投射一条光线，并将其投射到其体积表示中，查询MLP中沿该光线采样的场景属性，并将这些值合成为单一颜色。</li><li>NeRF渲染在较少人为的场景中表现出明显的伪影。当训练图像以多种分辨率观察场景内容时，从恢复的NeRF中获得的渲染在近距离视图中显得过于模糊，并且在远距离视图中包含混联伪影。一个直接的解决方案是采用离线光线追踪中使用的策略:通过在其足迹中行进多条光线对每个像素进行超采样。但是对于神经体积表示(如NeRF)来说，这是非常昂贵的，它需要数百个MLP评估来渲染单个光线，并且需要几个小时来重建单个场景。</li><li>In this paper, we take <strong>inspiration</strong> from <strong>the mipmapping approach used to prevent aliasing</strong> in computer graphics rendering pipelines.<ul><li>mipmap表示一组不同的离散下采样尺度的信号(通常是图像或纹理图)，并根据像素足迹到该射线相交的几何图形的投影选择适当的尺度用于射线。这种策略被称为预滤波，因为抗混叠的计算负担从渲染时间(如在蛮力超采样解决方案中)转移到预计算阶段-无论纹理渲染多少次，都只需要为给定纹理创建一次mipmap。</li></ul></li><li>Mip-NeRF: extends NeRF to simultaneously represent the prefiltered radiance field for a continuousspace of scales.<ul><li>mip-NeRF的输入是一个三维高斯分布，它表示亮度场应该被积分的区域。如图1所示，然后，我们可以通过沿着一个圆锥的间隔查询mip-NeRF来呈现一个预过滤的像素，使用近似于像素对应的锥形截锥的高斯函数。为了对三维位置及其周围的高斯区域进行编码，我们提出了一种新的特征表示:集成位置编码(IPE)。这是NeRF的位置编码(PE)的一种推广，它允许空间的一个区域具有紧凑的特征，而不是空间中的单个点。</li><li>Mip-NeRF极大地提高了NeRF的准确性，并且在以不同分辨率观察场景内容的情况下(即相机移动到离场景更近或更远的设置)，这种好处甚至更大。在我们提出的具有挑战性的多分辨率基准上，mip-NeRF能够相对于NeRF平均降低60%的错误率(参见图2的可视化)。Mip-NeRF的尺度感知结构还允许我们将NeRF用于分层采样[30]的单独的“粗”和“细”MLP合并为单个MLP。因此，mip-NeRF比NeRF略快(约7%)，并且有一半的参数。</li></ul></li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>我们的工作直接扩展了NeRF[30]，这是一种非常有影响力的技术，用于从观察到的图像中学习3D场景表示，以合成新颖的逼真视图。<br>我们回顾了计算机图形学和视图合成中使用的3D表示，包括最近引入的连续神经表示，如NeRF，重点是<strong>采样和混叠</strong>。</p><ul><li>Anti-aliasing in Rendering<ul><li>采样和混叠是计算机图形学中渲染算法发展过程中被广泛研究的基本问题。减少混叠(“抗混叠”)通常是通过<strong>超采样或预滤波</strong>来完成的。</li><li>基于超采样的技术[46]在渲染时每像素投射多条光线，以便采样更接近<strong>奈奎斯特频率</strong>。这是一种减少混叠的有效策略，但代价很高，因为运行时间通常随超采样率线性扩展。因此，超采样通常只在脱机渲染上下文中使用。而不是采样更多的光线来匹配奈奎斯特频率，基于预滤波的技术使用场景内容的低通滤波版本来降低奈奎斯特频率所需的渲染场景没有混叠。</li><li>预滤波技术[18,20,32,49]更适合实时渲染，因为场景内容的过滤版本可以提前预计算，并且可以在渲染时根据目标采样率使用正确的“比例”。在渲染环境中，预过滤可以被认为是通过每个像素跟踪一个锥而不是一条射线[1,16]:每当锥与场景内容相交时，在与锥的足迹对应的尺度上查询预先计算的场景内容的多尺度表示(例如稀疏体素八叉树[15,21]或mipmap[47])。</li><li>我们的工作从图形工作中获得灵感，并为NeRF提供了多尺度场景表示。我们的策略在两个关键方面不同于传统图形管道中使用的多尺度表示。<ul><li>首先，我们<strong>无法预先计算多尺度表示</strong>，因为在我们的问题设置中，场景的几何形状是未知的——我们正在使用计算机视觉恢复场景的模型，而不是渲染预定义的CGI资产。因此，Mip-NeRF必须在训练期间学习场景的预过滤表示</li><li>其次，<strong>我们的尺度概念是连续的</strong>，而不是离散的。mip-NeRF不是在固定数量的尺度上使用多个副本来表示场景(比如在mipmap中)，而是学习一个可以在任意尺度上查询的单一神经场景模型。</li></ul></li></ul></li><li>Scene Representations for View Synthesis<ul><li>对于视图合成任务，已经提出了各种场景表示:使用观察到的场景图像来恢复表示，支持从未观察到的摄像机视点渲染场景的新颖逼真图像。当场景的图像被密集捕获时，可以使用光场插值技术[9,14,22]来渲染新的视图，而无需重建场景的中间表示。与采样和混叠相关的问题已在此设置中进行了深入研究[7]。</li><li>从稀疏捕获的图像合成新视图的方法通常重建场景的3D几何形状和外观的明确表示。许多经典的视图合成算法使用基于网格的表示以及漫射[28]或视图相关[6,10,48]纹理。<ul><li>Mesh-based：<strong>基于网格</strong>的表示可以有效地存储，并且自然地与现有的图形渲染管道兼容。然而，由于不连续性和局部最小值，使用基于梯度的方法来优化网格几何和拓扑通常是困难的。因此，体积表示在视图合成中变得越来越流行。</li><li>早期的方法直接使用观察到的图像对体素网格着色[37]，而最近的体积方法使用基于梯度的学习来训练深度网络来预测场景的体素网格表示[12,25,29,38,41,53]。<strong>基于离散体素</strong>的表示对于视图合成是有效的，但它们不能很好地扩展到更高分辨率的场景</li><li>coordinate-based：计算机视觉和图形学研究的最新趋势是用<strong>基于坐标</strong>的神经表示取代这些离散表示，将3D场景表示为由mlp参数化的连续函数，这些函数从3D坐标映射到该位置的场景属性。一些最近的方法使用基于坐标的神经表征将场景建模为隐式曲面[31,50]，但最近的大多数视图合成方法都是基于体积NeRF表征[30]。NeRF启发了许多后续作品，将其连续神经体表示扩展到生成建模[8,36]，动态场景[23,33]，非刚性变形物体[13,34]，具有变化照明和遮挡物的摄影旅游设置[26,43]，以及用于重照明的反射建模[2,3,40]</li><li>在使用基于坐标的神经表示的视图合成上下文中，对采样和混叠问题的关注相对较少。用于视图合成的离散表示，如多边形网格和体素网格，可以使用传统的多尺度预滤波方法(如mipmaps和八叉树)有效地渲染而不存在混叠。然而，基于坐标的视图合成的神经表示目前只能使用超采样来抗锯齿，这加剧了它们已经缓慢的渲染过程。Takikawa等人最近的工作[42]提出了一种基于稀疏体素八叉树的多尺度表示，用于隐式表面的连续神经表示，但他们的方法<strong>要求场景几何是先验的</strong>，而不是我们的视图合成设置，其中唯一的输入是观察到的图像。</li><li>Mip-NeRF解决了这个开放的问题，在训练和测试期间实现了抗混叠图像的高效渲染，以及在训练期间使用多尺度图像。</li></ul></li></ul></li></ul><h3 id="Preliminaries-NeRF"><a href="#Preliminaries-NeRF" class="headerlink" title="Preliminaries: NeRF"></a>Preliminaries: NeRF</h3><p>NeRF采样：射线r(t) = o + td从相机的投影o的中心沿方向发射，使其穿过像素。用于确定相机预定义的近平面和远平面tn和tf之间排序距离t的向量。对于每个距离tk∈t，我们计算它在射线x = r(tk)上对应的3D位置，然后使用位置编码对每个位置进行编码，编码后输入进MLP得到密度和颜色</p><ul><li>NeRF的保真度主要取决于位置编码的使用，因为它允许将场景参数化的MLP表现为插值函数，其中L决定插值内核的带宽</li></ul><p>训练NeRF很简单:使用一组已知相机姿势的观察图像，我们使用梯度下降最小化所有输入像素值和所有预测像素值之间的平方差之和。<br>构建了两个MLP，一粗一精，两者结合进行预测color和density<br>$\begin{aligned}\min_{\Theta^c,\Theta^f}\sum_{\mathbf{r}\in\mathcal{R}}\left(\left|\mathbf{C}^<em>(\mathbf{r})-\mathbf{C}(\mathbf{r};\Theta^c,\mathbf{t}^c)\right|_2^2\+\left|\mathbf{C}^</em>(\mathbf{r})-\mathbf{C}(\mathbf{r};\Theta^f,\text{sort}(\mathbf{t}^c\cup\mathbf{t}^f))\right|_2^2\right)\end{aligned}$</p><ul><li>粗采样：均匀采样64个点$t^f$</li><li>精采样：经过粗MLP——权重参数$\Theta^c$计算得到的密度，然后计算出权重，使用逆变换采样得到128个精采样点</li><li>最后使用总共192个采样点，使用精MLP得出密度和颜色</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230721140121.png" alt="image.png"></p><p>NeRF的工作原理是沿着每个像素的射线提取点采样的位置编码特征(这里显示为点)。这些点采样特征忽略了每条射线所观察到的体积的形状和大小，因此两台不同的相机在不同尺度下对同一位置成像可能会产生<strong>相同的模糊点采样特征</strong>，从而显著降低了NeRF的性能。相比之下，Mip-NeRF投射锥而不是射线，并明确地对每个采样的锥形截体(在这里显示为梯形)的体积进行建模，从而解决了这种模糊性。</p><p>MipNeRF通过从每个像素投射一个锥体来改善这个问题。我们不是沿着每条射线进行点采样，而是将被投射的锥体划分为一系列锥形截锥(垂直于其轴线切割的锥体)。我们不是从空间中的无限小点构造位置编码(PE)特征，而是构造一个由每个圆锥截体所覆盖的体积的集成位置编码(IPE)表示。这些变化使MLP能够推断出每个锥形截锥体的大小和形状，而不仅仅是它的质心。锥形锥台和IPE特征的使用也使我们能够将NeRF的两个独立的“粗”和“细”MLP减少到一个单一的多尺度MLP，从而提高了训练和评估速度，并将模型大小减少了50%。</p><h2 id="Cone-Tracing-and-Positional-Encoding"><a href="#Cone-Tracing-and-Positional-Encoding" class="headerlink" title="Cone Tracing and Positional Encoding"></a>Cone Tracing and Positional Encoding</h2><p>mipNeRF中的图像一次渲染一个像素，对于该像素，我们从相机的投影中心o沿穿过像素中心的方向d投射一个锥体</p><ul><li>圆锥顶点为o，在像平面上半径$\dot r$，以$\frac{2}{\sqrt{12}}$缩放世界坐标系下的像素宽度得到。 </li></ul><p>位于两个t值[t0, t1]之间的锥形截锥内位置x的集合：</p><script type="math/tex; mode=display">\begin{aligned}\mathrm{F}(\mathbf{x},\mathbf{o},\mathbf{d},\dot{r},t_0,t_1)&=\mathbb{1}\bigg\{\bigg(t_0<\frac{\mathbf{d}^\mathrm{T}(\mathbf{x}-\mathbf{o})}{\left\|\mathbf{d}\right\|_2^2}<t_1\bigg)\\\land\bigg(\frac{\mathbf{d}^\mathrm{T}(\mathbf{x}-\mathbf{o})}{\left\|\mathbf{d}\right\|_2\left\|\mathbf{x}-\mathbf{o}\right\|_2}>\frac{1}{\sqrt{1+(\dot{r}/\left\|\mathbf{d}\right\|_2)^2}}\bigg)\bigg\},\end{aligned}</script><ul><li>$\mathbb{1}(\cdot)$指标函数，iff x在$(\mathbf{o},\mathbf{d},\dot{r},t_0,t_1)$定义的圆锥内F(x, ·) = 1</li><li>与 $\land$ ， 或$\vee$</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230721125154.png" alt="image.png"></p><p>我们现在必须构造一个圆锥形截体内体积的特征表示。理想情况下，这种特征表示应该与NeRF中使用的位置编码特征具有相似的形式，正如Mildenhall等人所表明的，这种特征表示对NeRF的成功至关重要[30]。对此有许多可行的方法(参见附录以进一步讨论)，但我们发现的最简单和最有效的解决方案是<strong>简单地计算圆锥锥内所有坐标的预期位置编码</strong>:</p><p>$\gamma^*(\mathbf{o},\mathbf{d},\dot{r},t_0,t_1)=\frac{\int\gamma(\mathbf{x})\mathrm{F}(\mathbf{x},\mathbf{o},\mathbf{d},\dot{r},t_0,t_1)d\mathbf{x}}{\int\mathrm{F}(\mathbf{x},\mathbf{o},\mathbf{d},\dot{r},t_0,t_1)d\mathbf{x}}.$</p><p>然而，目前尚不清楚如何有效地计算这样的特征，因为分子中的积分没有封闭形式的解。因此，我们用多元高斯函数近似锥形截锥体，这允许对所需特征进行有效逼近，我们将其称为“<strong>集成位置编码</strong>”(IPE)。</p><p>为了用<strong>多元高斯函数近似圆锥体</strong>，我们必须计算平均值和协方差of F(x，·)。因为<strong>假设每个圆锥截体都是圆形的</strong>，<strong>并且因为圆锥截体围绕锥轴是对称的</strong>，这样的<strong>高斯分布完全由三个值</strong>(除了o和d之外)表征:沿射线的平均距离$μ_t$，沿射线$σ^{2}_{t}$的方差，垂直于射线$σ^{2}_{r}$的方差:</p><p>$\begin{aligned}\mu_t&amp;=t_\mu+\frac{2t_\mu t_\delta^2}{3t_\mu^2+t_\delta^2},\quad\sigma_t^2=\frac{t_\delta^2}{3}-\frac{4t_\delta^4(12t_\mu^2-t_\delta^2)}{15(3t_\mu^2+t_\delta^2)^2},\\\sigma_r^2&amp;=\dot{r}^2\bigg(\frac{t_\mu^2}{4}+\frac{5t_\delta^2}{12}-\frac{4t_\delta^4}{15(3t_\mu^2+t_\delta^2)}\bigg).\end{aligned}$</p><p>其中$t_{\mu} = \frac{t_{0} + t_{1}}{2}$,$t_{\delta} = \frac{t_{1}-t_{0}}{2}$</p><p>我们可以将这个高斯函数从锥形截锥体的坐标系转换为<strong>如下的世界坐标</strong>:</p><p>$\mu=\mathbf{o}+\mu_t\mathbf{d},\quad\Sigma=\sigma_t^2\big(\mathbf{d}\mathbf{d}^\mathrm{T}\big)+\sigma_r^2\bigg(\mathbf{I}-\frac{\mathbf{d}\mathbf{d}^\mathrm{T}}{\left|\mathbf{d}\right|_2^2}\bigg)$,得到最终的多元高斯函数。</p><p>接下来，我们<strong>推导出IPE</strong>，它是根据前面提到的高斯分布的位置编码坐标的期望。为了实现这一点，首先将公式1中的PE重写为傅里叶特征[35,44]是有帮助的:</p><p>$\gamma(\mathbf{x})=\Big[\sin(\mathbf{x}),\cos(\mathbf{x}),\ldots,\sin\bigl(2^{L-1}\mathbf{x}\bigr),\cos\bigl(2^{L-1}\mathbf{x}\bigr)\Big]^{\mathrm{T}.}$</p><p>$\mathbf{P}=\begin{bmatrix}1&amp;0&amp;0&amp;2&amp;0&amp;0&amp; \cdots &amp;2^{L-1}&amp;0&amp;0\\0&amp;1&amp;0&amp;0&amp;2&amp;0&amp;\cdots&amp;0&amp;2^{L-1}&amp;0\\0&amp;0&amp;1&amp;0&amp;0&amp;2&amp; \cdots &amp;0&amp;0&amp;2^{L-1}\end{bmatrix},\gamma(\mathbf{x})=\begin{bmatrix}\sin(\mathbf{Px})\\\cos(\mathbf{Px})\end{bmatrix}.$</p><p>$\mathrm{(Cov[Ax,By]~=~A~Cov[x,y]B^T)}$</p><p>均值和协方差：$\mu_{\gamma}=\mathrm{P}\mu,\quad\Sigma_{\gamma}=\mathrm{P}\Sigma\mathrm{P}^{\mathrm{T}}.$</p><p>产生IPE特征的最后一步是计算这个提升的<strong>多元高斯的期望</strong>，由位置的正弦和余弦调制。这些期望有简单的封闭形式表达式:</p><p>$\begin{aligned}\operatorname{E}_{x\sim\mathcal{N}(\mu,\sigma^2)}[\sin(x)]&amp;=\sin(\mu)\exp\Big(-(^1/2)\sigma^2\Big),\\\operatorname{E}_{x\sim\mathcal{N}(\mu,\sigma^2)}[\cos(x)]&amp;=\cos(\mu)\exp\Big(-(^1/2)\sigma^2\Big).\end{aligned}$</p><p>我们看到这个期望的正弦或余弦仅仅是均值的正弦或余弦被方差的高斯函数衰减。有了这个，我们可以计算我们的<strong>最终IPE特征</strong>as 期望的正弦和余弦的平均值和协方差矩阵的对角线:</p><script type="math/tex; mode=display">\begin{aligned}\gamma(\mathbf{\mu},\mathbf{\Sigma})& =\mathrm{E}_{\mathbf{x}\sim\mathcal{N}(\mathbf{\mu}_\gamma,\mathbf{\Sigma}_\gamma)}[\gamma(\mathbf{x})]  \\&=\begin{bmatrix}\sin(\mathbf{\mu}_\gamma)\circ\exp(-(1/2)\mathrm{diag}(\mathbf{\Sigma}_\gamma))\\\cos(\mathbf{\mu}_\gamma)\circ\exp(-(1/2)\mathrm{diag}(\mathbf{\Sigma}_\gamma))\end{bmatrix}\end{aligned}</script><p>$\circ$表示逐元素的乘法。因为位置编码独立地编码每个维度，这种预期的编码只依赖于$\gamma(x)$的边际分布，并且只需要协方差矩阵的对角线(每个维度方差的向量)。Because$\sum_{\gamma}$由于其相对较大的尺寸，计算成本过高，我们直接计算$\sum_{\gamma}$的对角线:$\operatorname{diag}(\mathbf{\Sigma}_{\gamma})=\Big[\operatorname{diag}(\mathbf{\Sigma}),4\operatorname{diag}(\mathbf{\Sigma}),\ldots,4^{L-1}\operatorname{diag}(\mathbf{\Sigma})\Big]^\mathrm{T}$</p><p><strong>这个向量只依赖于3D位置的协方差Σ的对角线，可以计算为</strong>:</p><p>$\operatorname{diag}(\mathbf{\Sigma})=\sigma_t^2(\mathbf{d}\circ\mathbf{d})+\sigma_r^2\left(\mathbf{1}-\frac{\mathbf{d}\circ\mathbf{d}}{\left|\mathbf{d}\right|_2^2}\right).$</p><p>如果直接计算这些对角线，那么构建IPE特征的成本与构建PE特征的成本大致相同。</p><p>In a toy 1D domain :  IPE 和传统 PE 特征之间的差异<br>IPE 特征的行为直观：如果位置编码中的特定频率有一个大于用于构建 IPE 特征的区间宽度的周期，则<strong>该频率处的编码不受影响</strong>。但是如果周期小于区间（在这种情况下，该区间上的 PE 将重复振荡），<strong>那么该频率处的编码会缩小到零</strong>。（sin、cos: 许多交叉的-1和1导致积分后编码变为0，如图）<br>简而言之，IPE 保留了在区间上恒定的频率，并软“删除”频率在区间上变化，而 PE 保留了直到某个手动调整的超参数 L 的所有频率。<br>通过以这种方式缩放每个正弦和余弦，IPE 特征实际上是抗锯齿位置编码特征，可以平滑地编码空间体积的大小和形状。IPE 还有效地将 L 从超参数中删除：它可以简单地设置为非常大的值，然后永远不会调整</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/ipe_anim_horiz.gif" alt="ipe_anim_horiz.gif"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230721153610.png" alt="image.png"></p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>除了锥跟踪和IPE特征外，mip-NeRF的行为类似于NeRF，如第2.1节所述。<br>对于渲染的每个像素，而不是像NeRF中那样的光线，投射一个锥体。我们没有沿射线采样$t_{k}$的n个值，而是采样$t_{k}$的n + 1个值，<strong>计算跨越每对相邻采样$t_{k}$值的区间的IPE特征</strong>，如前所述，这些IPE特征作为输入传递到MLP中，以产生密度和颜色，如公式2所示。mip-NeRF中的渲染遵循公式3。</p><p>$\forall t_k\in\mathbf{t},\quad[\tau_k,\mathbf{c}_k]=\mathrm{MLP}(\gamma(\mathbf{r}(t_k));\Theta).$</p><p>$\begin{aligned}\mathbf{C}(\mathbf{r};\Theta,\mathbf{t})&amp;=\sum_kT_k(1-\exp(-\tau_k(t_{k+1}-t_k)))\mathbf{c}_k,\\\text{with}\quad T_k&amp;=\exp\left(-\sum_{k’&lt;k}\tau_{k’}(t_{k’+1}-t_{k’})\right),\end{aligned}$</p><p>我们的圆锥变换和 IPE 特征允许我们明确地将尺度编码到我们的输入特征中，从而使 MLP 能够学习场景的多尺度表示。因此，<strong>Mip-nerF 使用具有参数 Θ 的单个 MLP</strong>，我们在分层采样策略中反复查询。这有多个好处：模型大小被切割一半，渲染更准确，采样效率更高，整体算法更简单。我们的优化问题是：(loss)</p><script type="math/tex; mode=display">\min_{\Theta}\sum_{\mathbf{r}\in\mathcal{R}}(\lambda\left\|\mathbf{C}^{*}(\mathbf{r})-\mathbf{C}(\mathbf{r};\Theta,\mathbf{t}^{c})\right\|_{2}^{2}+\left\|\mathbf{C}^{*}(\mathbf{r})-\mathbf{C}(\mathbf{r};\Theta,\mathbf{t}^{f})\right\|_{2}^{2})</script><p>通过超参数$\lambda = 0.1$实现粗loss与精loss的平衡</p><p>采样点：</p><ul><li>NeRF：C64+F64+128</li><li>Mip-NeRF：C128+F128</li></ul><p>权重：<br>$w_k’=\frac{1}{2}(\max(w_{k-1},w_k)+\max(w_k,w_{k+1}))+\alpha.$</p><ul><li>We filter w with a <strong>2-tap max filter</strong> followed by a 2-tap blur filter (a “blurpool” [51]), which <strong>produces a wide and smooth upper envelope上包络</strong> on w</li><li>A hyperparameter α is added to that envelope <strong>before it is re-normalized to sum to 1</strong>, which ensures that some samples are drawn even in empty regions of space (we set α = 0.01 in all experiments). 确保了空区域也会抽取一些样本</li></ul><p>Mip-NeRF 在 JaxNeRF [11] 之上实现，JaxNeRF 是一种 JAX [4] 重新实现的 NeRF，它实现了比原始 TensorFlow 实现更高的精度并训练速度更快。<br>细节：</p><ul><li>1 million iterations of Adam [19] with a batch size of 4096</li><li>a learning rate that is annealed logarithmically from $5 · 10^{−4}$ to $5 · 10^{−6}$</li></ul><p>有关其他细节的补充以及 JaxNeRF 和 mip-NeRF 之间的一些额外差异，这些差异不会显着影响性能，并且是我们的主要贡献附带的：锥体追踪、IPE 和使用单个多尺度 MLP。</p><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><p>NeRF-blender dataset上评估PSNR、SSIM、LPIPS以及：<br>To enable easier comparison , we also present an <strong>“average” error metric</strong> that summarizes all three metrics: the geometric mean of $MSE = 10^{−PSNR/10}$, $\sqrt{1 − SSIM}$ (as per [5]), and LPIPS.</p><p>我们还报告了运行时(wall time的中位数和中值绝对偏差)，以及NeRF和mip-NeRF的每个变体的网络参数数量。所有 JaxNeRF 和 mip-NeRF 实验都在具有 32 个内核的 TPU v2 上进行训练</p><p>NeRF原始Blender数据集缺陷：所有相机的焦距和分辨率相同，且放在与对象相同的距离上<br>Mip-NeRF构建了自己的多尺度Blender benchmark：相机距离物体不同，可能放大或缩小</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Sampling </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Sampling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Encoding</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF/Encoding/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/NeRF/Encoding/</url>
      
        <content type="html"><![CDATA[<p>对输入x进行编码的方式<br><a href="https://docs.nerf.studio/en/latest/nerfology/model_components/visualize_encoders.html">Field Encoders - nerfstudio</a></p><span id="more"></span><h1 id="频率编码"><a href="#频率编码" class="headerlink" title="频率编码"></a>频率编码</h1><h2 id="NeRF"><a href="#NeRF" class="headerlink" title="NeRF"></a>NeRF</h2><script type="math/tex; mode=display">\gamma(p)=\left(\sin \left(2^{0} \pi p\right), \cos \left(2^{0} \pi p\right), \cdots, \sin \left(2^{L-1} \pi p\right), \cos \left(2^{L-1} \pi p\right)\right)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embedder</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        self.kwargs = kwargs</span><br><span class="line">        self.create_embedding_fn()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_embedding_fn</span>(<span class="params">self</span>):</span><br><span class="line">        embed_fns = []</span><br><span class="line">        d = self.kwargs[<span class="string">&#x27;input_dims&#x27;</span>]</span><br><span class="line">        out_dim = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> self.kwargs[<span class="string">&#x27;include_input&#x27;</span>]:</span><br><span class="line">            embed_fns.append(<span class="keyword">lambda</span> x: x)</span><br><span class="line">            out_dim += d</span><br><span class="line"></span><br><span class="line">        max_freq = self.kwargs[<span class="string">&#x27;max_freq_log2&#x27;</span>]</span><br><span class="line">        N_freqs = self.kwargs[<span class="string">&#x27;num_freqs&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.kwargs[<span class="string">&#x27;log_sampling&#x27;</span>]:</span><br><span class="line">            freq_bands = <span class="number">2.</span> ** torch.linspace(<span class="number">0.</span>, max_freq, N_freqs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            freq_bands = torch.linspace(<span class="number">2.</span>**<span class="number">0.</span>, <span class="number">2.</span>**max_freq, N_freqs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> freq <span class="keyword">in</span> freq_bands:</span><br><span class="line">            <span class="keyword">for</span> p_fn <span class="keyword">in</span> self.kwargs[<span class="string">&#x27;periodic_fns&#x27;</span>]:</span><br><span class="line">                embed_fns.append(<span class="keyword">lambda</span> x, p_fn=p_fn, freq=freq: p_fn(x * freq))</span><br><span class="line">                out_dim += d</span><br><span class="line"></span><br><span class="line">        self.embed_fns = embed_fns</span><br><span class="line">        self.out_dim = out_dim</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">embed</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.cat([fn(inputs) <span class="keyword">for</span> fn <span class="keyword">in</span> self.embed_fns], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_embedder</span>(<span class="params">multires, input_dims=<span class="number">3</span></span>):</span><br><span class="line">    embed_kwargs = &#123;</span><br><span class="line">        <span class="string">&#x27;include_input&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&#x27;input_dims&#x27;</span>: input_dims,</span><br><span class="line">        <span class="string">&#x27;max_freq_log2&#x27;</span>: multires-<span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;num_freqs&#x27;</span>: multires,</span><br><span class="line">        <span class="string">&#x27;log_sampling&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&#x27;periodic_fns&#x27;</span>: [torch.sin, torch.cos],</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    embedder_obj = Embedder(**embed_kwargs)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">embed</span>(<span class="params">x, eo=embedder_obj</span>): <span class="keyword">return</span> eo.embed(x)</span><br><span class="line">    <span class="keyword">return</span> embed, embedder_obj.out_dim</span><br><span class="line"></span><br><span class="line"><span class="comment"># multires &lt;=&gt; L</span></span><br><span class="line"><span class="comment"># use: get_embedder(args.multires, args.i_embed)</span></span><br></pre></td></tr></table></figure><h2 id="Instant-nsr-pl"><a href="#Instant-nsr-pl" class="headerlink" title="Instant-nsr-pl"></a>Instant-nsr-pl</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">get_encoding.py:</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CompositeEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoding, include_xyz=<span class="literal">False</span>, xyz_scale=<span class="number">1.</span>, xyz_offset=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(CompositeEncoding, self).__init__()</span><br><span class="line">        self.encoding = encoding</span><br><span class="line">        self.include_xyz, self.xyz_scale, self.xyz_offset = include_xyz, xyz_scale, xyz_offset</span><br><span class="line">        self.n_output_dims = <span class="built_in">int</span>(self.include_xyz) * self.encoding.n_input_dims + self.encoding.n_output_dims</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, *args</span>):</span><br><span class="line">        <span class="keyword">return</span> self.encoding(x, *args) <span class="keyword">if</span> <span class="keyword">not</span> self.include_xyz <span class="keyword">else</span> torch.cat([x * self.xyz_scale + self.xyz_offset, self.encoding(x, *args)], dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_encoding</span>(<span class="params">n_input_dims,conf</span>):</span><br><span class="line">    <span class="keyword">if</span> conf.otype == <span class="string">&#x27;HashGrid&#x27;</span>:</span><br><span class="line">        encoding = HashGrid(n_input_dims, conf)</span><br><span class="line">    <span class="keyword">elif</span> conf.otype == <span class="string">&#x27;ProgressiveBandHashGrid&#x27;</span>:</span><br><span class="line">        encoding = ProgressiveBandHashGrid(n_input_dims, conf)</span><br><span class="line">    <span class="keyword">elif</span> conf.otype == <span class="string">&#x27;VanillaFrequency&#x27;</span>:</span><br><span class="line">        encoding = VanillaFrequency(n_input_dims, conf)</span><br><span class="line">    <span class="keyword">elif</span> conf.otype == <span class="string">&#x27;SphericalHarmonics&#x27;</span>:</span><br><span class="line">        encoding = SphericalHarmonics(n_input_dims, conf)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    encoding = CompositeEncoding(encoding, include_xyz = conf.get(<span class="string">&#x27;include_xyz&#x27;</span>,<span class="literal">False</span>), xyz_scale =<span class="number">2</span>, xyz_offset = -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line">frequency.py：</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VanillaFrequency</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.N_freqs = config[<span class="string">&#x27;n_frequencies&#x27;</span>]</span><br><span class="line">        self.in_channels, self.n_input_dims = in_channels, in_channels</span><br><span class="line">        self.funcs = [torch.sin, torch.cos]</span><br><span class="line">        self.freq_bands = <span class="number">2</span>**torch.linspace(<span class="number">0</span>, self.N_freqs-<span class="number">1</span>, self.N_freqs)</span><br><span class="line">        self.n_output_dims = self.in_channels * (<span class="built_in">len</span>(self.funcs) * self.N_freqs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = []</span><br><span class="line">        <span class="keyword">for</span> freq <span class="keyword">in</span> <span class="built_in">zip</span>(self.freq_bands):</span><br><span class="line">            <span class="keyword">for</span> func <span class="keyword">in</span> self.funcs:</span><br><span class="line">                out += [func(freq*x)]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(out, -<span class="number">1</span>)     </span><br><span class="line"></span><br><span class="line">models/neus.py：</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">N</span>(nn.Module):</span><br><span class="line">    __init__:</span><br><span class="line">    self.encoding = get_encoding(n_input_dims = <span class="number">3</span>, config)</span><br><span class="line">    forward:</span><br><span class="line">    h = self.encoding(x)</span><br><span class="line"></span><br><span class="line">yaml:</span><br><span class="line">    xyz_encoding: </span><br><span class="line">      otype: HashGrid</span><br><span class="line">      include_xyz: true</span><br><span class="line">      n_frequencies: <span class="number">10</span></span><br></pre></td></tr></table></figure><h2 id="IPE"><a href="#IPE" class="headerlink" title="IPE"></a>IPE</h2><p>Mip-NeRF集成位置编码</p><p>对多元高斯近似截锥体的均值和协方差进行编码</p><script type="math/tex; mode=display">\begin{aligned}\gamma(\mathbf{\mu},\mathbf{\Sigma})& =\mathrm{E}_{\mathbf{x}\sim\mathcal{N}(\mathbf{\mu}_\gamma,\mathbf{\Sigma}_\gamma)}[\gamma(\mathbf{x})]  \\&=\begin{bmatrix}\sin(\mathbf{\mu}_\gamma)\circ\exp(-(1/2)\mathrm{diag}(\mathbf{\Sigma}_\gamma))\\\cos(\mathbf{\mu}_\gamma)\circ\exp(-(1/2)\mathrm{diag}(\mathbf{\Sigma}_\gamma))\end{bmatrix}\end{aligned}</script><p>IPE可以将大区域的高频编码求和为0<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/ipe_anim_horiz.gif" alt="ipe_anim_horiz.gif"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230721153610.png" alt="image.png"></p><h1 id="HashGrid"><a href="#HashGrid" class="headerlink" title="HashGrid"></a>HashGrid</h1><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230703160333.png" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">hashgrid.py:</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HashGrid</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_input_dims, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.n_input_dims = n_input_dims</span><br><span class="line">        <span class="keyword">with</span> torch.cuda.device(get_rank()):</span><br><span class="line">            self.encoding = tcnn.Encoding(self.n_input_dims, config_to_primitive(config))</span><br><span class="line">        self.n_output_dims = self.encoding.n_output_dims</span><br><span class="line">        self.n_level = config[<span class="string">&#x27;n_levels&#x27;</span>]</span><br><span class="line">        self.n_features_per_level = config[<span class="string">&#x27;n_features_per_level&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        enc = self.encoding(x)</span><br><span class="line">        <span class="keyword">return</span> enc</span><br><span class="line"></span><br><span class="line">models/neus.py：</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">N</span>(nn.Module):</span><br><span class="line">    __init__:</span><br><span class="line">    self.encoding = get_encoding(n_input_dims = <span class="number">3</span>, config)</span><br><span class="line">    forward:</span><br><span class="line">    h = self.encoding(x)</span><br><span class="line"></span><br><span class="line">yaml:</span><br><span class="line">    xyz_encoding: </span><br><span class="line">      otype: HashGrid</span><br><span class="line">      n_levels: <span class="number">16</span></span><br><span class="line">      n_features_per_level: <span class="number">2</span></span><br><span class="line">      log2_hashmap_size: <span class="number">19</span></span><br><span class="line">      base_resolution: <span class="number">16</span></span><br><span class="line">      per_level_scale: <span class="number">1.447269237440378</span></span><br><span class="line">      include_xyz: true</span><br></pre></td></tr></table></figure><h1 id="球面谐波编码"><a href="#球面谐波编码" class="headerlink" title="球面谐波编码"></a>球面谐波编码</h1><p>球面基函数——球谐函数</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/351289217">球谐函数介绍（Spherical Harmonics） - 知乎 (zhihu.com)</a></p></blockquote><p>$\{Y_\ell^m\}.$ 与二维的三角函数基类似，球面谐波为三维上的一组基函数，用来描述其他更加复杂的函数</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810145848.png" alt="image.png"><br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810150600.png" alt="image.png"></p><script type="math/tex; mode=display">\begin{aligned}y_l^m(\theta,\varphi)=\begin{cases}\sqrt{2}K_l^m\cos(m\varphi)P_l^m\big(\cos\theta\big),&m>0\\[2ex]\sqrt{2}K_l^m\sin(-m\varphi)P_l^{-m}\big(\cos\theta\big),&m<0\\[2ex]K_l^0P_l^0\big(\cos\theta\big),&m=0\end{cases} \\P_n(x)=\frac1{2^n\cdot n!}\frac{d^n}{dx^n}[(x^2-1)^n] \\P_l^m(x)=(-1)^m(1-x^2)^{m/2}\frac{d^m}{dx^m}(P_l(x)) \\K_{l}^{m}=\sqrt{\frac{\left(2l+1\right)}{4\pi}\frac{\left(l-\left|m\right|\right)!}{\left(l+\left|m\right|\right)!}}\end{aligned}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">spherical.py:</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SphericalHarmonics</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_input_dims, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.n_input_dims = n_input_dims</span><br><span class="line">        <span class="keyword">with</span> torch.cuda.device(get_rank()):</span><br><span class="line">            self.encoding = tcnn.Encoding(self.n_input_dims, config_to_primitive(config))</span><br><span class="line">        self.n_output_dims = self.encoding.n_output_dims</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        enc = self.encoding(x)</span><br><span class="line">        <span class="keyword">return</span> enc</span><br><span class="line"></span><br><span class="line">models/neus.py：</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">N</span>(nn.Module):</span><br><span class="line">    __init__:</span><br><span class="line">    self.encoding = get_encoding(n_input_dims = <span class="number">3</span>, config)</span><br><span class="line">    forward:</span><br><span class="line">    h = self.encoding(x)</span><br><span class="line"></span><br><span class="line">yaml:</span><br><span class="line">    dir_encoding: </span><br><span class="line">      otype: SphericalHarmonics</span><br><span class="line">      degree: <span class="number">4</span></span><br></pre></td></tr></table></figure><h2 id="IDE"><a href="#IDE" class="headerlink" title="IDE"></a>IDE</h2><p>在Ref-NeRF中，借鉴Mip-NeR中的IPE，提出了IDE，基于球面谐波，将高频部分的编码输出置为0</p><p>$\mathrm{IDE}(\hat{\boldsymbol{\omega}}_r,\kappa)=\left\{\mathbb{E}_{\hat{\boldsymbol{\omega}}\sim\mathrm{vMF}(\hat{\boldsymbol{\omega}}_r,\kappa)}[Y_\ell^m(\hat{\boldsymbol{\omega}})]\colon(\ell,m)\in\mathcal{M}_L\right\},$<br>$\mathcal{M}_{L}=\{(\ell,m):\ell=1,…,2^{L},m=0,…,\ell\}.$</p><p>$\mathbb{E}_{\hat{\boldsymbol{\omega}}\sim\mathrm{vMF}(\hat{\boldsymbol{\omega}}_r,\kappa)}[Y_\ell^m(\hat{\boldsymbol{\omega}})]=A_\ell(\kappa)Y_\ell^m(\hat{\boldsymbol{\omega}}_r),$<br>$A_{\ell}(\kappa)\approx\exp\left(-\frac{\ell(\ell+1)}{2\kappa}\right).$</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230810152303.png" alt="image.png"></p><p>use:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.sph_enc = generate_ide_fn(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">dir_enc = self.sph_enc(reflections, <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 将reflections编码，粗糙度为0</span></span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generalized_binomial_coeff</span>(<span class="params">a, k</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute generalized binomial coefficients.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> np.prod(a - np.arange(k)) / np.math.factorial(k)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">assoc_legendre_coeff</span>(<span class="params">l, m, k</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute associated Legendre polynomial coefficients.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      Returns the coefficient of the cos^k(theta)*sin^m(theta) term in the</span></span><br><span class="line"><span class="string">      (l, m)th associated Legendre polynomial, P_l^m(cos(theta)).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      Args:</span></span><br><span class="line"><span class="string">        l: associated Legendre polynomial degree.</span></span><br><span class="line"><span class="string">        m: associated Legendre polynomial order.</span></span><br><span class="line"><span class="string">        k: power of cos(theta).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      Returns:</span></span><br><span class="line"><span class="string">        A float, the coefficient of the term corresponding to the inputs.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> ((-<span class="number">1</span>)**m * <span class="number">2</span>**l * np.math.factorial(l) / np.math.factorial(k) /</span><br><span class="line">          np.math.factorial(l - k - m) *</span><br><span class="line">          generalized_binomial_coeff(<span class="number">0.5</span> * (l + k + m - <span class="number">1.0</span>), l))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sph_harm_coeff</span>(<span class="params">l, m, k</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Compute spherical harmonic coefficients.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">return</span> (np.sqrt(</span><br><span class="line">      (<span class="number">2.0</span> * l + <span class="number">1.0</span>) * np.math.factorial(l - m) /</span><br><span class="line">      (<span class="number">4.0</span> * np.pi * np.math.factorial(l + m))) * assoc_legendre_coeff(l, m, k))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_ml_array</span>(<span class="params">deg_view</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create a list with all pairs of (l, m) values to use in the encoding.&quot;&quot;&quot;</span></span><br><span class="line">    ml_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(deg_view):</span><br><span class="line">        l = <span class="number">2</span>**i</span><br><span class="line">        <span class="comment"># Only use nonnegative m values, later splitting real and imaginary parts.</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(l + <span class="number">1</span>):</span><br><span class="line">            ml_list.append((m, l))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert list into a numpy array.</span></span><br><span class="line">    ml_array = np.array(ml_list).T</span><br><span class="line">    <span class="keyword">return</span> ml_array</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_ide_fn</span>(<span class="params">deg_view</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Generate integrated directional encoding (IDE) function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      This function returns a function that computes the integrated directional</span></span><br><span class="line"><span class="string">      encoding from Equations 6-8 of arxiv.org/abs/2112.03907.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      Args:</span></span><br><span class="line"><span class="string">        deg_view: number of spherical harmonics degrees to use.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      Returns:</span></span><br><span class="line"><span class="string">        A function for evaluating integrated directional encoding.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      Raises:</span></span><br><span class="line"><span class="string">        ValueError: if deg_view is larger than 5.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> deg_view &gt; <span class="number">5</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Only deg_view of at most 5 is numerically stable.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    ml_array = get_ml_array(deg_view)</span><br><span class="line">    l_max = <span class="number">2</span>**(deg_view - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a matrix corresponding to ml_array holding all coefficients, which,</span></span><br><span class="line">    <span class="comment"># when multiplied (from the right) by the z coordinate Vandermonde matrix,</span></span><br><span class="line">    <span class="comment"># results in the z component of the encoding.</span></span><br><span class="line">    mat = np.zeros((l_max + <span class="number">1</span>, ml_array.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i, (m, l) <span class="keyword">in</span> <span class="built_in">enumerate</span>(ml_array.T):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(l - m + <span class="number">1</span>):</span><br><span class="line">            mat[k, i] = sph_harm_coeff(l, m, k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># mat = torch.from_numpy(mat.astype(np.float32)).cuda()</span></span><br><span class="line">    mat = torch.from_numpy(mat.astype(np.float32)).cpu()</span><br><span class="line">    ml_array = torch.from_numpy(ml_array.astype(np.float32)).cpu()</span><br><span class="line">    <span class="comment"># ml_array = torch.from_numpy(ml_array.astype(np.float32)).cuda()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">integrated_dir_enc_fn</span>(<span class="params">xyz, kappa_inv</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Function returning integrated directional encoding (IDE).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.</span></span><br><span class="line"><span class="string">          kappa_inv: [..., 1] reciprocal of the concentration parameter of the von</span></span><br><span class="line"><span class="string">            Mises-Fisher distribution.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          An array with the resulting IDE.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = xyz[..., <span class="number">0</span>:<span class="number">1</span>]</span><br><span class="line">        y = xyz[..., <span class="number">1</span>:<span class="number">2</span>]</span><br><span class="line">        z = xyz[..., <span class="number">2</span>:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute z Vandermonde matrix.</span></span><br><span class="line">        vmz = torch.concat([z**i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(mat.shape[<span class="number">0</span>])], dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute x+iy Vandermonde matrix.</span></span><br><span class="line">        vmxy = torch.concat([(x + <span class="number">1j</span> * y)**m <span class="keyword">for</span> m <span class="keyword">in</span> ml_array[<span class="number">0</span>, :]], dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get spherical harmonics.</span></span><br><span class="line">        sph_harms = vmxy * torch.matmul(vmz, mat)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Apply attenuation function using the von Mises-Fisher distribution</span></span><br><span class="line">        <span class="comment"># concentration parameter, kappa.</span></span><br><span class="line">        sigma = <span class="number">0.5</span> * ml_array[<span class="number">1</span>, :] * (ml_array[<span class="number">1</span>, :] + <span class="number">1</span>)</span><br><span class="line">        ide = sph_harms * torch.exp(-sigma * kappa_inv)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split into real and imaginary parts and return</span></span><br><span class="line">        <span class="keyword">return</span> torch.concat([torch.real(ide), torch.imag(ide)], dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> integrated_dir_enc_fn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_lat_long</span>():</span><br><span class="line">    res = (<span class="number">1080</span>, <span class="number">1080</span>*<span class="number">3</span>)</span><br><span class="line">    gy, gx = torch.meshgrid(torch.linspace(<span class="number">0.0</span> + <span class="number">1.0</span> / res[<span class="number">0</span>], <span class="number">1.0</span> - <span class="number">1.0</span> / res[<span class="number">0</span>], res[<span class="number">0</span>], device=<span class="string">&#x27;cuda&#x27;</span>),</span><br><span class="line">                            torch.linspace(-<span class="number">1.0</span> + <span class="number">1.0</span> / res[<span class="number">1</span>], <span class="number">1.0</span> - <span class="number">1.0</span> / res[<span class="number">1</span>], res[<span class="number">1</span>], device=<span class="string">&#x27;cuda&#x27;</span>),</span><br><span class="line">                            indexing=<span class="string">&#x27;ij&#x27;</span>) <span class="comment"># [h,w]</span></span><br><span class="line"></span><br><span class="line">    sintheta, costheta = torch.sin(gy * np.pi), torch.cos(gy * np.pi)</span><br><span class="line">    sinphi, cosphi = torch.sin(gx * np.pi), torch.cos(gx * np.pi)</span><br><span class="line">    reflvec = torch.stack((sintheta * sinphi, costheta, -sintheta * cosphi), dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> reflvec</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/NeRF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Encoding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF++</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/LargeScaleScene/NeRF++/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/LargeScaleScene/NeRF++/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>NeRF++: Analyzing and Improving Neural Radiance Fields</th></tr></thead><tbody><tr><td>Author</td><td>Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun</td></tr><tr><td>Conf/Jour</td><td>arXiv: Computer Vision and Pattern Recognition</td></tr><tr><td>Year</td><td>2020</td></tr><tr><td>Project</td><td><a href="https://github.com/Kai-46/nerfplusplus">Kai-46/nerfplusplus: improves over nerf in 360 capture of unbounded scenes (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4498415698728083457&amp;noteId=1798375069007427072">NeRF++: Analyzing and Improving Neural Radiance Fields. (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718165249.png" alt="image.png"></p><p>创新：一种前背景分离的方法<br>挑战：</p><ul><li>First, the training and testing of NeRF and NeRF++ on a single large-scale scene is quite <strong>time-consuming and memory-intensive</strong> —&gt; NGP解决了耗时</li><li>Second, <strong>small camera calibration errors</strong> may impede阻碍 photorealistic synthesis. Robust loss functions, such as the contextual loss (Mechrez et al., 2018), could be applied.</li><li>Third, photometric effects such as <strong>auto-exposure and vignetting渐晕</strong> can also be taken into account to increase image fidelity. This line of investigation is related to the lighting changes addressed in the orthogonal work of Martin-Brualla et al. (2020).</li></ul><span id="more"></span><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>We first remark on radiance fields and their potential ambiguities, namely <strong>the shape-radiance ambiguity</strong>, and analyze NeRF’s success in avoiding such ambiguities. Second, we <strong>address a parametrization issue</strong> involved in applying NeRF to 360◦ captures of objects within large-scale, unbounded 3D scenes</p><ul><li>inverted sphere parameterization<br>In this technical report, we first <strong>present an analysis of potential failure modes</strong> in NeRF, and an analysis of why NeRF avoids these failure modes in practice. Second, we present <strong>a novel spatial parameterization scheme</strong> that we call <strong>inverted sphere parameterization</strong> that allows NeRF to work on a new class of captures of unbounded scenes.</li></ul><p>Why NeRF success:  利用了亮度与观察方向相关的不对称MLP解决了shape-radiance ambiguity<br>In particular, we find that in theory, optimizing the 5D function from a set of training images can <strong>encounter critical degenerate solutions</strong> that fail to generalize to novel test views, in the absence of any regularization. Such phenomena are encapsulated封装 in the shape-radiance ambiguity (Figure 1, left), wherein one can fit a set of training images perfectly for an arbitrary incorrect geometry by a suitable choice of outgoing 2D radiance at each surface point. We empirically show that <strong>the specific MLP structure used in NeRF plays an important role in avoiding such ambiguities,</strong> yielding an impressive ability to synthesize novel views. Our analysis offers a new view into NeRF’s impressive success.</p><p>a spatial parameterization issue: 对于360度拍摄的图片，可以有两种参数化的方法</p><ul><li>对整个空间参数化，前背景融合进行建模，但由于分辨率有限而缺乏细节</li><li>只对前景物体即整个场景中的一个bound进行采样，这样会丢失掉背景元素</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718155708.png" alt="image.png"></p><ul><li>Shape-radiance ambiguity (left) and parameterization of unbounded scenes (right). <ul><li>Shaperadiance ambiguity: our theoretical analysis shows that, in the absence of explicit or implicit regularization, a set of training images can be fit independently of the recovered geometry (e.g., for incorrect scene geometry $\hat S$ rather than correct geometry $S^{∗}$) by exploiting view-dependent radiance to simulate the effect of the correct geometry. </li><li>Parameterization of unbounded scenes:with standard parameterization schemes, either (1) only a portion of the scene is modeled (red outline), leading to significant artifacts in background elements, or (2) the full scene is modeled (orange outline), which leads to an overall loss of details due to finite sampling resolution</li></ul></li></ul><p>In summary, we present an analysis on how NeRF manages to resolve the shape-radiance ambiguity, as well as a remedy for the parameterization of unbounded scenes in the case of 360◦ captures.</p><p>PRELIMINARIES: NeRF</p><h1 id="SHAPE-RADIANCE-AMBIGUITY"><a href="#SHAPE-RADIANCE-AMBIGUITY" class="headerlink" title="SHAPE-RADIANCE AMBIGUITY"></a>SHAPE-RADIANCE AMBIGUITY</h1><p>The capacity of NeRF to model view-dependent appearance leads to <strong>an inherent ambiguity between 3D shape and radiance</strong> that can admit degenerate solutions, in the absence of regularization. For an arbitrary, incorrect shape, one can show that there exists a family of radiance fields that perfectly explains the training images, but that generalizes poorly to novel test views.</p><ul><li>对于一个特定的场景，可以找到一组辐射场，完美解释训练图像，但在测试视图上的泛化性很差<ul><li>根据训练集训练出来的radiance field 可以看成一个球，形状与真实物体完全不同，但是在训练view上渲染出来的图像却与真实的图片相差很小，当改变很小的view时即使用一个不同于训练集的view时，图片发生很大的变化</li></ul></li><li>对于一个简单的几何物体，需要很复杂的radiance field来进行表示。ref : <a href="https://zhuanlan.zhihu.com/p/458166170">NeRF++论文部分解读 为何NeRF如此强大？ - 知乎 (zhihu.com)</a><ul><li>想一想这个问题，NeRF会把一个镜子重建成一个平面还是一个有深度的几何（类似镜像的世界）？答案是会重建出几何，而不是一个镜子平面</li></ul></li></ul><p>将单位圆的opacity设置为1，其他设置为0，在训练集上训练得出的结果：<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718163229.png" alt="image.png"></p><p>Why does NeRF avoid such degenerate solutions? We hypothesize that two related factors come to NeRF’s rescue:<br>1) incorrect geometry forces the radiance field to have higher intrinsic complexity (i.e., much higher frequencies) while in contrast<br>2) NeRF’s specific MLP structure implicitly encodes a smooth BRDF prior on surface reflectance.</p><ul><li>As σ deviates from the correct shape, c must in general become a high-frequency function with respect to d to reconstruct the input images. For the correct shape, the surface light field will generally be much smoother (in fact, constant for Lambertian materials). The higher complexity required for incorrect shapes is more difficult to represent with a limited capacity MLP.<ul><li>incorrect shape —&gt; complex c(surface light field)</li><li>correct shape —&gt; smoother surface light field</li></ul></li><li>In particular, NeRF’s specific MLP structure encodes an implicit prior favoring smooth surface reflectance functions where c is smooth with respect to d at any given surface point x. This MLP structure, shown in Figure 3, treats the scene position x and the viewing direction d asymmetrically不对称的: d is injected into the network close to the end of the MLP, meaning that there are fewer MLP parameters, as well as fewer non-linear activations, involved in the creation of view-dependent effects. In addition, the Fourier features used to encode the viewing direction consist only of low-frequency components, i.e，对于位置x，频率编码为$\gamma^{10}(\cdot)$, 而对于方向d，频率编码仅有$\gamma^{4}(\cdot)$。In other words, for a fixed x, the radiance c(x, d) has limited expressivity with respect to d.<ul><li>NeRF的MLP采用了不对称结构，方向d在MLP中只有少量的参数</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718164536.png" alt="image.png"></p><p>验证实验：</p><ul><li>NeRF MLP</li><li>vanilla MLP : 对x和d 都进行$\gamma^{10}(\cdot)$编码，并同时输入进MLP网络</li></ul><p>This result is consistent with our hypothesis that <strong>implicit regularization of reflectance</strong> in NeRF’s MLP model of radiance c <strong>helps recover correct solutions</strong></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718164556.png" alt="image.png"></p><h1 id="INVERTED-SPHERE-PARAMETRIZATION"><a href="#INVERTED-SPHERE-PARAMETRIZATION" class="headerlink" title="INVERTED SPHERE PARAMETRIZATION"></a>INVERTED SPHERE PARAMETRIZATION</h1><p>NeRF的NDC操作虽然解决了无限远的问题，但是对360度环绕拍摄的场景无法很好的处理远处背景</p><ul><li>如果bound前景物体，对背景的重建效果很差</li><li>如果bound整个场景，对物体的重建效果就会下降</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718164906.png" alt="image.png"></p><p>将整个场景分成两个部分<br>partition the scene space into two volumes</p><ul><li>an inner unit sphere前景：处于一个单位圆中<ul><li>The inner volume contains the foreground and all the cameras, while the outer volume contains the remainder of the environment.</li></ul></li><li>an outer volume represented by an inverted sphere covering the complement of the inner volume 背景：位置坐标由一个四维的向量表示，inverted sphere parameterization<ul><li>the outer volume contains the remainder of the environment.</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718165249.png" alt="image.png"></p><p>These two volumes are modelled with two separate NeRFs. To render the color for a ray, they are raycast individually, followed by a final compositition. </p><ul><li>No re-parameterization is needed for the inner NeRF, as that part of the scene is nicely bounded. </li><li>For the outer NeRF, we apply an <strong>inverted sphere parametrization.</strong> </li></ul><p>内部：(x, y, z)<br>外部：(x, y, z)需要参数化为$(x’,y’,z’,1/r)$ ，其中$x’^{2}+ y’^{2}+z’^{2}= 1$ ，(x’,y’,z’)，为与(x,y,z)同一条光线的单位方向向量，representing a direction on the sphere.</p><ul><li>$r = \sqrt{x^2+y^2+z^2}&gt;1$，所以 0 &lt; 1/r &lt; 1</li><li>i.e. 该点的坐标为 $r \cdot (x’,y’,z’) = (x,y,z)$</li><li>重参数化后的参数都是有界的，0 &lt; 1/r &lt; 1，$x’,y’,z’ \in [-1,1]$</li></ul><p>在光线$ray = o + td$中，相当于t被单位球分为两部分：</p><ul><li>inside: $t \in [0,t’]$</li><li>outside: $t \in [t’,\infty]$</li></ul><script type="math/tex; mode=display">\begin{aligned}\mathbf{C}(\mathbf{r})& =\underbrace{\int_{t=0}^{t^{\prime}}\sigma(\mathbf{o}+t\mathbf{d})\cdot\mathbf{c}(\mathbf{o}+t\mathbf{d},\mathbf{d})\cdot e^{-\int_{s=0}^{t}\sigma(\mathbf{o}+s\mathbf{d})ds}dt}_{\mathrm{(i)}}  \\&+\underbrace{e^{-\int_{s=0}^{t^{\prime}}\sigma(\mathbf{o}+s\mathbf{d})ds}}_{\mathrm{(ii)}}\cdot\underbrace{\int_{t=t^{\prime}}^{\infty}\sigma(\mathbf{o}+t\mathbf{d})\cdot\mathbf{c}(\mathbf{o}+t\mathbf{d},\mathbf{d})\cdot e^{-\int_{s=t^{\prime}}^{t}\sigma(\mathbf{o}+s\mathbf{d})ds}_{\mathrm{(iii)}}dt.}\end{aligned}</script><p>Terms (i) and (ii) are computed in Euclidean space, while term (iii) is computed in inverted sphere space with 1r as the integration variable.<br>In other words, we use $σ_{in}(o + td)$, $c_{in} (o + td, d)$ in terms (i) and (ii), and $σ_{out}(x′, y′, z′, 1/r)$, $c_{out} (x′, y′, z′, 1/r, d)$ in term (iii)</p><p>In order to compute term (iii) for the ray $r = o + td$, we first need to be able to evaluate $σ_{out}$ , $c_{out}$ at any 1/r; in other words, we need a way to compute (x′, y′, z′) corresponding to a given 1/r, so that $σ_{out}$, $c_{out}$ can take (x′, y′, z′, 1/r) as input..</p><div class="note success">            <p><a href="https://github.com/Kai-46/nerfplusplus/issues/19">about inverted sphere parameterization · Issue #19 · Kai-46/nerfplusplus (github.com)</a><br>为什么不直接用xyz除以r得到x’y’z’:</p><ul><li>xyz有时太大，数值误差</li><li>在代码中，计算xyz需要使用o和d，一般xyz是未知的，而r很好求得，因此使用这种方法<ul><li><code>bg_depth = torch.linspace(0., 1., N_samples).view([1, ] * len(dots_sh) + [N_samples,]).expand(dots_sh + [N_samples,]).to(rank)</code></li><li>bg_depth 即 1/r</li></ul></li></ul>          </div><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718192632.png" alt="image.png"><br>(x’,y’,z’)可由a和$\omega$得到<br>由$a = o + t_{a}d$得到$t_{a}$，a的位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">由： ||o+td|| = 1</span></span><br><span class="line"><span class="string">||d||^2*t^2 + 2*(o.d)*t + ||o||^2-1 = 0</span></span><br><span class="line"><span class="string">因此求ta的代码：</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">intersect_sphere</span>(<span class="params">rays_o, rays_d</span>):</span><br><span class="line">    odotd = torch.<span class="built_in">sum</span>(rays_o*rays_d, <span class="number">1</span>)</span><br><span class="line">    d_norm_sq = torch.<span class="built_in">sum</span>(rays_d**<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    o_norm_sq = torch.<span class="built_in">sum</span>(rays_o**<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    determinant = odotd**<span class="number">2</span>+(<span class="number">1</span>-o_norm_sq)*d_norm_sq</span><br><span class="line">    <span class="keyword">assert</span> torch.<span class="built_in">all</span>(determinant&gt;=<span class="number">0</span>), \</span><br><span class="line">        <span class="string">&#x27;Not all your cameras are bounded by the unit sphere; please make sure the cameras are normalized properly!&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> (torch.sqrt(determinant)-odotd)/d_norm_sq</span><br></pre></td></tr></table></figure><p>由$b = o + t_{b}d$得到b的位置，$\mathbf{d}^T(\mathbf{o}+t_b\mathbf{d})=0.$</p><p>$\omega=\arcsin|\mathbf{b}|-\arcsin(|\mathbf{b}|\cdot\frac{1}{r}).$，根据a的位置和角度$\omega$即可求得x’y’z’<br>具体算法ref: <a href="https://zh.wikipedia.org/wiki/%E7%BD%97%E5%BE%B7%E9%87%8C%E6%A0%BC%E6%97%8B%E8%BD%AC%E5%85%AC%E5%BC%8F">罗德里格旋转公式 - 维基百科，自由的百科全书 (wikipedia.org)</a></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230719125758.png" alt="image.png|500"></p><p>$\mathbf{v}_{\mathrm{rot}}=\mathbf{v}\cos\theta+(\mathbf{k}\times\mathbf{v})\sin\theta+\mathbf{k}(\mathbf{k}\cdot\mathbf{v})(1-\cos\theta).$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">depth2pts_outside</span>(<span class="params">ray_o, ray_d, depth</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    ray_o, ray_d: [..., 3]</span></span><br><span class="line"><span class="string">    depth: [...]; inverse of distance to sphere origin</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># note: d1 becomes negative if this mid point is behind camera</span></span><br><span class="line">    d1 = -torch.<span class="built_in">sum</span>(ray_d * ray_o, dim=-<span class="number">1</span>) / torch.<span class="built_in">sum</span>(ray_d * ray_d, dim=-<span class="number">1</span>)</span><br><span class="line">    p_mid = ray_o + d1.unsqueeze(-<span class="number">1</span>) * ray_d</span><br><span class="line">    p_mid_norm = torch.norm(p_mid, dim=-<span class="number">1</span>)</span><br><span class="line">    ray_d_cos = <span class="number">1.</span> / torch.norm(ray_d, dim=-<span class="number">1</span>)</span><br><span class="line">    d2 = torch.sqrt(<span class="number">1.</span> - p_mid_norm * p_mid_norm) * ray_d_cos</span><br><span class="line">    p_sphere = ray_o + (d1 + d2).unsqueeze(-<span class="number">1</span>) * ray_d</span><br><span class="line"></span><br><span class="line">    rot_axis = torch.cross(ray_o, p_sphere, dim=-<span class="number">1</span>)</span><br><span class="line">    rot_axis = rot_axis / torch.norm(rot_axis, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    phi = torch.asin(p_mid_norm)</span><br><span class="line">    theta = torch.asin(p_mid_norm * depth)  <span class="comment"># depth is inside [0, 1]</span></span><br><span class="line">    rot_angle = (phi - theta).unsqueeze(-<span class="number">1</span>)     <span class="comment"># [..., 1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># now rotate p_sphere</span></span><br><span class="line">    <span class="comment"># Rodrigues formula: https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula</span></span><br><span class="line">    p_sphere_new = p_sphere * torch.cos(rot_angle) + \</span><br><span class="line">                   torch.cross(rot_axis, p_sphere, dim=-<span class="number">1</span>) * torch.sin(rot_angle) + \</span><br><span class="line">                   rot_axis * torch.<span class="built_in">sum</span>(rot_axis*p_sphere, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>) * (<span class="number">1.</span>-torch.cos(rot_angle))</span><br><span class="line">    p_sphere_new = p_sphere_new / torch.norm(p_sphere_new, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    pts = torch.cat((p_sphere_new, depth.unsqueeze(-<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># now calculate conventional depth</span></span><br><span class="line">    depth_real = <span class="number">1.</span> / (depth + TINY_NUMBER) * torch.cos(theta) * ray_d_cos + d1</span><br><span class="line">    <span class="keyword">return</span> pts, depth_real</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/LargeScaleScene </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> LargeScaleScene </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeuDA</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/NeuDA/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/NeuDA/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>NeuDA: Neural Deformable Anchor for High-Fidelity Implicit Surface Reconstruction</th></tr></thead><tbody><tr><td>Author</td><td>Bowen Cai    ,Jinchi Huang,    Rongfei Jia    ,Chengfei Lv,    Huan Fu*</td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://3d-front-future.github.io/neuda/">NeuDA (3d-front-future.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4738274314005004289&amp;noteId=1876559968197187840">NeuDA: Neural Deformable Anchor for High-Fidelity Implicit Surface Reconstruction (readpaper.com)</a></td></tr></tbody></table></div><p>NeuDA变形后的grid距离Surface更近一些，即可以使采样点插值时更多依赖于表面，即渲染时也会更多地考虑到3D空间相邻的信息</p><p>创新：Deformable Anchors、HPE、$\mathcal{L}_{norm}$</p><ul><li>改进了NGP中的grid表示，8个顶点存储feature—&gt;存储锚点位置，锚点位置经过PE后输入进SDF网络</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718145119.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718144934.png" alt="image.png"></p><span id="more"></span><h1 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h1><p>One of the major limitations of this paper is that we follow an intuitive idea to propose NeuDA and conduct empirical studies to validate its performance. Although we <strong>can not provide strictly mathematical proof,</strong> we prudently respond to this concern and provide qualitative proof by reporting the anchor points’ deformation process in Figure 8.<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718153518.png" alt="image.png"></p><p>Taking a slice of grid voxels as an example, we can see the anchor points (e.g. orange points) move to object surfaces as training convergences, resulting in an implied adaptive representation. Intuitively, the SDF change has an increasing effect on geometry prediction as the anchor approaches the surfaces, while the SDF change of a position far from the object has weak effects. Thus, the optimization process may force those anchors (“yellow” points) to move to positions nearly around the object surfaces to better reflect the SDF changes. The deformable anchor shares some similar concepts with deformable convolution [4] and makes its movement process like a mesh deformation process. Moreover, as each query point has eight anchors, from another perspective, each anchor follows an individual mesh deformation process. <strong>Thereby, NeuDA may play an important role in learning and ensembling multiple 3D reconstruction models.</strong></p><p>直观地看，随着锚点接近物体表面，SDF变化对几何预测的影响越来越大，而远离物体位置的SDF变化对几何预测的影响较弱。</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>This paper studies neural implicit surface reconstruction. We find that previous works (e.g. NeuS) are likely to produce over-smoothing surfaces for small local geometry structures and surface abrupt regions. <strong>A possible reason is that the spatial context in 3D space has not been flexibly exploited</strong>. We take inspiration from the insight and propose NeuDA, namely Neural Deformable Anchors, as a solution. NeuDA is leveraging multi-level voxel grids, and is empowered by the core “<strong>Deformable Anchors (DA)</strong>“ representation approach and <strong>a simple hierarchical position encoding strategy</strong>.<br>The former maintains(DA) learnable anchor points at verities to enhance the capability of neural implicit model in <strong>handling complicated geometric structures</strong>, and<br>the latter(HPE) explores complementaries of high-frequency and low-frequency geometry properties in the multi-level anchor grid structure.<br>The comparisons with baselines and SOTA methods demonstrate the superiority of NeuDA in capturing high-fidelity typologies.</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>This paper studies implicit surface reconstruction leveraging differentiable ray casting. Previous works such as IDR [35] and NeuS [28] <strong>overlook the spatial context in 3D space</strong> when predicting and rendering the surface, thereby may fail to capture sharp local topologies such as <strong>small holes and structures</strong>. </p><p>To mitigate the limitation, we propose a flexible neural implicit representation <strong>leveraging hierarchical voxel grids</strong>, namely Neural Deformable Anchor (NeuDA), for high-fidelity surface reconstruction. NeuDA maintains the <strong>hierarchical anchor grids</strong> where each vertex stores a 3D position (or anchor) instead of the direct embedding (or feature). We optimize the anchor grids such that different local geometry structures can be adaptively encoded. Besides, we dig into the frequency encoding strategies and introduce a simple <strong>hierarchical positional encoding</strong> method for the hierarchical anchor structure to flexibly exploit the properties of high-frequency and low-frequency geometry and appearance. Experiments on both the DTU [8] and BlendedMVS [33] datasets demonstrate that NeuDA can produce promising mesh surfaces.</p><ul><li>分层锚网格：每个顶点存储3D位置或者锚点，而不是特征值</li><li>引入一种简单的分层位置编码方式，灵活地利用高频和低频的几何和外观属性</li></ul><p>3D surface reconstruction from multi-view images is one of the fundamental problems of the community. Typical Multi-view Stereo (MVS) approaches perform cross-view feature matching, depth fusion, and surface reconstruction (e.g., Poisson Surface Reconstruction) to obtain triangle meshes [9].Some methods have exploited the possibility of training end-to-end deep MVS models or employing deep networks to improve the accuracy of sub-tasks of the MVS pipeline.</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718141228.png" alt="image.png"><br>Neus 将物体表面定义为sdf的零水平集，but these methods have not explored the spatial context in 3D space.<br>As a result, they may struggle to recover fine-grain geometry in some local spaces, such as <strong>boundaries, holes, and other small structures</strong></p><ul><li>A straightforward solution is to query scene properties of a sampled 3D point by fusing its nearby features. <ul><li>For example, we can represent scenes as neural voxel fields where the embedding (or feature) at each vertex of the voxel encodes the geometry and appearance context.</li><li>Given a target point, we are able to aggregate the features of the surrounding eight vertices. As the scope of neighboring information is limited by the resolution of grids, multi-level (or hierarchical) voxel grids have been adopted to study different receptive fields. These approaches do obtain <strong>sharper surface details</strong> compared to baselines for most cases, （Multi res Hash grid）</li><li><strong>but still cannot capture detailed regions well</strong>. A possible reason is that the geometry features held by the voxel grids are uniformly distributed around 3D surfaces, while small structures are with complicated typologies and may need more flexible representations</li></ul></li></ul><p>ours NeuDA:</p><ul><li>代替多分辨率哈希网格中的每个顶点存取特征信息，NeuDA的每个网格顶点中存取的是3D点的位置/锚点信息，具体采样得到的3D点特征由，8个顶点坐标经过PE后，三线性插值频率嵌入得到。<ul><li>The input feature for a query point is obtained by directly interpolating the frequency embedding of its eight adjacent anchors.</li></ul></li><li>we present a simple yet effective <strong>hierarchical positional encoding policy</strong> that adopts a higher frequency band to a finer grid level</li><li><strong>It’s worth mentioning that NeuDA employs a shallower MLP</strong> (4 vs. 8 for NeuS and volSDF) to achieve better surface reconstruction performance due to the promising scene representation capability of the hierarchical deformable anchor structure.</li></ul><p>Related Work</p><ul><li>Neural Implicit Surface Reconstruction<ul><li>NeRF—&gt;Neus…<ul><li>Nevertheless, the above approaches extract geometry features from a single point along a casting ray, which may <strong>hinder the neighboring information sharing across sampled points around the surface</strong></li></ul></li><li>It is worth mentioning that the Mip-NeRF 虽然使用锥形光线(tracing an anti-aliased conical frustum)进行采样，将相邻的信息带入渲染过程，但是由于位置编码依赖the radius of the casting cone.因此很难应用到surface reconstruction</li></ul></li><li>Neural Explicit Representation<ul><li>Voxel和pointcloud等显式表示使得在模型优化过程中更容易将邻域信息注入到几何特征中<ul><li>DVGO [25] and Plenoxels [23] represent the scene as a voxel grid, and compute the opacity and color of each sampled point via trilinear interpolation of the neighboring voxels.</li><li>The Voxurf [31] further extends this single-level voxel feature to a hierarchical geometry feature by concatenating the neighboring feature stored voxel grid from different levels.</li><li>The Instant-NGP [18] and MonoSDF [37] use multiresolution hash encoding to achieve fast convergence and capture high-frequency and local details, but they might suffer from hash collision due to its compact representation.</li><li>Both of these methods leverage a multi-level grid scheme to enlarge the receptive field of the voxel grid and encourage more information sharing among neighboring voxels. <strong>Although the voxel-based methods have further improved the details of surface geometry</strong>, they may be suboptimal in that the geometry features held by the <strong>voxel grids are uniformly distributed around 3D surfaces</strong>, while <strong>small structures are with complicated typologies and may need more flexible representation</strong>.</li><li>但是体素方法斥候的几何特征均匀分布在3D表面，而微小的结构拥有复杂的typologies，需要更灵活的表示方法</li><li>Point-based methods [2, 12, 32] bypass this problem, since the point clouds, initially estimated from COLMAP [24], are naturally distributed on the 3D surface with complicated structures. Point-NeRF [32] proposes to model point-based radiance field, which <strong>uses an MLP network to aggregate the neural points</strong> in its neighborhood to regress the volume density and view-dependent radiance at that location. However, the point-based methods are also limited in practical application, since their <strong>reconstruction performance depends on the initially estimated point clouds that often have holes and outliers</strong>.</li><li>点云的方法依赖初始估计的点云，这些点云通常具有孔洞和离群</li></ul></li></ul></li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Preliminaries-Neus"><a href="#Preliminaries-Neus" class="headerlink" title="Preliminaries: Neus"></a>Preliminaries: Neus</h2><h2 id="Deformable-Anchors-DA"><a href="#Deformable-Anchors-DA" class="headerlink" title="Deformable Anchors (DA)"></a>Deformable Anchors (DA)</h2><p>目的：提高体素网格表示的灵活性</p><ul><li>从图中可以看出，NeuDA变形后的grid距离Surface更近一些，即可以使采样点插值时更多依赖于表面，即渲染时也会更多地考虑到3D空间相邻的信息</li></ul><p>沿着特定光线上的采样点：$p\in\mathbb{R}^3$</p><p>$\begin{aligned}\phi(p,\psi(G))&amp;=\sum_{v\in\mathcal{V}}w(p_v)\cdot\gamma(p_v+\triangle p_v),\\\psi(G)&amp;=\left\{p_v,\triangle p_v|v\in G\right\}.\end{aligned}$</p><ul><li>G: anchor grid</li><li>$\psi(G)$ ： a set of deformable anchors</li><li>$\gamma(p_v+\triangle p_v)$:  frequency encoding function</li><li>$w(p_v)$: cosine similarity as weight , measure the <strong>contributions of different anchors to the sampled point</strong><ul><li>$w(p_{n})=\frac{\hat{w}(p_{n})}{\sum_{n}\hat{w}(p_{n})},\quad\hat{w}(p_{n})=\frac{p\cdot p_{n}}{|p||p_{n}|}.$</li></ul></li></ul><p>$\begin{aligned}\mathcal{F}(x;\theta)&amp;=\mathcal{F}\left(\phi\left(p,\psi(G)\right);\theta\right)\\&amp;=\left(f(x;\theta),\hat{n}(x;\theta),z(x;\theta)\right).\end{aligned}$</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718145119.png" alt="image.png"></p><p>相比Neus，NeuDA具有</p><ul><li>HPE</li><li>法向损失$\mathcal{L}_{norm}$ </li><li>更浅的MLP</li></ul><h2 id="Hierarchical-Positional-Encoding"><a href="#Hierarchical-Positional-Encoding" class="headerlink" title="Hierarchical Positional Encoding"></a>Hierarchical Positional Encoding</h2><p>several levels of anchor grid（8 levels in this paper）</p><ul><li>bad: applying the standard positional encoding function [17] to each level followed by a concatenation operation would <strong>produce a large-dimension embedding</strong></li><li>ours: We argue that different anchor grid levels could have their own responsibilities for handling global structures or capturing detailed geometry variations.</li></ul><p>在水平L的网格中，给定锚点$p_{l}\in\mathbb{R}^3$，则the frequency encoding function: $\gamma(p_l)=\left(\sin(2^l\pi p_l),\cos(2^l\pi p_l)\right).$分别应用于$p_{l}$中的三个坐标值，然后每个L网格，经过interpolation operation返回a small 6-dimension embedding：$\phi(\hat{p}_{l})$</p><p>Finally, we concatenate multi-level embedding vectors to obtain the hierarchical positional encoding:</p><p>$\mathcal{H}(p)=(\phi(\hat{p}_0),\phi(\hat{p}_1),…,\phi(\hat{p}_{L-1})),$编码后的结果输入进SDF网络中</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718144934.png" alt="image.png"></p><h2 id="Objectives"><a href="#Objectives" class="headerlink" title="Objectives"></a>Objectives</h2><p>We minimize the mean absolute errors between the rendered and ground-truth pixel colors <strong>as the indirect supervision</strong> for the SDF prediction function: 间接监督SDF的预测</p><p>$\mathcal{L}_{c}=\frac1{\mathcal{R}}\sum_{r\in\mathcal{R}}\Big|C(r)-\hat{C}(r)\Big|,$</p><p>Eikonal term：<br>$\mathcal{L}_{reg}=\frac{1}{\mathcal{R}N}\sum_{r,i}(|\nabla f(\mathcal{H}(p_{r,i}))|_{2}-1)^{2},$</p><p>$\mathcal{L}_{mask}=\mathrm{BCE}(m_r,\sum_i^nT_{r,i}\alpha_{r,i}),$</p><p>$m_{r}$是射线r的掩码标签，是真实值，与权重累计opacity(预测值)进行BCE处理</p><p>本文额外添加了一个：NeuDA的SDF网络还输出一个预测的法向量$\hat n$ ，与sdf的梯度即真实法向量进行取差，并沿着光线求出该像素点的法向量之差作为法向量损失</p><p>$\mathcal{L}_{norm}=\sum_{r,i}T_{r,i}\alpha_{r,i}\left|\nabla f(\mathcal{H}(p_{r,i}))-\hat{n}_{r,i}\right|$</p><p>$\mathcal{L}=\mathcal{L}_{c}+\lambda_{eik}\mathcal{L}_{reg}+\lambda_{norm}\mathcal{L}_{norm}+\lambda_{mask}\mathcal{L}_{mask}.$</p><p>本文：</p><ul><li>$\lambda_{eik}=0.1$</li><li>$\lambda_{normal}=3 \times 10^{-5}$</li><li>$\lambda_{mask}=0.1$</li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
            <tag> Efficiency </tag>
            
            <tag> Encoding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparseNeuS</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Sparse/SparseNeuS/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Sparse/SparseNeuS/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse Views</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://www.xxlong.site/">Xiaoxiao Long</a>    <a href="https://clinplayer.github.io/">Cheng Lin</a>    Peng Wang    <a href="https://homepages.inf.ed.ac.uk/tkomura/">Taku Komura</a>   <a href="https://www.cs.hku.hk/people/academic-staff/wenping/">Wenping Wang</a></td></tr><tr><td>Conf/Jour</td><td><a href="https://eccv2022.ecva.net/">ECCV</a></td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://www.xxlong.site/SparseNeuS/">SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse Views (xxlong.site)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?noteId=1876524675392434432">SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse views (readpaper.com)</a></td></tr></tbody></table></div><p>从仅2~3张的稀疏输入中重建表面</p><ul><li>首先，我们提出了一个<strong>多层几何推理框架</strong>，以粗到细的方式恢复表面。</li><li>其次，我们采用了一种<strong>多尺度颜色混合方案</strong>，该方案联合评估局部和背景亮度一致性，以获得更可靠的颜色预测。</li><li>第三，采用<strong>一致性感知的微调方案</strong>，控制遮挡和图像噪声引起的不一致区域，得到准确、干净的重建。</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718133443.png" alt="image.png"></p><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>提出了一种新的基于神经渲染的表面重建方法SparseNeuS，用于从多视图图像中恢复表面。我们的方法可以推广到新的场景，并产生高质量的稀疏图像重建，这是以前的作品[44,49,32]所难以做到的。因此，<strong>为了使我们的方法推广到新的场景，我们引入了几何编码体来编码通用几何推理的几何信息。此外，针对稀疏视图设置困难的问题，提出了一系列策略</strong>。</p><ul><li>首先，我们提出了一个多层几何推理框架，以粗到细的方式恢复表面。</li><li>其次，我们采用了一种多尺度颜色混合方案，该方案联合评估局部和背景亮度一致性，以获得更可靠的颜色预测。</li><li>第三，采用一致性感知的微调方案，控制遮挡和图像噪声引起的不一致区域，得到准确、干净的重建。<br>实验表明，该方法在重建质量和计算效率方面都优于目前的方法。由于采用了符号距离场，我们的方法只能产生闭面重构。未来可能的方向包括利用其他表示，如无符号距离场来重建开放表面物体。</li></ul><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>介绍了一种新的基于神经渲染的多视图图像表面重建方法——SparseNeuS。当只提供稀疏图像作为输入时，这项任务变得更加困难，在这种情况下，现有的神经重建方法通常会产生不完整或扭曲的结果。此外，它们无法推广到未见过的新场景，阻碍了它们在实践中的应用。相反，<strong>SparseNeuS可以泛化到新的场景，并且可以很好地处理稀疏图像(少至2或3张)</strong>。<br>SparseNeuS采用有符号距离函数(SDF)作为表面表示，并通过<strong>引入几何编码体从图像特征中学习泛化先验</strong>，用于通用表面预测。此外，引入了几种策略来有效地利用稀疏视图进行高质量的重建，包括:<br>1)多级几何推理框架，以粗到精的方式恢复表面;<br>2)多尺度颜色混合方案，实现更可靠的颜色预测;<br>3)一致性感知微调方案，控制遮挡和噪声引起的不一致区域。<br>大量的实验表明，我们的方法不仅优于最先进的方法，而且具有良好的效率、通用性和灵活性。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>从多视图图像中重建三维几何结构是计算机视觉中的一个基本问题，已经被广泛研究了几十年。<strong>传统的多视图立体方法</strong>[2,8,36,18,37,7,19]通过在输入图像中寻找相应的匹配来重建三维几何形状。<strong>然而，当只有一组稀疏的图像作为输入时，图像噪声、弱纹理和反射使得这些方法难以建立密集和完整的匹配</strong>。</p><p>随着神经隐式表示的最新进展，<strong>神经表面重建方法</strong>[44,50,49,32]利用神经渲染来共同优化隐式几何和辐射场，通过最小化渲染视图和地面真实视图的差异。虽然这些方法可以产生似是而非的几何形状和逼真的新颖视图，但它们<strong>有两个主要的局限性</strong>。</p><ul><li>首先，现有的方法严重依赖于大量的输入视图，即密集视图，这在实践中通常是不可用的。</li><li>其次，它们需要耗时的逐场景优化重建，因此无法推广到新的场景。为了使这种重建方法适用于实际应用，需要解决这些限制。</li></ul><p>我们提出了一种新的多视图表面重建方法SparseNeuS，它具有两个明显的优点:</p><ul><li>它具有良好的泛化能力;</li><li>只需要一组稀疏的图像(少则2 ~ 3张)即可成功重建。</li></ul><p>SparseNeuS通过从图像特征中学习可泛化先验并分层地利用稀疏输入中编码的信息来实现这些目标。</p><p>为了学习可泛化先验，根据MVSNerf[3]，我们构建了一个<strong>几何编码体</strong>，该编码体聚集了<strong>来自多视图输入的2D图像特征</strong>，并使用这些信息潜在特征来推断3D几何。因此，我们的曲面预测网络采用混合表示作为输入，即xyz坐标和来自几何编码体的相应特征，来预测重建曲面的网络编码符号距离函数(SDF)。</p><p>我们的pipeline中最关键的部分是如何有效地整合来自稀疏输入图像的有限信息，通过神经渲染获得高质量的表面。为此，我们介绍了一些应对这一挑战的策略。</p><ul><li>首先是一种<strong>多级几何推理方案</strong>，由粗到细逐级构造曲面。我们使用级联体编码结构，即编码相对全局特征的粗体来获得高级几何形状，粗体引导的细体来细化几何形状。每个场景的微调过程被进一步纳入到该方案中，该方案以推断的几何形状为条件，以构建细微的细节来生成更细粒度的表面。这种多层次的方案将高质量的重建任务分为几个步骤。每一步都是基于前一步的几何图形，重点是构建更精细的细节。此外，由于该方案的层次性，大大提高了重建效率，因为可以丢弃大量远离粗糙表面的样本，从而不会增加精细几何推理的计算负担。</li><li>我们提出的第二个重要策略是用于新视图合成的<strong>多尺度颜色弯曲方案</strong>。鉴于稀疏图像中的信息有限，网络将难以直接回归准确的颜色以呈现新视图。因此，我们通过预测输入图像像素的线性混合权重来获得颜色来缓解这个问题。具体来说，<strong>我们采用基于像素和基于补丁的混合</strong>来共同评估局部和上下文的亮度一致性。当输入是稀疏的时，这种多尺度混合方案产生更可靠的颜色预测。</li><li>多视图3D重建的另一个挑战是，<strong>由于遮挡或图像噪声，3D表面点在不同视图之间通常没有一致的投影</strong>。在输入视图数量较少的情况下，几何推理对每个图像的依赖性进一步增加，从而加剧了问题并导致几何变形。为了应对这一挑战，我们在微调阶段提出了<strong>一致性感知的微调方案</strong>。该方案自动检测缺乏一致投影的区域，并在优化中排除这些区域。事实证明，该策略有效地使微调表面不易受遮挡和噪声的影响，从而更准确，更清洁，有助于高质量的重建。</li></ul><p>我们在DTU[11]和BlendedMVS[48]数据集上评估了我们的方法，并表明我们的方法在定量和定性上都优于最先进的无监督神经隐式表面重建方法</p><p>In Summary</p><ul><li>提出了一种基于神经绘制的表面重建方法。我们的方法学习了跨场景的可泛化先验，因此可以泛化到具有高质量几何结构的新场景进行3D重建。</li><li>我们的方法能够从稀疏的图像集(少至2或3张图像)进行高质量的重建。这是通过使用三种新策略有效地从稀疏输入图像推断3D表面来实现的:<ul><li>a)多级几何推理multi-level geometry reasoning;</li><li>b)多尺度配色multi-scale color blending;</li><li>c)一致性意识微调。consistency-aware fine-tuning</li></ul></li><li>我们的方法在重建质量和计算效率方面都优于最先进的方法</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li>Multi-view stereo(MVS)<ul><li>经典的MVS方法利用各种3D表示进行重建，例如:基于体素网格的[12,13,15,18,37,40]，基于3D点云的[7,19]，以及基于深度图的[2,8,36,42,47,10,27,26,25]。与体素网格和3D点云相比，深度图更加灵活，更适合并行计算，<strong>因此基于深度图的方法是最常用的，如著名的COLMAP</strong>[36]方法。<strong>基于深度图的方法首先估计每个图像的深度图，然后利用滤波操作将深度图融合在一起，形成全局点云，然后使用筛选泊松曲面重建[16]等网格算法进行进一步处理</strong>。这些方法在密集捕获的图像上取得了很好的效果。<strong>然而，由于图像数量有限，这些方法对图像噪声、弱纹理和反射更加敏感，使得这些方法难以产生完整的重建</strong></li></ul></li><li>Neural surface reconstruction<ul><li>近年来，三维几何的神经隐式表示已成功应用于形状建模[1,4,9,28,33,29]、新视图合成[39,24,30,21,34,38,38,43]和多视图三维重建[14,50,31,17,22,44,49,32,32,52,52,6]。</li><li>对于多视图重建任务，三维几何图形由神经网络表示，<strong>神经网络输出占用场或符号距离函数</strong>(SDF)。一些方法利用表面渲染[31]进行多视图重建，但它们总是<strong>需要额外的对象掩模</strong>[50,31]或深度先验[52]，这在实际应用中效率较低。</li><li>为了避免额外的掩码或深度先验，一些方法[44,49,32,6]<strong>利用体绘制进行重建</strong>。然而，它们也<strong>严重依赖于大量的图像来执行耗时的逐场景优化，因此无法推广到新的场景</strong>。</li><li>在泛化方面，已有一些成功的神经渲染尝试[51,45,3,23,5]。这些方法以稀疏视图为输入，利用图像的亮度信息生成新的视图，可以推广到未见场景。<strong>虽然这些方法可以生成合理的合成图像，但提取的几何图形往往存在噪声、不完整性和失真等问题</strong>。</li></ul></li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>给定几个(即三个)具有已知相机参数的视图，我们提出了一种<strong>分层恢复表面并跨场景进行泛化的新方法</strong>。如图2所示，我们的管道可以分为三个部分:<br>(1)几何推理。SparseNeuS首先构建级联几何编码体，对局部几何表面信息进行编码，并以粗到细的方式从体积中恢复表面(参见3.1节)。<br>(2)外观预测。SparseNeuS利用多尺度颜色混合模块通过汇总输入图像的信息来预测颜色，<strong>然后将估计的几何形状与预测的颜色相结合，使用体绘制来渲染合成视图</strong>(参见3.2节)。<br>(3)逐场景微调。最后，提出了一种一致性感知的微调方案，以进一步改善获得的具有细粒度细节的几何形状(参见3.3节)。<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230718133443.png" alt="image.png"><br><em>SparseNeuS概述。级联几何推理方案首先构建一个编码相对全局特征的粗体来获得基本几何，然后在粗层的引导下构建一个精细体来细化几何。最后，使用一致性感知微调策略来添加微妙的几何细节，从而产生具有细粒度表面的高质量重建。特别地，利用多尺度颜色混合模块进行更可靠的颜色预测</em></p><h2 id="Geometry-reasoning"><a href="#Geometry-reasoning" class="headerlink" title="Geometry reasoning"></a>Geometry reasoning</h2><p>SparseNeuS构建两种不同分辨率的级联几何编码体进行几何推理，聚合图像特征对局部几何信息进行编码。该方法首先从低分辨率的几何编码体中提取粗几何，然后用粗几何来指导精细层次的几何推理。</p><p><strong>几何编码体积</strong><br>对于N幅输入图像$\{I_i\}_{i=0}^{N-1}$捕获的场景，我们首先估计一个能够覆盖感兴趣区域的边界框。在居中输入图像的相机坐标系中定义边界框，然后网格化成规则体素。为了构造几何编码体M，通过二维特征提取网络从输入图像$\{I_i\}_{i=0}^{N-1}$中提取二维特征映射$\{F_{i}\}_{i=0}^{N-1}$。接下来，利用一张图像$I_{i}$的相机参数，将边界框的每个顶点v投影到每个特征映射$F_{i}$上，通过插值得到其特征$F_i(\pi_i(v))$，其中$\pi_i(v)$表示v在特征图$F_{i}$上的投影像素位置。为简便起见，我们将$F_i(\pi_i(v))$缩写为$F_{i}(v).$。</p><p>使用每个顶点的所有投影特征$\{F_i(v)\}_{i=0}^{N-1}$构造几何编码体积M。按照之前的方法[46,3]，我们首先计算顶点所有投影特征的方差来构建cost 体积B，然后应用稀疏的3D CNN $\mathbb{\Psi}$对代价体积B进行聚合，得到几何编码体积M:<br>$M=\psi(B),\quad B(v)=\mathrm{Var}\left(\{F_{i}(v)\}_{i=0}^{N-1}\right),$<br>其中Var为方差运算，计算每个顶点v的所有投影特征$\{F_i(v)\}_{i=0}^{N-1}$的方差。</p><p><strong>表面提取</strong><br>给定任意三维位置q, MLP网络$f_{θ}$以三维坐标及其对应的几何编码体M (q)的插值特征的组合作为输入，预测用于曲面表示的有符号距离函数(SDF) s(q)。其中，对其三维坐标进行位置编码PE，曲面提取操作表示为:$s(q)=f_\theta(\operatorname{PE}(q),M(q)).$</p><p><strong>级联体积方案</strong><br>为了平衡计算效率和重建精度，SparseNeuS构建了两种分辨率的级联几何编码体，以粗到精的方式进行几何推理。首先构建粗糙几何编码体来推断基本几何，该编码体呈现场景的全局结构，但由于体积分辨率有限，精度相对较低。在得到的粗几何结构的指导下，构建精细几何编码体，进一步细化表面细节。在精细级体积中，可以丢弃许多远离粗糙表面的顶点，这大大减少了计算内存负担，提高了效率。</p><h2 id="Appearance-prediction"><a href="#Appearance-prediction" class="headerlink" title="Appearance prediction"></a>Appearance prediction</h2><p>给定方向为d的射线上的任意3D位置q，我们通过汇总来自输入图像的外观信息来预测其颜色。在稀疏输入图像信息有限的情况下，网络很难直接回归颜色值来呈现新视图。与之前的作品[51,3]不同，sparseneus通过预测输入图像的混合权重来生成新的颜色。首先将一个位置q投影到输入图像上，得到相应的颜色$\{I_i(q)\}_{i=0}^{N-1}.$。然后使用估计的混合权值将来自不同视图的颜色混合在一起作为q的预测颜色。</p><p><strong>混合权重</strong><br>生成混合权值$\{w_i^q\}_{i=0}^{N-1}$的关键是考虑输入图像的摄影一致性。我们将q投影到特征映射$\{F_{i}\}_{i=0}^{N-1}$上，使用双线性插值提取相应的特征$\{F_i(q)\}_{i=0}^{N-1}$。此外，我们从不同的角度计算特征$\{F_i(q)\}_{i=0}^{N-1}$的均值和方差，以获取全局摄影一致性信息。每个特征$F_{i}(q)$与均值和方差连接在一起，然后输入到一个微小的MLP网络中生成一个新的特征$F_i^{\prime}(q)$。接下来，我们将新特征$F_i^{\prime}(q)$，查询射线相对于第i个输入图像的观察方向的观察方向$\Delta d_{i}=d{-}d_{i},$，以及三线性插值的体积编码特征M(q)输入MLP网络$f_c$，以生成混合权值:$w_{i}^{q}=f_{c}(F_{i}^{\prime}(q),M(q),\Delta d_{i}).$。最后，使用Softmax算子对混合权值$\{w_i^q\}_{i=0}^{N-1}$进行归一化。</p><p><strong>基于像素的颜色混合</strong><br>利用得到的混合权值，预测3D位置q的color $c_q$为其投影颜色$\{I_i(q)\}_{i=0}^{N-1}$在输入图像上的加权和。为了渲染查询光线的颜色，我们首先预测在光线上采样的3D点的颜色和SDF值。使用基于SDF的体绘制[44]，将采样点的颜色和SDF值聚合以获得光线的最终颜色。由于查询光线的颜色对应于合成图像的一个像素，因此我们将此操作命名为基于像素的混合。尽管对基于像素的混合所呈现的颜色的监督已经诱导出有效的几何推理，但像素的信息是局部的，缺乏上下文信息，因此当输入是稀疏的时，通常会导致不一致的表面斑块。</p><p><strong>基于补丁的颜色混合</strong><br>受经典patch 匹配的启发，我们考虑强制合成颜色和gt颜色上下文一致;即不仅在像素级，而且在贴片级。要渲染大小为k × k的patch的颜色，一种幼稚的实现是使用体渲染查询$k^2$射线的颜色，这会导致大量的计算量。因此，我们利用<strong>局部曲面假设和单应性变换</strong>来实现更有效的实现。</p><p>关键思想是估计采样点的局部平面，从而有效地导出局部patch。给定一个采样点q，我们利用SDF网络s(q)的性质，通过计算空间梯度来估计法向$n_{q}$，即$n_q=\nabla s(q)$。然后，我们对局部平面$(q,n_q)$上的一组点进行采样，将采样点投影到每个视图上，并对每个输入图像进行插值得到颜色。<strong>局部平面上的所有点与q共享相同的混合权值，因此只需要查询一次混合权值</strong>。利用局部平面假设，考虑查询三维位置的邻近几何信息，对局部补丁的上下文信息进行编码，增强了查询三维位置的几何一致性。通过采用基于patch的体绘制，合成区域比单个像素包含更多的全局信息，从而产生更有信息量和一致性的形状上下文，特别是在纹理较弱和强度变化的区域。</p><p><strong>体积渲染</strong><br>为了绘制经过场景的光线r的像素色C(r)或补丁色P (r)，我们查询光线上M个样本的像素色、补丁色$p_i$和sdf值$s_i$，然后利用Neus中体渲染函数将sdf值$s_i$转换为密度$σ_i$。最后，密度被用来沿着射线积累基于像素和基于补丁的颜色:$U(r)=\sum_{i=1}^MT_i\left(1-\exp\left(-\sigma_i\right)\right)u_i,\quad\mathrm{where}\quad T_i=\exp\left(-\sum_{j=1}^{i-1}\sigma_j\right)$<br>其中U (r)表示C (r)或P (r)， $u_{i}$表示第i个样本在射线上基于像素的颜色$c_{i}$或基于patch的颜色$p_{i}$。</p><h2 id="Per-scene-fine-tuning"><a href="#Per-scene-fine-tuning" class="headerlink" title="Per-scene fine-tuning"></a>Per-scene fine-tuning</h2><p>利用可推广的先验和有效的几何推理框架，在给定新场景的稀疏图像的情况下，SparseNeuS已经可以通过快速网络推理恢复几何表面。<strong>然而，由于稀疏输入视图中的信息有限，以及不同场景的高度多样性和复杂性，通用模型获得的几何形状可能包含不准确的异常值，缺乏微妙的细节</strong>。<br>因此，我们提出了一种新的微调方案，该方案以推断的几何形状为条件，重构细微细节并生成更细粒度的表面。由于网络推理给出的初始化，每个场景的优化可以快速收敛到一个高质量的曲面。</p><p><strong>精调网络Fine-tuning networks</strong><br>在微调中，我们直接优化得到的精细级几何编码体积和有符号距离函数(SDF)网络$f_θ$，而<strong>丢弃二维特征提取网络和三维稀疏CNN网络</strong>。<br>此外，在通用设置中使用的基于CNN的混合网络被一个微小的MLP网络所取代。虽然基于CNN的网络也可以用于逐场景微调，但通过实验，<strong>我们发现一个新的微型MLP可以加速微调而不会损失性能</strong>，因为MLP比基于CNN的网络小得多。MLP网络仍然输出混合权重$\{w_{i}^{q}\}_{i=0}^{N-1}$查询的3D位置q, 但是需要输入的组合3d坐标q,表面法向$n_q$,射线方向d, 预测SDFs(q), 和几何编码体积的插值特征M(q)。特别, 位置编码PE应用于3d位置q和射线方向d。<br>MLP网络$f_{c}^{\prime}$的定义是:$\{w_i^q\}_{i=0}^{N-1}=f_c^{\prime}(\mathrm{PE}(q),\mathrm{PE}(d),n_q,s(q),M(q))$,其中$\{w_i^q\}_{i=0}^{N-1}$为预测的混合权值，N为输入图像的个数。</p><p><strong>一致性感知的颜色损失</strong><br>我们观察到，在多视图立体中，<strong>由于投影可能被图像噪声遮挡或污染，3D表面点在不同视图之间往往没有一致的投影</strong>。因此，这些区域的误差处于次优状态，并且这些区域的预测曲面总是不准确和扭曲的。为了解决这个问题，我们提出了一种一致性感知的颜色损失来自动检测缺乏一致投影的区域，并在优化中排除这些区域:</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_{color}& =\sum_{r\in\mathbb{R}}O\left(r\right)\cdot\mathcal{D}_{pix}\left(C\left(r\right),\tilde{C}\left(r\right)\right)+\sum_{r\in\mathbb{R}}O\left(r\right)\cdot\mathcal{D}_{pat}\left(P\left(r\right),\tilde{P}\left(r\right)\right)  \\&+\lambda_0\sum_{r\in\mathbb{R}}log\left(O\left(r\right)\right)+\lambda_1\sum_{r\in\mathbb{R}}log\left(1-O\left(r\right)\right),\end{aligned}</script><p>其中r是一条查询射线，$\mathbb{R}$是所有查询射线的集合，$O (r)$是通过体绘制得到的沿射线r的累计权值之和。由式2，我们可以很容易地推导出$\begin{aligned}O\left(r\right)=\sum_{i=1}^{M}T_i\left(1-\exp\left(-\sigma_i\right)\right).\end{aligned}$。<br>C (r)和$\tilde{C}\left(r\right)$分别为查询射线的基于像素的渲染和gt的颜色，P (r)和$\tilde{P}\left(r\right)$分别为查询射线的基于patch的渲染和gt的颜色，$\mathcal{D}_{pix}$和$\mathcal{D}_{pat}$分别为渲染像素的颜色和渲染patch颜色的损失度量。经验上，我们选择$\mathcal{D}_{pix}$作为L1损失，$\mathcal{D}_{pat}$作为归一化互相关(NCC)损失。</p><p><strong>这个公式背后的原理是，投影不一致的点总是有比较大的颜色误差，在优化中不能最小化</strong>。因此，如果在优化过程中颜色误差难以最小化，我们强制将累积权值$O (r)$的和设为零，这样在优化过程中就会排除不一致的区域。为了控制一致性水平，我们引入了两个逻辑正则化术语:λ0/λ1的比值越小，保留的区域越多;否则，更多的区域被排除，表面更干净。</p><h2 id="Training-loss"><a href="#Training-loss" class="headerlink" title="Training loss"></a>Training loss</h2><p>通过增强合成颜色和地面真值颜色的一致性，SparseNeuS的训练不依赖于三维地面真值形状。总损失函数定义为三个损失项的加权和:<br>$\mathcal{L}=\mathcal{L}_\mathrm{color~}+\alpha\mathcal{L}_\mathrm{eik}+\beta\mathcal{L}_\mathrm{sparse}.$</p><p>我们注意到，在通用训练的早期阶段，估计的几何形状相对不准确，并且3D表面点可能有很大的误差，其中误差不能提供关于区域是否亮度一致的明确线索。我们在每个场景的微调中利用了一致性感知的颜色损失，并在通用模型的训练中删除了Eq. 3$\mathcal{L}_\mathrm{color~}$的最后两个一致性感知逻辑项。</p><p>对采样点应用Eikonal项[9]，对表面预测网络$f_θ$得到的SDF值进行正则化:<br>$\mathcal{L}_{eik}=\frac{1}{\left|\mathbb{Q}\right|}\sum_{q\in\mathbb{Q}}\left(\left|\nabla f_{\theta}\left(q\right)\right|_{2}-1\right)^{2},$</p><p>其中q为一个采样的三维点，$\mathbb{Q}$为所有采样点的集合，$\nabla f_{\theta}\left(q\right)$为网络$f_θ$相对于采样点q的梯度，$\left|\cdot\right|_{2}$为l2范数。Eikonal项强制网络$f_θ$具有单位l2范数梯度，这鼓励$f_θ$生成光滑的表面。</p><p>此外，由于体绘制中透光率的累积特性，可见面后面的不可见查询样本缺乏监督，导致可见面后面的自由面不可控。为了使我们的框架能够生成紧凑的几何曲面，我们<strong>采用稀疏正则化项来惩罚不可控的自由曲面</strong>:</p><p>$\mathcal{L}_{sparse}=\frac{1}{\left|\mathbb{Q}\right|}\sum_{q\in\mathbb{Q}}\exp\left(-\tau\cdot\left|s(q)\right|\right),$<br>其中$|s(q)|$为采样点q的绝对SDF值，τ为重新缩放SDF值的超参数。<strong>这一项将促使可见表面后面的点的SDF值远离0</strong>。在提取0级集SDF生成网格时，该项可以避免不可控的自由曲面。</p><h1 id="Datasets-and-Implementation"><a href="#Datasets-and-Implementation" class="headerlink" title="Datasets and Implementation"></a>Datasets and Implementation</h1><p><strong>Dataset</strong></p><ul><li>DTU: like IDR scene<ul><li>为了提高存储效率，我们使用分辨率为512 × 640的中心裁剪图像进行训练。我们观察到DTU数据集的图像包含大量的黑色背景，并且这些区域具有相当大的图像噪声，因此我们使用基于简单阈值的去噪策略来减轻训练图像中这些区域的噪声。可选地，RGB值为零的黑色背景可以用作简单的数据集，以鼓励这些区域的几何预测为空。</li></ul></li><li>BlendedMVS<ul><li>我们在BlendedMVS[48]数据集的7个具有挑战性的场景上进行了进一步测试。对于每个场景，我们选择一组分辨率为768 × 576的三幅图像作为输入。请注意，在每个场景微调阶段，我们仍然使用这三张图像进行优化，而不使用任何新图像。</li></ul></li></ul><p><strong>实现细节</strong><br>使用特征金字塔网络[20]作为图像特征提取网络，从输入图像中提取多尺度特征。我们使用类似U-Net的架构实现了稀疏的3D CNN网络，并使用torchsparse[41]作为3D稀疏卷积的实现。粗级和精细级几何编码体的分辨率分别为96 × 96 × 96和192 × 192 × 192。基于补丁的混合中使用的补丁大小为5 × 5。<br>我们采用两阶段的训练策略来训练我们的通用模型:</p><ul><li>第一阶段，首先训练粗级网络150k次迭代;</li><li>第二阶段，对精细级网络进行150k次迭代训练，对粗级网络进行固定训练。</li></ul><p>我们在两个RTX 2080Ti gpu上训练我们的模型，批处理大小为512射线。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>我们将我们的方法与三种最先进的方法进行比较:<br>1)通用的神经渲染方法，包括PixelNerf [51]， IBRNet[45]和MVSNerf[3]，其中我们使用密度阈值从学习的隐式场中提取网格;<br>2)基于逐场景优化的神经表面重建方法，包括IDR[50]、NeuS[44]、VolSDF[49]、UniSurf [32];<br>3)广泛使用的经典MVS方法COLMAP[35]，我们从COLMAP的输出点云中使用筛选泊松表面重建(filtered Poisson Surface Reconstruction)[16]重建网格。<br>所有方法都以三幅图像作为输入。</p><ul><li>Quantitative定量 comparisons.</li><li>Qualitative comparisons.</li><li>Ablations and analysis</li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Sparse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3DReconstruction </tag>
            
            <tag> Neus </tag>
            
            <tag> Sparse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PermutoSDF</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/PermutoSDF/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/PermutoSDF/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://radualexandru.github.io/">Radu Alexandru Rosu</a>, <a href="http://www.ais.uni-bonn.de/behnke/">Sven Behnke</a></td></tr><tr><td>Conf/Jour</td><td>CVPR</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://radualexandru.github.io/permuto_sdf/">PermutoSDF (radualexandru.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4738634003066650625&amp;noteId=1873869258829705728">PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices (readpaper.com)</a></td></tr></tbody></table></div><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230716172220.png" alt="image.png"></p><p>创新：用permutohedral lattice替换voxel hash encoding </p><ul><li><p>simplex,3D中的单纯形就是正四面体</p></li><li><p>几何细节光滑，通过曲率损失来实现</p></li><li>带Lipschitz常数的颜色MLP来训练，使得高频颜色与高频几何特征相匹配<ul><li>ref : <a href="https://readpaper.com/paper/4592561893885878273">[PDF] Learning Smooth Neural Functions via Lipschitz Regularization-论文阅读讨论-ReadPaper</a></li><li>$y=\sigma(\widehat{W}_ix+b_i),\quad\widehat{W}_i=m\left(W_i,\text{softplus}\left(k_i\right)\right)$</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230717135035.png" alt="image.png"></p><span id="more"></span><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>We proposed a combination of <strong>implicit surface representations and hash-based encoding methods</strong> for the task of reconstructing accurate geometry and appearance from unmasked posed color images. </p><p>We improved upon the voxel-based hash encoding by using a <strong>permutohedral lattice</strong> which is always faster in training and faster for inference in three and higher dimensions. Additionally, we proposed <strong>a simple regularization scheme</strong> that allows to recover fine geometrical detail at the level of pores毛孔 and wrinkles. Our full system can train in ≈ 30 min on an RTX 3090 GPU and <strong>render in real-time using sphere tracing</strong>. We believe this work together with the code release will help the community in a multitude of other tasks that require modeling signals using fast local features.</p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>To NGP_MHE and Neus_SDF<br>We propose improvements to the two areas by replacing the voxel hash encoding with <strong>a permutohedral lattice置换面体晶格</strong> which optimizes faster, especially for higher dimensions.</p><p>Accurate reconstruction geometry and appearance of scenes is an important component of many computer vision tasks. Recent Neural Radiance Field (NeRF)-like models represent the scene as a density and radiance field and, when supervised with enough input images, can render photorealistic novel views.</p><p>Speed：Works like INGP [18] further improve on NeRF by using a hash-based positional encoding which results in fast training and visually pleasing results. However, despite the photorealistic renderings, <strong>the reconstructed scene geometry can deviate severally from the ground-truth重建场景的几何表现不是很好</strong> For example, objects with high specularity高镜面 or view-dependent effects视觉相关影响 are often reconstructed as a cloud of low density; untextured regions can have arbitrary density in the reconstruction. </p><ul><li>无法处理高镜面、视觉相关的影响造成的低密度点云</li><li>无纹理区域在重建中密度随机</li></ul><p>SDF：Another line of recent methods tackles the issue of incorrect geometry by representing the surfaces of objects as binary occupancy or Signed Distance Function (SDF). This representation can also be optimized with volumetric rendering techniques that are supervised with RGB images. However, parametrization of the SDF as a single fully-connected Multi-Layer Perceptron (MLP) often leads to overly smooth geometry and color.</p><ul><li>SDF参数化为单个全连接MLP通常导致几何和颜色过于平滑</li></ul><p>In this work, we propose PermutoSDF, a method that combines the strengths of hash-based encodings and implicit surfaces. We represent the scene as an SDF and a color field, which we render using unbiased volumetric integration(Neus中提出的体积积分方法). A naive combination of these two methods would fail to reconstruct accurate surfaces however, <strong>since it lacks any inductive bias for the ambiguous cases like specular or untextured surfaces</strong>. 简单叠加NGP和Neus，依然对无纹理和镜面等模糊情况，缺乏任何归纳偏差<br>Attempting to regularize the SDF with a <strong>total variation loss</strong> or a <strong>curvature loss</strong> will produce a smoother geometry at the expense of losing smaller details. 如果使用总变异损失和曲率损失，几何更光滑，但是代价是丢失更小的细节</p><h2 id="In-this-work"><a href="#In-this-work" class="headerlink" title="In this work:"></a>In this work:</h2><p>In this work, we propose a regularization scheme that ensures both smooth geometry where needed and also reconstruction of fine details like pores and wrinkles 本文正则化方案，确保所需的平滑几何形状及可以重建毛孔、皱纹等细节<br>Furthermore, we improve upon the voxel hashing method of INGP by <strong>proposing permutohedral lattice hashing</strong>. The number of vertices per simplex (triangle, tetrahedron, . . . ) in this data structure scales linearly with dimensionality instead of exponentially as in the hyper-cubical voxel case 每个simplex的定点数随着维度数线性缩放，而不是指数缩放，We show that the <strong>permutohedral lattice</strong> performs better than voxels for 3D reconstruction and 4D background estimation.<br>Main contribution：</p><ul><li>a novel framework for optimizing neural implicit surfaces based on hash-encoding,</li><li>an extension of hash encoding to a permutohedral lattice which scales linearly with the input dimensions and allows for faster optimization, and</li><li>a regularization scheme that allows to recover accurate SDF geometry with a level of detail at the scale of pores and wrinkles.</li></ul><p>Related Work</p><ul><li>Classical Multi-View Reconstruction<ul><li>Multi-view 3D reconstruction has been studied for decades. The classical methods can be categorized as either <strong>depth map-based</strong> or <strong>volume-based</strong><ul><li>Depth map methods like COLMAP [25] reconstruct a depth map for each input view by matching photometrically consistent patches. The depth maps are fused to a global 3D point cloud and a watertight水密 surface is recovered using Poisson泊松 Reconstruction [12]. While COLMAP can give good results in most scenarios, it yields suboptimal次优 results on non-Lambertian surfaces. </li><li>Volume-based approaches fuse the depth maps into a volumetric structure (usually a Truncated Signed Distance Function) from which an explicit surface can be recovered via the marching cubes algorithm [15]. Volumetric methods work well when fusing multiple noisy depth maps but <strong>struggle with reconstructing thin surfaces and fine details</strong></li></ul></li></ul></li><li>NeRF Models<ul><li>A recent paradigm shift in 3D scene reconstruction has been the introduction of NeRF  </li><li>NeRFs represent the scene as density and radiance fields, parameterized by a MLP. Volumetric rendering is used to train them to match posed RGB images. This yields highly photorealistic renderings with view-dependent effects. However, <strong>the long training time of the original NeRF</strong> prompted a series of subsequent works to address this issue.</li></ul></li><li>Accelerating NeRF<ul><li>Two main areas were identified as problematic: <strong>the large number of ray samples that traverse empty space</strong> and <strong>the requirement to query a large MLP for each individual sample.</strong></li><li><a href="https://readpaper.com/paper/3044538714">[PDF] Neural Sparse Voxel Fields</a>  uses an octree to model only the occupied space and restricts samples to be generated only inside the occupied voxels. Features from the voxels are interpolated and a <strong>shallow MLP predicts color and density</strong>. This achieves significant speedups <strong>but requires complicated pruning剪枝 and updating of the octree structure</strong>.</li><li>DVGO  similarly models the scene with <strong>local features which are stored in a dense grid</strong> that is decoded by an MLP into view-dependent color. </li><li>Plenoxels completely removes any MLP and instead <strong>stores opacity and spherical harmonics (SH) coefficients at sparse voxel positions.</strong></li><li>Instant Neural Graphics Primitives proposes a hash-based encoding in which ray samples trilinearly interpolate features between eight positions from a hash map. A shallow MLP implemented as a fully fused CUDA kernel predicts color and density. Using a hash map for encoding has <strong>the advantage of not requiring complicated mechanisms for pruning or updating like in the case of octrees.</strong></li><li>In our work, we improve upon INGP by proposing <strong>a novel permutohedral lattice-based hash encoding</strong>, which is better suited for interpolating in high-dimensional spaces. We use our new encoding to reconstruct accurate 3D surfaces and model background as a 4D space.</li></ul></li><li>Implicit Representation<ul><li>Other works have focused on reconstructing the scene geometry using implicit surfaces. </li><li>SDFDiff discretizes离散化 SDF values on a dense grid and by defining a differentiable shading function can optimize the underlying geometry. However, their approach can neither recover arbitrary color values nor can it scale to higher-resolution geometry.</li><li>IDR and DVR represent the scene as SDF and occupancy map, respectively分别, and by using differentiable rendering can recover high-frequency geometry and color. However, both methods <strong>require 2D mask supervision</strong> for training which is not easy to obtain in practice.</li><li>In order to remove the requirement of mask supervision<ul><li>UNISURF optimizes an binary occupancy function through volumetric rendering. </li><li>VolSDF extends this idea to SDFs. </li><li>NeuS analyzes the biases caused by using volumetric rendering for optimizing SDFs and introduces an unbiased and occlusion-aware <strong>weighting scheme无偏碰撞的权重方案</strong> which allows to recover more accurate surfaces.</li></ul></li><li>In this work, we reconstruct a scene as SDF and color field without using any mask supervision. We model the scene <strong>using locally hashed features</strong> in order to recover finer detail than previous works. We also <strong>propose several regularizations</strong> which help to recover geometric detail at the level of pores and wrinkles.</li></ul></li></ul><h1 id="Method-Overview"><a href="#Method-Overview" class="headerlink" title="Method Overview"></a>Method Overview</h1><p>Given a series of images with poses $\{\mathcal{I}_{k}\}$, our task is to recover both surface S and appearance of the objects within.<br>We define the surface S as the zero level set of an SDF:$\mathcal{S}=\{\mathrm{x}\in\mathbb{R}^3|g(\mathrm{x})=0\}.$</p><p>$SDF,\chi = g(\operatorname{enc}(\mathbf{x};\theta_g);\Phi_g)$</p><ul><li>输入坐标x<br>$Color = c(\mathbf{h}_c,\mathbf{v},\mathbf{n},\chi;\Phi_c)$</li><li>$\mathbf{h}_c=\operatorname{enc}(\mathbf{x};\theta_c)$</li><li>$\mathbf{v}$为观察方向</li><li>$\mathbf{n}$为法向方向，sdf梯度</li><li>a learnable geometric feature $\chi$ which is output by the sdf network</li></ul><p>Note, that using two separate networks is crucial as we want to regularize each one individually in order to recover high-quality geometry.</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230716172220.png" alt="image.png"></p><h1 id="Permutohedral-Lattice-SDF-Rendering"><a href="#Permutohedral-Lattice-SDF-Rendering" class="headerlink" title="Permutohedral Lattice SDF Rendering"></a>Permutohedral Lattice SDF Rendering</h1><p>We now detail each network, the permutohedral lattice, and our training methodology.</p><h2 id="Volumetric-Rendering"><a href="#Volumetric-Rendering" class="headerlink" title="Volumetric Rendering"></a>Volumetric Rendering</h2><p>$\hat{C}(\mathbf{p})=\int_{t=0}^{+\infty}w(t)c(\mathbf{p}(t),\mathbf{v},\mathbf{n},\chi;\Phi_c),$</p><p>$\rho(t)=\max\left(\frac{-\frac{\mathrm{d}\psi_s}{\mathrm{d}t}(g(\mathbf{p}(t);\Phi_g))}{\psi_s(g(\mathbf{p}(t);\Phi_g))},0\right),$$\psi_s(x)=\frac{1}{1+e^{-ax}}$</p><p>$w(t)=T(t)\rho(t),\mathrm{~where~}T(t)=\exp\left(-\int_0^t\rho(u)\mathrm{d}u\right)$</p><h2 id="Hash-Encoding-with-Permutohedral-Lattice"><a href="#Hash-Encoding-with-Permutohedral-Lattice" class="headerlink" title="Hash Encoding with Permutohedral Lattice"></a>Hash Encoding with Permutohedral Lattice</h2><p>In order to facilitate learning of high-frequency details, INGP [18] proposed a hash-based encoding which maps a 3D spatial coordinate to a higher-dimensional space. The encoding maps a spatial position x into a cubical grid and linearly interpolates features from the hashed eight corners of the containing cube. A fast CUDA implementation interpolates over various multi-resolutional grids in parallel. The hash map is stored as a tensor of L levels, each containing up to T feature vectors with dimensionality F .</p><p>The speed of the encoding function is mostly determined by the number of accesses to the hash map as the operations to determine the eight hash indices are fast. Hence, it is of interest to reduce the memory accesses required to linearly interpolate features for position x. By using a <strong>tetrahedral lattice</strong> instead of a cubical one, <strong>memory accesses can be reduced</strong> by a factor of two as each simplex <strong>has only four vertices instead of eight</strong>. This advantage grows for higher dimensions when using a permutohedral lattice [1].<a href="https://readpaper.com/paper/1964772475">[PDF] Fast High‐Dimensional Filtering Using the Permutohedral Lattice-论文阅读讨论-ReadPaper</a></p><p>The permutohedral lattice divides the space into uniform simplices均匀单纯形 which form triangles and tetrahedra in 2D and 3D, respectively. The main advantage of this lattice is that <strong>given dimensionality d the number of vertices per simplex is d+1</strong>, which scales linearly instead of the exponential growth $2^{d}$ for hyper-cubical voxels. <strong>This ensures a low number of memory accesses to the hashmap and therefore fast optimization.</strong></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230717135035.png" alt="image.png"><br>Given a position x, the containing simplex can be obtained in $O(d^{2})$. Within the simplex, barycentric coordinates are calculated and d-linear interpolation is performed similar to INGP.<br>Similarly to INGP, we slice from lattices at multiple resolutions and concatenate the results. The final output is a high-dimensional encoding $h = enc(x; θ)$ of the input x <strong>given lattice features θ</strong>.</p><h2 id="4D-Background-Estimation"><a href="#4D-Background-Estimation" class="headerlink" title="4D Background Estimation"></a>4D Background Estimation</h2><p>For modeling the background, we follow the formulation of <strong>NeRF++</strong> which represents foreground volume as a unit sphere and background volume by an inverted sphere.</p><p>Points in the outer volume are represented using 4D positions (x′, y′, z′, 1/r) where (x′, y′, z′) is a unit-length directional vector and 1/r is the inverse distance.<br>对4D的背景进行编码时，a cubical voxel（NGP）需要查询16个顶点的特征值，而permutohedral lattice只需要查询5个顶点的特征值<br>We directly use this 4D coordinate to slice from a 4-dimensional lattice and obtain multi-resolutional features. A small MLP outputs the radiance and density which are volumetrically rendered and blended with the foreground. <strong>Please note that in 4D</strong>, the permutohedral lattice only needs to access five vertices(4d+1) for each simplex while a cubical voxel would need 16($2^{4}$). Our linear scaling with dimensionality is of significant advantage in this use case.</p><h1 id="PermutoSDF-Training-and-Regularization"><a href="#PermutoSDF-Training-and-Regularization" class="headerlink" title="PermutoSDF Training and Regularization"></a>PermutoSDF Training and Regularization</h1><p>简单融合使用loss：</p><ul><li>$\mathcal{L}_{\mathrm{rgb}}=\sum_p\lVert\hat{C}(\mathbf{p})-C(\mathbf{p})\rVert_2^2$</li><li>$\mathcal{L}_{\mathrm{eik}}=\sum_{x}\left(|\nabla g(\mathrm{enc}(\mathrm{x}))|-1\right)^2,$</li></ul><p>A naive combination of hash-based encoding and implicit surfaces can yield undesirable surfaces, though. While the model is regularized by the Eikonal loss, there are many surfaces that satisfy the Eikonal constraint.<br>For <strong>specular or untextured areas</strong>, the Eikonal regularization doesn’t provide enough information to properly recover the surface. To address this issue, we propose <strong>several regularizations</strong> that serve to both recover smoother surfaces and more detail.</p><h2 id="SDF-Regularization—-gt-smoother-surfaces"><a href="#SDF-Regularization—-gt-smoother-surfaces" class="headerlink" title="SDF Regularization—&gt;smoother surfaces"></a>SDF Regularization—&gt;smoother surfaces</h2><p>In order to aid the network in recovering smoother surfaces in reflective or untextured areas, we add a curvature loss on the SDF<br>Calculating the full 3×3 Hessian matrix can be expensive; so we approximate curvature as local deviation of the normal vector. Recall that we already have the normal $n = ∇g(enc(x))$ at each ray sample since it was required for the Eikonal loss. With this normal, we define a tangent vector切向量 $η$ by cross product with a random unit vector τ such that $η = n × τ$.Given this random vector in the tangent plane, we slightly perturb our sample x to obtain $\mathrm{x}_\epsilon=\mathrm{x}+\epsilon\eta.$. We obtain the normal at the new perturbed point as $\mathbf{n}_\epsilon=\nabla g(\mathrm{enc}(\mathbf{x}_\epsilon))$ and define a curvature loss based on the dot product between the normals at the original and perturbed points:<br>curvature loss: $\mathcal{L}_\mathrm{curv}=\sum_x(\mathbf{n}\cdot\mathbf{n}_\epsilon-1)^2.$<br>物理含义：<strong>表面上相邻两点的法向量应该是平行的</strong></p><h2 id="Color-Regularization详细解析深度学习中的-Lipschitz-条件-知乎-zhihu-com"><a href="#Color-Regularization详细解析深度学习中的-Lipschitz-条件-知乎-zhihu-com" class="headerlink" title="Color Regularization详细解析深度学习中的 Lipschitz 条件 - 知乎 (zhihu.com)"></a>Color Regularization<a href="https://zhuanlan.zhihu.com/p/389024283">详细解析深度学习中的 Lipschitz 条件 - 知乎 (zhihu.com)</a></h2><p>While the curvature regularization helps in recovering smooth surfaces, we observe that the network converges to an undesirable state where the geometry gets increasingly smoother while the color network learns to model all the high-frequency detail in order to drive the $\mathcal{L}_{rgb}$ to zero. Despite lowering $\mathcal{L}_{curv}$ during optimization, <strong>the SDF doesn’t regain back the lost detail</strong>. We show this behavior in Fig. 8. Recall that the color network is defined as $Color = c(\mathbf{h}_c,\mathbf{v},\mathbf{n},\chi;\Phi_c)$, with an input encoding of $h = enc(x; θ_{c})$. We observe that all the high-frequency detail learned by the color network has to be present in the weights of the MLP $Φ_{c}$ or the hashmap table $θ_{c}$ as all the other inputs are smooth. 颜色网络学习到的高频细节都在MLP的权重$Φ_{c}$或哈希表$θ_{c}$中，因为其他所有输入$\mathbf{v},\mathbf{n},\chi$都是平滑的</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230717141818.png" alt="image.png"></p><p>In order to recover fine geometric detail, we propose to learn a color mapping network that is itself smooth w.r.t. to its input such <strong>that large changes in color are matched with large changes in surfaces normal</strong>. Function smoothness can be studied in the context of Lipschitz continuous networks. A function f is <strong>k-Lipschitz continuous</strong> if it satisfies: $\underbrace{|f(d)-f(e)|}_{\text{change in the output}}\leq k\underbrace{|d-e|}_{\text{change in the input}}$<br>Intuitively, it sets k as an upper bound上限 for the rate of change of the function. We are interested in the color network being a smooth function (small k) such that high-frequency color is also reflected in high-detailed geometry. <em>There are several ways to enforce Lipschitz smoothness on a function . Most of them impose a hard 1-Lipschitz requirement or ignore effects such as network depth which makes them difficult to tune for our use case</em></p><h3 id="修改后的颜色MLP网络"><a href="#修改后的颜色MLP网络" class="headerlink" title="修改后的颜色MLP网络"></a>修改后的颜色MLP网络</h3><p>The recent work of Liu et al. <a href="https://readpaper.com/paper/4592561893885878273">[PDF] Learning Smooth Neural Functions via Lipschitz Regularization-论文阅读讨论-ReadPaper</a> provides a simple and interpretable framework for softly regularizing the Lipschitz constant of a network. Given an MLP layer $y =σ(W_{i}x+b_{i})$and a trainable Lipschitz bound $k_{i}$ for the layer, they replace the weight matrix $W_{i}$with:<br>$y=\sigma(\widehat{W}_ix+b_i),\quad\widehat{W}_i=m\left(W_i,\text{softplus}\left(k_i\right)\right)$</p><ul><li>$\text{softplus }(k_i)=\ln(1+e^{k_i})$</li><li>the function m(.) <strong>normalizes</strong> the weight matrix by rescaling each row of $W_{i}$ such that <strong>the absolute value of the row-sum</strong> is less than or equal to softplus (ki).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jax.numpy <span class="keyword">as</span> jnp </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalization</span>(<span class="params">Wi, softplus_ci</span>): <span class="comment"># L-inf norm</span></span><br><span class="line">    absrowsum = jnp.<span class="built_in">sum</span>(jnp.<span class="built_in">abs</span>(Wi), axis=<span class="number">1</span>) </span><br><span class="line">    scale = jnp.minimum(<span class="number">1.0</span>, softplus_ci/absrowsum) </span><br><span class="line"><span class="keyword">return</span> Wi * scale[:,<span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">y = sigma(normalization(Wi, softplus(ci))*x + bi)</span><br></pre></td></tr></table></figure><p>Since the product of per-layer Lipschitz constants $k_{i}$ is the Lipschitz bound for the whole network, we can regularize it using:</p><p>$\mathcal{L}_{\mathrm{Lipschitz}}=\prod_{l}^{i=1}\text{softplus}\left(k_{i}\right).$</p><p>In addition to regularizing the color MLP, we also apply weight decay of 0.01 to the color hashmap $θ_{c}$.</p><h2 id="Lipschitz-in-DL"><a href="#Lipschitz-in-DL" class="headerlink" title="Lipschitz in DL"></a>Lipschitz in DL</h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/389024283">详细解析深度学习中的 Lipschitz 条件 - 知乎 (zhihu.com)</a></p></blockquote><p>$\underbrace{|f(d)-f(e)|}_{\text{change in the output}}\leq k\underbrace{|d-e|}_{\text{change in the input}}$<br>$||f(x,y_1)-f(x,y_2)||\leq L||y_1-y_2||,$</p><p>Lipschitz continuous含义：存在一个实数 L ，使得对于函数f(x) 上的每对点，连接它们的线的斜率的绝对值不大于这个实数L。最小的 L 称为该函数的 Lipschitz 常数，意味着函数被一次函数上下夹逼。</p><p>eg：$f:[-3,7]\to\mathbb{R},f(x)=x^2:$ 符合利普希茨连续条件, L=14</p><p>(Bi-Lipschitz continuous $\begin{aligned}\frac{1}{L}||y_1-y_2||\leq||f(x,y_1)-f(x,y_2)||\leq L||y_1-y_2||,\end{aligned}$)</p><p>$\left\{\begin{array}{c}f(y)\leq f(x)+L||y-x||\\f(y)\geq f(x)-L||y-x||\end{array}\right.$<br>可以看出，x确定后，f(y) 被一个一次函数bound在一定的范围内</p><p> <a href="https://readpaper.com/paper/4592561893885878273">[PDF] Learning Smooth Neural Functions via Lipschitz Regularization-论文阅读讨论-ReadPaper</a></p><h2 id="Training-Schedule"><a href="#Training-Schedule" class="headerlink" title="Training Schedule"></a>Training Schedule</h2><p>Several scheduling considerations must be observed for our method. In Eq. 3, the sigmoid function $ψ_{s}(.)$ is parametrized with 1/a which is the standard deviation that controls the range of influence of the SDF towards the volume rendering. <strong>In NeuS</strong> 1/a is considered a learnable parameter which <strong>starts at a high value and decays towards zero as the network converges</strong></p><p>However, we found out that considering it as a learnable parameter can lead to the network missing thin object features due to the fact that large objects in the scene dominate the gradient towards a. <strong>Instead, we use a scheduled linear decay 1/a over 30 k iterations which we found to be robust for all the objects we tested.</strong></p><p>30k iteration前，1/a不变，30k以后，对1/a做线性衰减<br>100k前，loss：</p><p>In order to recover smooth surfaces, we train the first 100 k iterations using curvature loss:<br>$\mathcal{L}=\mathcal{L}_{\mathrm{rgb}}+\lambda_{1}\mathcal{L}_{\mathrm{eik}}+\lambda_{2}\mathcal{L}_{\mathrm{curv}}.$</p><p>后100k，loss：<br>For further 100 k iterations, we recover detail by <strong>removing the curvature loss and adding the regularization of the color network</strong> $λ_3 \cdot \mathcal{L}_{\mathrm{Lipschitz}}$ </p><p>初始10k，从粗到精，退火哈希map的水平L(多分辨率)<br>In addition, we initialize our network with the SDF of a sphere at the beginning of the optimization and anneal the levels L of the hash map in a coarse-to-fine manner over the course of the initial 10 k iterations. </p><h1 id="Acceleration"><a href="#Acceleration" class="headerlink" title="Acceleration"></a>Acceleration</h1><p>Similar to other volumetric rendering methods, a major bottleneck for the speed is the number of position samples considered for each ray. We use several methods to accelerate both training and inference.</p><h2 id="Occupancy-Grid"><a href="#Occupancy-Grid" class="headerlink" title="Occupancy Grid"></a>Occupancy Grid</h2><p>In order to concentrate more samples near the surface of the SDF and have fewer in empty space, we use an occupancy grid modeled as a dense grid of resolution $128^{3}.$ We maintain two versions of the grid, <strong>one with full precision</strong>, storing the SDF value at each voxel, and <strong>another containing only a binary occupancy bit</strong>. The grids are laid out in Morton order to ensure fast traversal. Note that differently from INGP, we store signed distance and not density in our grid. <strong>This allows us to use the SDF volume rendering equations to determine if a certain voxel has enough weight</strong> that it would meaningfully contribute to the integral Eq. 2 and therefore should be marked as occupied space.</p><p>Morton order: </p><ul><li>ref : <a href="https://zhuanlan.zhihu.com/p/468542418">Morton code理解 - 知乎 (zhihu.com)</a><br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230717154630.png" alt="image.png"></li></ul><h2 id="Sphere-Tracing"><a href="#Sphere-Tracing" class="headerlink" title="Sphere Tracing"></a>Sphere Tracing</h2><p>Having an SDF opens up a possibility for accelerating rendering at inference time by using sphere tracing. This can be significantly faster than volume rendering as most rays converge in 2-3 iterations towards the surface.<br>We create a ray sample at the first voxel along the ray that is marked as occupied. We run sphere tracing for a predefined number of iterations and march not-yet-converged samples towards the surface (indicated by their SDF being above a certain threshold). Once <strong>all samples have converged or we reached a maximum number of sphere traces</strong>, we <strong>sample the color network once and render</strong>. </p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230717145036.png" alt="image.png"></p><h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><p>在CUDA kernel中实现：</p><ul><li>permutohedral lattices，slices in parallel from all resolutions</li><li>The backward pass for updating the hashmap $\frac{∂h}{∂θ}$<ul><li>We use the chain rule to backpropagate the upstream gradients as:$\frac{\partial\mathcal{L}}{\partial\mathbf{h}}\frac{\partial\mathbf{h}}{\partial\theta}.$</li></ul></li><li>the partial derivative of encoding $h = enc(x; θ_{g})$ w.r.t. to spatial position x<ul><li>$\mathbf{n}=\nabla g(\mathrm{enc}(\mathbf{x}))$</li><li>Again, the chain rule is applied with the autograd partial derivative of g(.) as: $\frac{\partial g}{\partial\mathbf{h}}\frac{\partial\mathbf{h}}{\partial\mathbf{x}}$ to obtain the normal.</li></ul></li><li>Furthermore, since we use this normal as part of our loss function Leik, we support also double backward operations, i.e., we also implement CUDA kernels for $\partial(\frac{\partial\tilde{\mathcal{L}}}{\partial\mathbf{x}})/\partial\theta$ and $\partial(\frac{\partial\mathcal{L}}{\partial\mathbf{x}})\big/\partial(\frac{\partial\mathcal{L}}{\partial\mathbf{h}})$ Hence, we can run our optimization entirely within PyTorch’s autograd engine, without requiring any finite differences.<br>将梯度计算和反向传播在CUDA kernel中部署，可以在Pytorch中使用autograd实现反向传播，而不需要进行有限差分获得梯度</li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Efficiency </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
            <tag> Efficiency </tag>
            
            <tag> Encoding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neuralangelo</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/Neuralangelo/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/Neuralangelo/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Neuralangelo: High-Fidelity Neural Surface Reconstruction</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://mli0603.github.io/">Zhaoshuo Li</a><a href="https://tom94.net/">Thomas Müller</a><a href="https://research.nvidia.com/person/alex-evans">Alex Evans</a><a href="https://www.cs.jhu.edu/~rht/">Russell H. Taylor</a><a href="https://mathiasunberath.github.io/">Mathias Unberath</a><a href="https://mingyuliu.net/">Ming-Yu Liu</a><a href="https://chenhsuanlin.bitbucket.io/">Chen-Hsuan Lin</a></td></tr><tr><td>Conf/Jour</td><td>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://research.nvidia.com/labs/dir/neuralangelo/">Neuralangelo: High-Fidelity Neural Surface Reconstruction (nvidia.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4766570421235482625&amp;noteId=1871247300347519744">Neuralangelo: High-Fidelity Neural Surface Reconstruction (readpaper.com)</a></td></tr></tbody></table></div><p><strong>创新：新的计算梯度的方法——数值梯度、粗到精地逐步优化——数值梯度的补偿$\epsilon$，粗网格先激活，当$\epsilon$减小到精网格的空间大小时，逐步激活精网格</strong><br>SR Issue: Current methods struggle to recover detailed structures of real-world scenes<br>To address : present Neuralangelo (combines the representation power of multi-resolution 3D hash grids with neural surface rendering)</p><ul><li>numerical gradients for computing higher-order derivatives as a smoothing operation<ul><li><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230716140552.png" alt="image.png"></li></ul></li><li>coarse-to-fine optimization on the hash grids controlling different levels of details<br>even wo auxiliary inputs such as depth , Neuralangelo can effectively recover dense 3D surface structures from multi-view images with fidelity 保真 significantly surpassing previous methods, enabling detailed large-scale scene reconstruction from RGB video captures.</li></ul><p><strong>our future work</strong> to explore a more efficient sampling strategy to accelerate the training process.</p><span id="more"></span><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="paper"><a href="#paper" class="headerlink" title="paper"></a>paper</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul><li>DTU dataset</li><li>Tanks and Temples datase</li></ul><h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><p>Our hash encoding <strong>resolution spans 25 to 211 with 16 levels</strong>. Each hash entry has a channel size of 8. The maximum number of hash entries of each resolution is 222.<br><strong>We activate 4 and 8 hash resolutions at the beginning of optimization for DTU dataset and Tanks and Temples respectively</strong>, due to differences in scene scales. <strong>We enable a new hash resolution every 5000 iterations when the step size ε equals its grid cell size.</strong> For all experiments, we do not utilize auxiliary data such as segmentation(mask) or depth during the optimization process.</p><h3 id="Evaluation-criteria"><a href="#Evaluation-criteria" class="headerlink" title="Evaluation criteria."></a>Evaluation criteria.</h3><ul><li>Chamfer distance and F1 score for surface evaluation<ul><li><a href="https://readpaper.com/paper/2085905957">Large Scale Multi-view Stereopsis Evaluation-论文阅读讨论-ReadPaper</a></li><li><a href="https://readpaper.com/paper/2738551266">Tanks and temples: benchmarking large-scale scene reconstruction-论文阅读讨论-ReadPaper</a><ul><li>Chamfer 距离越小，表示预测分割与真实分割越接近</li><li>F1 得分的取值范围在 0 和 1 之间，越接近 1 表示模型的性能越好</li></ul></li></ul></li><li>We use peak signal-tonoise ratio (PSNR) to report image synthesis qualities.</li></ul><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><h2 id="数据集生成"><a href="#数据集生成" class="headerlink" title="数据集生成"></a>数据集生成</h2><p>colmap 数据生成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DATA_PATH=datasets/<span class="variable">$&#123;SEQUENCE&#125;</span>_ds<span class="variable">$&#123;DOWNSAMPLE_RATE&#125;</span></span><br><span class="line">bash projects/neuralangelo/scripts/run_colmap.sh <span class="variable">$&#123;DATA_PATH&#125;</span></span><br></pre></td></tr></table></figure><p>or</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">colmap gui Automatic reconstruction</span><br><span class="line">+</span><br><span class="line">BA: Bundle adjustment</span><br><span class="line">+</span><br><span class="line">Undistortion</span><br></pre></td></tr></table></figure><p>最后数据集：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DATA_PATH</span><br><span class="line">├─ database.db      (COLMAP database)</span><br><span class="line">├─ images           (undistorted input images)</span><br><span class="line">├─ images_raw       (raw input images)</span><br><span class="line">├─ sparse           (COLMAP data from SfM)</span><br><span class="line">│  ├─ cameras.bin   (camera parameters)</span><br><span class="line">│  ├─ images.bin    (images and camera poses)</span><br><span class="line">│  ├─ points3D.bin  (sparse point clouds)</span><br><span class="line">│  ├─ 0             (a directory containing individual SfM models. There could also be 1, 2... etc.)</span><br><span class="line">├─ run-colmap-geometric.sh 几何一致性稠密重建 example 脚本</span><br><span class="line">├─ run-colmap-photometric.sh 光度一致性稠密重建 example 脚本</span><br><span class="line">│  ...</span><br><span class="line">├─ stereo (COLMAP data for MVS, not used here)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">&#123;DATA_PATH&#125;/transforms.json</span></span><br><span class="line">python3 projects/neuralangelo/scripts/convert_data_to_json.py --data_dir $&#123;DATA_PATH&#125; --scene_type $&#123;SCENE_TYPE&#125;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># eg:</span></span></span><br><span class="line">python projects/neuralangelo/scripts/convert_data_to_json.py --data_dir ./inputs/Miku --scene_type object</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Config files projects/neuralangelo/configs/custom/&#123;SEQUENCE&#125;.yaml</span></span><br><span class="line">python3 projects/neuralangelo/scripts/generate_config.py --sequence_name $&#123;SEQUENCE&#125; --data_dir $&#123;DATA_PATH&#125; --scene_type $&#123;SCENE_TYPE&#125;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># eg：</span></span></span><br><span class="line">python projects/neuralangelo/scripts/generate_config.py --sequence_name Miku --data_dir ./inputs/Miku --scene_type object</span><br></pre></td></tr></table></figure><ul><li><code>SCENE_TYPE</code>: can be one of  <code>&#123;outdoor,indoor,object&#125;</code>.</li><li><code>SEQUENCE</code>: your custom name for the video sequence.</li></ul><h2 id="run"><a href="#run" class="headerlink" title="run"></a>run</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">EXPERIMENT=toy_example</span><br><span class="line">GROUP=example_group</span><br><span class="line">NAME=example_name</span><br><span class="line">CONFIG=projects/neuralangelo/configs/custom/<span class="variable">$&#123;EXPERIMENT&#125;</span>.yaml</span><br><span class="line">GPUS=1  <span class="comment"># use &gt;1 for multi-GPU training!</span></span><br><span class="line">torchrun --nproc_per_node=<span class="variable">$&#123;GPUS&#125;</span> train.py \</span><br><span class="line">    --logdir=logs/<span class="variable">$&#123;GROUP&#125;</span>/<span class="variable">$&#123;NAME&#125;</span> \</span><br><span class="line">    --config=<span class="variable">$&#123;CONFIG&#125;</span> \</span><br><span class="line">    --show_pbar</span><br><span class="line"></span><br><span class="line">eg:</span><br><span class="line">EXPERIMENT=Miku</span><br><span class="line">GROUP=dtu</span><br><span class="line">NAME=Miku</span><br><span class="line">CONFIG=projects/neuralangelo/configs/custom/<span class="variable">$&#123;EXPERIMENT&#125;</span>.yaml</span><br><span class="line">GPUS=1</span><br><span class="line">torchrun --nproc_per_node=<span class="variable">$&#123;GPUS&#125;</span> train.py --logdir=logs/<span class="variable">$&#123;GROUP&#125;</span>/<span class="variable">$&#123;NAME&#125;</span> --config=<span class="variable">$&#123;CONFIG&#125;</span> --show_pbar</span><br><span class="line">--data.readjust.scale=0.5 \</span><br><span class="line">--max_iter=20000 \</span><br><span class="line">--validation_iter=99999999 \</span><br><span class="line">--model.object.sdf.encoding.coarse2fine.step=200 \</span><br><span class="line">--model.object.sdf.encoding.hashgrid.dict_size=19 \</span><br><span class="line">--optim.sched.warm_up_end=200 \</span><br><span class="line">--optim.sched.two_steps=[12000,16000]</span><br><span class="line">--checkpoint <span class="variable">$&#123;CHECKPOINT&#125;</span> --resume</span><br><span class="line"></span><br><span class="line"><span class="comment"># shutdown after run</span></span><br><span class="line">&amp;&amp; /usr/bin/shutdown</span><br></pre></td></tr></table></figure><h2 id="Isosurface-extraction"><a href="#Isosurface-extraction" class="headerlink" title="Isosurface extraction"></a>Isosurface extraction</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">CHECKPOINT=logs/<span class="variable">$&#123;GROUP&#125;</span>/<span class="variable">$&#123;NAME&#125;</span>/xxx.pt</span><br><span class="line">OUTPUT_MESH=xxx.ply</span><br><span class="line">CONFIG=logs/<span class="variable">$&#123;GROUP&#125;</span>/<span class="variable">$&#123;NAME&#125;</span>/config.yaml</span><br><span class="line">RESOLUTION=2048</span><br><span class="line">BLOCK_RES=128</span><br><span class="line">GPUS=1  <span class="comment"># use &gt;1 for multi-GPU mesh extraction</span></span><br><span class="line">torchrun --nproc_per_node=<span class="variable">$&#123;GPUS&#125;</span> projects/neuralangelo/scripts/extract_mesh.py \</span><br><span class="line">    --config=<span class="variable">$&#123;CONFIG&#125;</span> \</span><br><span class="line">    --checkpoint=<span class="variable">$&#123;CHECKPOINT&#125;</span> \</span><br><span class="line">    --output_file=<span class="variable">$&#123;OUTPUT_MESH&#125;</span> \</span><br><span class="line">    --resolution=<span class="variable">$&#123;RESOLUTION&#125;</span> \</span><br><span class="line">    --block_res=<span class="variable">$&#123;BLOCK_RES&#125;</span></span><br><span class="line"></span><br><span class="line">eg:</span><br><span class="line">CHECKPOINT=logs/<span class="variable">$&#123;GROUP&#125;</span>/<span class="variable">$&#123;NAME&#125;</span>/epoch_00224_iteration_000020000_checkpoint.pt</span><br><span class="line">OUTPUT_MESH=logs/<span class="variable">$&#123;GROUP&#125;</span>/<span class="variable">$&#123;NAME&#125;</span>/Miku.ply</span><br><span class="line">CONFIG=logs/<span class="variable">$&#123;GROUP&#125;</span>/<span class="variable">$&#123;NAME&#125;</span>/config.yaml</span><br><span class="line">RESOLUTION=2048</span><br><span class="line">BLOCK_RES=128</span><br><span class="line">GPUS=1</span><br><span class="line">torchrun --nproc_per_node=<span class="variable">$&#123;GPUS&#125;</span> projects/neuralangelo/scripts/extract_mesh.py --config=<span class="variable">$&#123;CONFIG&#125;</span> --checkpoint=<span class="variable">$&#123;CHECKPOINT&#125;</span> --output_file=<span class="variable">$&#123;OUTPUT_MESH&#125;</span> --resolution=<span class="variable">$&#123;RESOLUTION&#125;</span> --block_res=<span class="variable">$&#123;BLOCK_RES&#125;</span> --textured</span><br></pre></td></tr></table></figure><ul><li>Add <code>--textured</code> to extract meshes with textures.</li><li>Add <code>--keep_lcc</code> to remove noises. May also remove thin structures.</li><li>Lower <code>BLOCK_RES</code> to reduce GPU memory usage.</li><li>Lower <code>RESOLUTION</code> to reduce mesh size.</li></ul><h2 id="EXP"><a href="#EXP" class="headerlink" title="EXP"></a>EXP</h2><h3 id="exp1"><a href="#exp1" class="headerlink" title="exp1"></a>exp1</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># config gen</span></span><br><span class="line">python projects/neuralangelo/scripts/generate_config.py --sequence_name Miku_exp1 --data_dir ./inputs/Miku_exp1 --scene_type object</span><br><span class="line"></span><br><span class="line"><span class="comment"># train</span></span><br><span class="line">EXPERIMENT=Miku_exp1</span><br><span class="line">GROUP=exp</span><br><span class="line">NAME=Miku_exp1</span><br><span class="line">CONFIG=projects/neuralangelo/configs/custom/<span class="variable">$&#123;EXPERIMENT&#125;</span>.yaml</span><br><span class="line">GPUS=1</span><br><span class="line">torchrun --nproc_per_node=<span class="variable">$&#123;GPUS&#125;</span> train.py --logdir=logs/<span class="variable">$&#123;GROUP&#125;</span>/<span class="variable">$&#123;NAME&#125;</span> --config=<span class="variable">$&#123;CONFIG&#125;</span> --show_pbar --optim.sched.two_steps=[12000,16000]</span><br><span class="line"><span class="comment">## other optional</span></span><br><span class="line">--data.readjust.scale=0.5 --max_iter=20000</span><br><span class="line">--validation_iter=99999999 \</span><br><span class="line">--model.object.sdf.encoding.coarse2fine.step=200 \</span><br><span class="line">--model.object.sdf.encoding.hashgrid.dict_size=19 \</span><br><span class="line">--optim.sched.warm_up_end=200 \</span><br><span class="line">--optim.sched.two_steps=[12000,16000]</span><br><span class="line">--checkpoint <span class="variable">$&#123;CHECKPOINT&#125;</span> --resume</span><br><span class="line"></span><br><span class="line"><span class="comment"># extraction</span></span><br><span class="line">CHECKPOINT=logs/<span class="variable">$&#123;GROUP&#125;</span>/<span class="variable">$&#123;NAME&#125;</span>/epoch_01000_iteration_000020000_checkpoint.pt</span><br><span class="line">OUTPUT_MESH=logs/<span class="variable">$&#123;GROUP&#125;</span>/<span class="variable">$&#123;NAME&#125;</span>/Miku_exp1.ply</span><br><span class="line">CONFIG=logs/<span class="variable">$&#123;GROUP&#125;</span>/<span class="variable">$&#123;NAME&#125;</span>/config.yaml</span><br><span class="line">RESOLUTION=2048</span><br><span class="line">BLOCK_RES=128</span><br><span class="line">GPUS=1</span><br><span class="line">torchrun --nproc_per_node=<span class="variable">$&#123;GPUS&#125;</span> projects/neuralangelo/scripts/extract_mesh.py --config=<span class="variable">$&#123;CONFIG&#125;</span> --checkpoint=<span class="variable">$&#123;CHECKPOINT&#125;</span> --output_file=<span class="variable">$&#123;OUTPUT_MESH&#125;</span> --resolution=<span class="variable">$&#123;RESOLUTION&#125;</span> --block_res=<span class="variable">$&#123;BLOCK_RES&#125;</span> --textured</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231006095200.png" alt="image.png|666"></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>We introduce Neuralangelo, an approach for photogrammetric neural surface reconstruction. The findings of Neuralangelo are simple yet effective: <strong>using numerical gradients for higher-order derivatives and a coarse-to-fine optimization strategy.</strong> Neuralangelo unlocks the representation power of multi-resolution hash encoding for neural surface reconstruction modeled as SDF. We show that Neuralangelo effectively <strong>recovers dense scene structures</strong> of both <strong>object-centric captures</strong> and <strong>large-scale indoor/outdoor scenes</strong> with extremely high fidelity, enabling detailed large-scale scene reconstruction from RGB videos. Our method currently samples pixels from images randomly without tracking their statistics and errors. <strong>Therefore, we use long training iterations to reduce the stochastics 随机指标 and ensure sufficient sampling of details</strong>. <strong>It is our future work to explore a more efficient sampling strategy to accelerate the training process.</strong></p><h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><p>The recovered surfaces provide structural information useful for <strong>many downstream applications</strong>，eg：</p><ul><li>3D asset generation for augmented/virtual/mixed reality——AR/VR/MR 的 3D 资产生成</li><li>environment mapping for autonomous navigation of robotics—— 机器人自主导航的环境映射</li></ul><p>Photogrammetric surface reconstruction using a monocular RGB camera is <strong>of particular interest</strong>, as it equips users with the capability of casually 随意地 creating digital twins of the real world using ubiquitous mobile devices.</p><p><strong>Multi-view stereo algorithms</strong> had been the method of choice for sparse 3D reconstruction，<strong>but an inherent drawback</strong> of these algorithms is their inability to handle ambiguous observations</p><ul><li>regions with large areas of homogeneous colors</li><li>repetitive texture patterns</li><li>strong color variations<br>This would result in inaccurate reconstructions with noisy or missing surfaces.</li></ul><p>Recenty, <strong>neural surface reconstruction methods</strong> have shown great potential in addressing these limitations. Despite the superiority of neural surface reconstruction methods over classical approaches, the recovered fidelity of current methods does not scale well with the capacity of MLPs. 恢复的保真度不能很好地与 MLP 的容量进行 scale</p><p><strong>Instant NGP</strong> introduces a hybrid 3D grid structure with a multi-resolution hash encoding and a lightweight MLP that is more expressive with a memory footprint loglinear to the resolution. NGP 的内存占用与分辨率 log 线性相关</p><p>本文 Neuralangelo = InstantNGP + Neus<br>Neuralangelo adopts Instant NGP as a neural SDF representation of the underlying 3D scene, optimized from multi-view image observations via neural surface rendering<br>Step:</p><ul><li>First, using numerical gradients to compute higher-order derivatives, such as surface normals for the eikonal regularization, is critical to stabilizing the optimization.</li><li>Second, a progressive optimization schedule plays an important role in recovering the structures at different levels of details</li></ul><p>We combine these two key ingredients and, via extensive experiments on standard benchmarks and real-world scenes, demonstrate significant improvements over image-based neural surface reconstruction methods inboth reconstruction accuracy 重建精度 and view synthesis quality 视图合成质量.</p><p>In summary：</p><ul><li>We present the Neuralangelo framework to naturally incorporate the representation power of multi-resolution hash encoding into neural SDF representations.</li><li>We present two simple techniques to improve the quality of hash-encoded surface reconstruction: <strong>higher-order derivatives with numerical gradients</strong> and <strong>coarse-to-fine optimization with a progressive level of details</strong>.</li></ul><p>RelatedWork：</p><ul><li>Multi-view surface reconstruction<ul><li>volumetric occupancy grid to represent the scene: Each voxel is visited and marked occupied if strict color constancy between the corresponding projected image pixels is satisfied. The photometric consistency assumption 光度一致性假设 typically fails due to autoexposure 自动曝光 or non-Lambertian materials 非朗伯材料, which are ubiquitous in the real world. Relaxing such color constancy constraints across views is important for realistic 3D reconstruction</li><li>Follow-up methods typically start with 3D point clouds from multi-view stereo techniques and then perform dense surface reconstruction. Reliance on the quality of the generated point clouds often leads to missing or noisy surfaces. <strong>Recent learning-based approaches</strong> augment the point cloud generation process with learned image features and cost volume construction(MVSnet, DeepMVS). <strong>_However, these approaches are inherently limited by 花费体积的分辨率 the resolution of the cost volume and fail to recover geometric details._</strong></li></ul></li><li>NeRF<ul><li>NeRF achieves remarkable photorealistic view synthesis with view-dependent effects. NeRF encodes 3D scenes with <strong>an MLP mapping 3D spatial locations to color and volume density</strong>. These predictions are composited into pixel colors using neural volume rendering. A problem of NeRF and its variants , however, is the question of how an isosurface of the volume density could be defined to represent the underlying 3D geometry. NeRF 的问题就是等值面不好找。Current practice often relies <strong>on heuristic thresholding 启发式阈值 on the density values</strong>; due to insufficient constraints on the level sets, however, such surfaces are often noisy and may not model the scene structures accurately. <strong>_Therefore, more direct modeling of surfaces is preferred for photogrammetric surface reconstruction problems._</strong></li></ul></li><li>Neural surface reconstruction<ul><li>For scene representations with better-defined 3D surfaces, implicit functions such as occupancy grids（UNISURF） or SDFs are preferred over simple volume density fields.To integrate with neural volume rendering [25], different techniques [41, 47] have been proposed to reparametrize the underlying representations back to volume density 将底层表征重新参数化回体密度.These designs of neural implicit functions enable more accurate surface prediction with view synthesis capabilities of unsacrificed quality.</li><li>Follow-up works extend the above approaches to realtime at the cost of surface fidelity 有牺牲保真度来实现实时建模的研究(Vox-Surf, Neus2), while others use auxiliary information to enhance the reconstruction results 其他的使用辅助信息增强重建结果(with patch warping, Geo-Nues, MonoSDF).<ul><li>Notably, NeuralWarp uses <strong>patch warping given co-visibility information from structure-frommotion (SfM)</strong> to guide surface optimization, but the patchwise planar assumption fails to capture highly-varying surfaces 补丁平面假设无法捕捉高度变化的表面.</li><li>Other methods utilize <strong>sparse point clouds from SfM</strong> to supervise the SDF, but their performances are upper-bounded by the quality of the point clouds, as with classical approaches</li><li>The use of <strong>depth and segmentation as auxiliary data</strong> has also been explored with unconstrained image collections or using scene representations with hash encodings.</li></ul></li><li>In contrast, our work Neuralangelo builds upon hash encodings to recover surfaces but <strong>without the need for auxiliary inputs</strong> used in prior work，本文的方法不需要输入一些辅助数据</li><li>Concurrent work also proposes coarse-to-fine optimization for improved surface details, where a displacement network corrects the shape predicted by a coarse network，并行的方法，使用位移网络来纠错粗网络预测的形状</li><li>In contrast, we use hierarchical hash grids and control the level of details based on our analysis of higher-order derivatives. 通过基于对高阶导数的分析来控制细节的级别</li></ul></li></ul><h1 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h1><p>Neuralangelo reconstructs dense structures of the scene from multi-view images. Neuralangelo samples 3D locations along camera view directions and uses a multi-resolution hash encoding to encode the positions. The encoded features are input to an SDF MLP and a color MLP to composite images using SDF-based volume rendering.</p><p>3D 位置—&gt;哈希编码后的位置信息—&gt;SDF/Color</p><h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><ul><li>Neural volume rendering.</li><li>Volume rendering of SDF</li><li>Multi-resolution hash encoding<ul><li>也有一种方式是 sparse voxel 结构，但是由于内存随着分辨率的增加呈现立方增长，太费内存占用，Hash encoding instead assumes <strong>no spatial hierarchy 空间层次结构</strong> and resolves collision automatically based on gradient averaging 梯度平均</li></ul></li></ul><h2 id="Numerical-Gradient-Computation"><a href="#Numerical-Gradient-Computation" class="headerlink" title="Numerical Gradient Computation"></a>Numerical Gradient Computation</h2><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230716140552.png" alt="image.png"></p><p><strong>w.r.t. : with respect to 相对于，就…而言</strong></p><p>We show in this section that the analytical gradient w.r.t. position of hash encoding suffers from localities. 相对于哈希编码位置，解析梯度受到局部性的影响 . Therefore, optimization updates only propagate to local hash grids, lacking non-local smoothness. <strong>We propose a simple fix</strong> to such a locality problem <strong>by using numerical gradients</strong>. ？？？差分类似</p><p>A special property of SDF is its differentiability with a gradient of the unit norm. The gradient of SDF satisfies the eikonal equation $∥∇f (x)∥_{2} = 1$ (almost everywhere). To enforce the optimized neural representation to be a valid SDF, the eikonal loss is typically imposed on the SDF predictions:<br>由于 SDF 梯度在每处都满足二范数等于 1，因此构建 Eikonal loss 来优化 SDF 的预测：</p><p>$\mathcal{L}_{\mathrm{eik}}=\frac{1}{N}\sum_{i=1}^{N}(|\nabla f(\mathbf{x}_i)|_2-1)^2,$N 是总采样点数</p><p>To allow for end-to-end optimization, <strong>a double backward operation</strong> on the SDF prediction f (x) is required.</p><h3 id="de-facto-先前的大部分方法"><a href="#de-facto-先前的大部分方法" class="headerlink" title="de-facto 先前的大部分方法"></a>de-facto 先前的大部分方法</h3><p>eikonal loss 反向传播到局部的哈希表项</p><p>The $de facto$ method for computing surface normals of SDFs ∇f (x) is to use analytical gradients. Analytical gradients of hash encoding w.r.t. position, however, are not continuous across space under trilinear interpolation. 哈希编码三线性插值下，哈希编码的解析梯度相对于位置在空间上是不连续的。<br>To find the sampling location in a voxel grid, each 3D point $x_{i}$ would first be scaled by the grid resolution $V_{l}$, written as $x_{i,l} = x_{i} · V_{l}.$ Let the coefficient for (tri-)linear interpolation be $β = x_{i,l} − ⌊x_{i,l}⌋$. The resulting <strong>feature vectors</strong> are</p><p>$\gamma_l(\mathbf{x}_{i,l})=\gamma_l(\lfloor\mathbf{x}_{i,l}\rfloor)\cdot(1-\beta)+\gamma_l(\lceil\mathbf{x}_{i,l}\rceil)\cdot\beta,$ where the rounded position$⌊x_{i,l⌋}, ⌈x_{i,l}⌉$correspond to the local grid cell corners. We note that rounding operations ⌊·⌋and ⌈·⌉ are non-differentiable, rounding 运算是不可微的，可以得到哈希编码相对于位置的微分：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial\gamma_{l}(\mathbf{x}_{i,l})}{\partial\mathbf{x}_{i}}& =\gamma_{l}(\lfloor\mathbf{x}_{i,l}\rfloor)\cdot(-\frac{\partial\beta}{\partial\mathbf{x}_{i}})+\gamma_{l}(\lceil\mathbf{x}_{i,l}\rceil)\cdot\frac{\partial\beta}{\partial\mathbf{x}_{i}}  \\&=\gamma_l(\lfloor\mathbf{x}_{i,l}\rfloor)\cdot(-V_l)+\gamma_l(\lceil\mathbf{x}_{i,l}\rceil)\cdot V_l.\end{aligned}</script><p>The derivative of hash encoding is local, i.e., when $x_{i}$ moves across grid cell borders, the corresponding hash entries will be different. Therefore, the eikonal loss defined in Eq. 5 only back-propagates to the locally sampled hash entries, i.e. $γl(⌊x_{i,l}⌋) and γl(⌈x_{i,l}⌉)$. When continuous surfaces (e.g. a flat wall) span multiple grid cells, these grid cells should produce coherent surface normals without sudden transitions. 当表面连续或者很大，跨过多个网格单元时，这些网格单元应该产生连贯的表面法线，而不会突然转变，为了确保表面表示的一致性，应对这些网格单元联合优化，但是分析梯度仅仅只局限于局部网格单元。To ensure consistency in surface representation, joint optimization of these grid cells is desirable. However, the analytical gradient is limited to local grid cells, unless all corresponding grid cells happen to be sampled and optimized simultaneously. Such sampling is not always guaranteed</p><h3 id="our-method"><a href="#our-method" class="headerlink" title="our method"></a>our method</h3><p>To overcome the locality of the analytical gradient of hash encoding, we propose to compute the surface normals using numerical gradients.<br>If the step size of the numerical gradient is smaller than the grid size of hash encoding, the numerical gradient would be equivalent to the analytical gradient; otherwise, hash entries of multiple grid cells would participate in the surface normal computation.</p><ul><li>math 表达上面的描述 - step size &lt; grid size ：numerical gradient = analytical gradient - step size &gt; grid size ：多个网格的哈希表项都参与表面法向的计算<br>Backpropagating through the surface normals thus allows hash entries of multiple grids to receive optimization updates simultaneously. Intuitively, numerical gradients with carefully chosen step sizes can be interpreted as a smoothing operation on the analytical gradient expression. <strong>numerical gradients 通过选择 step size 可以解释为 analytical gradient 表示的平滑操作</strong><br>An alternative of normal supervision is a teacher-student curriculum(Ref-NeRF, NeRFactor), where the predicted noisy normals are driven towards MLP outputs to exploit the smoothness of MLPs 利用 MLP 的平滑性，将预测的嘈杂法线作为 MLP 的输出，然而 loss 的解析梯度也只能反向传播到局部的单元网格来进行哈希编码. However, analytical gradients from such teacher-student losses still only back-propagate to local grid cells for hash encoding. In contrast, numerical gradients solve the locality issue without the need of additional networks.</li></ul><p>To compute the surface normals using the numerical gradient, additional SDF samples are needed. Given a sampled point $x_{i} = (x_{i}, y_{i}, z_{i})$, we additionally sample two points along each axis of the canonical coordinate around xi within a vicinity 范围内 of a step size of ε. For example, the x-component of the surface normal can be found as：$\nabla_xf(\mathbf{x}_i)=\frac{f\left(\gamma(\mathbf{x}_i+\epsilon_x)\right)-f\left(\gamma(\mathbf{x}_i-\epsilon_x)\right)}{2\epsilon},$ $\epsilon_{x} = [\epsilon, 0, 0]$</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230716140552.png" alt="image.png"></p><h2 id="Progressive-Levels-of-Details"><a href="#Progressive-Levels-of-Details" class="headerlink" title="Progressive Levels of Details"></a>Progressive Levels of Details</h2><p>Coarse-to-fine optimization can better shape the loss landscape to avoid falling into false local minima. Such a strategy has found many applications in computer vision, such as image-based registration .<br>Neuralangelo also adopts a coarse-to-fine optimization scheme to reconstruct the surfaces with progressive levels of details. Using numerical gradients for the higher-order derivatives naturally enables Neuralangelo to perform coarse-to-fine optimization from two perspectives.</p><ul><li>Step size ε. As previously discussed, numerical gradients can be interpreted as a smoothing operation where the step size ε controls the resolution and the amount of recovered details. Imposing $\mathcal{L}_{eik}$ with a larger ε for numerical surface normal computation ensures the surface normal is consistent at a larger scale, thus producing consistent and continuous surfaces. On the other hand, imposing $\mathcal{L}_{eik}$ with a smaller ε affects a smaller region and avoids smoothing details. In practice, we initialize the step size ε to the coarsest hash grid size and exponentially decrease it matching different hash grid sizes throughout the optimization process. 初始化$\epsilon$ 为最粗的哈希网格大小，并在整个优化过程中匹配不同的网格大小，以指数方式减小它</li><li>Hash grid resolution V . If all hash grids are activated from the start of the optimization, to capture geometric details, fine hash grids must first “unlearn” from the coarse optimization with large step size ε and “relearn” with a smaller ε. If such a process is unsuccessful due to converged optimization, geometric details would be lost. <strong>Therefore, we only enable an initial set of coarse hash grids and progressively activate finer hash grids throughout optimization when ε decreases to their spatial size.</strong> 粗网格先激活，当$\epsilon$减小到精网格的空间大小时，逐步激活精网格，可以更好的捕捉到细节。The relearning process can thus be avoided to better capture the details. In practice, we also apply weight decay over all parameters to avoid single-resolution features dominating the final results.</li></ul><h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p>To further encourage the smoothness of the reconstructed surfaces, we impose a prior by regularizing the mean curvature of SDF 通过正则化平均曲率施加先验<br>The mean curvature is computed from <strong>discrete Laplacian</strong> similar to the surface normal computation, otherwise, the second-order analytical gradients of hash encoding are zero everywhere when using trilinear interpolation. The curvature loss Lcurv is defined as:$\mathcal{L}_{\mathtt{curv}}=\frac{1}{N}\sum_{i=1}^{N}\left|\nabla^{2}f(\mathbf{x}_{i})\right|.$<br>We note that the samples used for the surface normal computation in Eq. 8：$\nabla_xf(\mathbf{x}_i)=\frac{f\left(\gamma(\mathbf{x}_i+\epsilon_x)\right)-f\left(\gamma(\mathbf{x}_i-\epsilon_x)\right)}{2\epsilon},$ are sufficient for curvature computation.<br>The total loss is defined as the weighted sum of losses:<br>$\mathcal{L}=\mathcal{L}_{\mathrm{RGB}}+w_{\mathrm{eik}}\mathcal{L}_{\mathrm{eik}}+w_{\mathrm{curv}}\mathcal{L}_{\mathrm{curv}}.$<br>All network parameters, including MLPs and hash encoding, are trained jointly end-to-end.所有网络参数，端到端的联合训练</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Efficiency </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
            <tag> Efficiency </tag>
            
            <tag> Encoding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NerfAcc</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Sampling/NerfAcc/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Sampling/NerfAcc/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>NerfAcc: Efficient Sampling Accelerates NeRFs</th></tr></thead><tbody><tr><td>Author</td><td>Li, Ruilong and Gao, Hang and Tancik, Matthew and Kanazawa, Angjoo</td></tr><tr><td>Conf/Jour</td><td>arXiv preprint arXiv:2305.04966</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://www.nerfacc.com/en/latest/">NerfAcc Documentation — nerfacc 0.5.3 documentation</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4753910162394202113&amp;noteId=1865424779362415616">NerfAcc: Efficient Sampling Accelerates NeRFs (readpaper.com)</a></td></tr></tbody></table></div><p>一种可以加速NeRF的高效采样策略<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230711120805.png" alt="image.png"></p><p>NerfAcc = Instant-NGP的Occupancy Grid + Mip-NeRF 360的Proposal Network</p><p><code>pip install nerfacc</code></p><span id="more"></span><p>用法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">import</span> nerfacc</span><br><span class="line"></span><br><span class="line">radiance_field = ...  <span class="comment"># network: a NeRF model</span></span><br><span class="line">rays_o: Tensor = ...  <span class="comment"># ray origins. (n_rays, 3)</span></span><br><span class="line">rays_d: Tensor = ...  <span class="comment"># ray normalized directions. (n_rays, 3)</span></span><br><span class="line">optimizer = ...  <span class="comment"># optimizer</span></span><br><span class="line"></span><br><span class="line">estimator = nerfacc.OccGridEstimator(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigma_fn</span>(<span class="params"></span></span><br><span class="line"><span class="params">   t_starts: Tensor, t_ends:Tensor, ray_indices: Tensor</span></span><br><span class="line"><span class="params"></span>) -&gt; Tensor:</span><br><span class="line">   <span class="string">&quot;&quot;&quot; Define how to query density for the estimator.&quot;&quot;&quot;</span></span><br><span class="line">   t_origins = rays_o[ray_indices]  <span class="comment"># (n_samples, 3)</span></span><br><span class="line">   t_dirs = rays_d[ray_indices]  <span class="comment"># (n_samples, 3)</span></span><br><span class="line">   positions = t_origins + t_dirs * (t_starts + t_ends)[:, <span class="literal">None</span>] / <span class="number">2.0</span></span><br><span class="line">   sigmas = radiance_field.query_density(positions)</span><br><span class="line">   <span class="keyword">return</span> sigmas  <span class="comment"># (n_samples,)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rgb_sigma_fn</span>(<span class="params"></span></span><br><span class="line"><span class="params">   t_starts: Tensor, t_ends: Tensor, ray_indices: Tensor</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[Tensor, Tensor]:</span><br><span class="line">   <span class="string">&quot;&quot;&quot; Query rgb and density values from a user-defined radiance field. &quot;&quot;&quot;</span></span><br><span class="line">   t_origins = rays_o[ray_indices]  <span class="comment"># (n_samples, 3)</span></span><br><span class="line">   t_dirs = rays_d[ray_indices]  <span class="comment"># (n_samples, 3)</span></span><br><span class="line">   positions = t_origins + t_dirs * (t_starts + t_ends)[:, <span class="literal">None</span>] / <span class="number">2.0</span></span><br><span class="line">   rgbs, sigmas = radiance_field(positions, condition=t_dirs)</span><br><span class="line">   <span class="keyword">return</span> rgbs, sigmas  <span class="comment"># (n_samples, 3), (n_samples,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Efficient Raymarching:</span></span><br><span class="line"><span class="comment"># ray_indices: (n_samples,). t_starts: (n_samples,). t_ends: (n_samples,).</span></span><br><span class="line">ray_indices, t_starts, t_ends = estimator.sampling(</span><br><span class="line">   rays_o, rays_d, sigma_fn=sigma_fn, near_plane=<span class="number">0.2</span>, far_plane=<span class="number">1.0</span>,</span><br><span class="line">   early_stop_eps=<span class="number">1e-4</span>, alpha_thre=<span class="number">1e-2</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Differentiable Volumetric Rendering.</span></span><br><span class="line"><span class="comment"># colors: (n_rays, 3). opaicity: (n_rays, 1). depth: (n_rays, 1).</span></span><br><span class="line">color, opacity, depth, extras = nerfacc.rendering(</span><br><span class="line">   t_starts, t_ends, ray_indices, n_rays=rays_o.shape[<span class="number">0</span>], rgb_sigma_fn=rgb_sigma_fn</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimize: Both the network and rays will receive gradients</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss = F.mse_loss(color, color_gt)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><h2 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h2><p>Abstract and Introduction and Related Works</p><p>本文集中讨论各个采样方法对NeRF的加速效果，在统一透射率估计概念下，证明改进的采样通常适用于NeRF<br>为了促进未来的实验，提出了一个即插即用的Python工具箱——NerfAcc，提供了灵活的Api，将目前的采样方法合并进了NeRF的相关工作</p><p>可以使相关工作都有一定的加速<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230711120805.png" alt="image.png"></p><p>大多NeRF方法都有相似的体渲染pipeline：沿着光线创建采样点，然后通过$\alpha$累加<br>有大量工作集中在发展高效的辐射场表示，很少通过高效采样来对神经体渲染的计算花费进行关注，即使有在论文中也不是作为主要的方法被提出。<br>InstantNGP和Plenoxels都是用高定制的CUDA实现在光线行进时完成空间跳跃，与各自的辐射场实现紧密耦合。</p><p>本文揭示了各种采样方法的复杂性，其经常被忽视但很重要</p><ul><li>NeRF codebase：对NeRF的改进有很多，但每个代码库都是针对特定任务定制的，并且只支持一种采样方法<ul><li>虽然这些方法都是同样的pipeline ，但是做迁移时依然困难：requires non-trivial efforts.</li></ul></li><li>NeRF frameworks：将多种NeRF方法融合进一个框架，例如NeRF-Factory、Nerfstudio和Kaolin-Wisp。<ul><li>NeRF- factory提供了一系列具有原始实现的NeRF变体如NeRF++、Mip-NeRF、Mip-NeRF360等等，并专注于全面的基准测试。</li><li>Nerfstudio整合了现有文献中介绍的关键技术，并为社区提供了易于构建的模块化组件</li><li>Kaolin-Wisp实现了一组基于体素的NeRF论文</li><li>然而，这些框架旨在鼓励研究人员在框架内进行开发，而对使用自己的代码库的用户没有好处</li></ul></li><li>本文NeRFAcc即插即用，可以方便地整合到自己的代码库中</li></ul><h2 id="Importance-Sampling-via-Transmittance"><a href="#Importance-Sampling-via-Transmittance" class="headerlink" title="Importance Sampling via Transmittance"></a>Importance Sampling via Transmittance</h2><p>目前有许多采样方法</p><ul><li>Plenoxels uses a sparse gridInstant</li><li>NGP uses an occupancy grid</li><li>NeRF employs a coarse-to-fine strategy</li><li>Mip-NeRF 360 proposes proposal networks.</li><li>他们以完全不同的方式运行</li></ul><p><strong><em>transmittance is all you need for importance sampling</em></strong></p><p>每个方法本质上都有他自己的沿光线创建透射率估计的方式：transmittance estimator<br>这种观点可以使得不同的方法可以在本文NeRFAcc中统一起来</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230711125031.png" alt="image.png"></p><h3 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h3><p>高效采样在图形学中是一个被广泛讨论的问题，其中重点是识别对最终渲染做出最重要贡献的区域。这一目标通常通过重要性抽样来实现，其目的是根据概率密度函数(PDF)——$p(t)$ 在$t_{near},t_{far}$间来分布样本，通过积分计算累积分布函数(CDF)样本采用逆变换采样法生成。$\begin{aligned}F(t)=\int_{t_n}^tp(v)dv,\end{aligned}$<br>采样点：$t=F^{-1}(u)\quad\text{where}\quad u\sim\mathcal{U}[0,1].$<br>每个采样点的贡献由权重表示：$w(t)=T(t)\sigma(t)$<br>颜色：$C(\mathbf{r})=\int_{t_n}^{t_f}T(t)\:\sigma(t)\:c(t)\:dt$<br>$T(t)=\exp\left(-\int_{t_n}^t\left.\sigma(s)\:ds\right).\right.$</p><p>因此：$p(t)=T(t)\sigma(t)$ ,则累计分布函数由$\begin{aligned}F(t)=\int_{t_n}^tp(v)dv\end{aligned}$：</p><script type="math/tex; mode=display">\begin{aligned}F(t)& =\int_{t_n}^tT(v)\sigma(v)dv  \\&=\int_{t_n}^t\frac{d}{dv}\left[-\exp\left(-\int_{t_n}^v\sigma(s)ds\right)\right]dv \\&=1-\exp\left(-\int_{t_n}^t\sigma(s)ds\right) \\&=1-T(t).\end{aligned}</script><p>因此，对CDF进行逆采样相当于对透射率T(t)进行逆采样。一个透射率估计量足以确定最优样本。直观地说，这意味着在透射率变化很快的区域(<em>这正是光线照射到物体表面时所发生的情况</em>)周围放置更多的样本，累计透光率可以通过$1-T(t)$直接计算而不需要积分计算。<br>由于NeRF场景几何不是预定义的，而是动态优化的，在NeRF优化过程中，辐射场在迭代之间发生变化，需要在每一步k动态更新透射率估计量$\mathcal{F}:T(t)^{k-1}\mapsto T(t)^k.$</p><p>从不断变化的辐射场中准确估计透射率变得更加困难，目前的方法是使用either exponential moving average (EMA) or stochastic gradient descent (SGD)来作为更新函数$\mathcal{F}$,我们注意到也许有其他的更新函数可以被探索出来。</p><p>现有的高效采样方法:</p><ul><li>Uniform.每个点对结果的贡献相等，采样过程相当于沿射线均匀采样。Note!!!:每个使用均匀采样的NeRF模型都固有地假设这种线性透射率衰减</li><li>Spatial Skipping.识别空区域并在采样期间跳过它们，用保守阈值对射线的密度进行二值化。为了在优化期间更新透射率估计器：<ul><li>InstantNGP直接更新cached density通过exponential moving average (EMA) ：<ul><li>$\sigma(t_i)^k=\gamma\cdot\sigma(t_i)^{k-1}+(1-\gamma)\cdot\sigma(t_i)^k$</li></ul></li><li>Plenoxels通过渲染损失的梯度下降来更新密度</li></ul></li><li>PDF方法.用离散样本沿射线直接估计PDF<ul><li>NeRF中粗网络训练使用体积渲染损失来输出一组密度，然后进行逆变换采样</li><li>Mip-NeRF中在粗网络使用一个小得多的MLP，即Proposal Network，以加快PDF的构建</li><li>两者都是使用梯度下降法来更新透射率估计器</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230711130845.png" alt="image.png"></p><p>$T(t_{i}) = 1-F(t_{i})=1-\int_{t_n}^{t_i}p(v)dv$</p><h3 id="Design-Spaces"><a href="#Design-Spaces" class="headerlink" title="Design Spaces"></a>Design Spaces</h3><p>Choice of Representations. 显式or隐式，体素or点云orSDFor占据场<br>透光率估计器可以使用显式体素，MLP或混合表示。根据估计量是否显式，它可以使用基于规则的EMA或带有一些监督的梯度下降进行更新。<br>通常基于体素的估计器比隐式估计器更快，但是有更多的aliasing issues混叠问题。<br>透过率估计器的表示可以显著受益于辐射场表示的进步。例如，Nerfacto模型对亮度场和采样模块使用最新的混合表示HashEncoding，在野外设置中实现了最佳的质量-速度权衡。</p><p>Handling Unbounded Scenes.<br>对于无界区域也就是$t_{near},t_{far}$之外的区域，沿着射线密集取样是不可能的。与图形渲染中使用的mipmap类似，一般的解决方案是随着光线走得更远而进行更粗的采样，因为更远的物体在图像平面上出现的像素更少<br>这可以通过创建一个双目标映射函数来实现: $\Phi:s\in[s_{n},s_{f}]\mapsto\dot{t}\in[t_{n},+\infty]$，相关论文引入了不同的映射函数</p><h3 id="Discussions"><a href="#Discussions" class="headerlink" title="Discussions"></a>Discussions</h3><p>Pros and Cons.</p><ul><li>uniform assumption采样是最容易实现的，但在大多数情况下效率最低</li><li>Spatial skipping更有效的技术，因为大多数3D空间是空的，但它仍然在被占用但闭塞的区域内均匀采样，这些区域对最终渲染贡献不大</li><li>PDF-based estimators 通常提供更准确的透射率估计，使样本更集中在高贡献区域(例如，表面)，并在空区域和遮挡区域中更分散。<ul><li>然而，这也意味着样本总是在整个空间中展开，没有任何跳跃。此外，由于(1)沿射线估算透光率的分段线性假设或(2)透光率估计器的潜在体素表示，目前的方法都在体渲染中引入了混叠效应。</li><li>最近的一项工作，Zip-NeRF，解决了与这两个确切问题相关的混叠问题(在他们的工作中称为“z-混叠”和“xy-混叠”)，这在我们的统一框架下自然揭示出来。</li></ul></li></ul><p>Implementation Difficulties.</p><ul><li>目前有效采样的实现都是高度定制的，并与每篇论文中提出的特定辐射场紧密集成。例如，在Instant-NGP和Plenoxels中，空间跳过是用定制的CUDA内核实现的。Mip-NeRF 360、K-planes和Nerfacto实现了一个提议网络，但它与它们的存储库紧密集成，只能支持存储库附带的有限类型的辐射场</li><li>然而，如前所示，采样过程独立于辐射场表示，因此它应该很容易在不同的NeRF变体之间转移。由于各种各样的实现细节，从头开始正确地实现有效的采样方法通常需要大量的工作。因此，拥有一个易于从存储库转移到存储库的实现对于支持NeRF的未来研究是有价值的。</li></ul><p>Insights from Unified Formulation.</p><ul><li>通过透射率估计器来理解采样光谱，为研究新的采样策略铺平了道路。例如，我们的框架揭示了来自Instant-NGP的占用网格和来自Mip-NeRF 360的建议网络不是相互排斥的，而是互补的，因为它们都旨在估计沿射线的透射率</li><li>因此，将它们结合起来变得很简单:首先可以使用占用网格计算透光率，然后使用建议网络细化估计的透光率。这样既可以跳过空白空间，又可以将样品集中到表面上。我们在第4.4节中探讨了这种方法，并证明它克服了提案网络方法的局限性，该方法总是对整个空间进行采样。此外，该公式可能会揭示诸如如何利用深度信息或其他先验信息增强采样程序等问题，我们鼓励读者进一步研究。</li></ul><h2 id="NerfAcc-Toolbox"><a href="#NerfAcc-Toolbox" class="headerlink" title="NerfAcc Toolbox"></a>NerfAcc Toolbox</h2><h3 id="Design-Principles"><a href="#Design-Principles" class="headerlink" title="Design Principles"></a>Design Principles</h3><p>这个库的设计目标如下:</p><ul><li>Plug-and-play.</li><li>Efficiency &amp; Flexibility.</li><li>Radiance Field Complexity.包括基于密度的辐射场和基于SDF的辐射场等等、静态和动态场景等等</li></ul><h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><p>NerfAcc结合了两种可以与辐射场表示解耦的高级采样方法，即<strong>来自Instant-NGP的占用网格和来自Mip-NeRF 360的提议网络</strong>。</p><p>伪代码：<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230711134348.png" alt="image.png"></p><p>Sample as Interval.</p><p>使用$(t_{0}, t_{1}, r)$表示每个采样点(<em>沿着第r条射线的区间的开始t0和结束t1</em>)，这种基于间隔的表示提供了三个关键优势：</p><ul><li>首先，将样本表示为区间而不是单个点，可以支持基于锥形射线的抗混叠研究如Mip-NeRF和MipNeRF 360。</li><li>其次，由于几乎在所有情况下ti都不需要梯度，使用(t0, t1, r)而不是(x0, x1)来表示间隔，可以将采样过程从可微计算图中分离出来，从而最大化其速度。</li><li>最后，附加到每个样本上的射线id r支持不同数量的样本跨越一个打包张量的射线，我们将在下一段中讨论。在Nerfstudio中采用了类似的表示来支持各种辐射场。</li></ul><p>Packed Tensor.</p><p>为了支持空间跳变采样，有必要考虑到每条射线可能导致不同数量的有效样本。将数据存储为具有形状(n_rays, n_samples，…)的张量和具有形状(n_rays, n_samples，…)的额外掩码，以指示哪些样本是有效的，但是当大部分空间为空时，会导致显着的低效内存使用。为了解决这个问题，在NerfAcc中，我们将样本表示为形状为(all_samples，…)的“压缩张量”，其中只存储有效的样本(参见算法1)。为了跟踪每个样本的相关射线，我们还托管了一个形状为(n_rays, 2)的整数张量，它存储了压缩张量中的起始索引和该射线上的样本数量。这种方法类似于Instant-NGP和PyTorch3D中使用的方法。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ray0_id_in_packed = (n_rays,2)[0][0] #第0条光线在压缩张量中的起始索引</span><br><span class="line">ray0_count_in_packed = (n_rays,2)[0][1] #第0条光线上采样点的数量</span><br></pre></td></tr></table></figure><p>No Gradient Filtering.<br>在重要性采样后，不准确的透射率估计可能导致一些样本位于空白或闭塞的空间，特别是在占用网格等空间跳过方法中。这些样本可以在包含在PyTorch的可微计算图中之前通过使用禁用梯度的辐射场来评估它们的透射率来进行过滤。由于在滤波过程中不需要反向传递，这比在计算图中保留所有样本要快得多(~ 10倍)。实际上，在此过程中，透射率低于10−4的样品被忽略，对渲染质量几乎没有影响。请注意，该策略的灵感来自于Instant-NGP的实现。</p><h3 id="Case-Studies"><a href="#Case-Studies" class="headerlink" title="Case Studies"></a>Case Studies</h3><p>我们在七篇论文中展示了NerfAcc在三种类型的NeRF上的灵活性:</p><ul><li>静态NeRF (NeRF， TensoRF， Instant-NGP );</li><li>动态nerf (D-NeRF， K-Planes  TiNeuVox );</li><li>以及用于相机优化的NeRF变化(BARF)</li></ul><p>尽管这些方法中的许多，例如Instant-NGP, TensoRF, TiNeuVox和K-Planes，已经在效率上进行了高度优化，但我们仍然能够大大加快它们的训练速度，并在几乎所有情况下获得略好的性能。值得一提的是，TensoRF, TiNeuVox, K-Planes和BARF的实验是通过将NerfAcc集成到官方代码库中进行的，大约需要更改100行代码。我们的实验结果，包括我们的基线结果，如表2a,2b和2c所示，所有这些实验都是在相同的物理环境下进行的，使用单个NVIDIA RTX A5000 GPU进行比较。除了本文报道的实验外，NerfAcc还被集成到一些流行的开源项目中，如用于基于密度的nerf的nerfstudio，以及用于基于sdf的nerf的sdfstudio和instant-nsr-pl。</p><ul><li>Static NeRFs.在本任务中，我们实验了三种NeRF变体，包括原始的基于mlp的NeRF、TensoRF和Instant-NGP。我们展示了NerfAcc在有界场景(NeRF-Synthetic数据集，Tank&amp;Template数据集)和无界场景(360数据集)上与基于mlp和基于体素的辐射场一起工作。值得注意的是，使用NerfAcc，可以用纯Python代码训练一个instantngp模型，并获得比官方纯CUDA实现稍好的性能，如表2a所示。</li><li>Dynamic NeRFs. 在本任务中，我们将NerfAcc工具箱应用于T-NeRF， K-Planes和TiNeuVox，涵盖了合成(D-NeRF)和“野外”captures1(伴随HyperNeRF)。当应用占用网格方法来加速这些动态方法时，我们在所有帧之间共享占用网格，而不是用它来表示静态场景。换句话说，我们不是用它来表示一个区域在单个时间戳上的不透明度，而是用它来表示该区域在所有时间戳上的最大不透明度。这不是最优的，但仍然使渲染非常有效，因为这些数据集中有有限的移动。</li><li>NeRFs for Camera Optimization.在本任务中，我们使用NerfAcc工具箱对带有摄动相机的NeRFSynthetic数据集进行BARF。目标是对多视点图像的辐射场和相机外源进行联合优化。我们观察到，NerfAcc提供的空间跳跃采样加快了训练速度，显著提高了图像质量和相机姿态重建。这些改进可以归因于我们的抽样过程中强制的稀疏性。这一发现可能为未来的研究提供有趣的途径。</li><li>Analysis of Different Sampling Approaches.表2a中的结果表明，占用网格和提议网络采样之间的选择可以显著影响不同数据集上的运行时间和性能。由于每种方法都依赖于一组不同的超参数，因此通过扫描超参数空间来系统地比较两种方法是至关重要的。我们改变了占用网格的分辨率和行进步长，以及提议网络方法的样本数量和提议网络的大小。我们在图5中绘制了NeRF-Synthetic和Mip-NeRF 360数据集的每种方法的帕累托曲线。<ul><li>该分析表明，占用网格采样适用于NeRF-Synthetic数据集，而提议网络方法在360数据集上表现更好。这可能是因为NeRF-Synthetic数据集包含更多可以使用占用网格方法有效跳过的空白空间。然而，在真实的、无界的数据中，占用网格方法的使用受到边界框和缺乏可跳过的空白空间的限制，这使得建议网络方法更加有效。这些实验使用了来自Instant-NGP的辐射场，具有相同的训练配方。</li></ul></li></ul><h3 id="Combined-Sampling"><a href="#Combined-Sampling" class="headerlink" title="Combined Sampling"></a>Combined Sampling</h3><p>第3节中介绍的透光率估计器的统一概念的一个好处是，它可以直接结合两种不同的采样方法，因为它们本质上都提供了可用于重要性采样的透光率估计。例如，我们发现简单地在提议网络的顶部堆叠一个占用网格，可以显著减少光线的数量，并缩小NeRF-Synthetic数据集上剩余光线的近远平面。与仅使用建议网络进行重要性采样相比，这将导致质量的提高，从31.40dB提高到32.35dB，并将训练时间从5.2分钟减少到4.3分钟。图6显示了一个带有FICUS场景的示例，其中使用组合采样清除了浮动对象。本实验使用Instant-NGP中的HashEncoding作为亮度场表示。</p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>总之，本文强调了先进的采样方法对提高神经辐射场(NeRF)优化和渲染效率的重要影响。我们证明了先进的采样可以显著加快各种最近的NeRF论文的训练，同时保持高质量的结果。NerfAcc是一个灵活的Python工具箱，它的开发使研究人员能够轻松地将高级采样方法合并到nerf相关方法中。探索和比较先进的采样方法是开发更有效和更容易获得的基于nerf的方法的重要步骤。所提出的结果还表明，通过先进的采样策略，可以进一步研究提高NeRF和其他相关技术的性能。</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Sampling </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Sampling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Instant-NSR代码理解</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/Instant-NSR-code/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/Instant-NSR-code/</url>
      
        <content type="html"><![CDATA[<p>对<a href="https://github.com/zhaofuq/Instant-NSR">Instant-NSR</a>代码的理解</p><span id="more"></span><h1 id="训练流程图"><a href="#训练流程图" class="headerlink" title="训练流程图"></a>训练流程图</h1><iframe frameborder="0" style="width:100%;height:1158px;" src="https://viewer.diagrams.net/?highlight=0000ff&edit=_blank&layers=1&nav=1&title=train.drawio#R7V1bc6M4Fv41lNNb1S5AIOAxjpPp3eqdnZme2tndFxc2ss02Bi%2FgJJ5fvxI3A1In6gSQaNMPHVvmIs7hO3cdKeDu8PxT7B73f488FCi66j0rYKnoumEAA%2F8hI%2Bd8RNctLR%2FZxb6Xj9UGvvh%2FomJQLUZPvoeSxoFpFAWpf2wObqIwRJu0MebGcfTUPGwbBc27Ht0doga%2BbNyAHv3D99J9Pmrr1mX8E%2FJ3%2B%2FLOGnTyXw5ueXDxJMne9aKn2hC4V8BdHEVp%2FunwfIcCQr2SLvl5D9%2F4tZpYjMKU54T%2Fxr%2BEUfpg7f72z5W2%2FOvu51%2B9Pz%2Faen6ZRzc4FU9czDY9lySIo1PoIXIVVQGLp72foi9Hd0N%2BfcJcx2P79BDgbxr%2BuI3CtOCiBvF3N%2FB3If6ywbNEMR6gp108ySOKU%2FRcGyoe4ycUHVAan%2FEh5a8qNPNzyreqIPHThUPAKA7Z17hTvVVu8VbsqmtfCIc%2FFLT7DjpqFBnT2PXDVYji7QsE1b6foF3QD6gN8pk0%2BTRVpcln9EU9gyIR8jAKi69RnO6jXRS6wf1ldNF8Ky%2FHfI6iY0G6%2F6I0PRe0c09p1CQsevbTf5HT52bx7d%2B1X5bPxZWzL%2BfyS4gft3YS%2Bfrv%2Bm%2BX07Jv5XncTEyiU7xBL5CqgGfqxjuUvnAcyI8jdHzxlYhR4Kb%2BY1Padc5gWspExxQPZJ8JHRIUz7M%2FK%2Fxgyc2HboVQB5ixmxJH0xiYcYbEDJwww4sZwIkZUyrMaEAkh7Uafy%2Fcfo3DDf5e2M3JYc9N9pVa7J%2Fdmi4VvwFLRs5DlD5F8VdaHu6jw%2FqUCJGFVtN80GxaFlYmWV0Wmr1ZX%2BYkDHnRYfKiA3aNjuLUXyI%2FTGvGqN18mwBQ52rtX%2BuC%2BbyLa7Rem2pSb3%2BTTAqECULeCmFwnNO9H%2B6kM0w0vUk%2FHTIsExYae7NMSrlaI%2BJmH0UJwmOVOIMBnsZiHTeICf93It5wRqaPSUanW3yABo%2FPGbHK3%2FGnHfn7M%2Frt4efyirkx2cl1P%2BIP7oHwLlwn2R%2FsxpWieLXddncbtXHlxNuu0k0Ytm8%2FyGORm6NwM9S9hD3jce8mUTzE7ehH7COyEqBt2o0oAa24gKabfE6OBnuTJfa1GcFv1%2Bylxn5dtVtSGb7VA07WW3c8LjW%2BJDwu5%2F0t76aMBBWyWSJnB1rzZugHMKLNus6INtu9CUWLIuahyLzkZKzZRTdYpUceMVzr6q3SgcV3%2BeJtQKfcAprqkKGL%2BlNFDkX1M0oowuHnS5vUSdI4%2BoruogBTHCzDKCTya%2BsHQWuISpUQavkbN7gtfjj4npcJPxY7mgzrgSM67ahRHDEYDNH7Yog%2Bef38aQOVV28YUumNct5cok4%2BKeY0IWMwIFPFPwaJlOlCg8pNzKiyY0YXZmtlp97GsXuuHXAkQa%2BkduVXImmGprbel%2FyKnQbMSiI1Cghy19rzH9%2FlW7fdZ6Ylgw2ZfVZKUjv9rpwBfqJsEtX1ep3XyvPjxgWT4x7FaLV340MU%2BpukPUdm%2FKO3OQf5lNyw04cPT4dV4J5RTF5SPXsudY%2FtFBRichzwGDSaDMln0JzVyJ4ZI3W1RW5aPCHJPrzEzh%2FgiS9cXm0KoxW8ztZ3xtZGSqvL21%2FR6gfEwCtP8%2B77bU6eu8q01ZIECqqvPxodN6f40U1PWE0EUZJUZm3%2BzM2fvuuxO%2FdKizCzZD5p28BmlOVU6epGxLo3CxsIjWaOK2JdOpuvV7PJFc0EEnlR0kceuHmsS8Xjct7fjFj7faVpFXMxI1ngWW5Nz7B%2FUX5MvG35MQ%2FhzhRzSUc9ZAqXmw5nuNzqLeZB5x7CqAcNKWncVjNaOlIVHbel0xdXFUivaocLhkBG%2Ff2wDLGvGiDtxAYUDRBAB7278mOwCvMP%2BLC48jQC97D2XPwhj6tnp6dRvNnPs2Pnt557uMH0dfGzkSiPfhfgk%2B%2FWKHXJF3RMOvONPrzPxxEd7G%2BXz5gOXT4DWJV4vaUsgTPZqdxV47zRfmBLZaeW856ExaiERau8ATKs5IFlhdhs%2BlviFpqouAUYqU8LaJ%2B2swCp2WGM1FrMQixCZvlxszLBVrm%2BRMAkxa%2BBn6Q3mTiaZ8flCwkLAXTz4YNiLYUFjVvPEaL0tUcgBcr4sNYjEEGwaB6YpRZYh7aO89Cjj9%2BsKGQdm03lKZMuKw9t3HMxIQ19hE26vZ%2BnJE4xRbBpo7HtfLAWUzFrrvuLYFuTJuDWBLz1uLJFsOmY2KQJ5NIE%2Fu7gvlMXTPJduHyHrQyladFBAXaGUu1LvhvTegt%2B%2BW5zyndDMvlOh3STzR55p4DlzV8c%2FQzkuS8fxKvqlPnn7NDPv93UYgJ39Uv4aXm2OscPpf7lLyRK4Ic32Q9EaD3g%2F3Wyojc%2FU%2FsgX7luyym3hDvlxpRo5oZqicDXoSqXU27Q0f5NTFCDnbYKqnmQLQznn05rFH%2BOEhl75MAmfGyWK8NKpIDe4DOtEOGHD2%2F825BrhUg573brtZqe%2Bz3%2FLiFiKiOvgIxj0JDRWKvc%2BoPMtOCaHzK8YWBDrgXXpj7xuHseO3Lx%2BOq6hw3AY1MuJ88U2jfzR%2BWxJhWPDXYZKgn80daMRD3hVEYR1bBN4UyhHTJHhg6LFx1y9cg06FLSzPjv1tDvo4c1KPuEl5Y%2F0Od0ZJjZUdbsi5h0g4sDSvYjpCUjbjcoJU06pIMfzyMu6dhoqamM6uRhiUn3Vt8%2BPo6Qkqye0cNSUmg56Lg0osnbJ9WUzLenO5jmOxF4buom6NJ0nTROWZZjnaxbwrf0vZfvIztmjVY1jgPUOQQUaqvwWwO29tzqC7lQopStyoncZiBbGw65vLYslMyb%2F4Ytuwoi16tFs%2FNU0Cn1g2ROwDYn6PpcHNMhkL%2FntrLj2jK4cA0GxzVtKBbJi3n296b5Btzlh9aYQ0Z0Ve04s9EPC5rrQm2ozRn9ZZn59Y5Y8HA2wcb5%2Feff73%2F9z%2FHTs%2F7rR%2F8%2F5dYk12YTtYTe20Qt1GlRyySyUMFaznKUGULbbEquKtAmLEMI6Y0qxtSSz2btdjVoDz5rSs3wSxjesD2UKzAJ6bA9pLM10ilpTS%2F326iaHTOikiwrSbP7spHsqaEOP1x4oxaWXNlqSEctxgGXlnaWAC7WtDEcN1wsXu1iyaVdLKErtMYlErl5DOWqa7RoC%2BL5%2FKcCbmnrUbRJramtRS6Mwl%2FWEpfeXBRrKmLkxwevySAbPmiT4ZOb7AlA6JCOdAjRGIW%2Bw0KETvaTTbnArdZXVzu9qythGpB50rX9wrnc3rSEUYfAYnJvne9sRqhmdU2d1gCPfT5oby%2Bb1YJDej%2FHaOVPGFsgAVYMTdd6o6PQpT3jUvE2b5MCW67WVjbdpGAUUQGz9MHliQo4U%2BkPP1x413w7ksGFXvM9CrhAvZkZlgEu6gQXXrg4Gq926XxH8ffxmK4r%2FfKpdD26hU0nJhhsokS0A%2BnQdusEkW%2FRCowUIrT%2FSBrU7WLXkzMQabT8PY1RLVEqjWFQQrt7v%2F20yKWMhPSz2juAq7QmZkkZozf60VG%2Bw2pDEe7HjWBYlmwRDIf2yuSNyLW1pujMhEMX%2BeZZHTnlQZt8GqNic1D6VQ0zJrODw%2BzgdmTlSu04Y3VkeWR1b44ss%2FBWaIcAccUg31vd%2FO2iZY7iZjAQVl6aJLt5WPfqhEIDg7QvZEHbaQRWMbPakz5hku9Ki6U6wQfDqWUeZ4rEB21vieD4tXOBjiPU%2BojKLqUAI9s5rJTShBq9IxdTjPom5nGOSIDQYY56%2B90RYES4JmdEHyaM8GLE4cSINpRf%2BNIsayBBBzdvbC8%2FQgzxWkRoVezIEaLxeoN57lCYO0j7g8nGHYUKMcSrENpMVe4dxbGVBf4Aldulcos%2F4K8Pim2REXuhOA%2FKvak4qmLT8JJ3T4v3sU4z27KNFeliMk%2FvK9KlMQorGcyD5P%2FbjGeLW%2BUWkhHyQScjmKn489Vy0ZKAi0Lr1Mauoxh1nuwDLaE6ik4gomO02Xe1PCCI1m6wSlJ07OiKQbTp9oJJ6qYJEThYKDlWJnmwOKL1h3Q62uQ1Yp2%2BdHQFhklAvEFAMFpjsamsChUQdNB2jZJ0lTc9VrMRdXbww5mEiNGaGXMIGIgxGYjpot0Im5dXuodQJ4gp5d3riNFFIqacZj1%2BGO1WxzSu8BIdUXhzUUWkO0%2BSIUN%2FIIeS3Qnn6XNaV1Z3rqIveung8y6MOWZlpxYgs1jNxi1GnV9%2FINMpBpwStNrs0ebrMfLDsh9mxooLjfG7gQVbjebEbujIZnG92u37acTUNRsZEYBh2chYOnwV1UXdyEpGpwU2lYVaFzodKKj1qJUQKJreWjw%2BaA84Ng3FhpLrOBmdRcHdb0GsRUFnJLvaDHkW7o757sfsRgNKL1swP7pxQnYv%2FTD4nYvuiAPf9WLgYTwe03nN4Bt4JrXdZZfN7WOHnMWlLA4sayVyA8%2FikrAkMomsbhh4AtsjEWhLgoR5mDfpzO3SGfllNjw6GnUOYFmreRh4ItWdV6ejh%2B3yFcJa%2B5yH%2FcDy9%2Fg0PHJaLgRmUu4wDC89EdYDK5%2FUSuMP%2BApmbe%2F49h2vI33SbimuM%2FrKMtdGdLIyh62z6QIJAvOji%2BeTdQrvyLdMz0dy4izrojyrvQnvvG4W38u11TaKD27a6azX5I0oLp9%2FvpI3lfIfAGNVn8Zal9BFYyK2BzgVa73dgwC8MUlgiPQgAB2TzGKQWGqECcH36iKYZvFMQr%2B7vRsM0%2B9mtT3qze8GU3r8Hbjhjk8JLeECdHzKi57CrIxLPoyYWjuKC1gZL62nRffsRW9Tufw7UMJbRAKEFpEAuogEuyexHyb%2Bhu5wKCNMGEvRB4bJVErxDpjwllIAWyhMGKUUhIN%2BuMOj63X0PAasGOJVylRE8Q6sMPoQsKksNOUB6LYDZH%2B7HCp%2FPbg7JKNiUS%2FbqJVoYWkWYM2HxIvYzddHjpccBjxreoUu6oVizewah%2FmLJUbIY6FmdjnNmkwMUbxdHVxsaz%2Bv0mgV7o7ySUXDMav9PcvSZZYNUdtYchgzQqxYfAtktAZgiK4RhxneAI5YO8KgAzjhce7GsXu%2B2SrmYnaJgOYwminmMo%2BDekVqBR%2B%2FDSI3BXqekFVdd73G4wkKtvPsczZapjSuK50BW%2BkMw7EY65Z0HdLAhr3ZO2J96Tqw67AmleY1YMPy2y8o9vGDZxn3F6oJX4A6AERyFiPty%2FUpA3j9bkOo323QfncG23tLcRZkqc89VJw7strw3lbsO8UG2SpS%2FBPdsSvZu5lQ2JzW3wJd7Z1Y5y%2FV53U14G6%2B7rJX7R%2BnNPBDVIx7bvz1H%2FgyfpoXo6hmc1DPRvvS0E578SFjrxTmOolquVH3GBbr438Tw0ArtG4JY7PE3riRzBsVMMVqczoqUKRiVMzKZnVCUQFjLj4WGvku%2F0vU%2BxWA%2BrKrVQVqyNrjvXijKWjblR7vHtu0H3OIHgkTiw6QBTefojjwaqPXwDWgNblmqsx14AKYxlrSbyoLVbEhS5NCxXYUW71oUjJik94M5OBbxTbIh1snO9ghK28X2cjijgyS6xjK4j5bjmvju%2BTYLZo74OEHciDW1eR3fOFlbS4GmchtPvJQKPMFvg%2FIbojvsKiuWhzs5E9gFhd0ltk0oXKrZq0lLHJucdZ91mPCUmydXPNarHzKQGAsPdYtg%2Bm%2F92fmT%2FUXb1f4Jm8nNlOo6W7SKx%2BqzDLFfeGRLhPoc1tvym9Wk3NjXqY3hon%2F0%2F5PiJ7Io0UJXcXyo8owQ6eSMyakVStz57zeOt7qkwh7uwjj9Vmg0EZ5Ju2z%2BCR%2FOQrxxeqYMLT4YrSbn0DCCxLI2yoPCu2cXk6zBhKinBLaofeL3D81Xis4a%2F%2F2B3b%2BP8mIN20ONbX6pzWhx7Ic4LzcK7rh9vUGPboJX%2BaZpyhMItojF09RrOMN1bn8a1DUYlDUsebaoMLMmoTZ24UZr9MChRaNQ9ppyVZnoXie%2Fb3J%2Fl%2BRriDF0kpyqO81RnRVlXAVRjuoaEONFVSsurQPgilLbO3LyDHFW2IOhQYCIF1i%2FuV0OLjx%2BY9s%2Fbh8UNFbzqbDatdnDlnwYo3G2%2Bzzfef1Gi2hXiOkvcZca0QhWhUdOqV%2F4y2n8hSFvfSQXoe9JaYrsbZVPyQMdcMdusFf73SSzl5otN79UWNiADbVuWMyck0OKyKm95ZosoQuRx552y%2BL1923hC5Htmh3P6umKwQbfuFM8rTZUKPZRa1xn5q1GGjLwH10WJ8SQcaxabXWymgqYBTDsQDVmwtv0S78mbF24keVcNQm1CqjNQhgKKQ%2BJZzQGMDYJRxv3bElNAZg0eUSRJKdSC8jiU04w76YbBVioHDHRWimf%2ByA4XXwLaEOvkU7%2BIn72OzaKxtaoN1WLsKhUubvJqi8BSq8sQFbaGzAYlTB0tYzsXNktpZt2AYPy1ZmdcLqzVYuL3ydtrKu2lSFjAzmsi1R%2Fl%2FlFGnN8L4mTqTZvAEBW2j%2B32YEBCTX%2FtjfhzRcxBsAYhPMbzEAWqtaBRoANu8G3rZQ59KmYzqZAUBa1OalGevIjb1%2FkXuTlS95JC3fOSt2w69SGwa6CkFVSF7FpRn15qwm%2B%2F2ZBnRO%2F6pMAwBeZcjQZgHDYSSv%2BVOWD55vgihBMm4gYVvN%2BIpjMXIuHakM%2FDWOSOPd6ref8DPu%2F541wQf3%2Fwc%3D"></iframe><h1 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h1><h2 id="Neus网络"><a href="#Neus网络" class="headerlink" title="Neus网络"></a>Neus网络</h2><p><code>in network_sdf.py NeRFNetwork(NeRFRenderer)</code></p><p>Instant-NSR训练隐式模型的网络由两个串联的MLP组成：</p><ul><li>一个具有2个隐藏层的SDF MLP $𝑚_{𝑠}$<ul><li>每个隐藏层宽度为64个神经元</li><li>用Softplus替换了原始的ReLU激活函数，并且对所有隐藏层的激活函数设置了𝛽=100，SDF MLP使用哈希编码函数(NGP)将3D位置映射为32个输出值。</li></ul></li><li>一个具有3个隐藏层的颜色MLP $𝑚_{𝑐}$<ul><li>每个隐藏层宽度为64个神经元</li></ul></li></ul><p>$m_{s}$训练SDF的网络表示为:$(x,F_{geo})=m_{s}(p,F_{hash}).$</p><ul><li>input<ul><li>每个三维采样点的3个输入空间位置值</li><li>来自哈希编码位置的32个输出值</li></ul></li><li>output<ul><li>sdf值，然后我们将截断的函数应用于输出SDF值，该值使用sigmoid激活将其映射到<code>[−1,1]</code>：<code>sigma = F.relu(h[..., 0])</code></li><li>15维的$F_{geo}$值</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/m_s.png" alt="m_s.png"></p><p>$m_{c}$训练颜色的网络表示为：$\hat{C}=m_{c}(\mathrm{p},\mathrm{n},\mathrm{v},F_{geo}).$</p><ul><li>input<ul><li>视角方向在球谐函数基础上分解为4阶及以下的前16个系数</li><li>SDF MLP的15维的输出值$F_{geo}$</li><li>每个三维采样点的3个输入空间位置值</li><li>用有限差分函数估计SDF梯度的3个正态值</li></ul></li><li>output<ul><li>RGB: 3</li></ul></li></ul><p>用sigmoid激活将输出的RGB颜色值映射到[0,1]范围</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/m_c.png" alt="m_c.png"></p><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>我们在论文中证明了我们约10分钟的训练结果与原始NeuS的约8小时优化结果是可比较的。在优化阶段，我们假设感兴趣的区域最初位于单位球内。</p><ul><li>分层采样：我们在PyTorch实现中采用了NeRF的分层采样策略，其中粗采样和细采样的数量分别为64和64。我们每批次采样4,096条光线，并使用单个NVIDIA RTX 3090 GPU进行为期6,000次迭代的模型训练，训练时间为12分钟。</li><li>法线计算：为了近似梯度以进行高效的法线计算，我们采用有限差分函数$𝑓 ′ (𝑥) = (𝑓 (𝑥 + Δ𝑥) − (𝑓 𝑥 − Δ𝑥))/2Δ𝑥$。在我们的PyTorch实现中，我们将近似步长设置为Δ𝑥 = 0.005，并在训练结束时将其减小到Δ𝑥 = 0.0005。</li><li>Loss：我们通过最小化Huber损失$L_{𝑐𝑜𝑙𝑜r}$和Eikonal损失$L_{𝑒𝑖𝑘}$来优化我们的模型。这两个损失使用经验系数𝜆进行平衡，在我们的实验中将其设置为0.1。</li><li>Adam：我们选择Adam优化器，初始学习率为$10^{−2}$，并在训练过程中将其降低到$1.6 \times 10^{-3}$。</li></ul><h4 id="有限差分法："><a href="#有限差分法：" class="headerlink" title="有限差分法："></a>有限差分法：</h4><p><a href="https://zh.wikipedia.org/wiki/%E6%9C%89%E9%99%90%E5%B7%AE%E5%88%86%E6%B3%95">有限差分法 - 维基百科，自由的百科全书 (wikipedia.org)</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def gradient(self, x, bound, epsilon=0.0005):</span><br><span class="line">    #not allowed auto gradient, using fd instead</span><br><span class="line">    return self.finite_difference_normals_approximator(x, bound, epsilon)</span><br><span class="line"></span><br><span class="line">def finite_difference_normals_approximator(self, x, bound, epsilon = 0.0005): # 有限差分法</span><br><span class="line">    # finite difference</span><br><span class="line">    # f(x+h, y, z), f(x, y+h, z), f(x, y, z+h) - f(x-h, y, z), f(x, y-h, z), f(x, y, z-h)</span><br><span class="line">    pos_x = x + torch.tensor([[epsilon, 0.00, 0.00]], device=x.device)</span><br><span class="line">    dist_dx_pos = self.forward_sdf(pos_x.clamp(-bound, bound), bound)[:,:1]</span><br><span class="line">    pos_y = x + torch.tensor([[0.00, epsilon, 0.00]], device=x.device)</span><br><span class="line">    dist_dy_pos = self.forward_sdf(pos_y.clamp(-bound, bound), bound)[:,:1]</span><br><span class="line">    pos_z = x + torch.tensor([[0.00, 0.00, epsilon]], device=x.device)</span><br><span class="line">    dist_dz_pos = self.forward_sdf(pos_z.clamp(-bound, bound), bound)[:,:1]</span><br><span class="line"></span><br><span class="line">    neg_x = x + torch.tensor([[-epsilon, 0.00, 0.00]], device=x.device)</span><br><span class="line">    dist_dx_neg = self.forward_sdf(neg_x.clamp(-bound, bound), bound)[:,:1]</span><br><span class="line">    neg_y = x + torch.tensor([[0.00, -epsilon, 0.00]], device=x.device)</span><br><span class="line">    dist_dy_neg  = self.forward_sdf(neg_y.clamp(-bound, bound), bound)[:,:1]</span><br><span class="line">    neg_z = x + torch.tensor([[0.00, 0.00, -epsilon]], device=x.device)</span><br><span class="line">    dist_dz_neg  = self.forward_sdf(neg_z.clamp(-bound, bound), bound)[:,:1]</span><br><span class="line"></span><br><span class="line">    return torch.cat([0.5*(dist_dx_pos - dist_dx_neg) / epsilon, 0.5*(dist_dy_pos - dist_dy_neg) / epsilon, 0.5*(dist_dz_pos - dist_dz_neg) / epsilon], dim=-1)</span><br></pre></td></tr></table></figure><h2 id="Neural-Tracking-Implementation-Details"><a href="#Neural-Tracking-Implementation-Details" class="headerlink" title="Neural Tracking Implementation Details"></a>Neural Tracking Implementation Details</h2><p>在第4节中，我们提出了一种神经跟踪流程，它将传统的非刚性跟踪和神经变形网络以一种由粗到精的方式结合在一起。我们通过高斯-牛顿方法解决非刚性跟踪问题，并在接下来的内容中介绍了详细信息。<br>跟踪细节: 在进行非刚性跟踪之前，我们通过计算规范网格上的测地距离来对ED节点进行采样。我们计算平均边长，并将其乘以一个半径比例，用于控制压缩程度，以获得影响半径𝑟。通过所有的实验，我们发现简单地调整为0.075也可以得到很好的结果。给定𝑟，我们按Y轴对顶点进行排序，并在距离𝑟之外时从现有ED节点集合中选择ED节点。此外，当ED节点影响相同的顶点时，我们可以将它们连接起来，然后提前构建ED图以进行后续优化。</p><p>$\mathrm{(c,\sigma)=\phi^{o}(p’+\phi^{d}(p’,t),d).}$<br>网络结构: 我们改进阶段的关键包括规范辐射场$𝜙^𝑜$和变形网络$𝜙^𝑑$。</p><ul><li><p>$𝜙^𝑜$具有与Instant-NGP相同的网络结构，包括三维哈希编码和两个串联的MLP: 密度和颜色。</p><ul><li>三维坐标通过哈希编码映射为64维特征，作为密度MLP的输入。然后，密度MLP具有2个隐藏层（每个隐藏层有64个隐藏维度），并输出1维密度和15维几何特征。</li><li>几何特征与方向编码连接在一起，并输入到具有3个隐藏层的颜色MLP中。最后，我们可以获得每个坐标点的密度值和RGB值。</li></ul></li><li><p>$𝜙^{𝑑}$包括四维哈希编码和单个MLP。</p><ul><li>四维哈希编码具有32个哈希表，将输入（p′，𝑡）映射到64维特征。通过我们的2个隐藏层变形MLP（每个隐藏层具有128个隐藏维度），最终可以得到Δp′。</li></ul></li></ul><h3 id="训练细节-1"><a href="#训练细节-1" class="headerlink" title="训练细节"></a>训练细节</h3><p>训练细节。我们分别训练$𝜙^𝑜$和 $𝜙^{𝑑}$ 。我们首先利用多视图图像来训练规范表示 $𝜙^𝑜$。当 PSNR 值稳定下来（通常在100个epoch之后），我们冻结 $𝜙^𝑜$ 的参数。然后，我们训练变形网络 $𝜙^{𝑑}$ 来预测每帧的变形位移。我们构建了一个PyTorch CUDA扩展库来实现快速训练。我们首先将规范帧中的ED节点转换到当前帧，然后构建一个KNN体素。具体而言，我们的体素分辨率是2563到5123，并且对于KNN体素中的每个体素，我们通过堆查询4到12个最近邻的ED节点。基于KNN体素，我们可以快速查询体素中的任何3D点，并获取邻居和对应的蒙皮权重，以通过非刚性跟踪计算坐标。</p><h2 id="Neural-Blending-Implementation-Details"><a href="#Neural-Blending-Implementation-Details" class="headerlink" title="Neural Blending Implementation Details"></a>Neural Blending Implementation Details</h2><p>U-Net:<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230706205127.png" alt="image.png"></p><h3 id="训练细节-2"><a href="#训练细节-2" class="headerlink" title="训练细节"></a>训练细节</h3><p>在训练过程中，我们引入了一个称为遮挡映射的新维度，它是两个变形深度图之间的差异计算得出的。然后，我们将遮挡映射（1维）和两个变形的RGBD通道（4维）作为网络输入，进一步帮助U-net网络优化混合权重。在传统的逐像素神经纹理混合过程中，混合结果仅从两个变形图像生成。然而，如果目标视图在相邻虚拟视图中都被遮挡，将导致严重的伪影问题。因此，我们使用纹理渲染结果作为额外输入，以恢复由于遮挡而丢失的部分。为了有效地渲染，我们首先将输入图像降采样为512×512作为网络输入，然后通过双线性插值上采样权重映射以生成最终的2K图像。为了避免混合网络过度拟合额外的纹理渲染输入，我们在训练过程中应用高斯模糊操作来模拟低分辨率的纹理渲染图像。这个操作有助于网络专注于选定的相邻视图的细节，同时从额外的纹理渲染输入中恢复缺失的部分。此外，我们选择Adam [Diederik P Kingma et al.2014]优化器，初始学习率为1e-4，权重衰减率为5e-5。我们在一台单独的NVIDIA RTX 3090 GPU上使用Twindom [web.twindom.com]数据集对神经纹理混合模型进行了两天的预训练。</p><h1 id="编码方式"><a href="#编码方式" class="headerlink" title="编码方式"></a>编码方式</h1><p>Instant-NSR自己集成了<code>cuda.C++</code>程序，可以实现哈希编码而不需要安装tiny-cuda-nn扩展</p><h2 id="encoding-py"><a href="#encoding-py" class="headerlink" title="encoding.py"></a>encoding.py</h2><h2 id="hashencoder"><a href="#hashencoder" class="headerlink" title="hashencoder"></a>hashencoder</h2><h2 id="psencoder"><a href="#psencoder" class="headerlink" title="psencoder"></a>psencoder</h2><h2 id="shencoder"><a href="#shencoder" class="headerlink" title="shencoder"></a>shencoder</h2>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Efficiency </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
            <tag> Code </tag>
            
            <tag> Efficiency </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Instant-nsr-pl创建项目</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF-Mine/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/NeRF-Mine/</url>
      
        <content type="html"><![CDATA[<p>本项目<a href="https://github.com/yq010105/NeRF-Mine">yq010105/NeRF-Mine (github.com)</a>基于<a href="https://github.com/bennyguo/instant-nsr-pl">Instant-nsr-pl</a>(NSR,NGP,PytorchLightning)代码构建</p><ul><li>保留 omegaconf、nerfacc、Mip-nerf_loss，类似文件结构</li><li>去除 pytorch-lightning 框架，使用 pytorch</li></ul><p>NeRF 主要部分：</p><ul><li>神经网络结构 —&gt; 训练出来模型，即 3D 模型的隐式表达<ul><li>网络类型一般为 MLP，相当于训练一个函数，输入采样点的位置，可以输出该点的信息(eg: density, sdf, color…)</li></ul></li><li><a href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF/Sampling">采样方式</a>：沿着光线进行采样获取采样点</li><li><a href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF/Encoding">位置编码</a>：对采样点的位置 xyz 和方向 dir 进行编码，使得 MLP 的输入为高频的信息</li><li><a href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF/Math">数学相关</a>：光线的生成、坐标变换、体渲染公式、BRDF……</li><li>体渲染函数：<ul><li>NeRF：$\mathrm{C}(r)=\int_{\mathrm{t}_{\mathrm{n}}}^{\mathrm{t}_{\mathrm{f}}} \mathrm{T}(\mathrm{t}) \sigma(\mathrm{r}(\mathrm{t})) \mathrm{c}(\mathrm{r}(\mathrm{t}), \mathrm{d}) \mathrm{dt} =\sum_{i=1}^{N} T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right) \mathbf{c}_{i}$<ul><li>不透明度$\sigma$，累计透光率 —&gt; 权重</li><li>颜色值</li></ul></li><li>Neus：$C(\mathbf{o},\mathbf{v})=\int_{0}^{+\infty}w(t)c(\mathbf{p}(t),\mathbf{v})\mathrm{d}t$<ul><li>sdf, dirs, gradients, invs —&gt; $\alpha$ —&gt; 权重</li><li>颜色值</li></ul></li><li>NeRO：$\mathbf{c}(\omega_{0})=\mathbf{c}_{\mathrm{diffuse}}+\mathbf{c}_{\mathrm{specular}} =\int_{\Omega}(1-m)\frac{\mathbf{a}}{\pi}L(\omega_{i})(\omega_{i}\cdot\mathbf{n})d\omega_{i} + \int_{\Omega}\frac{DFG}{4(\omega_{i}\cdot\mathbf{n})(\omega_{0}\cdot\mathbf{n})}L(\omega_{i})(\omega_{i}\cdot\mathbf{n})d\omega_{i}$<ul><li>漫反射颜色：Light(直射光)，金属度 m、反照率 a</li><li>镜面反射颜色：Light(直射光+间接光)，金属度 m、反照率 a、粗糙度$\rho$ ，碰撞概率 occ_prob，间接光碰撞 human 的 human_light</li><li>详情见<a href="NeRO-code.md">NeRO Code</a></li></ul></li></ul></li><li>隐式模型导出(.stl、.obj、.ply 等)显式模型(Marching Cube)：利用 trimesh，torchmcubes，mcubes 等库<ul><li>根据 sdf 和 threshold，获取物体表面的 vertices 和 faces(如需还要生成 vertices 对应的 colors)。</li><li>然后根据 vertices、faces 和 colors，由 trimesh 生成 mesh 并导出模型为 obj 等格式</li></ul></li></ul><p>Future：</p><ul><li>[ ] 消除颜色 or 纹理与几何的歧义，Neus(X—&gt;MLP—&gt;SDF)的方法会将物体的纹理建模到物体的几何中</li><li>[x] 只关注前景物体的建模，可以结合 SAM 将图片中的 interest object 分割出来: <strong>Rembg</strong>分割后效果也不好</li></ul><span id="more"></span><p>NeRF-Mine 文件结构：</p><ul><li>confs/ 配置文件<ul><li>dtu.yaml</li></ul></li><li>encoder/ 编码方式<ul><li>get_encoding.py</li><li>frequency.py</li><li>hashgrid.py</li><li>spherical.py</li></ul></li><li>process_data/ 处理数据集<ul><li>dtu.py</li></ul></li><li>models/ 放一些网络的结构和网络的运行和方法<ul><li>network.py 基本网络结构</li><li>neus.py neus 的网络结构</li><li>utils.py</li></ul></li><li>systems/ 训练的程序<ul><li>neus.py 训练 neus 的程序</li></ul></li><li>utils/ 工具类函数</li><li>run.py 主程序</li><li>inputs/ 数据集</li><li>outputs/ 输出和 log 文件<ul><li>logs filepath: /root/tf-logs/name_in_conf/trial_name</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line">python run.py --config confs/neus-dtu.yaml --train</span><br><span class="line"><span class="comment"># 恢复训练</span></span><br><span class="line">python run.py --config confs/neus-dtu.yaml --train --resume ckpt_path</span><br><span class="line"></span><br><span class="line"><span class="comment"># test to 生成mesh + video</span></span><br><span class="line">python run.py --config confs/neus-dtu.yaml --<span class="built_in">test</span> --resume ckpt_path</span><br></pre></td></tr></table></figure><h1 id="Question2023-9-17"><a href="#Question2023-9-17" class="headerlink" title="Question2023.9.17"></a>Question2023.9.17</h1><ul><li>mesh 精度：<ul><li>一个可以评价模型质量的指标，目前大部分方法都只能通过定性的观察来判断，而<strong>定量的比较只能比较渲染图片，而不能比较模型</strong></li><li>改进<ul><li>x Method1，提高前景 occupy grid 的分辨率，虽然细节颜色更加正确，<strong>但同时带来 eikonal 项损失难收敛的问题</strong></li><li>x Method2，将前景和背景的 loss 分开反向传播，loss_fg 只反向传播到 fg 的 MLP。由于无法准确预测光线/像素是背景还是前景，因此重建效果很差。<ul><li>x bg 的 loss 只有 L1_rgb 的话，求出来的 loss 没有 grad_fn，无法反向传播，添加条件 if loss_bg.grad_fn is not None，效果不好</li></ul></li><li>Method3，先用之前方法训练得到一个深度 mask</li></ul></li></ul></li><li>mesh 颜色：<ul><li>neus 方式逆变换采样训练出来的 color，会分布在整个空间中，因此虽然 render 出来的视频效果很好，但是 mesh 表面点的颜色会被稀释</li></ul></li></ul><blockquote><p><a href="https://github.com/Totoro97/NeuS/issues/48">How to reconstruct texture after generating mesh ? · Issue #48 · Totoro97/NeuS (github.com)</a> &gt;<a href="https://github.com/bmild/nerf/issues/44">What can we do with our own trained model? · Issue #44 · bmild/nerf (github.com)</a></p></blockquote><p>Neus: 表面点的颜色<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230917192416.png" alt="image.png|666"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/00300000_88_158.gif" alt="00300000_88_158.gif"></p><p>如果采用更快速的 NGP+Neus 的方法，由于使用了占空网格的方式采样，因此不会将表面点的颜色散射到空间背景中，这样在 extract mesh 的时候，使用简单的法向量模拟方向向量，即可得到很好的效果</p><p>Instant-nsr-pl 表面点颜色：<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230917192510.png" alt="image.png|666"></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p><a href="https://github.com/qiyun71/NeRF-Mine/blob/main/README.md">NeRF-Mine/README.md</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># conda 技巧</span></span><br><span class="line">conda remove -n  需要删除的环境名 --all</span><br><span class="line">conda <span class="built_in">env</span> create --file xxx.yaml</span><br><span class="line">conda <span class="built_in">env</span> update --file xxx.yaml --prune</span><br><span class="line">    <span class="comment">## --prune: uninstalls dependencies which were removed from `xxx.yml`</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># conda 清华源</span></span><br><span class="line">vi  ~/.condarc</span><br><span class="line">修改为以下内容</span><br><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: <span class="literal">true</span></span><br><span class="line">default_channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure><h2 id="EXP"><a href="#EXP" class="headerlink" title="EXP"></a>EXP</h2><h3 id="exp1"><a href="#exp1" class="headerlink" title="exp1"></a>exp1</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">python imgs2poses.py E:\3\Work\EXP\exp1\dataset\Miku_exp1</span><br><span class="line">python gen_cameras.py E:\3\Work\EXP\exp1\dataset\Miku_exp1</span><br><span class="line"></span><br><span class="line"># 训练</span><br><span class="line">python run.py --config confs/neus-dtu_like.yaml --train</span><br><span class="line"># 恢复训练</span><br><span class="line">python run.py --config confs/neus-dtu_like.yaml --train --resume ckpt_path</span><br><span class="line"></span><br><span class="line"># test to 生成mesh + video</span><br><span class="line">python run.py --config confs/neus-dtu_like.yaml --test --resume ckpt_path</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231006095349.png" alt="image.png|333"></p><h3 id="exp2"><a href="#exp2" class="headerlink" title="exp2"></a>exp2</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">python imgs2poses.py E:\3\Work\EXP\exp2\dataset\Miku_exp2</span><br><span class="line">python gen_cameras.py E:\3\Work\EXP\exp2\dataset\Miku_exp2</span><br><span class="line"></span><br><span class="line"># 训练</span><br><span class="line">python run.py --config confs/neus-dtu_like.yaml --train</span><br><span class="line"># 恢复训练</span><br><span class="line">python run.py --config confs/neus-dtu_like.yaml --train --resume ckpt_path</span><br><span class="line"></span><br><span class="line"># test to 生成mesh + video</span><br><span class="line">python run.py --config confs/neus-dtu_like.yaml --test --resume ckpt_path</span><br></pre></td></tr></table></figure><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p><a href="Datasets.md">数据集</a>：</p><div class="table-container"><table><thead><tr><th>Paper</th><th>Dataset</th><th>Link</th></tr></thead><tbody><tr><td>NeRF</td><td>nerf_synthetic,nerf_llff_data,LINEMOD,deepvoxels</td><td><a href="https://github.com/bmild/nerf#project-page--video--paper--data">bmild/nerf: Code release for NeRF (Neural Radiance Fields) (github.com)</a></td></tr><tr><td>Neus</td><td>dtu,BlenderMVS,custom</td><td><a href="https://github.com/Totoro97/NeuS#project-page---paper--data">Totoro97/NeuS: Code release for NeuS (github.com)</a></td></tr><tr><td>Point-NeRF</td><td>dtu,nerf_synthetic,ScanNet,Tanks and temple</td><td><a href="https://github.com/Xharlie/pointnerf#data-preparation">Xharlie/pointnerf: Point-NeRF: Point-based Neural Radiance Fields (github.com)</a></td></tr></tbody></table></div><p>自定义数据集：<br><a href="https://github.com/Totoro97/NeuS/tree/main/preprocess_custom_data">NeuS/preprocess_custom_data at main · Totoro97/NeuS (github.com)</a> - <a href="NeuS.md#Neus使用自制数据集">Neus_custom_data</a> - <a href="Neus-Instant-nsr-pl.md#自定义数据集">Neus-Instant-nsr-pl</a> - 大概需要的视图数量 20Simple/40Complex ref: <a href="https://arxiv.org/abs/2310.00684">https://arxiv.org/abs/2310.00684</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;case_name&gt;</span><br><span class="line">|-- cameras_xxx.npz    # camera parameters</span><br><span class="line">    |-- camera_mat_&#123;&#125;.npy，intrinsic</span><br><span class="line">    |-- camera_mat_inv_&#123;&#125;.npy</span><br><span class="line">    |-- world_mat_&#123;&#125;.npy，intrinsic @ w2c --&gt; world to pixel</span><br><span class="line">    |-- world_mat_inv_&#123;&#125;.npy</span><br><span class="line">    |-- scale_mat_&#123;&#125;.npy ，根据手动清除point得到的sparse_points_interest.ply</span><br><span class="line">    |-- scale_mat_inv_&#123;&#125;.npy</span><br><span class="line">|-- image</span><br><span class="line">    |-- 000.png        # target image for each view</span><br><span class="line">    |-- 001.png</span><br><span class="line">    ...</span><br><span class="line">|-- mask</span><br><span class="line">    |-- 000.png        # target mask each view (For unmasked setting, set all pixels as 255)</span><br><span class="line">    |-- 001.png</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>neuralangelo 提供了 blender 插件可以可视化 colmap 数据，但是会出现 image plane 与 camera plane 不重合的情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DATA_PATH</span><br><span class="line">├─ database.db      (COLMAP database)</span><br><span class="line">├─ images           (undistorted input images)</span><br><span class="line">├─ images_raw       (raw input images)</span><br><span class="line">├─ sparse           (COLMAP data from SfM)</span><br><span class="line">│  ├─ cameras.bin   (camera parameters)</span><br><span class="line">│  ├─ images.bin    (images and camera poses)</span><br><span class="line">│  ├─ points3D.bin  (sparse point clouds)</span><br><span class="line">│  ├─ 0             (a directory containing individual SfM models. There could also be 1, 2... etc.)</span><br><span class="line">│  ...</span><br><span class="line">├─ stereo (COLMAP data for MVS, not used here)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231004155843.png" alt="image.png|666"></p><p><strong>需要对 colmap 数据做(BA and) Undistortion</strong></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231004160230.png" alt="image.png|666"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231004160314.png" alt="image.png|666"></p><h1 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h1><h2 id="confs-配置文件"><a href="#confs-配置文件" class="headerlink" title="confs 配置文件"></a>confs 配置文件</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">$&#123;model.name&#125;-$&#123;dataset.name&#125;-$&#123;basename:$&#123;dataset.root_dir&#125;&#125;</span></span><br><span class="line"><span class="attr">seed:</span> <span class="number">7</span> <span class="comment">#3407</span></span><br><span class="line"><span class="attr">tag:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">dataset:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">dtu</span></span><br><span class="line">  <span class="attr">root_dir:</span> <span class="string">./inputs/dtu_scan24</span></span><br><span class="line">  <span class="attr">render_cameras_name:</span> <span class="string">cameras_sphere.npz</span></span><br><span class="line">  <span class="attr">object_cameras_name:</span> <span class="string">cameras_sphere.npz</span></span><br><span class="line">  <span class="attr">img_downscale:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">apply_mask:</span> <span class="literal">False</span></span><br><span class="line">  <span class="attr">test_steps:</span> <span class="number">60</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p><strong>omegaconf</strong>获取 yaml 中参数<br><strong>argparse</strong>获取终端输入的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ============ Register OmegaConf Recolvers ============= #</span></span><br><span class="line">OmegaConf.register_new_resolver(<span class="string">&#x27;calc_exp_lr_decay_rate&#x27;</span>, <span class="keyword">lambda</span> factor, n: factor**(<span class="number">1.</span>/n))</span><br><span class="line">OmegaConf.register_new_resolver(<span class="string">&#x27;add&#x27;</span>, <span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">OmegaConf.register_new_resolver(<span class="string">&#x27;sub&#x27;</span>, <span class="keyword">lambda</span> a, b: a - b)</span><br><span class="line">OmegaConf.register_new_resolver(<span class="string">&#x27;mul&#x27;</span>, <span class="keyword">lambda</span> a, b: a * b)</span><br><span class="line">OmegaConf.register_new_resolver(<span class="string">&#x27;div&#x27;</span>, <span class="keyword">lambda</span> a, b: a / b)</span><br><span class="line">OmegaConf.register_new_resolver(<span class="string">&#x27;idiv&#x27;</span>, <span class="keyword">lambda</span> a, b: a // b)</span><br><span class="line">OmegaConf.register_new_resolver(<span class="string">&#x27;basename&#x27;</span>, <span class="keyword">lambda</span> p: os.path.basename(p))</span><br><span class="line"><span class="comment"># ======================================================= #</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_config</span>(<span class="params">*yaml_files, cli_args=[]</span>):</span><br><span class="line">    yaml_confs = [OmegaConf.load(f) <span class="keyword">for</span> f <span class="keyword">in</span> yaml_files]</span><br><span class="line">    cli_conf = OmegaConf.from_cli(cli_args)</span><br><span class="line">    conf = OmegaConf.merge(*yaml_confs, cli_conf)</span><br><span class="line">    OmegaConf.resolve(conf) <span class="comment"># 就地解析所有配置文件的内插$</span></span><br><span class="line">    <span class="keyword">return</span> conf</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">config_to_primitive</span>(<span class="params">config, resolve=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">return</span> OmegaConf.to_container(config, resolve=resolve)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_args</span>():</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--config&#x27;</span>, required=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">&#x27;path to config file&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--resume&#x27;</span>, default=<span class="literal">None</span>, <span class="built_in">help</span>=<span class="string">&#x27;path to the weights to be resumed&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    group = parser.add_mutually_exclusive_group(required=<span class="literal">True</span>)</span><br><span class="line">    group.add_argument(<span class="string">&#x27;--train&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>)</span><br><span class="line">    group.add_argument(<span class="string">&#x27;--mesh&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>)</span><br><span class="line">    group.add_argument(<span class="string">&#x27;--test&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--outputs_dir&#x27;</span>, default=<span class="string">&#x27;./outputs&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--logs_dir&#x27;</span>, default=<span class="string">&#x27;/root/tf-logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    args, extras = parser.parse_known_args()</span><br><span class="line">    <span class="keyword">return</span> args, extras</span><br></pre></td></tr></table></figure><h2 id="run-py-主程序"><a href="#run-py-主程序" class="headerlink" title="run.py 主程序"></a>run.py 主程序</h2><p><strong>获取配置文件和终端输入</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># extras：其他config_parser中没有添加的arg</span></span><br><span class="line">args, extras = get_args()</span><br><span class="line"><span class="keyword">from</span> utils.misc <span class="keyword">import</span> load_config, seed_everything</span><br><span class="line">config = load_config(args.config, cli_args=extras)</span><br><span class="line"></span><br><span class="line">config.trainer.outputs_dir = config.get(<span class="string">&#x27;outputs_dir&#x27;</span>) <span class="keyword">or</span> os.path.join(args.outputs_dir, config.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># args.resume : /root/autodl-tmp/new/NeRF-Mine/outputs/neus-dtu-Miku/@20230815-154030/ckpt/ckpt_000200.pth</span></span><br><span class="line"><span class="keyword">if</span> args.resume <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    config.trainer.trial_name = args.resume.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">3</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    config.trainer.trial_name = config.get(<span class="string">&#x27;trial_name&#x27;</span>) <span class="keyword">or</span> (config.tag + datetime.now().strftime(<span class="string">&#x27;@%Y%m%d-%H%M%S&#x27;</span>))</span><br><span class="line"></span><br><span class="line">config.trainer.logs_dir = config.get(<span class="string">&#x27;logs_dir&#x27;</span>) <span class="keyword">or</span> os.path.join(args.logs_dir, config.name ,config.trainer.trial_name)</span><br><span class="line">config.trainer.save_dir = config.get(<span class="string">&#x27;save_dir&#x27;</span>) <span class="keyword">or</span> os.path.join(config.trainer.outputs_dir, config.trainer.trial_name, <span class="string">&#x27;save&#x27;</span>)</span><br><span class="line">config.trainer.ckpt_dir = config.get(<span class="string">&#x27;ckpt_dir&#x27;</span>) <span class="keyword">or</span> os.path.join(config.trainer.outputs_dir, config.trainer.trial_name, <span class="string">&#x27;ckpt&#x27;</span>)</span><br><span class="line">config.trainer.code_dir = config.get(<span class="string">&#x27;code_dir&#x27;</span>) <span class="keyword">or</span> os.path.join(config.trainer.outputs_dir, config.trainer.trial_name, <span class="string">&#x27;code&#x27;</span>)</span><br><span class="line">config.trainer.config_dir = config.get(<span class="string">&#x27;config_dir&#x27;</span>) <span class="keyword">or</span> os.path.join(config.trainer.outputs_dir, config.trainer.trial_name, <span class="string">&#x27;config&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="string">&#x27;seed&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> config:</span><br><span class="line">    config.seed = <span class="built_in">int</span>(time.time() * <span class="number">1000</span>) % <span class="number">1000</span></span><br><span class="line">seed_everything(config.seed)</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]  =  <span class="string">&quot;TRUE&quot;</span></span><br></pre></td></tr></table></figure><p><strong>global seed setting</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seed_everything</span>(<span class="params">seed</span>):</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    os.environ[<span class="string">&#x27;PYTHONHASHSEED&#x27;</span>] = <span class="built_in">str</span>(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br></pre></td></tr></table></figure><h3 id="根据配置导入模块"><a href="#根据配置导入模块" class="headerlink" title="根据配置导入模块"></a>根据配置导入模块</h3><ul><li>from models.neus import NeuSModel<ul><li>model = NeuSModel(config.model)</li></ul></li><li>from systems.neus import Trainer<ul><li>trainer = Trainer(model, config.trainer, args)</li></ul></li><li>from process_data.dtu import NeRFDataset<ul><li>train_dm = NeRFDataset(config.dataset,stage=’train’)<ul><li>train_loader = torch.utils.data.DataLoader(train_dm, batch_size=3, shuffle=True)</li></ul></li><li>val_dm = NeRFDataset(config.dataset,stage = ‘valid’)<ul><li>val_loader = torch.utils.data.DataLoader(val_dm, batch_size=1)</li></ul></li><li>test_dm = NeRFDataset(config.dataset,stage=’test’)<ul><li>test_loader = torch.utils.data.DataLoader(test_dm, batch_size=1)</li></ul></li></ul></li></ul><p>运行：</p><ul><li>trainer.train(train_loader, val_loader)</li><li>trainer.test(test_loader)</li><li>trainer.mesh()</li></ul><h2 id="process-data"><a href="#process-data" class="headerlink" title="process_data"></a>process_data</h2><p><strong>train</strong></p><p>for data in train_loader:</p><p>data:</p><ul><li>pose: torch.Size([4, 4])</li><li>direction: torch.Size([960, 544, 3])</li><li>index: torch.Size([1])</li><li>H: 960</li><li>W: 544</li><li>image: torch.Size([960, 544, 3])</li><li>mask: torch.Size([960, 544])</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;pose&#x27;</span>: tensor([[[-<span class="number">0.0898</span>,  <span class="number">0.8295</span>, -<span class="number">0.5512</span>,  <span class="number">1.3804</span>],</span><br><span class="line">         [ <span class="number">0.0600</span>,  <span class="number">0.5570</span>,  <span class="number">0.8284</span>, -<span class="number">2.0745</span>],</span><br><span class="line">         [ <span class="number">0.9941</span>,  <span class="number">0.0413</span>, -<span class="number">0.0998</span>,  <span class="number">0.0576</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">1.0000</span>]]], device=<span class="string">&#x27;cuda:0&#x27;</span>),</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;direction&#x27;</span>: tensor([[[[-<span class="number">0.3379</span>, -<span class="number">0.5977</span>,  <span class="number">1.0000</span>],</span><br><span class="line">          [-<span class="number">0.3354</span>, -<span class="number">0.5977</span>,  <span class="number">1.0000</span>],</span><br><span class="line">          [-<span class="number">0.3329</span>, -<span class="number">0.5977</span>,  <span class="number">1.0000</span>],</span><br><span class="line">          ...,</span><br><span class="line">         [[-<span class="number">0.3379</span>,  <span class="number">0.5990</span>,  <span class="number">1.0000</span>],</span><br><span class="line">          [-<span class="number">0.3354</span>,  <span class="number">0.5990</span>,  <span class="number">1.0000</span>],</span><br><span class="line">          [-<span class="number">0.3329</span>,  <span class="number">0.5990</span>,  <span class="number">1.0000</span>],</span><br><span class="line">          ...,</span><br><span class="line">          [ <span class="number">0.3341</span>,  <span class="number">0.5990</span>,  <span class="number">1.0000</span>],</span><br><span class="line">          [ <span class="number">0.3366</span>,  <span class="number">0.5990</span>,  <span class="number">1.0000</span>],</span><br><span class="line">          [ <span class="number">0.3391</span>,  <span class="number">0.5990</span>,  <span class="number">1.0000</span>]]]], device=<span class="string">&#x27;cuda:0&#x27;</span>),</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;index&#x27;</span>: tensor([<span class="number">124</span>]),</span><br><span class="line"><span class="string">&#x27;H&#x27;</span>: [<span class="string">&#x27;480&#x27;</span>],</span><br><span class="line"><span class="string">&#x27;W&#x27;</span>: [<span class="string">&#x27;272&#x27;</span>],</span><br><span class="line"><span class="string">&#x27;image&#x27;</span>: tensor([[[[<span class="number">0.3242</span>, <span class="number">0.3438</span>, <span class="number">0.3164</span>],</span><br><span class="line">          [<span class="number">0.3281</span>, <span class="number">0.3477</span>, <span class="number">0.3203</span>],</span><br><span class="line">          [<span class="number">0.3320</span>, <span class="number">0.3398</span>, <span class="number">0.3125</span>],</span><br><span class="line">         ...,</span><br><span class="line">         [[<span class="number">0.5977</span>, <span class="number">0.6133</span>, <span class="number">0.5938</span>],</span><br><span class="line">          [<span class="number">0.5977</span>, <span class="number">0.6133</span>, <span class="number">0.5938</span>],</span><br><span class="line">          [<span class="number">0.5977</span>, <span class="number">0.6133</span>, <span class="number">0.5938</span>],</span><br><span class="line">          ...,</span><br><span class="line">          [<span class="number">0.6289</span>, <span class="number">0.7539</span>, <span class="number">0.8789</span>],</span><br><span class="line">          [<span class="number">0.6328</span>, <span class="number">0.7578</span>, <span class="number">0.8828</span>],</span><br><span class="line">          [<span class="number">0.6328</span>, <span class="number">0.7578</span>, <span class="number">0.8828</span>]]]], device=<span class="string">&#x27;cuda:0&#x27;</span>),</span><br><span class="line"><span class="string">&#x27;mask&#x27;</span>: tensor([[[<span class="number">0.9961</span>, <span class="number">0.9961</span>, <span class="number">0.9961</span>,  ..., <span class="number">0.9961</span>, <span class="number">0.9961</span>, <span class="number">0.9961</span>],</span><br><span class="line">     [<span class="number">0.9961</span>, <span class="number">0.9961</span>, <span class="number">0.9961</span>,  ..., <span class="number">0.9961</span>, <span class="number">0.9961</span>, <span class="number">0.9961</span>],</span><br><span class="line">     [<span class="number">0.9961</span>, <span class="number">0.9961</span>, <span class="number">0.9961</span>,  ..., <span class="number">0.9961</span>, <span class="number">0.9961</span>, <span class="number">0.9961</span>],</span><br><span class="line">     ...,</span><br><span class="line">     [<span class="number">0.9961</span>, <span class="number">0.9961</span>, <span class="number">0.9961</span>,  ..., <span class="number">0.9961</span>, <span class="number">0.9961</span>, <span class="number">0.9961</span>],</span><br><span class="line">     [<span class="number">0.9961</span>, <span class="number">0.9961</span>, <span class="number">0.9961</span>,  ..., <span class="number">0.9961</span>, <span class="number">0.9961</span>, <span class="number">0.9961</span>],</span><br><span class="line">     [<span class="number">0.9961</span>, <span class="number">0.9961</span>, <span class="number">0.9961</span>,  ..., <span class="number">0.9961</span>, <span class="number">0.9961</span>, <span class="number">0.9961</span>]]],</span><br><span class="line">   device=<span class="string">&#x27;cuda:0&#x27;</span>)&#125;</span><br></pre></td></tr></table></figure><h2 id="other-ref-code"><a href="#other-ref-code" class="headerlink" title="other ref code"></a>other ref code</h2><p>DTU 提取 mesh 后，通过 object masks 移除背景<a href="https://github.com/fdarmon/NeuralWarp/blob/main/evaluation/mesh_filtering.py#L108">NeuralWarp/evaluation/mesh_filtering.py at main · fdarmon/NeuralWarp (github.com)</a><br>DTU 数据集的深度图数据可视化，<br>调整可视化时的<code>v_min</code>和<code>v_max</code><a href="https://github.com/doubleZ0108/MVS/blob/master/utils/read_and_visualize_pfm.py">MVS/utils/read_and_visualize_pfm.py at master · doubleZ0108/MVS (github.com)</a></p><h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p><a href="Metrics.md#Metrics">Metrics</a></p><h2 id="Excel-Function"><a href="#Excel-Function" class="headerlink" title="Excel Function"></a>Excel Function</h2><p>将 C5：23.98 22.79 25.21 26.03 28.32 29.80 27.45 28.89 26.03 28.93 32.47 30.78 29.37 34.23 33.95，按空格拆分填入 C3 到 Q3<br><code>=TRIM(MID(SUBSTITUTE($C$5,&quot; &quot;,REPT(&quot; &quot;,LEN($C$5))),(COLUMN()-COLUMN($C$3))*LEN($C$5)+1,LEN($C$5)))</code></p><h1 id="Code-BUG"><a href="#Code-BUG" class="headerlink" title="Code BUG"></a>Code BUG</h1><ul><li>[x] tf-logs 在测试时会新加一个文件夹问题</li><li>[x] 训练过程中出现的 loss 错误</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">Error</span><br><span class="line">1.</span><br><span class="line">torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 23.69 GiB total capacity; 20.89 GiB already allocated; 23.69 MiB free; 22.11 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF  0% 0/2 [00:01&lt;?, ?it/s]</span><br><span class="line"></span><br><span class="line">with torch.no_grad():</span><br><span class="line">    val_epoch</span><br><span class="line"></span><br><span class="line">2.</span><br><span class="line">[2023-07-25 21:10:06,584] INFO: ==&gt;Training Epoch 1, lr = 0.010000</span><br><span class="line">loss=0.3620 (nan), lr=0.008147: : 100% 178/178 [00:15&lt;00:00, 11.85it/s]</span><br><span class="line">[2023-07-25 21:10:21,609] INFO: ==&gt;Training Epoch 2, lr = 0.008147</span><br><span class="line">loss=0.1856 (0.3541), lr=0.006637: : 100% 178/178 [00:13&lt;00:00, 13.39it/s]</span><br><span class="line">[2023-07-25 21:10:34,907] INFO: ==&gt;Training Epoch 3, lr = 0.006637</span><br><span class="line">loss=0.1963 (0.2716), lr=0.005408: : 100% 178/178 [00:13&lt;00:00, 13.45it/s]</span><br><span class="line">[2023-07-25 21:10:48,139] INFO: ==&gt;Training Epoch 4, lr = 0.005408</span><br><span class="line">loss=nan (nan), lr=0.004406: : 100% 178/178 [00:06&lt;00:00, 26.05it/s]</span><br><span class="line">[2023-07-25 21:10:54,974] INFO: ==&gt;Training Epoch 5, lr = 0.004406</span><br><span class="line">loss=nan (nan), lr=0.003589: : 100% 178/178 [00:03&lt;00:00, 46.24it/s]</span><br><span class="line">[2023-07-25 21:10:58,824] INFO: ==&gt;Validation at epoch 5</span><br><span class="line">0% 0/2 [00:00&lt;?, ?it/s]/root/NeRF-Mine/utils/mixins.py:160: RuntimeWarning: invalid value encountered i</span><br><span class="line">n divide</span><br><span class="line">img = (img - img.min()) / (img.max() - img.min())</span><br><span class="line">/root/NeRF-Mine/utils/mixins.py:169: RuntimeWarning: invalid value encountered in cast</span><br><span class="line">img = (img * 255.).astype(np.uint8)</span><br><span class="line">psnr=4.844281196594238: : 100% 2/2 [00:05&lt;00:00, 2.52s/it]</span><br><span class="line">[2023-07-25 21:11:03,865] INFO: ==&gt;Training Epoch 6, lr = 0.003589</span><br><span class="line">loss=nan (nan), lr=0.002924: : 100% 178/178 [00:03&lt;00:00, 47.74it/s]</span><br><span class="line">[2023-07-25 21:11:07,595] INFO: ==&gt;Training Epoch 7, lr = 0.002924</span><br><span class="line"></span><br><span class="line">inv_s  ----&gt; nan ， loss_mask ----&gt; nan , loss_eikonal ----&gt; nan</span><br><span class="line">都有问题</span><br><span class="line">sdf_grad_samples: 突然变为 0,3</span><br><span class="line"></span><br><span class="line">guess1: lr太高1e-2 √</span><br><span class="line"></span><br><span class="line">3.</span><br><span class="line">训练出来的test_Video和mesh位置不准确 --&gt; 数据集加载时的c2w没处理</span><br><span class="line"></span><br><span class="line">4.</span><br><span class="line">训练的效果不好且很慢 --&gt; 没有使用processHashGrid</span><br><span class="line">lr太低可能陷入局部最优</span><br><span class="line"></span><br><span class="line">NOTE：训练过程中loss突然变得很大</span><br><span class="line">TODO：防止过拟合--&gt;添加 torch.cuda.amp.GradScaler() 解决 loss为nan或inf的问题</span><br></pre></td></tr></table></figure><ul><li>[x] 导出 mesh 区域错误</li></ul><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230806160731.png" alt="image.png|500"></p><ul><li>可能是 mesh 网格的 ijk 区域大小设置有问题</li><li>或没有将 bound 进行坐标变换到训练时的世界坐标系</li></ul><h2 id="Add"><a href="#Add" class="headerlink" title="Add"></a>Add</h2><h3 id="Floaters-No-More"><a href="#Floaters-No-More" class="headerlink" title="Floaters No More"></a>Floaters No More</h3><ul><li>OOM</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">0% 0/60 [00:00&lt;?, ?it/s]Traceback (most recent call last):</span><br><span class="line">File &quot;run.py&quot;, line 97, in &lt;module&gt;</span><br><span class="line">main()</span><br><span class="line">File &quot;run.py&quot;, line 88, in main</span><br><span class="line">trainer.train(train_loader, val_loader)</span><br><span class="line">File &quot;/root/autodl-tmp/NeRF-Mine/systems/neus.py&quot;, line 164, in train</span><br><span class="line">self.train_epoch(train_loader)</span><br><span class="line">File &quot;/root/autodl-tmp/NeRF-Mine/systems/neus.py&quot;, line 197, in train_epoch</span><br><span class="line">self.scaler.scale(loss).backward()</span><br><span class="line">File &quot;/root/miniconda3/envs/neus/lib/python3.8/site-packages/torch/_tensor.py&quot;, line 488</span><br><span class="line">, in backward</span><br><span class="line">torch.autograd.backward(</span><br><span class="line">File &quot;/root/miniconda3/envs/neus/lib/python3.8/site-packages/torch/autograd/__init__.py&quot;</span><br><span class="line">, line 197, in backward</span><br><span class="line">Variable._execution_engine.run_backward( # Calls into the C++ engine to run the backw</span><br><span class="line">ard pass</span><br><span class="line">File &quot;/root/miniconda3/envs/neus/lib/python3.8/site-packages/torch/autograd/function.py&quot;</span><br><span class="line">, line 267, in apply</span><br><span class="line">return user_fn(self, *args)</span><br><span class="line">File &quot;/root/autodl-tmp/NeRF-Mine/models/neus.py&quot;, line 29, in backward</span><br><span class="line">return grad_output_colors * scaling.unsqueeze(-1), grad_output_sigmas * scaling, grad_</span><br><span class="line">output_ray_dist</span><br><span class="line">torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 13.58 GiB (GPU 0; 23.70</span><br><span class="line">GiB total capacity; 5.30 GiB already allocated; 7.14 GiB free; 14.86 GiB reserved in tota</span><br><span class="line">l by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to a</span><br><span class="line">void fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</span><br><span class="line">0% 0/60 [00:00&lt;?, ?it/s]</span><br></pre></td></tr></table></figure><h2 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h2><h3 id="Nerfacc"><a href="#Nerfacc" class="headerlink" title="Nerfacc"></a>Nerfacc</h3><p>0.3.5 —&gt; 0.5.3</p><p><strong>error1</strong>：loss_eikonal 一直增大</p><ul><li>在 fg 中添加了 def alpha_fn(t_starts,t_ends,ray_indices):</li><li>在 fg 中使用 ray_aabb_intersect 计算 near 和 far</li></ul><p>效果差原因：</p><ul><li>0.5.3 由于 Contraction 在射线遍历时低效，不再使用 ContractionType，因此对于背景 bg 使用 self.scene_aabb 会出现问题</li></ul><p>解决 test：</p><ul><li>对于背景的 unbounded 采用 prop 网格</li></ul><p><strong>error1.5</strong>:</p><ul><li>loss_rgb_mse 和 l1 损失为 nan</li><li>解决：背景 color 值为负数</li></ul><p><strong>error2</strong>: 对 unbounded 采用 prop 网格后，由于网络参数太多，出现 OOM</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">OOM</span><br><span class="line">torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 162.00 MiB (GPU 0; 23.70 GiB total capacity; 21.16 GiB already allocated; 150.56 MiB free; 22.31 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</span><br><span class="line"></span><br><span class="line">set PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32</span><br></pre></td></tr></table></figure><blockquote><p><a href="https://blog.csdn.net/MirageTanker/article/details/127998036">通过设置 PYTORCH<em>CUDA_ALLOC_CONF 中的 max_split_size_mb 解决 Pytorch 的显存碎片化导致的 CUDA:Out Of Memory 问题</em>梦音 Yune 的博客-CSDN 博客</a></p></blockquote><ul><li>max_split_size_mb 设置后，显存也不足</li><li>调小 prop 的网格参数，prop_network 主要由 Hash Table 和 MLP 两部分组成，调小 HashTable 的 n_levels</li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Neus </tag>
            
            <tag> InstantNGP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Instant-NSR</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/Instant-NSR/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/Instant-NSR/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Human Performance Modeling and Rendering via Neural Animated Mesh</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://zhaofuq.github.io/">Fuqiang Zhao</a>, <a href="https://nowheretrix.github.io/">Yuheng Jiang</a>, Kaixin Yao, <a href="https://jiakai-zhang.github.io/">Jiakai Zhang</a>, <a href="https://aoliao12138.github.io/">Liao Wang</a>, Haizhao Dai, Yuhui Zhong, <a href="https://cn.linkedin.com/in/yingliangzhang">Yingliang Zhang</a> <a href="https://wuminye.com/">Minye Wu</a>, <a href="http://xu-lan.com/">Lan Xu</a>, <a href="https://sist.shanghaitech.edu.cn/2020/0707/c7499a53862/page.htm">Jingyi Yu</a></td></tr><tr><td>Conf/Jour</td><td>SIGGRAPH Asia 2022</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://zhaofuq.github.io/NeuralAM/">Human Performance Modeling and Rendering via Neural Animated Mesh (zhaofuq.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4669786505854582785&amp;noteId=1856547936358141696">Human Performance Modeling and Rendering via Neural Animated Mesh (readpaper.com)</a></td></tr></tbody></table></div><p>可以理解为对Neus使用多分辨率哈希编码进行加速</p><ul><li>使用TSDF代替SDF</li><li>有限差分函数计算SDF的梯度，在tiny-CUDAnn并未集成，公开了自己的CUDAC++代码</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/pipeline.jpg" alt="pipeline"></p><p>不足：</p><ul><li>数据集需要手动mask</li></ul><span id="more"></span><p>数据集图片：<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/004.png" style="width:50%;"></p><h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>将truncated(截断) signed distance field(TSDF)与multi-resolution hash encoding结合在一起<br>进一步提出了hybrid neural tracker混合神经追踪器生成动画的meshs</p><ul><li>它将显式非刚性跟踪与隐式动态变形结合在一个自监督框架中<ul><li>前者提供coarse扭曲回规范空间</li><li>后者则使用与我们的重构器中的4D哈希编码相同的隐式方式进一步预测位移</li></ul></li></ul><p>我们讨论了使用获得的动画网格的渲染方案，从动态纹理到在各种带宽设置下的透视渲染。为了在质量和带宽之间取得复杂的平衡，我们提出了一种分层解决方案：</p><ul><li>首先渲染覆盖表演者的6个虚拟视图，然后进行遮挡感知的神经纹理混合。我们在各种基于网格的应用程序和各种平台上展示了我们方法的有效性，例如通过移动AR将虚拟人表演插入真实环境，或者通过VR头显身临其境地观看才艺展示。</li></ul><p>ACM分类：Computing methodologies → Computational photography; Image-based rendering.<br>Keywords：virtual human, neural rendering, human modeling, human performance capture</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>目标：在VR or AR场景中，可以身临其境地观看人类表演。具体而言，用户可以像与表演者面对面一样进行互动，简单到改变视角，复杂到通过触手可及的方式进行内容编辑。</p><p>目前显式重建方法的不足：迄今为止，生成体积人类表演的最广泛应用工作流程仍然是通过重建和跟踪动态网格，并为每一帧贴上纹理贴图。实际上，无论是基于摄影测量的重建还是基于3D扫描的重建仍然耗时，并容易受到遮挡和缺乏纹理的影响，从而导致产生洞和噪声。为了满足最低程度的沉浸式观看要求，最终重建的序列需要经验丰富的艺术家进行大量修复工作。</p><p>NeRF: 静态场景和动态场景</p><ul><li>静态场景通过多个观察视图，绕过显式重建，专注合成逼真的新视图<ul><li>[Lombardi等，2019; Tewari等，2020; Wu等，2020]</li><li>[Mildenhall等，2020] NeRF用MLP取代了传统的几何和外观概念</li><li>最初的NeRF及其加速方案[Müller等，2022; Yu等，2021a]主要关注静态场景</li></ul></li><li>动态场景中的实时渲染需要在空间和质量之间达到复杂的平衡<ul><li>[Peng等，2021b; Tretschk等，2020; Zhang等，2021; Zhao等，2022]旨在将这种神经表示扩展到具有时间作为潜在变量的动态场景中</li><li>空间[Müller等，2022; Sara Fridovich-Keil和Alex Yu等，2022; Yu等，2021a]</li><li>质量[Suo等，2021; Wang等，2022]</li></ul></li><li>在可动人类角色的背景下，也可以通过将参数化人体模型作为先验条件[Bagautdinov等，2021; Habermann等，2021; Liu等，2021; Xiang等，2021]来生成神经角色。然而，它们的最终质量在很大程度上依赖于预扫描或参数化模板和繁琐耗时的每个角色训练。</li></ul><p>目前的神经方法提取出来的mesh不适应现有的基于网格的动画流水线：</p><ul><li>神经方法的另一个主要挑战是它们的结果，即经过训练的神经网络，不能直接支持现有的基于网格的动画流水线，无论是流媒体还是渲染。尽管可能可以从网络中提取网格，例如通过对NeRF的密度场进行阈值处理，然后进行三角剖分，但所产生的几何形状往往过于嘈杂，无法适应先前的基于网格的流水线[Collet等人，2015年]。</li></ul><p>将NeRF的密度场换成SDF，可以得到高质量mesh，但是仍然很费时：(例如Neus)</p><ul><li>开创性的神经隐式表面[Munkberg等人，2021年；Wang等人，2021a]在体积渲染公式中将密度场替换为有符号距离场（SDF），可以恢复出非常高质量的几何形状。然而，这些方法仍然需要很长的训练时间，无法扩展以生成动态网格序列。事实上，即使可以重构网格序列，很少有工作强调应用基于神经网络的网格压缩来支持流媒体或高质量播放的神经渲染。</li></ul><h3 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h3><p>我们提出了一个全面的神经建模和渲染流程，用于支持多视角摄像机球顶捕捉的人类表演的高质量重建、压缩和渲染。我们遵循传统的动画网格工作流程，但采用一种新的高效神经技术类别，如图1所示。</p><ul><li>InstantNGP+Neus：具体而言，在网格重建方面，我们提出了一种高度加速的神经表面重建器Instant-NSR，类似于InstantNGP [Müller等，2022]，但基于NeuS [Wang等，2021a]的SDF体积公式进行表面重建。</li><li>TSDF：我们发现直接采用多分辨率哈希编码可能会导致梯度爆炸问题，因此无法在基于特征哈希的网络优化的早期迭代中稳定收敛。相反，我们提出了截断SDF（TSDF）公式，该公式在先前的3D融合框架[Newcombe等，2015，2011]中得到应用，并通过Sigmoid映射实现。我们的开源PyTorch实现将训练时间从NeuS的8小时缩短到10分钟，并且具有相当的质量。</li><li>我们还提出了一种基于有限差分的CUDA实现，用于近似估计法线，这可以进一步缩短训练时间，但质量略有降低。</li></ul><p>动画的meshs：<br>我们应用Instant-NSR生成高质量的网格序列，然后开始对几何进行压缩，并使用动画网格编码时间对应的信息。具体来说，我们引入了一个混合神经跟踪器，<strong>将显式的非刚性跟踪和隐式的动态变形结合在一个自监督框架中</strong>。</p><ul><li>在显式阶段，我们采用了基于嵌入变形的快速非刚性跟踪方案[Newcombe等人，2015年；Xu等人，2019b]。它从各个帧生成粗糙的初始变形，以维持高效率。</li><li>在隐式阶段，我们学习规范空间中4D坐标（3D变形位置+1D时间戳）的每个顶点位移，以保留精细的几何细节。我们再次采用CUDA核心的4D哈希编码方案和Instant-NSR中的体积渲染公式，以进行快速和自监督训练，以获得动画网格。</li></ul><p>渲染方面：<br>粗暴的方法是采用动态纹理[Collet等人，2015; UVAtlas 2011]来支持流式沉浸式体验。<br>相反，我们讨论了在不同带宽设置下的策略，从在整个序列中使用单个纹理映射到从所有圆顶摄像头流式传输视频以进行高质量的图像重建[Buehler等人，2001]。我们展示了一种妥协方案，首先渲染涵盖不同观察者视角的6个虚拟视图，然后进行神经纹理混合，考虑遮挡和视角依赖性，以在质量和带宽之间实现复杂的平衡。最后，我们展示了各种虚拟和增强现实应用的内容播放，包括在移动AR平台上将虚拟人类表演插入真实环境，以及使用VR头显沉浸式地观看超高质量的才艺表演节目。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>总结起来，我们的主要贡献包括：  </p><ul><li>我们引入了一种新颖的流程，以在神经时代era始终更新动态人物表演的建模、跟踪和渲染，在效果和效率两方面都与现有系统相比具有优势。  </li><li>我们提出了一种快速的神经表面重建方案，通过将TSDF体积渲染与哈希编码相结合，在几分钟内生成高质量的隐式表面。  </li><li>我们提出了一种混合神经跟踪方法，以自我监督的方式将显式和隐式的运动表示相结合，生成与传统制作工具兼容的动画网格。  </li><li>我们提出了一种分层神经渲染方法，用于逼真的人物表演合成，并展示了我们的流程在各种基于网格的应用和沉浸式自由视角虚拟现实/增强现实体验中的能力。</li></ul><h2 id="RelatedWork"><a href="#RelatedWork" class="headerlink" title="RelatedWork"></a>RelatedWork</h2><h3 id="神经人类建模"><a href="#神经人类建模" class="headerlink" title="神经人类建模"></a>神经人类建模</h3><p>Neural Human Modeling</p><ul><li>神经隐式表示最近作为传统场景表示（如网格、点云和体素网格）的有希望替代方法出现。由于其连续性特点，神经隐式表示在理论上可以以无限分辨率渲染图像。最近，NeRF（Mildenhall等，2020）及其后续工作（Chen等，2021a,b; Park等，2021; Peng等，2021b; Pumarola等，2021; Tiwari等，2021; Wang等，2021c,b,d; Zhang等，2021; Zhao等，2022）利用体积表示，通过对5D坐标表示进行光线行进计算辐射。基于经典的体积渲染技术，它们在新视角合成方面取得了令人印象深刻的结果。然而，由于体积渲染的模糊性（Zhang等，2020），这些方法仍然受到几何质量较低的影响。</li><li>一些基于表面渲染的方法（Niemeyer等，2019, 2020; Saito等，2021; Yariv等，2020）尝试通过隐式微分获得梯度，直接优化底层表面。<ul><li>UNISURF（Oechsle等，2021）是一种混合方法，通过体积渲染学习隐式表面，并鼓励体积表示收敛到一个表面。</li><li>与UNISURF不同，NeuS（Wang等，2021a）提供了从符号距离函数（SDF）到体积渲染密度的无偏转换。UNISURF通过占用值表示表面，并逐渐减少一些预定义步骤中的采样区域，使占用值收敛到表面，而NeuS通过SDF表示场景，因此表面可以作为SDF的零水平集自然提取，从而比UNISURF具有更好的重建精度。</li><li>所有这些方法的共同点是它们依赖于NeRF训练过程，在渲染过程中在网络上进行光线行进推理，这在训练和推理过程中计算量很大。<strong>因此，它们只适用于静态场景重建，无法处理需要无法接受的训练时间的动态场景</strong>。</li></ul></li><li>最近，一些NeRF扩展（Müller等，2022; Sara Fridovich-Keil和Alex Yu等，2022; Wang等，2022; Yu等，2021a; Zhang等，2022）被提出以加速NeRF的训练和渲染，但未加速隐式表面重建。最近的一项研究（Munkberg et al.，2021）通过利用高效可微分光栅化来优化显式网格表示，但仍需要几个小时的训练时间。我们的目标是提出一种快速的隐式表面重建方法，只需要几分钟的训练时间，并利用多视角图像进行重建。</li></ul><h3 id="人类行为捕捉"><a href="#人类行为捕捉" class="headerlink" title="人类行为捕捉"></a>人类行为捕捉</h3><p>Human Performance Capture.</p><ul><li>最近，通过传统的非刚性融合流程提出了具有实时性能的自由形态动态重建。从开创性的作品DynamicFusion [Newcombe等，2015年]开始，它通过GPU求解器和先进的多摄像头系统获益，Fusion4D [Dou等，2016年]和motion2fusion [Dou等，2017年]将非刚性跟踪流程扩展到具有挑战性动作的动态场景捕捉。<ul><li>KillingFusion [Slavcheva等，2017年]和SobolevFusion [Slavcheva等，2018年]对运动场提出了更多约束，以支持拓扑变化和快速帧间运动。</li><li>DoubleFusion [Yu等，2019年]通过添加身体模板SMPL [Loper等，2015年]作为先验知识，将内部身体和外部表面组合在一起，更稳健地捕捉人体活动。</li><li>得益于人体运动表示，UnstructuredFusion [Xu等，2019b年]实现了非结构化多视角设置。</li><li>Function4d [Yu等，2021b年]将时间体积融合与隐式函数相结合，生成完整的几何形状，并能处理拓扑变化。</li><li>然而，这些方法都利用了深度传感器精度有限的深度图。与此同时，这些方法及其后续工作[Jiang等，2022b年; Xu等，2019b年; Yu等，2021b年]对RGB和深度传感器的校准非常敏感。</li></ul></li><li>基于数字人体建模，一些先前的工作通过网格压缩探索支持自由视点视频。实现几何一致性压缩的关键是可靠地建立存储为3D网格序列的所有帧之间的对应关系。<strong>在行为捕捉的背景下，由于大规模非刚性变形随时间容易失去跟踪，这个任务仍然具有挑战性</strong>。拓扑变化和退化重构（例如，无纹理区域）导致额外的问题。现有的仅考虑形状的描述符[Litman和Bronstein 2013；Windheuser等，2014]对噪声敏感，而密集形状对应要求拓扑一致性，主要应用于零亏格表面。早期的匹配方案采用了强假设，包括等距或共形几何、测地线或扩散约束、局部几何一致性等，在马尔可夫随机场（MRF）或随机决策森林（RDF）框架下进行。最近的神经方法尝试从多视深度图或全景深度图中训练特征描述符，以分类不同的身体区域，以提高鲁棒性。</li><li>一旦构建完成，动画网格可以被有效压缩以进一步节省存储空间。基于主成分分析（PCA）的方法[Alexa和Müller 2000；Luo等人2013；Vasa和Skala 2007]旨在识别人体的不同几何集群（手臂、手、腿、躯干、头等），而[Gupta等人2002；Mamou等人2009]在网格上进行预分割以确保连接的一致性。时空模型可以进一步预测顶点轨迹以形成顶点组[Mamou等人2009；Ibarria和Rossignac 2003；Luo等人2013]。然而，一个被大部分忽视的问题是序列的渲染：在传统的计算机图形中，一个具有足够分辨率的单一预渲染纹理就足够渲染序列中的所有网格。然而，在真实捕捉到的序列中，由于遮挡、相机的校准误差以及颜色或光照不一致，几乎不可能产生具有可比质量的单一纹理。此外，使用单一纹理映射会失去视角依赖性。相反，我们的方法通过多视角RGB图像恢复人体几何形状，并通过传统的管线和神经形变网络进行非刚性跟踪，从而能够自然地处理人体活动。</li></ul><div class="note info">            <p>单一视角恢复物体的纹理，质量很低</p><ul><li>在传统的计算机图形中，一个具有足够分辨率的单一预渲染纹理就足够渲染序列中的所有网格</li><li>然而，在真实捕捉到的序列中，由于遮挡、相机的校准误差以及颜色或光照不一致，几乎不可能产生具有可比质量的单一纹理</li><li>此外，使用单一纹理映射会失去视角依赖性</li><li>相反，我们的方法通过多视角RGB图像恢复人体几何形状，并通过传统的管线和神经形变网络进行非刚性跟踪，从而能够自然地处理人体活动</li></ul>          </div><h3 id="神经人类渲染"><a href="#神经人类渲染" class="headerlink" title="神经人类渲染"></a>神经人类渲染</h3><p>Neural Human Rendering</p><ul><li>在逼真的新颖视图合成和3D场景建模领域，基于不同数据代理的可微神经渲染取得了令人印象深刻的结果，并变得越来越受欢迎。采用各种数据表示以获得更好的性能和特性，如点云[Aliev等，2020; Suo等，2020; Wu等，2020]，体素[Lombardi等，2019]，纹理网格[Liu等，2019; Shysheya等，2019; Thies等，2019]或隐式函数[Kellnhofer等，2021; Mildenhall等，2020; Park等，2019]和混合神经融合[Jiang等，2022a; Sun等，2021; Suo等，2021]。<ul><li>最近，[Li等，2020; Park等，2020; Pumarola等，2021; Wang等，2022]将神经辐射场[Mildenhall等，2020]扩展到动态环境中。</li><li>[Hu等，2021; Peng等，2021a，b; Zhao等，2022]利用人类先验SMPL[Loper等，2015]模型作为锚点，并使用线性混合蒙皮算法来扭曲辐射场。</li><li>此外，[Jiang等，2022a; Sun等，2021]将动态神经渲染和融合扩展到人类物体交互场景中。</li><li>然而，对于上述绝大多数方法来说，仍然需要密集的空间视图以实现高保真度的新视图渲染。基于图像的融合方法学习相邻视图的融合权重，并以轻量级方式合成逼真的新视图。</li><li>[Wang等，2021b]学习融合权重以获得泛化能力。</li><li>[Suo等，2021]使用遮挡地图作为融合权重估计的指导。</li><li>[Jiang等，2022a]将基于图像的融合与每顶点纹理结合起来解决遮挡问题。</li><li>相比之下，我们的神经融合方案结合了通过规范空间渲染获得的时空信息，同时将内存压缩到极限，也能确保融合结果的保真度。</li></ul></li></ul><h2 id="InstantNSR"><a href="#InstantNSR" class="headerlink" title="InstantNSR"></a>InstantNSR</h2><p>INSTANT NEURAL SURFACE RECONSTRUCTION</p><h3 id="体渲染函数形式"><a href="#体渲染函数形式" class="headerlink" title="体渲染函数形式"></a>体渲染函数形式</h3><p>NeRF：通过输入(𝑥, 𝑦, 𝑧, 𝜃, 𝜙)来获得$\sigma$和RGB值<br>Neus：用SDF代替了$\sigma$场，从而实现了更确定和准确的几何形状提取</p><p>本文采用NeRF类似体渲染方式，通过沿着光线累加n个采样点的几何和外观属性来获得像素的颜色值Color，然而SDF的表示中没有定义$\sigma$ 。本文遵循Neus将密度替换为不透明度密度$\rho(t)$，基于采样点位置$\mathbf{p}(t)$和累计密度分布CDF：$\Phi_s(x) = \frac{1}{1+e^{-bx}}$，其中b为inv_s是可以训练的超参数，随着网络训练的收敛，逐渐增加到较大数量。<br>体渲染函数与Neus类似：</p><p>$\rho(t)=\max\left(\frac{-\frac{\mathrm{d}\Phi_s}{\mathrm{d}t}(f(\mathbf{p}(t)))}{\Phi_s(f(\mathbf{p}(t)))},0\right)$<br>$\alpha_i=\max\left(\frac{\Phi_s(f(\mathbf{p}(t_i))))-\Phi_s(f(\mathbf{p}(t_{i+1})))}{\Phi_s(f(\mathbf{p}(t_i)))},0\right) =\max\left(1 - \frac{\Phi_s(f(\mathbf{p}(t_{i+1})))}{\Phi_s(f(\mathbf{p}(t_i)))},0\right)$<br>$\hat{C}=\sum_{i=1}^n T_i\alpha_i c_i$，离散累计透射率：$T_i=\prod_{j=1}^{i-1}(1-\alpha_j)$</p><h3 id="Truncated-SDF-Hash-Grids"><a href="#Truncated-SDF-Hash-Grids" class="headerlink" title="Truncated SDF Hash Grids"></a>Truncated SDF Hash Grids</h3><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/pipeline.jpg" alt="pipeline"></p><p>在这里，我们引入了一种类似于Instant-NGP的神经表面重构器InstantNSR，它可以从密集的多视图图像中高效地生成高质量的表面。哈希编码提供了高效的表示学习和查询方案，TSDF表示可以大大提高网络训练的稳定性。<br>具体而言，给定光线上的样本点 p(t)，我们首先通过插值从 L 级哈希网格的八个顶点处获取特征向量。我们将获取的特征向量连接成 $Fhash ∈ R^{𝐿×𝑑}$，然后将其输入到我们的 SDF 网络$m_{s}$中，该网络由一个浅层的 MLP 组成。$m_{s}$输出样本点的 SDF 值 𝑥</p><div class="note info">            <p>$m_{s}$训练SDF的网络表示为:$(x,F_{geo})=m_{s}(p,F_{hash}).$</p>          </div><p>单纯地将SDF表示应用于哈希编码框架将在优化过程中引入几个收敛问题。原始的基于SDF的方法利用累积密度分布$\Phi_s(x) = \frac{1}{1+e^{-bx}}$来计算alpha值，这会产生数值问题。当𝑏增加时，术语−𝑏𝑥将成为一个较大的正数，并使得$𝑒^{−𝑏𝑥}$接近无穷大。这种数值不稳定性使得损失函数在训练过程中很容易发散。</p><p>为了避免$\Phi_s(x)$中的值溢出，我们引入截断有符号距离函数（TSDF），表示为$\hat x$。由于TSDF的值范围为-1到1，这种修改确保了数值稳定性。我们将SDF输出应用于Sigmoid函数𝜋(·)，而不是直接截断SDF，以实现更好的收敛性并避免梯度消失问题。<br>$\pi(x)=\frac{1-e^{-bx}}{1+e^{-bx}}.$, 此时$\Phi_s(x) = \frac{1}{1+e^{-b \cdot \pi(x)}}$</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230717165905.png" alt="image.png"></p><p>类似于Instant-NGP [Müller et al. 2022]，我们将样本点的几何编码$𝐹_{𝑔𝑒𝑜}$、位置p和视线方向v输入到一个颜色网络$𝑚_{𝑐}$中，以预测其颜色$\hat C$。此外，我们还将点的法线n作为输入的一部分。法线即为SDF的梯度。<em>引入n的原因是为了隐式地对输出的SDF进行规范化，基于这样一个观察：如果邻近采样点的法线也相近，颜色网络倾向于输出相似的颜色。由于我们将法线定义为SDF的一阶导数，法线的梯度可以反向传播到SDF中。将法线添加到输入中可以使重建的表面更加平滑，尤其是对于无纹理区域。</em></p><div class="note info">            <p>$m_{c}$训练颜色的网络表示为：$\hat{C}=m_{c}(\mathrm{p},\mathrm{n},\mathrm{v},F_{geo}).$</p>          </div><h3 id="损失函数loss"><a href="#损失函数loss" class="headerlink" title="损失函数loss"></a>损失函数loss</h3><script type="math/tex; mode=display">\begin{gathered}\mathcal{L}=\mathcal{L}_{color}+\lambda\mathcal{L}_{eik}, \\\mathcal{L}_{color}=\frac{1}{b}\sum_{i}^{b}\mathcal{R}(\hat{C},C), \\\mathcal{L}_{eik}=\frac{1}{nb}\sum_{k,i}^{n,b}(|\mathrm{n}|-1)^{2} \end{gathered}</script><div class="note success">            <p>对于类似于Instant-NGP的基于CUDA的加速，我们采用有限差分函数来近似计算方程6和8中的梯度，以实现高效的法线计算。这种策略避免了繁琐的二阶梯度反向传播，而这在现有的库如tiny-CUDAnn [Müller 2021]中仍然没有得到完全支持。为了促进更加忠实的CUDA实现的未来工作，我们将公开发布我们的版本。</p>          </div><h2 id="神经动画网格"><a href="#神经动画网格" class="headerlink" title="神经动画网格"></a>神经动画网格</h2><p>NEURAL ANIMATION MESH</p><p>尽管我们的快速神经表面重建(第3节)为渲染提供了高精度的几何图形，但由于内存占用大，单独保存每帧的几何图形效率低下，限制了应用范围。为此，我们着手从高精度几何结构中构建拓扑一致的网格，以进一步压缩数据，并启用时间编辑效果。动画网格的生成是基于查找帧之间的几何对应关系。如图5所示，<strong>我们提出了一种神经跟踪管道，将非刚性跟踪和神经变形网络以粗到细的方式结合在一起</strong>。它包括如下所述的两个阶段</p><h3 id="粗追踪阶段"><a href="#粗追踪阶段" class="headerlink" title="粗追踪阶段"></a>粗追踪阶段</h3><p>Coarse Tracking Stage</p><p>在粗跟踪阶段，我们采用传统的非刚性跟踪方法[Guo等，2015年；Jiang等，2022年a；Newcombe等，2015年；Xu等，2019年a，b；Yu等，2019年]基于嵌入变形的方法来建立粗略对应关系。<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230706171346.png" alt="image.png"></p><p>如图4所示，我们在第一帧建立一个规范空间，并将第一帧的重建结果作为跟踪参考的规范网格。基于基于图形的表示方法[Newcombe等，2015年]，我们通过计算规范网格上的测地线距离均匀采样嵌入变形（ED）节点[Sumner等，2007年]，并使用这些节点驱动其进入目标帧。同时，我们用ED节点对人体运动$𝐻 = \{ 𝑇_{𝑖} , 𝑥_{𝑖} \}$进行参数化，其中$𝑥_{𝑖}$是采样的ED节点坐标，$𝑇_{𝑖}$是刚性变换。一旦获得优化的运动𝐻，我们可以使用线性混合蒙皮（LBS）[Loper等，2015年]将规范网格的顶点转换到实时空间:</p><p>$\mathrm{v’}_j=\sum_{i\in\mathcal{N}(\mathrm{v}_j)}\omega(\mathrm{x}_i,\mathrm{v}_j)T_i\mathrm{v}_j.$</p><p>在规范网格上，$v_{𝑗}$ 是一个顶点，$N(v_{𝑗})$ 表示顶点的相邻 ED 节点</p><p>$\omega(\mathrm{x}_{i},\mathrm{v}_{j})=\left(1-\left|\mathrm{v}_{j}-\mathrm{x}_{i}\right|_{2}^{2}/r^{2}\right)^{3}$ 是第𝑖个节点 $𝑥_{𝑖}$对$𝑣_{𝑗}$ 的影响权重。𝑟 是影响半径，在我们的实验中设置为 0.075。</p><h4 id="变形场估计"><a href="#变形场估计" class="headerlink" title="变形场估计"></a>变形场估计</h4><p>Warping Field Estimation</p><p>然而，由于规范帧和当前帧之间存在较大的运动差异，直接计算两者之间的运动𝐻是不现实的，这将导致非刚性跟踪失败。相反，我们跟踪相邻帧，然后将优化后的局部变形场𝑊传播到规范空间，以获得每帧的运动𝐻。遵循传统的非刚性跟踪流程[Li et al. 2009; Newcombe et al. 2015]，我们使用非刚性迭代最近点（ICP）算法在帧𝑡 − 1和帧𝑡之间搜索点对点和点对平面的对应关系。由于我们已经有每帧的几何信息，我们可以直接使用几何信息进行跟踪，而不是使用深度输入。</p><ul><li><p>利用几何信息作为输入的优势有两个方面:</p><ul><li>首先，它可以防止融合算法受到深度传感器引起的输入噪声的干扰；</li><li>其次，我们能够在欧拉空间中建立初始的局部对应关系，以进行跟踪优化，而不仅仅在单个射线上找到对应关系[Newcombe et al. 2011]。</li></ul></li><li><p>用于优化变形场𝑊𝑡的能量函数被定义为：<br>$E(W_t,V_{t-1},M_t,G)=\lambda_\text{data}E_\text{data}(W_t,V_{t-1},M_t)+\lambda_\text{reg}E_\text{reg}(W_t,G),$在这里 $𝑉_{𝑡 −1}$ 是使用扭曲场$𝑊_{𝑡 −1}$ 从规范空间变形的动画网格；$𝑀_{𝑡}$ 是从（第3节）获得的当前帧的重构几何体。</p></li><li><p>数据项用于最小化拟合误差，公式为:<br>$E_{\mathrm{clata}}=\sum_{\mathrm{v}_{j}\in C}\lambda_{\mathrm{point}}\left|\mathrm{v’}_{j}-\mathrm{c}_{j}\right|_{2}^{2}+\lambda_{\mathrm{plane}}\left|\mathrm{n}_{\mathrm{c}_{j}}^{\mathrm{T}}\left(\mathrm{v’}_{j}-\mathrm{c}_{j}\right)\right|^{2},$其中$v’_𝑗$是从规范空间变换得到的变形顶点 $v_𝑗$，$c_𝑗$ 是当前帧网格上的对应顶点。C 表示从非刚性 ICP 中获取的初始对应顶点对集合。权重 𝜆 用于平衡不同项的相对重要性。在我们的实验中，我们设置：<br>$𝜆_{data} = 1，𝜆_{reg} = 20，𝜆_{point} = 0.2 和 𝜆_{reg} = 0.8$</p></li><li><p>为了约束ED节点运动的平滑性，我们采用了局部尽可能刚性的正则化方法:<br>$E_{\mathrm{reg}}=\sum_{\mathrm{x}_{i}}\sum_{\mathrm{x}_{j}\in\mathcal{N}(\mathrm{x}_{i})}\omega\left(\mathrm{x}_{i},\mathrm{x}_{j}\right)\psi_{\mathrm{reg}}\left\Vert\mathrm{T}_{i}\mathrm{x}_{i}-\mathrm{T}_{j}\mathrm{x}_{i}\right\Vert_{2}^{2},$</p></li></ul><p>其中$x_𝑖，x_𝑗$是共享ED图𝐺上边的ED节点。$𝜔 (x_𝑖，x_𝑗)$定义了边的权重。$T_𝑖，T_𝑗$是ED节点的变换。$𝜓_𝑟𝑒𝑔$是保持不连续性的Huber惩罚项。然后我们通过GPU上的Gauss-Newton求解器来解决非线性最小二乘问题。更多细节请参考[Newcombe et al. 2015; Yu et al. 2019]。</p><h3 id="追踪细化阶段"><a href="#追踪细化阶段" class="headerlink" title="追踪细化阶段"></a>追踪细化阶段</h3><p>Tracking Refinement Stage</p><p>我们的显式粗糙非刚性跟踪阶段在复杂运动方面具有表现力强的表示能力，但仍然受到自由度的限制。因此，在细化阶段，我们采用了一种称为变形网络的神经方案，为几何跟踪提供更多的自由度，纠正不对齐问题并保留细粒度的几何细节，这是对现有工作的重大进展[Newcombe等人，2015年；Slavcheva等人，2017年；Xu等人，2019年b；Yu等人，2018年]。<br>我们采取两个步骤来实现这个目标:</p><ul><li>首先，我们利用原始的Instant-NGP [Müller等人，2022年]以快速方式在规范空间中获得辐射场$𝜙^{𝑜}：(c, 𝜎) = 𝜙^𝑜(p, d)$，其中c表示颜色场，𝜎表示密度场，p是规范空间中的点，d是射线方向。在获得这个辐射场之后，我们冻结模型参数。</li><li>其次，我们训练一个变形网络$𝜙^𝑑$来预测每帧的变形位移，并进一步微调帧的运动𝐻。第二步的核心是建立粗糙运动估计和光度损失方案之间的联系，以便我们可以使用输入图像在端到端监督中优化变形位移。我们引入了可微分的体渲染管线，与跟踪的ED节点合作实现监督。</li></ul><p>由于存在粗糙的运动𝐻，我们将规范帧中的ED节点转换为当前帧，并沿着目标视图的像素射线采样点。对于每个样本点p，我们找到其𝑘个最近的ED节点，并插值它们的反变形以回到规范空间。规范空间中的变形点表示为p’。我们将p’和当前帧的时间戳𝑡作为变形网络$𝜙^𝑑$的输入。网络$𝜙^𝑑$将预测位移Δp’。我们采用哈希编码方案[Müller等人，2022年]对$𝜙^𝑑$进行编码，以使其训练和推理高效。样本点p的密度和颜色的计算如下：<br>$\mathrm{(c,\sigma)=\phi^{o}(p’+\phi^{d}(p’,t),d).}$</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230706200640.png" alt="image.png"></p><h3 id="动画网格生成"><a href="#动画网格生成" class="headerlink" title="动画网格生成"></a>动画网格生成</h3><p>Animation Mesh Generation</p><p>使用变形网络可以为我们提供从当前帧到规范帧的更好对应关系。我们可以利用细粒度的对应关系作为非刚性跟踪算法中更好的先验信息，重新优化运动为𝐻 ′。为了生成当前帧的动画网格，我们只需要将𝐻 ′ 应用于规范网格的顶点，并保持拓扑关系不变。<br>如图6所示，我们利用精细的变形场参数来跟踪和获取实时帧的动画网格。与保存原始网格序列和网络参数不同，我们只需要保存规范网格和运动场，以支持回放、实时渲染和编辑。</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230706201011.png" alt="image.png"></p><h2 id="渲染"><a href="#渲染" class="headerlink" title="渲染"></a>渲染</h2><p>RENDERING</p><p>神经动画网格重建提供准确的几何信息，能够为各种渲染方法（如纹理网格渲染和基于图像的渲染）提供高质量的渲染效果。这些方法可以基于我们的神经动画网格在新视角下产生逼真的图像质量，但是要么缺乏视角相关的效果，要么需要大量的参考视角存储空间。<br>在本节中，我们提出了一种结合了<strong>显式二维纹理映射和隐式神经纹理混合</strong>的方法，该方法能够流式传输并保持高渲染质量。为此，我们的方法通过降低4D视频序列的比特率来实现流式应用，并旨在具有更低的存储需求。</p><h3 id="外观蒸馏"><a href="#外观蒸馏" class="headerlink" title="外观蒸馏"></a>外观蒸馏</h3><p>Appearance Distillation</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230706192541.png" alt="image.png"></p><p>请注意，我们的神经动画网格可以减少几何存储空间，但我们仍然有数十个参考视图，其视频需要存储空间。关键的想法是将所有参考视图的外观信息浓缩到更少的图像中。我们通过基于图像的渲染来实现这个目标。具体而言，我们预先定义了六个虚拟视点，均匀地围绕并朝向中心的表演者。这些视图具有足够的视场角，以便在图中显示的视图中可以看到整个表演者，如图8所示。我们使用无结构光图法[Gortler et al. 1996]来合成这些视图图像，该方法根据给定的几何计算相邻参考视图的像素混合。合成的视图通过从不同的身体部位和其他视图混合像素来编码原始参考视图的外观信息。以这种方式需要存储的视图数量减少到六个。然而，这种减少会导致信息丢失，从而降低蒸馏过程中结果的质量。<strong>因此，我们使用神经网络部署了一种神经混合方法来补偿信息损失</strong>。</p><h3 id="神经混合"><a href="#神经混合" class="headerlink" title="神经混合"></a>神经混合</h3><p>Neural Blending</p><p>我们引入了一种神经混合方案，可以将相邻虚拟视图中的细粒度纹理细节混合到新视图中。与[Suo等人，2021]类似，神经混合的输入不仅包括目标相机的两个相邻视图，还包括它们的深度图以实现遮挡感知。然而，由于视图稀疏性，一些自遮挡部分可能无法通过任何相邻的虚拟视图观察到。为了填补新视图中的这种区域，我们还采用了纹理动画网格渲染结果作为混合组件的一部分。图9展示了我们混合模块的流程。<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230706192853.png" alt="image.png"></p><ul><li><p>遮挡感知：目标视图中的大部分外观信息可以通过其两个相邻的虚拟视图观察到，由于自遮挡而导致的缺失部分可以在我们的稀疏视图重新渲染设置中使用2D纹理图进行恢复。基于这个假设，我们提出了一种混合方法，通过使用选定的视图通过混合网络重新渲染动画网格。给定围绕表演者的𝑘个输入图像，我们首先生成目标视图的深度图($𝐷𝑟_𝑡$ )、两个输入视图的深度图($𝐷𝑟_{1}$和$𝐷𝑟_{2}$)，以及目标视图中的粗糙渲染图像$𝐼_{𝑟}$，其中使用了第4节中描述的纹理网格。然后，我们使用$𝐷𝑟_{𝑡}$将输入图像$𝐼_1$和$𝐼_2$变形到目标视图，表示为$𝐼_{1,𝑡}和𝐼_{2,𝑡}$。我们还将源视图的深度图变形到目标视图，并获得$𝐷𝑟_{1,𝑡}$和$𝐷𝑟_{2,𝑡}$，以获得遮挡图$𝑂_{𝑖} = 𝐷𝑟_{𝑡} − 𝐷𝑟_{𝑖,𝑡} (𝑖 = 1, 2)$，它表示了遮挡信息。</p></li><li><p>混合网络：由于自遮挡和几何代理不准确，$𝐼_{1,𝑡}$和$𝐼_{2,𝑡}$可能不正确。仅使用二维纹理进行混合会产生明显的伪影。因此，我们引入了一个混合网络 $𝑈_{𝑏}$，该网络利用了多视图环境中的固有全局信息，并利用像素级的混合地图 𝑊 融合了相邻输入视图的局部精细几何和纹理信息，其可表示为：$W=U_{\boldsymbol{b}}(I_{1,\boldsymbol{t}},O_1,I_{2,\boldsymbol{t}},O_2).$</p><ul><li>$𝑈_{𝑏}$的网络结构类似于U-Net，网络输出两个通道的特征图𝑊 = (𝑊1, 𝑊2)，分别表示扭曲图像的混合权重。</li></ul></li></ul><p>为了实时性能，深度图在低分辨率（512 × 512）上生成。为了实现逼真的渲染，我们需要将深度图和混合图都上采样到2K分辨率。<br>然而，简单的上采样会导致边界附近的严重锯齿效应，这是由于深度推断模糊性造成的。因此，我们提出了一种边界感知方案，以改进深度图上的人体边界区域。具体而言，我们使用双线性插值来上采样$𝐷r_{𝑡}$。然后，应用腐蚀操作来提取边界区域。边界区域内的深度值通过使用第4节中描述的流程重新计算，并形成2K分辨率的$\hat 𝐷𝑟_{𝑡}$。然后，我们使用$\hat 𝐷𝑟_{𝑡}$将原始高分辨率输入图像变形为目标视图，得到$\hat 𝐼_{𝑖,𝑡}$。因此，我们最终的纹理融合结果可以表示为：<br>$I=\hat{W}_1\cdot\hat{I}_{1,t}+\hat{W}_2\cdot\hat{I}_{2,t}+(1-\hat{W}_2-\hat{W}_2)\cdot I_r,$<br>其中 $\hat 𝑊$ 是通过双线性插值直接上采样的高分辨率混合图；$𝐼_{r}$是对应纹理动画网格的图像。我们使用 Twindom [twindom [n. d.]] 数据集生成训练样本。合成训练数据包含 1,500 个带纹理的人体模型，并在每个模型周围渲染 360 个新视角和深度图。在训练过程中，我们随机选择六个视角，其相对姿态满足预定义的关系作为参考视角，而其他视角则是目标视角的真实值。我们通过均方误差损失函数将混合图像 𝐼 和对应的真实图像 𝐼 ′ 进行比对，以监督网络训练。</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Implementation Details and Running-time analysis</p><p>整个系统运行在单个RTX3090上</p><h3 id="各阶段花费时间"><a href="#各阶段花费时间" class="headerlink" title="各阶段花费时间"></a>各阶段花费时间</h3><div class="table-container"><table><thead><tr><th>Stage</th><th>Action</th><th>Avg Time</th></tr></thead><tbody><tr><td>Preprocessing</td><td>background matting</td><td>∼ 1 min</td></tr><tr><td>Neural Surface</td><td>fast surface reconstruction</td><td>∼ 10 mins</td></tr><tr><td>Neural Tracking</td><td>coarse tracking</td><td>∼ 57 ms</td></tr><tr><td></td><td>neural deformation</td><td>∼ 2 mins</td></tr><tr><td>Neural Rendering</td><td>2D texture generation</td><td>∼ 30s</td></tr><tr><td></td><td>sparse view generation</td><td>∼ 2s</td></tr><tr><td></td><td>neural texture blending</td><td>∼ 25ms</td></tr></tbody></table></div><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>对于每次表演，我们提供了80个预先校准和同步的RGB摄像机以2048 × 1536分辨率和25-30帧/秒捕获的视频序列，摄像机在表演者周围以4个不同纬度的圆圈均匀布置。我们还使用现成的背景抠图方法提供动态性能的前景分割</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><ul><li>渲染效果比较<ul><li>商业软件Agisoft PhotoScan</li><li>NeuralHumanFVV</li><li>基于神经图像的渲染方法IBRNet</li><li>动态神经辐射场ST-NeRF</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230706202348.png" alt="image.png"></p><ul><li>定量比较，我们采用峰值信噪比(PSNR)、结构相似指数(SSIM)、平均绝对误差(MAE)和学习感知图像斑块相似度(LPIPS)</li><li>几何比较</li></ul><h3 id="消融研究"><a href="#消融研究" class="headerlink" title="消融研究"></a>消融研究</h3><h3 id="沉浸体验"><a href="#沉浸体验" class="headerlink" title="沉浸体验"></a>沉浸体验</h3><h3 id="Limitations-and-Discussion"><a href="#Limitations-and-Discussion" class="headerlink" title="Limitations and Discussion"></a>Limitations and Discussion</h3><p>作为升级基于网格的工作流程的试验，我们的方法能够生成与现有后期制作工具兼容的动态人体资产，并为可流式沉浸式体验提供照片级真实人体建模和渲染。<br>尽管具备这些引人注目的功能，我们的流程仍然存在一些限制。在这里，我们提供详细分析并讨论潜在的未来扩展。</p><ul><li>首先，我们的方法依赖于基于图像的绿屏分割来分离前景表演，但无法提供视图一致的抠像结果。因此，我们的方法容易出现分割错误，仍然无法提供具有透明度细节的高质量头发渲染。受到最近的工作[Barron等人，2022年；Luo等人，2022年]的启发，我们计划将背景和透明度头发的显式建模引入我们的神经框架中。</li><li>此外，我们的渲染流程依赖于动画网格，因此受到跟踪质量的限制。即使使用虚拟视图的神经融合，我们的方法在处理极快速运动、严重的拓扑变化（如脱衣服）或复杂的人物-物体交互时仍然容易出现渲染伪影。这仍然是一个活跃研究领域中尚未解决的问题。如表1所示的运行时分析结果，我们的流程仅支持实时播放，仍无法实现实时即时建模和渲染以用于实时直播流媒体。瓶颈在于Instant-NSR的变形，尤其是隐式表面学习。为了加速，我们指出采用有限差分进行基于CUDA的法线估计，但这种近似的法线会降低表面细节。一个更忠实的CUDA实现需要数月的工程工作，以弄清楚MLP和哈希特征的二阶梯度的反向传播，而这在现有的CUDA库或代码库[Müller，2021年；Müller等人，2022年]中并没有得到充分支持。为了推动未来朝着这个方向的研究工作，我们将公开我们的PyTorch实现。</li></ul><p>对于管道设计，我们的方法分别促进了各种神经技术用于人体表演的重建、追踪和渲染，与最近的隐式NeRF类框架相比[Tretschk等人，2020；Zhang等人，2021]。进一步设计和传输紧凑的神经表示，并将其解码为几何、动作或外观，以支持各种下游应用，这是很有前景的。我们认为我们的方法是朝着这个最终目标迈出的坚实一步，通过中间利用基于动画网格的工作流程并将其推进到神经时代。此外，由于我们的方法能够高效地生成动态人体素材，我们计划扩大我们的穹顶级数据集，并探索生成可动画的数字人类[Xiang等人，2021]。将光线和材料明确分解到我们的方法中以实现可重新照明的人体建模也是很有趣的。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>我们提出了一种综合的神经建模和渲染方法，用于从密集的多视角RGB输入中重建、压缩和渲染高质量的人体表演。我们的核心思想是将传统的动画网格工作流提升到神经时代，采用一类高效的全新神经技术。我们的神经表面重建器Instant-NSR通过将基于TSDF的体积渲染与哈希编码相结合，能够在几分钟内高效生成高质量的表面。我们的混合追踪器以自监督的方式压缩几何信息并编码时间信息，生成动画网格，支持使用动态纹理或光照图进行各种常规基于网格的渲染。我们进一步提出了一种分层神经渲染方案，在质量和带宽之间取得了精细的平衡。我们展示了我们的方法在各种基于网格的应用和虚拟增强现实中以4D人体表演回放的形式的照片级沉浸式体验方面的能力。我们相信，我们的方法是从成熟的基于网格的工作流向神经人体建模和渲染的强有力过渡，它为忠实记录人类表演迈出了坚实的一步，具有众多在娱乐、游戏和虚拟现实/增强现实以及元宇宙中沉浸体验方面的潜在应用。</p><h2 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h2><h3 id="神经网络："><a href="#神经网络：" class="headerlink" title="神经网络："></a>神经网络：</h3><p>Instant-NSR由两个串联的MLP组成：</p><ul><li>一个具有2个隐藏层的SDF MLP $𝑚_{𝑠}$，用Softplus替换了原始的ReLU激活函数，并且对所有隐藏层的激活函数设置了𝛽=100，SDF MLP使用哈希编码函数[Müller等人，2022]将3D位置映射为32个输出值。</li><li>一个具有3个隐藏层的颜色MLP $𝑚_{𝑐}$</li><li>每个隐藏层宽度为64个神经元</li></ul><p>$m_{s}$训练SDF的网络表示为:$(x,F_{geo})=m_{s}(p,F_{hash}).$</p><ul><li>input<ul><li>每个三维采样点的3个输入空间位置值</li><li>来自哈希编码位置的32个输出值</li></ul></li><li>output<ul><li>sdf值，然后我们将截断的函数应用于输出SDF值，该值使用sigmoid激活将其映射到<code>[−1,1]</code>。</li><li>15维的$F_{geo}$值</li><li>共16个输出值</li></ul></li></ul><p>$m_{c}$训练颜色的网络表示为：$\hat{C}=m_{c}(\mathrm{p},\mathrm{n},\mathrm{v},F_{geo}).$</p><ul><li>input<ul><li>每个三维采样点的3个输入空间位置值</li><li>用有限差分函数估计SDF梯度的3个正态值</li><li>视角方向在球谐函数基础上分解为4阶及以下的前16个系数</li><li>SDF MLP的15维的输出值$F_{geo}$</li></ul></li><li>output<ul><li>RGB: 3</li></ul></li></ul><p>用sigmoid激活将输出的RGB颜色值映射到[0,1]范围</p><h4 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h4><p>我们在论文中证明了我们约10分钟的训练结果与原始NeuS的约8小时优化结果是可比较的[Wang et al. 2021a]。在优化阶段，我们假设感兴趣的区域最初位于单位球内。我们在PyTorch实现中采用了[Mildenhall et al. 2021]的分层采样策略，其中粗采样和细采样的数量分别为64和64。我们每批次采样4,096条光线，并使用单个NVIDIA RTX 3090 GPU进行为期6,000次迭代的模型训练，训练时间为12分钟。为了近似梯度以进行高效的法线计算，我们采用有限差分函数$𝑓 ′ (𝑥) = (𝑓 (𝑥 + Δ𝑥) − (𝑓 𝑥 − Δ𝑥))/2Δ𝑥$，如第3.2节所述。在我们的PyTorch实现中，我们将近似步长设置为Δ𝑥 = 0.005，并在训练结束时将其减小到Δ𝑥 = 0.0005。我们通过最小化Huber损失L𝑐𝑜𝑙𝑜𝑟和Eikonal损失L𝑒𝑖𝑘来优化我们的模型。这两个损失使用经验系数𝜆进行平衡，在我们的实验中将其设置为0.1。此外，我们选择Adam [Diederik P Kingma et al.2014]优化器，初始学习率为1𝑒 − 2，并在训练过程中将其降低到1.6𝑒 − 3。</p><h3 id="Neural-Tracking-Implementation-Details"><a href="#Neural-Tracking-Implementation-Details" class="headerlink" title="Neural Tracking Implementation Details"></a>Neural Tracking Implementation Details</h3><p>在第4节中，我们提出了一种神经跟踪流程，它将传统的非刚性跟踪和神经变形网络以一种由粗到精的方式结合在一起。我们通过高斯-牛顿方法解决非刚性跟踪问题，并在接下来的内容中介绍了详细信息。<br>跟踪细节: 在进行非刚性跟踪之前，我们通过计算规范网格上的测地距离来对ED节点进行采样。我们计算平均边长，并将其乘以一个半径比例，用于控制压缩程度，以获得影响半径𝑟。通过所有的实验，我们发现简单地调整为0.075也可以得到很好的结果。给定𝑟，我们按Y轴对顶点进行排序，并在距离𝑟之外时从现有ED节点集合中选择ED节点。此外，当ED节点影响相同的顶点时，我们可以将它们连接起来，然后提前构建ED图以进行后续优化。</p><p>$\mathrm{(c,\sigma)=\phi^{o}(p’+\phi^{d}(p’,t),d).}$<br>网络结构: 我们改进阶段的关键包括规范辐射场$𝜙^𝑜$和变形网络$𝜙^𝑑$。</p><ul><li><p>$𝜙^𝑜$具有与Instant-NGP相同的网络结构，包括三维哈希编码和两个串联的MLP: 密度和颜色。</p><ul><li>三维坐标通过哈希编码映射为64维特征，作为密度MLP的输入。然后，密度MLP具有2个隐藏层（每个隐藏层有64个隐藏维度），并输出1维密度和15维几何特征。</li><li>几何特征与方向编码连接在一起，并输入到具有3个隐藏层的颜色MLP中。最后，我们可以获得每个坐标点的密度值和RGB值。</li></ul></li><li><p>$𝜙^{𝑑}$包括四维哈希编码和单个MLP。</p><ul><li>四维哈希编码具有32个哈希表，将输入（p′，𝑡）映射到64维特征。通过我们的2个隐藏层变形MLP（每个隐藏层具有128个隐藏维度），最终可以得到Δp′。</li></ul></li></ul><h4 id="训练细节-1"><a href="#训练细节-1" class="headerlink" title="训练细节"></a>训练细节</h4><p>训练细节。我们分别训练$𝜙^𝑜$和 $𝜙^{𝑑}$ 。我们首先利用多视图图像来训练规范表示 $𝜙^𝑜$。当 PSNR 值稳定下来（通常在100个epoch之后），我们冻结 $𝜙^𝑜$ 的参数。然后，我们训练变形网络 $𝜙^{𝑑}$ 来预测每帧的变形位移。我们构建了一个PyTorch CUDA扩展库来实现快速训练。我们首先将规范帧中的ED节点转换到当前帧，然后构建一个KNN体素。具体而言，我们的体素分辨率是2563到5123，并且对于KNN体素中的每个体素，我们通过堆查询4到12个最近邻的ED节点。基于KNN体素，我们可以快速查询体素中的任何3D点，并获取邻居和对应的蒙皮权重，以通过非刚性跟踪计算坐标。</p><h3 id="Neural-Blending-Implementation-Details"><a href="#Neural-Blending-Implementation-Details" class="headerlink" title="Neural Blending Implementation Details"></a>Neural Blending Implementation Details</h3><p>U-Net:<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230706205127.png" alt="image.png"></p><h4 id="训练细节-2"><a href="#训练细节-2" class="headerlink" title="训练细节"></a>训练细节</h4><p>在训练过程中，我们引入了一个称为遮挡映射的新维度，它是两个变形深度图之间的差异计算得出的。然后，我们将遮挡映射（1维）和两个变形的RGBD通道（4维）作为网络输入，进一步帮助U-net网络优化混合权重。在传统的逐像素神经纹理混合过程中，混合结果仅从两个变形图像生成。然而，如果目标视图在相邻虚拟视图中都被遮挡，将导致严重的伪影问题。因此，我们使用纹理渲染结果作为额外输入，以恢复由于遮挡而丢失的部分。为了有效地渲染，我们首先将输入图像降采样为512×512作为网络输入，然后通过双线性插值上采样权重映射以生成最终的2K图像。为了避免混合网络过度拟合额外的纹理渲染输入，我们在训练过程中应用高斯模糊操作来模拟低分辨率的纹理渲染图像。这个操作有助于网络专注于选定的相邻视图的细节，同时从额外的纹理渲染输入中恢复缺失的部分。此外，我们选择Adam [Diederik P Kingma et al.2014]优化器，初始学习率为1e-4，权重衰减率为5e-5。我们在一台单独的NVIDIA RTX 3090 GPU上使用Twindom [web.twindom.com]数据集对神经纹理混合模型进行了两天的预训练。</p><h4 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h4><p>训练数据集。为了训练一个更具适应性的神经纹理融合网络，我们构建了一个大规模的合成多视角数据集。我们利用Twindom [web.twindom.com] 数据集中的预扫描模型生成多视角图像。具体而言，我们重新渲染预扫描模型以生成用于融合的六个固定视角，并在球体上采样180个虚拟目标视角来训练网络。为了增强我们网络的生成能力，我们通过给3D模型添加更具挑战性的姿势来扩大训练数据集。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>如<a href="Neus-Instant-nsr-pl.md">instant-nsr-pl</a>中环境配置tiny-cuda-nn</p><p>创建conda环境：</p><ul><li><code>conda create -n nsr python=3.8</code></li><li><code>conda activate nsr</code></li><li><code>pip install -r requirements.txt</code></li><li><code>pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch</code></li></ul><p>数据集：a test dataset <a href="https://drive.google.com/drive/folders/180qoFqABXjBDwW2hHa14A6bmV-Sj1qqJ?usp=sharing">dance</a></p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/385352354">Pytorch 指定GPU - 知乎 (zhihu.com)</a></p></blockquote><p><code>CUDA_VISIBLE_DEVICES=x python xxx.py</code></p><ul><li>类似于py文件中<code>os.environ[&#39;CUDA_VISIBLE_DEVICES&#39;]=&#39;x&#39;</code>，只不过它用起来更加方便。只能指定一张卡而且不能再程序运行中途换卡，适用于一次run，全程只需要占单卡的.py文件</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Instant-NSR Training</span><br><span class="line">CUDA_VISIBLE_DEVICES=$&#123;CUDA_DEVICE&#125; python train_nerf.py &quot;$&#123;INPUTS&#125;/dance&quot;  --workspace &quot;$&#123;WORKSPACE&#125;&quot; --downscale 1 --network sdf</span><br><span class="line"></span><br><span class="line">eg:</span><br><span class="line">python train_nerf.py &quot;inputs/pipaxing-singleframe&quot;  --workspace &quot;/root/tf-logs&quot; --downscale 1 --network sdf</span><br></pre></td></tr></table></figure><h3 id="提取网格"><a href="#提取网格" class="headerlink" title="提取网格"></a>提取网格</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Instant-NSR Mesh extraction</span><br><span class="line">CUDA_VISIBLE_DEVICES=$&#123;CUDA_DEVICE&#125; python train_nerf.py &quot;$&#123;INPUTS&#125;/dance&quot;  --workspace &quot;$&#123;WORKSPACE&#125;&quot; --downscale 1 --network sdf -mode mesh</span><br><span class="line"></span><br><span class="line">eg: </span><br><span class="line">python train_nerf.py &quot;inputs/pipaxing-singleframe&quot;  --workspace &quot;/root/tf-logs&quot; --downscale 1 --network sdf --mode mesh</span><br></pre></td></tr></table></figure><p>resolution: 1024</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230706153113.png" alt="image.png"></p><h3 id="生成特定的目标相机图片"><a href="#生成特定的目标相机图片" class="headerlink" title="生成特定的目标相机图片"></a>生成特定的目标相机图片</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Instant-NSR Rendering</span><br><span class="line">CUDA_VISIBLE_DEVICES=$&#123;CUDA_DEVICE&#125; python train_nerf.py &quot;$&#123;INPUTS&#125;/dance&quot;  --workspace &quot;$&#123;WORKSPACE&#125;&quot; --downscale 1 --network sdf -mode render</span><br><span class="line"></span><br><span class="line">eg:</span><br><span class="line">python train_nerf.py &quot;inputs/pipaxing-singleframe&quot;  --workspace &quot;/root/tf-logs&quot; --downscale 1 --network sdf --mode render</span><br></pre></td></tr></table></figure><div style="display:flex; "> <div> <img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/0006.png" style="width:73%"> <p style="text-align:center;">render_img</p> </div><div> <img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/0006_depth.png" style="width:73%"> <p style="text-align:center;">depth_img</p> </div> <div> <img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/0006_normal.png" style="width:73%"> <p style="text-align:center;">normal_img</p> </div> </div><h1 id="NeRF-Mine-基于InstantNSR在没有mask的情况下生成结果很差，放弃该项目"><a href="#NeRF-Mine-基于InstantNSR在没有mask的情况下生成结果很差，放弃该项目" class="headerlink" title="NeRF-Mine: 基于InstantNSR在没有mask的情况下生成结果很差，放弃该项目"></a><del>NeRF-Mine: 基于InstantNSR在没有mask的情况下生成结果很差，放弃该项目</del></h1><p>方便在本地和服务器之间拷贝</p><p>基于<a href="https://github.com/zhaofuq/Instant-NSR">instant-NSR</a> = <a href="https://github.com/Totoro97/NeuS">Neus</a>+ <a href="https://github.com/NVlabs/instant-ngp">InstantNGP</a></p><h1 id="环境配置-1"><a href="#环境配置-1" class="headerlink" title="环境配置"></a>环境配置</h1><p>选择RTX3090单卡，镜像配置：</p><ul><li>PyTorch  1.10.0</li><li>Python  3.8(ubuntu20.04)</li><li>Cuda  11.3</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">source /etc/network_turbo</span><br><span class="line"></span><br><span class="line">git clone https://github.com/yq010105/NeRF-Mine.git</span><br><span class="line"></span><br><span class="line">cd NeRF-Mine</span><br></pre></td></tr></table></figure><ul><li><code>conda create -n nsr python=3.8</code></li><li><code>conda activate nsr</code></li><li><code>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple</code></li><li>可选<code>pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch</code></li></ul><h2 id="error"><a href="#error" class="headerlink" title="error"></a>error</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">Failed to build pysdf  </span><br><span class="line">ERROR: Could not build wheels for pysdf, which is required to install pyproject.toml-based projects</span><br><span class="line"></span><br><span class="line">先取消pysdf的安装，安装完其他后再进行pysdf的安装</span><br><span class="line"></span><br><span class="line">2</span><br><span class="line">加载cpp扩展时，锁住：</span><br><span class="line">File &quot;run.py&quot;, line 6, in &lt;module&gt;  </span><br><span class="line">from models.network_sdf import NeRFNetwork  </span><br><span class="line">File &quot;/root/autodl-tmp/NeRF-Mine/models/network_sdf.py&quot;, line 8, in &lt;module&gt;  </span><br><span class="line">from encoder.encoding import get_encoder  </span><br><span class="line">File &quot;/root/autodl-tmp/NeRF-Mine/encoder/encoding.py&quot;, line 8, in &lt;module&gt;  </span><br><span class="line">from encoder.shencoder import SHEncoder  </span><br><span class="line">File &quot;/root/autodl-tmp/NeRF-Mine/encoder/shencoder/__init__.py&quot;, line 1, in &lt;module&gt;  </span><br><span class="line">from .sphere_harmonics import SHEncoder  </span><br><span class="line">File &quot;/root/autodl-tmp/NeRF-Mine/encoder/shencoder/sphere_harmonics.py&quot;, line 9, in &lt;module&gt;  </span><br><span class="line">from .backend import _backend  </span><br><span class="line">File &quot;/root/autodl-tmp/NeRF-Mine/encoder/shencoder/backend.py&quot;, line 6, in &lt;module&gt;  </span><br><span class="line">_backend = load(name=&#x27;_sh_encoder&#x27;,  </span><br><span class="line">File &quot;/root/miniconda3/envs/nsr/lib/python3.8/site-packages/torch/utils/cpp_extension.py&quot;, line 1284, in load  </span><br><span class="line">return _jit_compile(  </span><br><span class="line">File &quot;/root/miniconda3/envs/nsr/lib/python3.8/site-packages/torch/utils/cpp_extension.py&quot;, line 1523, in _jit_compile  </span><br><span class="line">baton.wait()  </span><br><span class="line">File &quot;/root/miniconda3/envs/nsr/lib/python3.8/site-packages/torch/utils/file_baton.py&quot;, line 42, in wait  </span><br><span class="line">time.sleep(self.wait_seconds)  </span><br><span class="line">KeyboardInterrupt</span><br><span class="line"></span><br><span class="line">原因：</span><br><span class="line">当加载hash编码后，快速加载了shen编码，导致cpp_extension.py依然被占用，因此，只需加一个延时，让两个库导入时后者慢一步，解除程序的占用</span><br><span class="line">方法：</span><br><span class="line">删除/root/.cache/torch_extensions/py38_cu117/下文件_hash_encoder</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">from encoder.hashencoder import HashEncoder</span><br><span class="line">之间添加延时 time.sleep(10)</span><br><span class="line">from encoder.shencoder import SHEncoder</span><br><span class="line">Note!延时在运行过第一次后可以注释掉</span><br></pre></td></tr></table></figure><blockquote><p>error2:<a href="https://blog.csdn.net/qq_38677322/article/details/109696077">(21条消息) torch.utils.cpp_extension.load卡住无响应_zParquet的博客-CSDN博客</a></p></blockquote><h1 id="在示例数据集上训练"><a href="#在示例数据集上训练" class="headerlink" title="在示例数据集上训练"></a>在示例数据集上训练</h1><p>下载InstantNSR提供的示例数据集：a test dataset <a href="https://drive.google.com/drive/folders/180qoFqABXjBDwW2hHa14A6bmV-Sj1qqJ?usp=sharing">dance</a>，放入inputs目录下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 开始训练</span><br><span class="line">python run.py --conf confs/dtu.conf --downscale 1 --network sdf</span><br><span class="line">or</span><br><span class="line">python run.py --conf confs/dtu.conf</span><br><span class="line"></span><br><span class="line"># 提取网格mesh</span><br><span class="line">python run.py --downscale 1 --network sdf --mode mesh</span><br><span class="line"></span><br><span class="line"># 生成特定的目标相机图片</span><br><span class="line">python run.py --downscale 1 --network sdf --mode render</span><br></pre></td></tr></table></figure><p>训练结果不理想：猜想是由于没有去除后面的背景，可以说nsr是一个依赖mask的方法</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230710211129.png" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Efficiency </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
            <tag> Efficiency </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>InstantNGP环境配置和tiny-cuda-nn用法</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Efficiency/NeRF-InstantNGP-code/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Efficiency/NeRF-InstantNGP-code/</url>
      
        <content type="html"><![CDATA[<p>tiny-cuda-nn在python中的用法:<a href="https://github.com/nvlabs/tiny-cuda-nn#pytorch-extension">NVlabs/tiny-cuda-nn: Lightning fast C++/CUDA neural network framework (github.com)</a></p><p>InstantNGP环境配置和使用，由于需要使用GUI，且笔记本GPU配置太低，因此没有具体训练的过程，只是进行了环境的配置。</p><span id="more"></span><h1 id="Tiny-cuda-nn"><a href="#Tiny-cuda-nn" class="headerlink" title="Tiny-cuda-nn"></a>Tiny-cuda-nn</h1><blockquote><p><a href="https://github.com/nvlabs/tiny-cuda-nn/blob/master/samples/mlp_learning_an_image_pytorch.py">tiny-cuda-nn/samples/mlp_learning_an_image_pytorch.py at master · NVlabs/tiny-cuda-nn · GitHub</a></p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">model = tcnn.NetworkWithInputEncoding(n_input_dims=2, </span><br><span class="line">                        n_output_dims=n_channels, </span><br><span class="line">                        encoding_config=config[&quot;encoding&quot;], </span><br><span class="line">                        network_config=config[&quot;network&quot;]).to(device)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># encoding_config = </span><br><span class="line">&quot;encoding&quot;: &#123;</span><br><span class="line">    &quot;otype&quot;: &quot;HashGrid&quot;,</span><br><span class="line">    &quot;n_levels&quot;: 16,</span><br><span class="line">    &quot;n_features_per_level&quot;: 2,</span><br><span class="line">    &quot;log2_hashmap_size&quot;: 15,</span><br><span class="line">    &quot;base_resolution&quot;: 16,</span><br><span class="line">    &quot;per_level_scale&quot;: 1.5</span><br><span class="line">&#125;,</span><br><span class="line"># network_config = </span><br><span class="line">&quot;network&quot;: &#123;</span><br><span class="line">    &quot;otype&quot;: &quot;FullyFusedMLP&quot;,</span><br><span class="line">    &quot;activation&quot;: &quot;ReLU&quot;,</span><br><span class="line">    &quot;output_activation&quot;: &quot;None&quot;,</span><br><span class="line">    &quot;n_neurons&quot;: 64,</span><br><span class="line">    &quot;n_hidden_layers&quot;: 2</span><br><span class="line">&#125;</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure><p>encoding: <a href="https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/data/readme/multiresolution-hash-encoding-diagram.png">multiresolution hash encoding</a> (<a href="https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf">technical paper</a>)</p><ul><li>n_levels: 多分辨率个数L=16</li><li>n_features_per_level: 特征向量的维度F=2</li><li>log2_hashmap_size: log2每个分辨率下特征向量个数$log_{2}T=15$</li><li>base_resolution: $N_{min} = 16$</li><li>per_level_scale: 每个分辨率下的scale=1.5？？？</li></ul><p>network: a lightning fast <a href="https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/data/readme/fully-fused-mlp-diagram.png">“fully fused” multi-layer perceptron</a> (<a href="https://tom94.net/data/publications/mueller21realtime/mueller21realtime.pdf">technical paper</a>)</p><ul><li>activation: 激活函数 “ReLU”</li><li>output_activation: 输出层激活函数无</li><li>n_neurons: 64</li><li>n_hidden_layers: 隐藏层数2</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">image =  Image(args.image, device) # model</span><br><span class="line"></span><br><span class="line">model = tcnn.NetworkWithInputEncoding(n_input_dims=2, </span><br><span class="line">                                                                    n_output_dims=n_channels, </span><br><span class="line">                                                                    encoding_config=config[&quot;encoding&quot;], </span><br><span class="line">                                                                    network_config=config[&quot;network&quot;]).to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=0.01)</span><br><span class="line"></span><br><span class="line">batch_size = 2**18</span><br><span class="line">interval = 10</span><br><span class="line"></span><br><span class="line">print(f&quot;Beginning optimization with &#123;args.n_steps&#125; training steps.&quot;)</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    batch = torch.rand([batch_size, 2], device=device, dtype=torch.float32)</span><br><span class="line">    traced_image = torch.jit.trace(image, batch) </span><br><span class="line">    # 对 `image` 进行跟踪，记录其在给定输入数据上的执行过程，并生成一个跟踪模型。</span><br><span class="line">    # 生成的跟踪模型可以被保存、加载和执行，而且通常具有比原始模型更高的执行效率。</span><br><span class="line">    # 只能跟踪具有固定输入形状的模型或函数</span><br><span class="line">except:</span><br><span class="line">    # If tracing causes an error, fall back to regular execution</span><br><span class="line">    print(f&quot;WARNING: PyTorch JIT trace failed. Performance will be slightly worse than regular.&quot;)</span><br><span class="line">    traced_image = image</span><br><span class="line"></span><br><span class="line">for i in range(args.n_steps):</span><br><span class="line">    batch = torch.rand([batch_size, 2], device=device, dtype=torch.float32)</span><br><span class="line">    targets = traced_image(batch)</span><br><span class="line">    output = model(batch)</span><br><span class="line"></span><br><span class="line">    relative_l2_error = (output - targets.to(output.dtype))**2 / (output.detach()**2 + 0.01)</span><br><span class="line">    loss = relative_l2_error.mean()</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">tcnn.free_temporary_memory()</span><br></pre></td></tr></table></figure><h1 id="NGP环境配置及运行"><a href="#NGP环境配置及运行" class="headerlink" title="NGP环境配置及运行"></a>NGP环境配置及运行</h1><p>配置前下载：</p><ul><li>clone repository <ul><li><code>git clone https://github.com/NVlabs/instant-ngp.git</code></li></ul></li><li>download instant-ngp.exe<ul><li><a href="https://github.com/NVlabs/instant-ngp/releases/tag/continuous">Release Development release · NVlabs/instant-ngp (github.com)</a></li></ul></li></ul><p>配置环境:</p><blockquote><p><a href="https://www.youtube.com/watch?v=3TWxO1PftMc">Updated: Making a NeRF animation with NVIDIA’s Instant NGP - YouTube</a></p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda create -n ngp python=3.10 </span><br><span class="line">conda activate ngp </span><br><span class="line">cd C:\Users\ehaines\Documents\_documents\Github\instant-ngp </span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Set environment in Anaconda</span><br><span class="line">conda activate ngp</span><br><span class="line"></span><br><span class="line"># Pull images from movie; I&#x27;ve put movie directory &quot;chesterwood&quot; in the instant-ngp directory for simplicity. Change &quot;fps 2&quot; to whatever is needed to give you around 100 images.</span><br><span class="line">cd C:\Users\(your path here)\Github\instant-ngp</span><br><span class="line">cd chesterwood</span><br><span class="line">python ..\scripts\colmap2nerf.py --video_in IMG_9471.MOV --video_fps 2 --run_colmap --overwrite</span><br><span class="line"># NOTE! This line is a bit different than shown in the video, as advice on aabb_scale&#x27;s use has changed. Also, I usually want to delete a few images after extracting them, so I don&#x27;t do an exhaustive match at this point. In fact, I usually hit break (Control-C) when I see &quot;Feature extraction&quot; starting, as the images have all been extracted at that point.</span><br><span class="line"></span><br><span class="line">#After you delete any blurry or useless frames, continue below to match cameras.</span><br><span class="line"></span><br><span class="line"># Camera match given set of images. Do for any set of images. Run from directory containing your &quot;images&quot; directory.</span><br><span class="line">python C:\Users\(your path here)\Github\instant-ngp\scripts\colmap2nerf.py --colmap_matcher exhaustive --run_colmap --aabb_scale 16 --overwrite</span><br><span class="line"># For videos or closely related sets of shots, you can take out the &quot;--colmap_matcher exhaustive&quot; from the line above, since your images are in order. This saves a few minutes. You could also leave off &quot;--aabb_scale 16&quot; or put 64, the new default; the docs say it is worth playing with this number, see nerf_dataset_tips.md for how (short version: edit it in transforms.json). In my limited testing, I personally have not seen a difference.</span><br><span class="line"></span><br><span class="line"># run interactive instant-ngp - run from the main directory &quot;instant-ngp&quot;</span><br><span class="line">cd ..</span><br><span class="line">instant-ngp chesterwood</span><br></pre></td></tr></table></figure><p>GPU配置太低，无法运行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">出现错误：CUDA_ERROR_OUT_OF_MEMORY</span><br><span class="line">Uncaught exception: D:/a/instant-ngp/instant-ngp/dependencies/tiny-cuda- nn/include\tiny-cuda-nn/gpu_memory.h:590 cuMemSetAccess(m_base_address + m_size, n_bytes_to_allocate, &amp;access_desc, 1) failed with error CUDA_ERROR_OUT_OF_MEMORY</span><br><span class="line"></span><br><span class="line">原因：GPU硬件配置太低1050Ti and only 4GB of VRAM</span><br><span class="line">nvidia-smi  </span><br><span class="line">Tue Jul 4 14:47:36 2023  </span><br><span class="line">+---------------------------------------------------------------------------------------+  </span><br><span class="line">| NVIDIA-SMI 531.79 Driver Version: 531.79 CUDA Version: 12.1 |  </span><br><span class="line">|-----------------------------------------+----------------------+----------------------+  </span><br><span class="line">| GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC |  </span><br><span class="line">| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |  </span><br><span class="line">| | | MIG M. |  </span><br><span class="line">|=========================================+======================+======================|  </span><br><span class="line">| 0 NVIDIA GeForce GTX 1050 Ti WDDM | 00000000:01:00.0 Off | N/A |  </span><br><span class="line">| N/A 42C P8 N/A / N/A| 478MiB / 4096MiB | 2% Default |  </span><br><span class="line">| | | N/A |  </span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Efficiency </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> InstantNGP </tag>
            
            <tag> Code </tag>
            
            <tag> Efficiency </tag>
            
            <tag> Encoding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Instant-nsr-pl的代码理解</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/Neus-Instant-nsr-pl-code/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/Neus-Instant-nsr-pl-code/</url>
      
        <content type="html"><![CDATA[<p>Instant Neus的代码理解<br>参考了：</p><ul><li><a href="https://github.com/kwea123/ngp_pl">ngp_pl</a>: Great Instant-NGP implementation in PyTorch-Lightning! Background model and GUI supported.<ul><li><a href="https://lightning.ai/docs/pytorch/1.9.5/">Welcome to ⚡ PyTorch Lightning — PyTorch Lightning 1.9.5 documentation</a></li></ul></li><li><a href="https://github.com/zhaofuq/Instant-NSR">Instant-NSR</a>: NeuS implementation using multiresolution hash encoding.</li></ul><span id="more"></span><iframe frameborder="0" style="width:100%;height:593px;" src="https://viewer.diagrams.net/?highlight=0000ff&edit=_blank&layers=1&nav=1&title=neus.drawio#R7Ztbj6M2FIB%2FTaTZhx2BMSR53Emm3YdWXWlUtfMUecEBdwhGxkyS%2FvraxA4QkyxNNzHZ5mE0%2BIC5nMN3fC5k5M1Wm58ZypNfaYTTEXCizcibjwAAkzEQ%2F6Rku5O4AQx2kpiRSMlqwQv5Gyuho6QliXDROpBTmnKSt4UhzTIc8pYMMUbX7cOWNG1fNUcxNgQvIUpN6R8k4slOOgHjWv4ZkzjRV3aD6W7PCumD1ZMUCYrouiHynkfejFHKd1urzQynUntaL7t5Px3Zu78xhjPeZ4Lz%2B5f5b1%2FYxzmcv7LPPA3nDH301b3xrX5gHInnV0PKeEJjmqH0uZY%2BMVpmEZZndcSoPuYXSnMhdIXwL8z5VhkTlZwKUcJXqdqLN4T%2FKac%2F%2Bmr02tgz36gzV4OtHmScbRuT5PC1ua%2BeVo30vCXNuLoRNxDj3fPKhzyqRiUqaMlCfEJ3%2BnVELMb8xHFgb2yBCaYrLO5PzGM4RZy8t%2B8Dqdc13h9XW1RsKKP%2BCwOr876jtFRX4gyRDLPHJeGG8WvTSmusE8LxS44qLawF4G0zdqv2HTOON6eVaypDT4DqhVTuwlfsrGv0XEfJkgZ20LmQ%2BoI7H2fzAXry4dnkAxh8iJVkSeKS4QXNOVkJ3bDi4cMJVBw7qOgVUq%2BsoIOV8TVZmd5ZOZsVrycrY5useAYrNFtUy8kC5zRMFoW4%2F1OLyjBI2ceW1kiBhiKXlMlXSWpRTs70oDC0KfTA2yorOKNveEZTcQ5vntFMgrUkaXogQimJMzEMhd6wkD9JrRIR6n5SO1Ykiioqu2zUtuIFzAQO%2FNm0w0p%2Bh5XApazkgrtDO9uhjXs6NNfpfimu49HGxz3aV8RvxqN5jm2PNjni0Sotao%2B2U2yEOEopioQL%2BvF9m15yxh0GCq7qzLy7Mzs%2F03f6erMjr8GVcn3HoLDA6fJRElfgG3BjgW035t7rYf%2BBkr4FMRdapcQsiVWU5AznjIa4KKolavC0QOsJvzu503I%2BLbAvLVZzftfMVctc8IEXKxqVKRYRMs6Hz8rEOiuBoUdDa0WCcrkZlizdPjEUvskX41vqa9fsL6DMSbsm37VKgw5V7uPb71%2BzhXe%2Fc77f6Z2ZT636HTM1r9JHksWVy7mBirwPbTsdz2rS5zYwqaH5FigtTGpqLIAy7QkKsFrC0rfZrGHpvtVC%2FNFFzFB0A7gE1otYntV15cZxAX1rJMdeiyv1e80ayVcRaK0RGyIj%2B8LucBgZUIXE6clIO%2FZyLTLS%2B5Mh3yojZoWkXlIGGnzB6dBI0b3KOynnkOL3JAVazVL0bbYbiGJEInF1OuzvIvZFEZ2veB3ITK%2BJDDRrIndkeiPTN7H3rBYUQUfPvRRwONW202CnWmlUA3m2a8eTaDPApcc%2FKJB1NHv3zFyFI2DmhI12vNSi0OjooDuPs3KFGeL4QUxrdOkfPpgq%2F%2FEa9TrgDto%2BEXZ9heR12PJijXs4oMb9zflEr7dPtBpGQKtJ1VmFB7dpYauFh942BnY%2FnjXXPelnxdpXPKI8x1n0ILYHuLodRImd35pdN0oc35xHHBAtfavadmExAxgjqZLEaIIGiM1BUAg6kqvrBoXQ%2FEaTLCulyQZ%2BFX7rCHwRJjh8WxAZoonh%2Fyf8C%2FyDlHjsWw7%2FfNfQ%2FtCd3XDCP9i7hWfV20HT27WrSEP%2BFv0wPoBdXe%2FvFB%2BIYf176Wpf42fn3vM%2F"></iframe><p>使用了PyTorch Lightning库</p><h1 id="文件结构："><a href="#文件结构：" class="headerlink" title="文件结构："></a>文件结构：</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">├───configs  # 配置文件</span><br><span class="line">│ nerf-blender.yaml  </span><br><span class="line">│ nerf-colmap.yaml  </span><br><span class="line">│ neus-blender.yaml  </span><br><span class="line">│ neus-bmvs.yaml  </span><br><span class="line">│ neus-colmap.yaml  </span><br><span class="line">│ neus-dtu.yaml  </span><br><span class="line">│  </span><br><span class="line">├───datasets  # 数据集加载</span><br><span class="line">│ blender.py  </span><br><span class="line">│ colmap.py  </span><br><span class="line">│  colmap_utils.py  </span><br><span class="line">│  dtu.py  </span><br><span class="line">│  utils.py  </span><br><span class="line">│  __init__.py  </span><br><span class="line">│  </span><br><span class="line">├───models  # model的神经网络结构和model的运算</span><br><span class="line">│  base.py  </span><br><span class="line">│  geometry.py  </span><br><span class="line">│  nerf.py  </span><br><span class="line">│  network_utils.py  </span><br><span class="line">│  neus.py  </span><br><span class="line">│  ray_utils.py  </span><br><span class="line">│  texture.py  </span><br><span class="line">│  utils.py  </span><br><span class="line">│  __init__.py  </span><br><span class="line">│  </span><br><span class="line">├───scripts  # 自定义数据集时gen_poses+run_colmap生成三个bin文件[&#x27;cameras&#x27;, &#x27;images&#x27;, &#x27;points3D&#x27;]</span><br><span class="line">│ imgs2poses.py  </span><br><span class="line">│  </span><br><span class="line">├───systems  # model模型加载和训练时每步的操作</span><br><span class="line">│  base.py  </span><br><span class="line">│  criterions.py  </span><br><span class="line">│  nerf.py  </span><br><span class="line">│  neus.py  </span><br><span class="line">│  utils.py  </span><br><span class="line">│  __init__.py  </span><br><span class="line">│  </span><br><span class="line">└───utils  </span><br><span class="line">│ callbacks.py  </span><br><span class="line">│ loggers.py  </span><br><span class="line">│ misc.py  </span><br><span class="line">│ mixins.py  </span><br><span class="line">│ obj.py  </span><br><span class="line">│ __init__.py  </span><br></pre></td></tr></table></figure><h1 id="fit流程伪代码"><a href="#fit流程伪代码" class="headerlink" title="fit流程伪代码"></a>fit流程伪代码</h1><p>train.fit()结构的伪代码</p><blockquote><p><a href="https://lightning.ai/docs/pytorch/1.9.5/common/lightning_module.html#hooks">LightningModule — PyTorch Lightning 1.9.5 documentation</a></p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">def fit(self):</span><br><span class="line">    if global_rank == 0:</span><br><span class="line">        # prepare data is called on GLOBAL_ZERO only</span><br><span class="line">        prepare_data()</span><br><span class="line"></span><br><span class="line">    configure_callbacks()</span><br><span class="line"></span><br><span class="line">    with parallel(devices):</span><br><span class="line">        # devices can be GPUs, TPUs, ...</span><br><span class="line">        train_on_device(model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def train_on_device(model):</span><br><span class="line">    # called PER DEVICE</span><br><span class="line">    setup(&quot;fit&quot;)</span><br><span class="line">    configure_optimizers()</span><br><span class="line">    on_fit_start()</span><br><span class="line"></span><br><span class="line">    # the sanity check runs here</span><br><span class="line"></span><br><span class="line">    on_train_start()</span><br><span class="line">    for epoch in epochs:</span><br><span class="line">        fit_loop()</span><br><span class="line">    on_train_end()</span><br><span class="line"></span><br><span class="line">    on_fit_end()</span><br><span class="line">    teardown(&quot;fit&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fit_loop():</span><br><span class="line">    on_train_epoch_start()</span><br><span class="line"></span><br><span class="line">    for batch in train_dataloader():</span><br><span class="line">        on_train_batch_start()</span><br><span class="line"></span><br><span class="line">        on_before_batch_transfer()</span><br><span class="line">        transfer_batch_to_device()</span><br><span class="line">        on_after_batch_transfer()</span><br><span class="line"></span><br><span class="line">        training_step()</span><br><span class="line"></span><br><span class="line">        on_before_zero_grad()</span><br><span class="line">        optimizer_zero_grad()</span><br><span class="line"></span><br><span class="line">        on_before_backward()</span><br><span class="line">        backward()</span><br><span class="line">        on_after_backward()</span><br><span class="line"></span><br><span class="line">        on_before_optimizer_step()</span><br><span class="line">        configure_gradient_clipping()</span><br><span class="line">        optimizer_step()</span><br><span class="line"></span><br><span class="line">        on_train_batch_end()</span><br><span class="line"></span><br><span class="line">        if should_check_val:</span><br><span class="line">            val_loop()</span><br><span class="line">    # end training epoch</span><br><span class="line">    training_epoch_end()</span><br><span class="line"></span><br><span class="line">    on_train_epoch_end()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def val_loop():</span><br><span class="line">    on_validation_model_eval()  # calls `model.eval()`</span><br><span class="line">    torch.set_grad_enabled(False)</span><br><span class="line"></span><br><span class="line">    on_validation_start()</span><br><span class="line">    on_validation_epoch_start()</span><br><span class="line"></span><br><span class="line">    val_outs = []</span><br><span class="line">    for batch_idx, batch in enumerate(val_dataloader()):</span><br><span class="line">        on_validation_batch_start(batch, batch_idx)</span><br><span class="line"></span><br><span class="line">        batch = on_before_batch_transfer(batch)</span><br><span class="line">        batch = transfer_batch_to_device(batch)</span><br><span class="line">        batch = on_after_batch_transfer(batch)</span><br><span class="line"></span><br><span class="line">        out = validation_step(batch, batch_idx)</span><br><span class="line"></span><br><span class="line">        on_validation_batch_end(batch, batch_idx)</span><br><span class="line">        val_outs.append(out)</span><br><span class="line"></span><br><span class="line">    validation_epoch_end(val_outs)</span><br><span class="line"></span><br><span class="line">    on_validation_epoch_end()</span><br><span class="line">    on_validation_end()</span><br><span class="line"></span><br><span class="line">    # set up for train</span><br><span class="line">    on_validation_model_train()  # calls `model.train()`</span><br><span class="line">    torch.set_grad_enabled(True)</span><br></pre></td></tr></table></figure><h1 id="datasets"><a href="#datasets" class="headerlink" title="datasets"></a>datasets</h1><h2 id="init"><a href="#init" class="headerlink" title="init"></a>init</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">datasets = &#123;&#125;</span><br><span class="line"></span><br><span class="line">def register(name):</span><br><span class="line">    def decorator(cls):</span><br><span class="line">        datasets[name] = cls</span><br><span class="line">        return cls</span><br><span class="line">    return decorator</span><br><span class="line"></span><br><span class="line">def make(name, config): # dtu ,config.datasets</span><br><span class="line">    dataset = datasets[name](config) # dataset = datasets[&#x27;dtu&#x27;](config)</span><br><span class="line">    return dataset</span><br><span class="line"></span><br><span class="line">from . import blender, colmap, dtu</span><br></pre></td></tr></table></figure><h2 id="dtu"><a href="#dtu" class="headerlink" title="dtu"></a>dtu</h2><p>数据集加载操作，返回一个DataLoader</p><ul><li>load_K_Rt_from_P，从P矩阵中恢复K、R和T</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_K_Rt_from_P</span>(<span class="params">P=<span class="literal">None</span></span>):</span><br><span class="line">    out = cv2.decomposeProjectionMatrix(P)</span><br><span class="line">    K = out[<span class="number">0</span>]</span><br><span class="line">    R = out[<span class="number">1</span>]</span><br><span class="line">    t = out[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    K = K / K[<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">    intrinsics = np.eye(<span class="number">4</span>)</span><br><span class="line">    intrinsics[:<span class="number">3</span>, :<span class="number">3</span>] = K</span><br><span class="line"></span><br><span class="line">    pose = np.eye(<span class="number">4</span>, dtype=np.float32)</span><br><span class="line">    pose[:<span class="number">3</span>, :<span class="number">3</span>] = R.transpose()</span><br><span class="line">    pose[:<span class="number">3</span>, <span class="number">3</span>] = (t[:<span class="number">3</span>] / t[<span class="number">3</span>])[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> intrinsics, pose</span><br></pre></td></tr></table></figure><ul><li>create_spheric_poses，创建一个球形的相机位姿，用于测试时的位姿和输出视频</li></ul><p>ref: torch.cross: <a href="https://zh.wikipedia.org/wiki/%E5%8F%89%E7%A7%AF#%E7%9F%A9%E9%98%B5%E8%A1%A8%E7%A4%BA">叉积 - 维基百科，自由的百科全书 (wikipedia.org)</a></p><div class="note warning">            <p>理解特征值和特征向量是什么？torch.linalg.eig</p>          </div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成球形相机姿态, 测试使用</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_spheric_poses</span>(<span class="params">cameras, n_steps=<span class="number">120</span></span>): <span class="comment"># camears: (n_images,3) , n_steps: 60</span></span><br><span class="line">    center = torch.as_tensor([<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">0.</span>], dtype=cameras.dtype, device=cameras.device)</span><br><span class="line">    cam_center = F.normalize(cameras.mean(<span class="number">0</span>), p=<span class="number">2</span>, dim=-<span class="number">1</span>) * cameras.mean(<span class="number">0</span>).norm(<span class="number">2</span>) <span class="comment"># normalize: 将输入张量归一化为单位范数</span></span><br><span class="line">    <span class="comment"># cam_center: torch.Size([3])</span></span><br><span class="line">    eigvecs = torch.linalg.eig(cameras.T @ cameras).eigenvectors  <span class="comment"># eig: 计算方阵的特征值和特征向量, .eigenvectors: 返回特征向量</span></span><br><span class="line">    <span class="comment"># eigvecs: torch.Size([3, 3])</span></span><br><span class="line">    rot_axis = F.normalize(eigvecs[:,<span class="number">1</span>].real.<span class="built_in">float</span>(), p=<span class="number">2</span>, dim=-<span class="number">1</span>) <span class="comment"># torch.Size([3]) 中间一列绕y轴旋转</span></span><br><span class="line">    up = rot_axis   <span class="comment"># torch.Size([3])</span></span><br><span class="line">    rot_dir = torch.cross(rot_axis, cam_center) <span class="comment"># cross: 计算两个向量的叉积, (3,)x(3,)=(3,)</span></span><br><span class="line">    max_angle = (F.normalize(cameras, p=<span class="number">2</span>, dim=-<span class="number">1</span>) * F.normalize(cam_center, p=<span class="number">2</span>, dim=-<span class="number">1</span>)).<span class="built_in">sum</span>(-<span class="number">1</span>).acos().<span class="built_in">max</span>()</span><br><span class="line">    <span class="comment"># max_angle: torch.Size([]) ,一个标量</span></span><br><span class="line">    <span class="comment"># 相机位置每个点与相机中心的夹角</span></span><br><span class="line">    all_c2w = []</span><br><span class="line">    <span class="keyword">for</span> theta <span class="keyword">in</span> torch.linspace(-max_angle, max_angle, n_steps):</span><br><span class="line">        cam_pos = cam_center * math.cos(theta) + rot_dir * math.sin(theta) <span class="comment"># torch.Size([3])</span></span><br><span class="line">        l = F.normalize(center - cam_pos, p=<span class="number">2</span>, dim=<span class="number">0</span>) <span class="comment"># torch.Size([3])</span></span><br><span class="line">        s = F.normalize(l.cross(up), p=<span class="number">2</span>, dim=<span class="number">0</span>)    <span class="comment"># torch.Size([3])</span></span><br><span class="line">        u = F.normalize(s.cross(l), p=<span class="number">2</span>, dim=<span class="number">0</span>)   <span class="comment"># torch.Size([3])</span></span><br><span class="line">        c2w = torch.cat([torch.stack([s, u, -l], dim=<span class="number">1</span>), cam_pos[:,<span class="literal">None</span>]], axis=<span class="number">1</span>) <span class="comment"># (3,4)</span></span><br><span class="line">        all_c2w.append(c2w) <span class="comment"># (n_steps, 3, 4)</span></span><br><span class="line"></span><br><span class="line">    all_c2w = torch.stack(all_c2w, dim=<span class="number">0</span>)   <span class="comment"># (n_steps, 3, 4)</span></span><br></pre></td></tr></table></figure><ul><li>DTUDatasetBase, <code>setup(self,config,split)</code><ul><li><strong>load</strong> cameras_file and <code>imread(/root_dir/image/*.png)</code> <ul><li><code>cv2.imread : shape[0] is H , shape[1] is W</code> is different frome PIL image</li></ul></li><li>img_downscale or img_wh <strong>to downscale</strong></li><li>n_images(数据集图片数量) <code>max([int(k.split(&#39;_&#39;)[-1]) for k in cams.keys()]) + 1</code></li><li>for i in range(n_images): <ul><li><code>P = (world_mat @ scale_mat)[:3,:4]</code></li><li><code>K, c2w = load_K_Rt_from_P(P)</code></li><li><code>fx, fy, cx, cy = K[0,0] * self.factor, K[1,1] * self.factor, K[0,2] * self.factor, K[1,2] * self.factor</code><ul><li><code>self.factor = w / W</code></li></ul></li><li><code>directions = get_ray_directions(w, h, fx, fy, cx, cy)</code> 得到的光线方向i.e. rays_d为类似NeRF的计算方式，y与j反向，z远离物体的方向<ul><li>self.directions append directions</li></ul></li><li>c2w  to tensor float and for c2w, flip the sign of input camera coordinate yz<ul><li>c2w_ = c2w.clone</li><li><code>c2w_[:3,1:3] *= -1.</code> because Neus DTU data is different from blender or blender</li><li>all_c2w append c2w_</li></ul></li><li>if train or val<ul><li>open i:06d.png image (PIL image) (size : w,h)</li><li>resize to w,h by Image.BICUBIC</li><li>TF(torchvision.transforms.functional).to_tensor() : CHW<ul><li><code>CHW.permute(1, 2, 0)[...,:3]</code> : HWC</li></ul></li><li>open mask and covert(‘L’) , resize , to_tensor</li><li>all_fg_mask append mask</li><li>all_images append img</li></ul></li></ul></li><li>all_c2w : stack all_c2w</li><li>if test<ul><li>all_c2w = 创建一个球形相机位姿create_spheric_poses</li><li>all_images = zeros(n_test_traj_steps,h,w,3) dtype = torch.float32</li><li>all_fg_masks = zeros(n_test_traj_steps,h,w) dtype = torch.float32</li><li><code>directions = directions[0]</code></li></ul></li><li>else <ul><li>all_images = stack all_images</li><li>all_fg_masks = stack al_images</li><li>directions = stack self.directions</li></ul></li><li>.float().to(self.rank) <ul><li>self.rank = get_rank()  = 0 ,1 ,2 … gpu序号</li></ul></li><li></li></ul></li><li>DTUDataset 继承Dataset和DTUDatasetBase<ul><li><code>__init__ , __len__ , __getitem__</code></li></ul></li><li>DTUIterableDataset 继承IterableDataset 和 DTUDatasetBase<ul><li><code>__init__ , __iter__</code></li></ul></li><li>DTUDataModule 继承pl.LightningDataModule<ul><li>@datasets.register(‘dtu’)</li><li><code>__init__(self,config)</code></li><li>setup(self,stage)<ul><li><code>stage in [None , &#39;fit&#39;]</code> : train_dataset = DTUIterableDataset(self.config,’train’)</li><li><code>stage in [None , &#39;fit&#39; , &#39;validate&#39;]</code> : val_dataset = DTUDataset(self.config, self.config.get(‘val_split’,’train’))</li><li><code>stage in [None , &#39;test&#39;]</code> : test_dataset = DTUDataset(self.config , self.config.get(‘test_split’,’test’))</li><li><code>stage in [None , &#39;predict&#39;]</code> : predict_dataset = DTUDataset(self.config, ‘train’)</li></ul></li><li>prepare_data</li><li>general_loader(self,dataset,batch_size)<ul><li>return DataLoader(dataset,num_workers=os.cpu_count(),batch_size,pin_memory=True,sampler=None)</li></ul></li><li>train_dataloader(self)<ul><li>return self.general_loader(self.train_dataset,batch_size=1)</li></ul></li><li>val_dataloader(self)<ul><li>return self.general_loader(self.val_dataset,batch_size=1)</li></ul></li><li>test_dataloader(self)<ul><li>return self.general_loader(self.test_dataset,batch_size=1)</li></ul></li><li>predict_dataloader(self)<ul><li>return self.general_loader(self.predict_dataset,batch_size=1)</li></ul></li></ul></li></ul><h1 id="models"><a href="#models" class="headerlink" title="models"></a>models</h1><h2 id="init-1"><a href="#init-1" class="headerlink" title="init"></a>init</h2><p><code>@models.register(&#39;neus&#39;)</code> 修饰器的作用：</p><ul><li>主要是为了实例化NeuSModel()的同时，在models字典中同时存入一个NeuSModel()值，对应的key为’neus’</li></ul><p>当运行 <code>neus_model = NeuSModel()</code> 时，即例如运行<code>self.texture = models.make(self.config.texture.name, self.config.texture)</code>时 ，会运行<code>neus_model = register(&#39;neus&#39;)(NeusModel)</code><br>返回给neus_model的值为decorator(cls) 函数的返回值，即NeusModel</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">models = &#123;&#125;</span><br><span class="line"></span><br><span class="line">def register(name):</span><br><span class="line">    def decorator(cls):</span><br><span class="line">        models[name] = cls</span><br><span class="line">        return cls</span><br><span class="line">    return decorator</span><br><span class="line"></span><br><span class="line">def make(name, config):</span><br><span class="line">    model = models[name](config)</span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line">from . import nerf, neus, geometry, texture</span><br></pre></td></tr></table></figure><h2 id="base"><a href="#base" class="headerlink" title="base"></a>base</h2><p>BaseModel ， 继承nn.Module</p><ul><li><code>__init__(self,config)</code><ul><li>self.confg = config , self.rank = get(rank)</li><li>self.setup()<ul><li>如果有config.weights，则load_state_dict(torch.load(config.weights))</li></ul></li></ul></li><li>setup()</li><li>update_step(self,epoch,global_step)</li><li>train(self,mode=True)<ul><li>return super().train(mode=mode)</li></ul></li><li>eval(self)<ul><li>return super().eval()</li></ul></li><li>regularizations(self,out)<ul><li>return {}</li></ul></li><li>@torch.no_gard() export(self,export_config)<ul><li>return {}</li></ul></li></ul><p>其他model需要继承于BaseModel</p><h2 id="neus"><a href="#neus" class="headerlink" title="neus"></a>neus</h2><p>Neus中的两个网络</p><h3 id="VarianceNetwork-继承nn-Module"><a href="#VarianceNetwork-继承nn-Module" class="headerlink" title="VarianceNetwork 继承nn.Module"></a>VarianceNetwork 继承nn.Module</h3><p>sigmoid函数的s参数在训练中变化</p><ul><li><code>__init__(self,config)</code><ul><li>self.config, self.init_val</li><li>self.register_parameter来自nn.Module注册一个参数</li><li>if self.modulate<ul><li>True: mod_start_steps, reach_max_steps, max_inv_s</li><li>False: none</li></ul></li></ul></li><li>@property 将该函数变为类的属性: inv_s()<ul><li>$val = e^{variance * 10.0}$</li><li>if self.modulate and do_mod<ul><li>val = val.clamp_max(mod_val)</li></ul></li><li>return val</li></ul></li><li>forward(self,x)<ul><li><code>return torch.ones([len(x), 1], device=self.variance.device) * self.inv_s</code> <ul><li>输入长度x1大小的inv_s</li></ul></li></ul></li><li>update_step(self,epoch,global_step)<ul><li>if self.modulate<ul><li>…</li></ul></li></ul></li></ul><h3 id="NeuSModel-继承BaseModel"><a href="#NeuSModel-继承BaseModel" class="headerlink" title="NeuSModel 继承BaseModel"></a>NeuSModel 继承BaseModel</h3><p>@models.register(‘neus’)<br><div class="note info">            <p>得到经过网络渲染后的一系列参数:<br>position: n_samples x 3</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">forward_:</span><br><span class="line">&#123;**out,</span><br><span class="line">**&#123;k + &#x27;_bg&#x27;: v for k, v in out_bg.items()&#125;,</span><br><span class="line">**&#123;k + &#x27;_full&#x27;: v for k, v in out_full.items()&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">forward:</span><br><span class="line">**&#123;**out,</span><br><span class="line">**&#123;k + &#x27;_bg&#x27;: v for k, v in out_bg.items()&#125;,</span><br><span class="line">**&#123;k + &#x27;_full&#x27;: v for k, v in out_full.items()&#125;</span><br><span class="line">&#125;</span><br><span class="line">+ inv_s</span><br></pre></td></tr></table></figure><ul><li>out: <ul><li>rgb, normal : n_rays x 3</li><li>opacity=权重累加, depth , rays_valid: opacity&gt;0 : n_rays x 1</li><li><code>num_samples: torch.as_tensor([len(t_starts)],dtype = torch.int32,device=rays.device)</code></li><li>if self.training:<ul><li>update: <ul><li>sdf, : n_samples x 1 </li><li>sdf_grad,  : n_samples x 3 </li><li>(weights, midpoints, dists, ray_indices).view(-1) : n_samples x 1</li></ul></li></ul></li></ul></li></ul><p>and</p><ul><li>if learned_background:<ul><li>out_bg:<ul><li>rgb, opacity, depth, rays_valid, num_samples</li><li>if self.training:<ul><li>update: (weights, midpoints, intervals, ray_indices).view(-1)</li></ul></li></ul></li></ul></li><li>else: rgb = None, num_samples = 0, rays_valid = 0</li></ul><p>and</p><ul><li>out_full <ul><li>rgb: <code>out_rgb + out_bg_rgb * (1.0 - out_opacity)</code> , n_rays x 1 </li><li>num_samples: out_num + out_bg_num , n_samples + n_samples_bg</li><li>rays_valid: out_valid + out_bg_valid , n_rays x 1</li></ul></li></ul>          </div></p><ul><li>setup<ul><li>self.geometry</li><li>self.texture</li><li>self.geometry.contraction_type</li><li>if self.config.learned_background<ul><li>self.geometry_bg</li><li>self.texture_bg</li><li>self.geometry_bg.contraction_type</li><li>self.near_plane_bg, self.far_plane_bg</li><li>self.cone_angle_bg = $10^{\frac{log_{10}(far.plane.bg)}{num.samples.per.ray.bg}}-1 = 10^{\frac{log_{10}(10^{3})}{64}}-1$</li></ul></li><li>self.variace = VarianceNetwork(self.config.variance)</li><li>self.register_buffer(‘scene_aabb’)<ul><li>即将 self.scene_aabb放在模型缓冲区，可以与参数一起保存，对这个变量进行优化</li></ul></li><li>if self.config.grid_prune 使用nerfacc中整合的InstantNGP中的占据网格，跳过空间中空白的部分<ul><li>self.occupancy_grid = OccupancyGrid(roi_aabb, resolution=128 , contraction_type=AABB)</li><li>if self.learned_background：<ul><li>self.occupancy_grid_bg = OccupancyGrid(roi_aabb, resolution=256 , contraction_type=UN_BOUNDED_SPHERE)</li></ul></li></ul></li><li>self.randomized = true</li><li>self.background_color = None</li><li>self.render_step_size = $1.732 \times 2 \times \frac{radius}{num.samples.per.ray}$</li></ul></li></ul><hr><ul><li>update_step(self,epoch,global_step)<ul><li>update_module_step(m,epoch,global_step)<ul><li>m: self.geometry, self.texture,self.variance<ul><li>if learned_background self.geometry_bg, self_texture_bg</li></ul></li></ul></li><li>cos_anneal_end = config.cos_anneal_end = 20000</li><li>if cos_anneal_end == 0: self.cos_anneal_end = 1.0<ul><li>else :min(1.0, global_step / cos_anneal_end)</li></ul></li><li>occ_eval_fn(x)  <code>x: Nx3</code><ul><li>sdf = self.geometry(x,…)</li><li><code>inv_s = self.variance(torch.zeros([1,3]))[:,:1].clip(1e-6,1e6)</code></li><li><code>inv_s = inv_s.expand(sdf.shape[0],1)</code></li><li><code>estimated_next_sdf = sdf[...,None] - self.render_step_size * 0.5</code><ul><li>$next = sdf - 1.732 \times 2 \times \frac{radius}{n.samples.perray} \cdot 0.5 = sdf - cos \cdot dist \cdot 0.5$<ul><li>$cos = 2 \cdot \sqrt{3}$</li></ul></li></ul></li><li><code>estimated_prev_sdf = sdf[...,None] + self.render_step_size * 0.5</code></li><li><code>prev_cdf = torch.sigmoid(estimated_prev_sdf * inv_s)</code></li><li><code>next_cdf = torch.sigmoid(estimated_prev_sdf * inv_s)</code></li><li><code>p = prev_cdf - next_cdf , c = prev_cdf</code></li><li><code>alpha = ((p + 1e-5) / (c + 1e-5)).view(-1, 1).clip(0.0, 1.0)</code>  Nx1</li><li>return alpha</li></ul></li><li>occ_eval_fn_bg(x)<ul><li><code>density, _ = self.geometry_bg(x)</code></li><li><code>return density[...,None] * self.render_step_size_bg</code></li></ul></li><li>if self.training(在nn.Module中可以这样判断是否为训练模式) and self.grid_prune<ul><li>self.occupancy_grid.every_n_step ：nerfacc的占据网格每n步更新一次</li><li>if learned_background: self.occupancy_grid_bg.every_n_step</li></ul></li></ul></li></ul><hr><ul><li>isosurface：判断是否等值面<ul><li>mesh = self.geometry.isosurface()</li><li>return mesh</li></ul></li></ul><hr><ul><li>get_alpha：获取$\alpha$值<ul><li><code>inv_s = self.variance(torch.zeros([1,3]))[:,:1].clip(1e-6,1e6)</code></li><li><code>inv_s = inv_s.expand(sdf.shape[0],1)</code></li><li><code>true_cos = (dirs * normal).sum(-1, keepdim=True)</code></li><li><code>iter_cos = -(F.relu(-true_cos * 0.5 + 0.5) * (1.0 - self.cos_anneal_ratio)+F.relu(-true_cos) * self.cos_anneal_ratio)</code></li><li><code>estimated_next_sdf = sdf[...,None] + iter_cos * dists.reshape(-1, 1) * 0.5</code></li><li><code>estimated_prev_sdf = sdf[...,None] - iter_cos * dists.reshape(-1, 1) * 0.5</code></li><li><code>prev_cdf = torch.sigmoid(estimated_prev_sdf * inv_s)</code></li><li><code>next_cdf = torch.sigmoid(estimated_next_sdf * inv_s)</code></li><li><code>p = prev_cdf - next_cdf</code></li><li><code>c = prev_cdf</code></li><li><code>alpha = ((p + 1e-5) / (c + 1e-5)).view(-1).clip(0.0, 1.0)</code> N,</li></ul></li></ul><hr><ul><li>forward_bg_：背景的输出<ul><li><code>n_rays = rays.shape[0]</code>, <code>rays_o, rays_d = rays[:, 0:3], rays[:, 3:6]</code></li><li>sigma_fn(t_starts, t_ends, ray_indices) <ul><li>ref: <a href="https://www.nerfacc.com/en/v0.3.5/apis/rendering.html">Volumetric Rendering — nerfacc 0.3.5 documentation</a></li><li><code>density, _ = self.geometry_bg(positions)</code></li><li>return <code>density[...,None]</code></li></ul></li><li><code>_, t_max = ray_aabb_intersect(rays_o, rays_d, self.scene_aabb)</code></li><li><code>near_plane = torch.where(t_max &gt; 1e9, self.near_plane_bg, t_max)</code><ul><li>if t_max &gt; 1e9, near_plane = self.near_plane_bg, else near_plane = t_max</li></ul></li><li>with torch.no_grad():<ul><li>ray_indices, t_starts, t_ends = ray_marching()</li><li>ref:<a href="https://www.nerfacc.com/en/v0.3.5/apis/rendering.html">Volumetric Rendering — nerfacc 0.3.5 documentation</a></li></ul></li><li>ray_indices = ray_indices.long() 为<code>N_rays</code></li><li>t_origins = rays_o[ray_indices] <code>N_rays x 3</code></li><li>t_dirs = rays_d[ray_indices] <code>N_rays x 3</code></li><li>midpoints = (t_starts + t_ends) / 2.<code>n_samples x 1</code></li><li>positions = t_origins + t_dirs * midpoints  <code>n_samples x 3</code></li><li>intervals = t_ends - t_starts 为<code>n_samples x 1</code><ul><li><strong><em>n_samples = N_rays1 </em> n_samples_ray1 + N_rays2 <em> n_samples_ray2 + …</em></strong></li></ul></li><li>density, feature = self.geometry_bg(positions)<ul><li>density : n_samples x 1</li><li>feature : n_samples x feature_dim</li></ul></li><li>rgb = self.texture_bg(feature, t_dirs)<ul><li>rgb: n_samples x 3</li></ul></li><li>weights = render_weight_from_density(t_starts, t_ends, density[…,None], ray_indices=ray_indices, n_rays=n_rays)<ul><li>从密度中得到权重: $w_i = T_i(1 - exp(-\sigma_i\delta_i)), \quad\textrm{where}\quad T_i = exp(-\sum_{j=1}^{i-1}\sigma_j\delta_j)$</li><li>ref: <a href="https://www.nerfacc.com/en/v0.3.5/apis/generated/nerfacc.render_weight_from_density.html#nerfacc.render_weight_from_density">nerfacc.render_weight_from_density — nerfacc 0.3.5 documentation</a></li><li>n_samples x 1</li></ul></li><li>opacity = accumulate_along_rays(weights, ray_indices, values=None, n_rays=n_rays)<ul><li>ref: <a href="https://www.nerfacc.com/en/v0.3.5/apis/generated/nerfacc.accumulate_along_rays.html#nerfacc.accumulate_along_rays">nerfacc.accumulate_along_rays — nerfacc 0.3.5 documentation</a></li><li>n_rays, 1</li></ul></li><li>depth = accumulate_along_rays(weights, ray_indices, values=midpoints, n_rays=n_rays)<ul><li>n_rays, 1</li></ul></li><li>comp_rgb = accumulate_along_rays(weights, ray_indices, values=rgb, n_rays=n_rays)<ul><li>n_rays , 3</li></ul></li><li>comp_rgb = comp_rgb + self.background_color * (1.0 - opacity)<ul><li>n_rays , 3</li></ul></li><li>return out </li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">out = &#123;</span><br><span class="line">    <span class="string">&#x27;comp_rgb&#x27;</span>: comp_rgb, <span class="comment"># n_rays, 1</span></span><br><span class="line">    <span class="string">&#x27;opacity&#x27;</span>: opacity, <span class="comment"># n_rays, 1</span></span><br><span class="line">    <span class="string">&#x27;depth&#x27;</span>: depth, <span class="comment"># n_rays, 1</span></span><br><span class="line">    <span class="string">&#x27;rays_valid&#x27;</span>: opacity &gt; <span class="number">0</span>, <span class="comment"># n_rays, 1</span></span><br><span class="line">    <span class="string">&#x27;num_samples&#x27;</span>: torch.as_tensor([<span class="built_in">len</span>(t_starts)], dtype=torch.int32,device=rays.device) <span class="comment"># n_samples</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> self.training:</span><br><span class="line">    out.update(&#123;</span><br><span class="line">        <span class="string">&#x27;weights&#x27;</span>: weights.view(-<span class="number">1</span>), <span class="comment">#  n_samples x 1</span></span><br><span class="line">        <span class="string">&#x27;points&#x27;</span>: midpoints.view(-<span class="number">1</span>), <span class="comment"># n_samples x 1</span></span><br><span class="line">        <span class="string">&#x27;intervals&#x27;</span>: intervals.view(-<span class="number">1</span>), <span class="comment"># n_samples x 1</span></span><br><span class="line">        <span class="string">&#x27;ray_indices&#x27;</span>: ray_indices.view(-<span class="number">1</span>) <span class="comment"># n_samples</span></span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure><hr><ul><li>forward_：前景输出<ul><li><code>n_rays = rays.shape[0]</code>, <code>rays_o, rays_d = rays[:, 0:3], rays[:, 3:6]</code></li><li>with torch.no_grad():<ul><li>ray_indices, t_starts, t_ends = ray_marching(…)</li><li>ref:<a href="https://www.nerfacc.com/en/v0.3.5/apis/rendering.html">Volumetric Rendering — nerfacc 0.3.5 documentation</a></li></ul></li><li>ray_indices = ray_indices.long()</li><li>t_origins = rays_o[ray_indices]</li><li>t_dirs = rays_d[ray_indices]</li><li>midpoints = (t_starts + t_ends) / 2.</li><li>positions = t_origins + t_dirs * midpoints</li><li>dists = t_ends - t_starts</li><li>sdf, sdf_grad, feature = self.geometry(positions, with_grad=True, with_feature=True)<ul><li>sdf : n_samples x 1 </li><li>sdf_grad: n_samples x 3</li><li>feature: n_samples x feature_dim</li></ul></li><li>normal = F.normalize(sdf_grad, p=2, dim=-1) 法向量：将sdf的梯度归一化<ul><li>normal: n_samples x 3</li></ul></li><li>alpha = self.get_alpha(sdf, normal, t_dirs, dists)[…,None]<ul><li>n_samples x 1</li></ul></li><li>rgb = self.texture(feature, t_dirs, normal)<ul><li>n_samples x 3</li></ul></li><li>weights = render_weight_from_alpha(alpha, ray_indices=ray_indices, n_rays=n_rays)<ul><li>从$\alpha$中得到权重：$w_i = T_i\alpha_i, \quad\textrm{where}\quad T_i = \prod_{j=1}^{i-1}(1-\alpha_j)$</li><li>ref: <a href="https://www.nerfacc.com/en/v0.3.5/apis/generated/nerfacc.render_weight_from_alpha.html#nerfacc.render_weight_from_alpha">nerfacc.render_weight_from_alpha — nerfacc 0.3.5 documentation</a></li><li>n_samples x 1</li></ul></li><li>opacity = accumulate_along_rays(weights, ray_indices, values=None, n_rays=n_rays)<ul><li>n_rays, 1</li></ul></li><li>depth = accumulate_along_rays(weights, ray_indices, values=midpoints, n_rays=n_rays)<ul><li>n_rays, 1</li></ul></li><li>comp_rgb = accumulate_along_rays(weights, ray_indices, values=rgb, n_rays=n_rays)<ul><li>n_rays, 3</li></ul></li><li>comp_normal = accumulate_along_rays(weights, ray_indices, values=normal, n_rays=n_rays)<ul><li>n_rays, 3</li></ul></li><li>comp_normal = F.normalize(comp_normal, p=2, dim=-1)<ul><li>n_rays, 3</li></ul></li><li>return <code>&#123;**out, **&#123;k + &#39;_bg&#39;: v for k, v in out_bg.items()&#125;, **&#123;k + &#39;_full&#39;: v for k, v in ut_full.items()&#125;&#125;</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">out = &#123;</span><br><span class="line">    <span class="string">&#x27;comp_rgb&#x27;</span>: comp_rgb, <span class="comment"># n_rays, 3</span></span><br><span class="line">    <span class="string">&#x27;comp_normal&#x27;</span>: comp_normal, <span class="comment"># n_rays, 3</span></span><br><span class="line">    <span class="string">&#x27;opacity&#x27;</span>: opacity, <span class="comment"># n_rays, 1</span></span><br><span class="line">    <span class="string">&#x27;depth&#x27;</span>: depth, <span class="comment"># n_rays, 1</span></span><br><span class="line">    <span class="string">&#x27;rays_valid&#x27;</span>: opacity &gt; <span class="number">0</span>, <span class="comment"># n_rays, 1</span></span><br><span class="line">    <span class="string">&#x27;num_samples&#x27;</span>: torch.as_tensor([<span class="built_in">len</span>(t_starts)], dtype=torch.int32, device=rays.device) <span class="comment">#n_samples</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> self.training:</span><br><span class="line">    out.update(&#123;</span><br><span class="line">        <span class="string">&#x27;sdf_samples&#x27;</span>: sdf, <span class="comment"># n_samples , 1</span></span><br><span class="line">        <span class="string">&#x27;sdf_grad_samples&#x27;</span>: sdf_grad, <span class="comment"># n_samples , 3</span></span><br><span class="line">        <span class="string">&#x27;weights&#x27;</span>: weights.view(-<span class="number">1</span>), <span class="comment"># n_samples , 1</span></span><br><span class="line">        <span class="string">&#x27;points&#x27;</span>: midpoints.view(-<span class="number">1</span>), <span class="comment"># n_samples , 1</span></span><br><span class="line">        <span class="string">&#x27;intervals&#x27;</span>: dists.view(-<span class="number">1</span>), <span class="comment"># n_samples , 1</span></span><br><span class="line">        <span class="string">&#x27;ray_indices&#x27;</span>: ray_indices.view(-<span class="number">1</span>)   <span class="comment"># n_samples             </span></span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> self.config.learned_background:</span><br><span class="line">    out_bg = self.forward_bg_(rays)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    out_bg = &#123;</span><br><span class="line">        <span class="string">&#x27;comp_rgb&#x27;</span>: self.background_color[<span class="literal">None</span>,:].expand(*comp_rgb.shape),</span><br><span class="line">        <span class="string">&#x27;num_samples&#x27;</span>: torch.zeros_like(out[<span class="string">&#x27;num_samples&#x27;</span>]),</span><br><span class="line">        <span class="string">&#x27;rays_valid&#x27;</span>: torch.zeros_like(out[<span class="string">&#x27;rays_valid&#x27;</span>])</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">out_full = &#123;</span><br><span class="line">    <span class="string">&#x27;comp_rgb&#x27;</span>: out[<span class="string">&#x27;comp_rgb&#x27;</span>] + out_bg[<span class="string">&#x27;comp_rgb&#x27;</span>] * (<span class="number">1.0</span> - out[<span class="string">&#x27;opacity&#x27;</span>]),</span><br><span class="line">    <span class="string">&#x27;num_samples&#x27;</span>: out[<span class="string">&#x27;num_samples&#x27;</span>] + out_bg[<span class="string">&#x27;num_samples&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;rays_valid&#x27;</span>: out[<span class="string">&#x27;rays_valid&#x27;</span>] | out_bg[<span class="string">&#x27;rays_valid&#x27;</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><ul><li><p>forward(rays)</p><ul><li>if self.training <ul><li>out = self.forward_(rays)</li></ul></li><li>else<ul><li>out = chunk_batch(self.forward_, self.config.ray_chunk, True, rays)</li></ul></li><li>return <code>&#123;**out, &#39;inv_s&#39;: self.variance.inv_s&#125;</code></li></ul></li><li><p>train(self, mode=True)</p><ul><li>self.randomized = <code>mode and self.config.randomized</code></li><li>return super().train(mode=mode)</li></ul></li><li><p>eval</p><ul><li>self.randomized = False</li><li>return super().eval()</li></ul></li><li><p>regularizations</p><ul><li>losses = {}</li><li>losses.update(self.geometry.regularizations(out))</li><li>losses.update(self.texture.regularizations(out))</li><li>return losses</li></ul></li></ul><p>@torch.no_grad()</p><ul><li>export：导出带有texture的mesh</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">export</span>(<span class="params">self, export_config</span>):</span><br><span class="line">    mesh = self.isosurface()</span><br><span class="line">    <span class="keyword">if</span> export_config.export_vertex_color:</span><br><span class="line">        _, sdf_grad, feature = chunk_batch(self.geometry, export_config.chunk_size, <span class="literal">False</span>, mesh[<span class="string">&#x27;v_pos&#x27;</span>].to(self.rank), with_grad=<span class="literal">True</span>, with_feature=<span class="literal">True</span>)</span><br><span class="line">        normal = F.normalize(sdf_grad, p=<span class="number">2</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        rgb = self.texture(feature, -normal, normal) <span class="comment"># set the viewing directions to the normal to get &quot;albedo&quot;</span></span><br><span class="line">        mesh[<span class="string">&#x27;v_rgb&#x27;</span>] = rgb.cpu()</span><br><span class="line">    <span class="keyword">return</span> mesh</span><br></pre></td></tr></table></figure><h2 id="network-utils"><a href="#network-utils" class="headerlink" title="network_utils"></a>network_utils</h2><div class="note info">            <p>各种编码方式和MLP网络</p><ul><li>VanillaFrequency, ProgressiveBandHashGrid, tcnn.Encoding</li><li>VanillaMLP, tcnn.Network<br>可以使用的方法：</li><li>get_encoding, get_mlp, get_encoding_with_network</li></ul>          </div><ul><li>VanillaFrequency 继承nn.Module<ul><li><code>__init__(self, in_channels, config)</code><ul><li>self.N_freqs 即L</li><li>self.in_channels, self.n_input_dims = in_channels, in_channels</li><li><code>self.funcs = [torch.sin, torch.cos]</code></li><li><code>self.freq_bands = 2**torch.linspace(0, self.N_freqs-1, self.N_freqs)</code></li><li>self.n_output_dims = self.in_channels <em> (len(self.funcs) </em> self.N_freqs) = 3x2xL</li><li>self.n_masking_step = config.get(‘n_masking_step’, 0)</li><li>self.update_step 每步开始前都需要更新出mask</li></ul></li><li>forward(self,x)<ul><li>out = []</li><li>for freq , mask in zip(self.freq_bands, self.mask):<ul><li>for func in self.funcs:<ul><li><code>out += [func(freq*x) * mask]</code>    </li></ul></li></ul></li><li>return torch.cat(out, -1)   </li></ul></li><li>update_step(self,epoch,global_step)<ul><li>if self.n_masking_step &lt;= 0 or global_step is None:<ul><li>self.mask = torch.ones(self.N_freqs, dtype=torch.float32) 与L相同形状的全1张量</li></ul></li><li>else:<ul><li>self.mask = (1. - torch.cos(math.pi <em> (global_step / self.n_masking_step </em> self.N_freqs - torch.arange(0, self.N_freqs)).clamp(0, 1))) / 2.<ul><li>mask = $\left(1-cos\left(\pi \cdot \left(\frac{global.step \cdot L}{n.masking.step}-arrange\right).clamp\right)\right) \cdot 0.5$</li></ul></li><li>rank_zero_debug(f’Update mask: {global_step}/{self.n_masking_step} {self.mask}’)</li></ul></li></ul></li></ul></li></ul><p>Vanilla：最初始的<br>即NeRF中的频率编码方式<br>$\gamma(p)=\left(\sin \left(2^{0} \pi p\right), \cos \left(2^{0} \pi p\right), \cdots, \sin \left(2^{L-1} \pi p\right), \cos \left(2^{L-1} \pi p\right)\right)$</p><ul><li><p>ProgressiveBandHashGrid，继承nn.Module</p><ul><li><code>__init__(self, in_channels,config)</code><ul><li>self.n_input_dims = in_channels</li><li>encoding_config = config.copy()</li><li>encoding_config[‘otype’] = ‘HashGrid’</li><li>with torch.cuda.device(get_rank()):<ul><li>self.encoding = tcnn.Encoding(in_channels, encoding_config) 使用哈希编码</li></ul></li><li>self.n_output_dims = self.encoding.n_output_dims</li><li>self.n_level = config[‘n_levels’]，分辨率个数</li><li>self.n_features_per_level = config[‘n_features_per_level’]，特征向量维数</li><li>self.start_level, self.start_step, self.update_steps = config[‘start_level’], config[‘start_step’], config[‘update_steps’]</li><li>self.current_level = self.start_level</li><li>self.mask = torch.zeros(self.n_level * self.n_features_per_level, dtype=torch.float32, device=get_rank())</li></ul></li><li>forward(self,x)<ul><li>enc = self.encoding(x)</li><li>enc = enc * self.mask ,第一个step，mask为0，之后每过update_steps，更新一次mask</li><li>return enc</li></ul></li><li>update_step(self,epoch,global_step)<ul><li>current_level = min(self.start_level + max(global_step - self.start_step, 0) // self.update_steps, self.n_level)<ul><li>min(1+max(global_step-0,0)//update_steps, 16)</li></ul></li><li>if current_level &gt; self.current_level:<ul><li>rank_zero_debug(f’Update current level to {current_level}’)</li></ul></li><li>self.current_level = current_level</li><li><code>self.mask[:self.current_level * self.n_features_per_level] = 1.</code><ul><li>mask从0到(当前分辨率x特征向量维数) 置为1</li></ul></li></ul></li></ul></li><li><p>CompositeEncoding，继承nn.Module</p><ul><li><code>__init__(self, encoding , include_xyz = True, xyz_scale=1 , xyz_offset=0)</code><ul><li>self.encoding = encoding</li><li>self.include_xyz, self.xyz_scale, self.xyz_offset = include_xyz, xyz_scale, xyz_offset</li><li>self.n_output_dims = int(self.include_xyz) * self.encoding.n_input_dims + self.encoding.n_output_dims<ul><li>$o.dim = int(TorF) \cdot n.idim +n.odim$</li></ul></li></ul></li><li><code>forward(self,x,*args)</code><ul><li>return <code>self.encoding(x, *args) if not self.include_xyz else torch.cat([x * self.xyz_scale + self.xyz_offset, self.encoding(x, *args)], dim=-1)</code><ul><li>if include_xyz: <code>torch.cat([x * self.xyz_scale + self.xyz_offset, self.encoding(x, *args)], dim=-1)</code> 将输入x变为2x-1，并与编码后的输入cat起来</li><li>else : <code>self.encoding(x, *args)</code></li></ul></li></ul></li><li>update(self, epoch, global_step)<ul><li>update_module_step(self.encoding, epoch, global_step)</li></ul></li></ul></li></ul><div class="note info">            <p>config_to_primitive在utils/misc.py中，<code>return OmegaConf.to_container(config, resolve=resolve)</code><br>由于OmegaConf config objects很占用内存，因此使用to_container转化为原始的容器，如dict。如果resolve值设置为 True，则将在转换期间解析内插<code>${foo}</code>。</p>          </div><ul><li>get_encoding(n_input_dims, config)<ul><li><code>if config.otype ==  &#39;VanillaFrequency&#39;:</code><ul><li>encoding = VanillaFrequency(n_input_dims, config_to_primitive(config))</li></ul></li><li><code>elif config.otype == &#39;ProgressiveBandHashGrid&#39;:</code><ul><li>encoding = ProgressiveBandHashGrid(n_input_dims, config_to_primitive(config))</li></ul></li><li>else:<ul><li>with torch.cuda.device(get_rank()):<ul><li>encoding = tcnn.Encoding(n_input_dims, config_to_primitive(config)) </li></ul></li></ul></li><li><code>encoding = CompositeEncoding(encoding, include_xyz=config.get(&#39;include_xyz&#39;, False), xyz_scale=2., xyz_offset=-1.)</code></li><li>return encoding</li></ul></li></ul><hr><ul><li>VanillaMLP , 继承nn.Module<ul><li>NeRF中的MLP</li><li><code>__init__(self,dim_in,dim_out,config)</code><ul><li>一共n_hidden_layers个隐藏层，每个隐藏层有n_neurons个节点</li><li><code>Sequential(*self.layers)</code>将每层都加入ModuleList中，并在内部实现forward，可以不写forward<ul><li>ref: <a href="https://zhuanlan.zhihu.com/p/75206669">详解PyTorch中的ModuleList和Sequential - 知乎 (zhihu.com)</a></li></ul></li><li>get_activation：utils中的激活函数方法，根据不同的output_activation选择不同的激活函数</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">self.n_neurons, self.n_hidden_layers = config[<span class="string">&#x27;n_neurons&#x27;</span>], config[<span class="string">&#x27;n_hidden_layers&#x27;</span>]</span><br><span class="line">self.sphere_init, self.weight_norm = config.get(<span class="string">&#x27;sphere_init&#x27;</span>, <span class="literal">False</span>), config.get(<span class="string">&#x27;weight_norm&#x27;</span>, <span class="literal">False</span>)</span><br><span class="line">self.sphere_init_radius = config.get(<span class="string">&#x27;sphere_init_radius&#x27;</span>, <span class="number">0.5</span>)</span><br><span class="line">self.layers = [self.make_linear(dim_in, self.n_neurons, is_first=<span class="literal">True</span>, is_last=<span class="literal">False</span>), self.make_activation()]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_hidden_layers - <span class="number">1</span>):</span><br><span class="line">    self.layers += [self.make_linear(self.n_neurons, self.n_neurons, is_first=<span class="literal">False</span>, is_last=<span class="literal">False</span>), self.make_activation()]</span><br><span class="line">self.layers += [self.make_linear(self.n_neurons, dim_out, is_first=<span class="literal">False</span>, is_last=<span class="literal">True</span>)]</span><br><span class="line">self.layers = nn.Sequential(*self.layers)</span><br><span class="line">self.output_activation = get_activation(config[<span class="string">&#x27;output_activation&#x27;</span>])</span><br></pre></td></tr></table></figure><ul><li>VanillaMLP 接上<ul><li>forward(self,x)，Sequential可以直接使用，而不需构建一个循环从ModuleList中依次执行<ul><li>x = self.layers(x.float())</li><li>x = self.output_activation(x)</li><li>return x</li></ul></li><li>make_linear(self,dim_in,dim_out,is_first,is_last)<ul><li>layer = nn.Linear(dim_in, dim_out, bias=True) # network without bias will degrade quality</li><li>if self.sphere_init: 初始化每层的权重和偏置(常数或者正态分布)<ul><li>if is_last:<ul><li>torch.nn.init.constant_(layer.bias, -self.sphere_init_radius)</li><li>torch.nn.init.normal_(layer.weight, mean=math.sqrt(math.pi) / math.sqrt(dim_in), std=0.0001)</li></ul></li><li>elif is_first:<ul><li>torch.nn.init.constant_(layer.bias, 0.0)</li><li>torch.nn.init.constant_(layer.weight[:, 3:], 0.0)</li><li>torch.nn.init.normal_(layer.weight[:, :3], 0.0, math.sqrt(2) / math.sqrt(dim_out))</li></ul></li><li>else:<ul><li>torch.nn.init.constant_(layer.bias, 0.0)</li><li>torch.nn.init.normal_(layer.weight, 0.0, math.sqrt(2) / math.sqrt(dim_out))</li></ul></li></ul></li><li>else:<ul><li>torch.nn.init.constant_(layer.bias, 0.0)</li><li>torch.nn.init.kaiming_uniform_(layer.weight, nonlinearity=’relu’)</li></ul></li><li>if self.weight_norm: <ul><li>layer = nn.utils.weight_normal(layer)</li></ul></li><li>return layer</li></ul></li><li>make_activation<ul><li>if self.sphere_init:<ul><li>return nn.Softplus(beta=100)</li></ul></li><li>else:<ul><li>return nn.ReLU(inplace=True)</li></ul></li></ul></li></ul></li></ul><hr><ul><li>sphere_init_tcnn_network(n_input_dims, n_output_dims, config, network)<ul><li>rank_zero_debug(‘Initialize tcnn MLP to approximately represent a sphere.’)</li><li>padto = 16 if config.otype == ‘FullyFusedMLP’ else 8</li><li>n_input_dims = n_input_dims + (padto - n_input_dims % padto) % padto<ul><li>$ni = ni + (padto - ni \% padto) \% padto$ 取余数</li></ul></li><li>n_output_dims = n_output_dims + (padto - n_output_dims % padto) % padto</li><li><code>data = list(network.parameters())[0].data</code></li><li><code>assert data.shape[0] == (n_input_dims + n_output_dims) * config.n_neurons + (config.n_hidden_layers - 1) * config.n_neurons**2</code>   </li><li><code>new_data = []</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># first layer</span></span><br><span class="line">weight = torch.zeros((config.n_neurons, n_input_dims)).to(data)</span><br><span class="line">torch.nn.init.constant_(weight[:, <span class="number">3</span>:], <span class="number">0.0</span>)</span><br><span class="line">torch.nn.init.normal_(weight[:, :<span class="number">3</span>], <span class="number">0.0</span>, math.sqrt(<span class="number">2</span>) / math.sqrt(config.n_neurons))</span><br><span class="line">new_data.append(weight.flatten())</span><br><span class="line"><span class="comment"># hidden layers</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(config.n_hidden_layers - <span class="number">1</span>):</span><br><span class="line">    weight = torch.zeros((config.n_neurons, config.n_neurons)).to(data)</span><br><span class="line">    torch.nn.init.normal_(weight, <span class="number">0.0</span>, math.sqrt(<span class="number">2</span>) / math.sqrt(config.n_neurons))</span><br><span class="line">    new_data.append(weight.flatten())</span><br><span class="line"><span class="comment"># last layer</span></span><br><span class="line">weight = torch.zeros((n_output_dims, config.n_neurons)).to(data)</span><br><span class="line">torch.nn.init.normal_(weight, mean=math.sqrt(math.pi) / math.sqrt(config.n_neurons), std=<span class="number">0.0001</span>)</span><br><span class="line">new_data.append(weight.flatten())</span><br><span class="line">new_data = torch.cat(new_data)</span><br><span class="line">data.copy_(new_data)</span><br></pre></td></tr></table></figure><ul><li><p>get_mlp(n_input_dims, n_output_dims, config)</p><ul><li><code>if config.otype == &#39;VanillaMLP&#39;:</code><ul><li>network = VanillaMLP(n_input_dims, n_output_dims, config_to_primitive(config))</li></ul></li><li><code>else:</code><ul><li>with torch.cuda.device(get_rank()):<ul><li>network = tcnn.Network(n_input_dims, n_output_dims, config_to_primitive(config))</li><li>if config.get(‘sphere_init’, False):<ul><li>sphere_init_tcnn_network(n_input_dims, n_output_dims, config, network)</li></ul></li></ul></li></ul></li><li>return network 返回一个model</li></ul></li><li><p>EncodingWithNetwork，继承nn.Module</p><ul><li><code>__init__(self,encoding,network)</code><ul><li>self.encoding, self.network = encoding, network</li></ul></li><li>forward(self,x)<ul><li>return self.network(self.encoding(x))</li></ul></li><li>update_step(self, epoch, global_step)<ul><li>update_module_step(self.encoding, epoch, global_step)</li><li>update_module_step(self.network, epoch, global_step)</li></ul></li></ul></li><li><p>get_encoding_with_network(n_input_dims, n_output_dims, encoding_config, network_config)</p><ul><li><code>if encoding_config.otype in [&#39;VanillaFrequency&#39;, &#39;ProgressiveBandHashGrid&#39;] or network_config.otype in [&#39;VanillaMLP&#39;]:</code><ul><li>encoding = get_encoding(n_input_dims, encoding_config)</li><li>network = get_mlp(encoding.n_output_dims, n_output_dims, network_config)</li><li>encoding_with_network = EncodingWithNetwork(encoding, network)</li></ul></li><li>else:<ul><li>with torch.cuda.device(get_rank()):<ul><li>encoding_with_network = tcnn.NetworkWithInputEncoding(n_input_dims,n_output_dims,encoding_config,network_config)</li></ul></li></ul></li><li>return encoding_with_network</li></ul></li></ul><h2 id="geometry"><a href="#geometry" class="headerlink" title="geometry"></a>geometry</h2><div class="note info">            <p>输入点的位置position经过MLP网络得到背景density, feature或者前景物体sdf, sdf_grad, feature</p>          </div><ul><li>contract_to_unisphere，根据contraction_type，将位置x缩放到合适大小<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">contract_to_unisphere</span>(<span class="params">x, radius, contraction_type</span>):</span><br><span class="line">    <span class="keyword">if</span> contraction_type == ContractionType.AABB:</span><br><span class="line">        x = scale_anything(x, (-radius, radius), (<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">elif</span> contraction_type == ContractionType.UN_BOUNDED_SPHERE:</span><br><span class="line">        x = scale_anything(x, (-radius, radius), (<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        x = x * <span class="number">2</span> - <span class="number">1</span>  <span class="comment"># aabb is at [-1, 1]</span></span><br><span class="line">        mag = x.norm(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        mask = mag.squeeze(-<span class="number">1</span>) &gt; <span class="number">1</span></span><br><span class="line">        x[mask] = (<span class="number">2</span> - <span class="number">1</span> / mag[mask]) * (x[mask] / mag[mask])</span><br><span class="line">        x = x / <span class="number">4</span> + <span class="number">0.5</span>  <span class="comment"># [-inf, inf] is at [0, 1]</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></li></ul><p>将等值面三角网格化</p><ul><li>MarchingCubeHelper 继承nn.Module<ul><li><code>__init__(resolution, use_torch=True)</code><ul><li>self.resolution = resolution</li><li>self.use_torch = use_torch</li><li>self.points_range = (0, 1)</li><li>if self.use_torch:<ul><li>import torchmcubes</li><li>self.mc_func = torchmcubes.marching_cubes</li></ul></li><li>else:<ul><li>import mcubes</li><li>self.mc_func = mcubes.marching_cubes</li></ul></li><li>self.verts = None</li></ul></li><li>grid_vertices()<ul><li>if self.verts is None:<ul><li><code>x, y, z = torch.linspace(*self.points_range, self.resolution), torch.linspace(*self.points_range, self.resolution), torch.linspace(*self.points_range, self.resolution)</code><ul><li><code>x: torch.Size([resolution])</code></li></ul></li><li>x, y, z = torch.meshgrid(x, y, z, indexing=’ij’)<ul><li><code>x: torch.Size([resolution, resolution, resolution])</code></li></ul></li><li><code>verts = torch.cat([x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)], dim=-1).reshape(-1, 3)</code><ul><li><code>verts: torch.Size([resolution ** 3, 3])</code></li></ul></li><li>self.verts = verts</li></ul></li><li>return self.verts</li></ul></li><li>forward(self,level,threshold=0.)<ul><li>level = level.float().view(self.resolution, self.resolution, self.resolution)</li><li>if self.use_torch:<ul><li><code>verts, faces = self.mc_func(level.to(get_rank()), threshold)</code></li><li><code>verts, faces = verts.cpu(), faces.cpu().long()</code></li></ul></li><li>else:<ul><li><code>verts, faces = self.mc_func(-level.numpy(), threshold)</code> # transform to numpy</li><li><code>verts, faces = torch.from_numpy(verts.astype(np.float32)), torch.from_numpy(faces.astype(np.int64))</code></li></ul></li><li>verts = verts / (self.resolution - 1.)</li><li>return {‘v_pos’: verts, ‘t_pos_idx’: faces}</li></ul></li></ul></li></ul><p>获得等值面的mesh网格，包括vertices和faces</p><ul><li>BaseImplicitGeometry，继承BaseModel<ul><li><code>__init__(self,config)</code><ul><li><code>if self.config.isosurface is not None:</code><ul><li><code>assert self.config.isosurface.method in [&#39;mc&#39;, &#39;mc-torch&#39;]</code></li><li><code>if self.config.isosurface.method == &#39;mc-torch&#39;:</code><ul><li><code>raise NotImplementedError(&quot;Please do not use mc-torch. It currently has some scaling issues I haven&#39;t fixed yet.&quot;)</code></li></ul></li><li><code>self.helper = MarchingCubeHelper(self.config.isosurface.resolution, use_torch=self.config.isosurface.method==&#39;mc-torch&#39;)</code></li></ul></li><li>self.contraction_type = None</li><li>self.radius = self.config.radius</li></ul></li><li>forward_level(self,points)<ul><li>raise NotImplementedError</li></ul></li><li>isosurface_(self,vmin,vmax): 返回mesh</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batch_func</span>(<span class="params">x</span>):</span><br><span class="line">    x = torch.stack([</span><br><span class="line">        scale_anything(x[...,<span class="number">0</span>], (<span class="number">0</span>, <span class="number">1</span>), (vmin[<span class="number">0</span>], vmax[<span class="number">0</span>])),</span><br><span class="line">        scale_anything(x[...,<span class="number">1</span>], (<span class="number">0</span>, <span class="number">1</span>), (vmin[<span class="number">1</span>], vmax[<span class="number">1</span>])),</span><br><span class="line">        scale_anything(x[...,<span class="number">2</span>], (<span class="number">0</span>, <span class="number">1</span>), (vmin[<span class="number">2</span>], vmax[<span class="number">2</span>])),</span><br><span class="line">    ], dim=-<span class="number">1</span>).to(self.rank)</span><br><span class="line">    rv = self.forward_level(x).cpu() <span class="comment"># 为 -density</span></span><br><span class="line">    cleanup()</span><br><span class="line">    <span class="keyword">return</span> rv</span><br><span class="line"><span class="comment"># self.helper.grid_vertices(): torch.Size([resolution ** 3, 3])</span></span><br><span class="line">level = chunk_batch(batch_func, self.config.isosurface.chunk, <span class="literal">True</span>, self.helper.grid_vertices())</span><br><span class="line">mesh = self.helper(level, threshold=self.config.isosurface.threshold)</span><br><span class="line">mesh[<span class="string">&#x27;v_pos&#x27;</span>] = torch.stack([</span><br><span class="line">    scale_anything(mesh[<span class="string">&#x27;v_pos&#x27;</span>][...,<span class="number">0</span>], (<span class="number">0</span>, <span class="number">1</span>), (vmin[<span class="number">0</span>], vmax[<span class="number">0</span>])),</span><br><span class="line">    scale_anything(mesh[<span class="string">&#x27;v_pos&#x27;</span>][...,<span class="number">1</span>], (<span class="number">0</span>, <span class="number">1</span>), (vmin[<span class="number">1</span>], vmax[<span class="number">1</span>])),</span><br><span class="line">    scale_anything(mesh[<span class="string">&#x27;v_pos&#x27;</span>][...,<span class="number">2</span>], (<span class="number">0</span>, <span class="number">1</span>), (vmin[<span class="number">2</span>], vmax[<span class="number">2</span>]))</span><br><span class="line">], dim=-<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> mesh</span><br><span class="line"></span><br><span class="line"><span class="keyword">in</span> utils:</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scale_anything</span>(<span class="params">dat, inp_scale, tgt_scale</span>):</span><br><span class="line">    <span class="keyword">if</span> inp_scale <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        inp_scale = [dat.<span class="built_in">min</span>(), dat.<span class="built_in">max</span>()]</span><br><span class="line">    dat = (dat  - inp_scale[<span class="number">0</span>]) / (inp_scale[<span class="number">1</span>] - inp_scale[<span class="number">0</span>])</span><br><span class="line">    dat = dat * (tgt_scale[<span class="number">1</span>] - tgt_scale[<span class="number">0</span>]) + tgt_scale[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> dat</span><br></pre></td></tr></table></figure><ul><li>BaseImplicitGeometry 接上<ul><li>@torch.no_grad()</li><li>isosurface(self) <ul><li>if self.config.isosurface is None:<ul><li>raise NotImplementedError</li></ul></li><li>mesh_coarse = self.isosurface_((-self.radius, -self.radius, -self.radius), (self.radius, self.radius, self.radius))</li><li>vmin, vmax = mesh_coarse[‘v_pos’].amin(dim=0), mesh_coarse[‘v_pos’].amax(dim=0)</li><li>vmin_ = (vmin - (vmax - vmin) * 0.1).clamp(-self.radius, self.radius)</li><li>vmax_ = (vmax + (vmax - vmin) * 0.1).clamp(-self.radius, self.radius)</li><li>mesh_fine = self.isosurface_(vmin_, vmax_)</li><li>return mesh_fine  返回vertices和faces</li></ul></li></ul></li></ul><p>背景几何：density和feature<br>@models.register(‘volume-density’)</p><ul><li>VolumeDensity 继承BaseImplicitGeometry<ul><li>setup()<ul><li>self.n_input_dims = self.config.get(‘n_input_dims’, 3)</li><li>self.n_output_dims = self.config.feature_dim</li><li>self.encoding_with_network = get_encoding_with_network(self.n_input_dims, self.n_output_dims, self.config.xyz_encoding_config, self.config.mlp_network_config)</li></ul></li><li>forward(self,points) 根据编码方式和网络，得到density和feature<ul><li>points = contract_to_unisphere(points, self.radius, self.contraction_type)</li><li><code>out = self.encoding_with_network(points.view(-1, self.n_input_dims)).view(*points.shape[:-1], self.n_output_dims).float()</code></li><li>density, feature = out[…,0], out</li><li>if ‘density_activation’ in self.config:<ul><li>density = get_activation(self.config.density_activation)(density + float(self.config.density_bias))</li></ul></li><li>if ‘feature_activation’ in self.config:<ul><li>feature = get_activation(self.config.feature_activation)(feature)</li></ul></li><li>return density, feature</li></ul></li><li>forward_level(self,points) 根据编码方式和网络，得到-density，方便进行判断等值面isosurface<ul><li>points = contract_to_unisphere(points, self.radius, self.contraction_type)</li><li><code>density = self.encoding_with_network(points.reshape(-1, self.n_input_dims)).reshape(*points.shape[:-1], self.n_output_dims)[...,0]</code></li><li>if ‘density_activation’ in self.config:<ul><li>density = get_activation(self.config.density_activation)(density + float(self.config.density_bias))</li></ul></li><li>return -density</li></ul></li><li>update_step(self,epoch, global_step)<ul><li>update_module_step(self.encoding_with_network, epoch, global_step)</li></ul></li></ul></li></ul><p>前景物体几何：sdf, sdf_grad, feature<br>@models.register(‘volume-sdf’)</p><ul><li>VolumeSDF<ul><li>setup()<ul><li>self.n_output_dims = self.config.feature_dim</li><li>encoding = get_encoding(3, self.config.xyz_encoding_config)</li><li>network = get_mlp(encoding.n_output_dims, self.n_output_dims, self.config.mlp_network_config)</li><li>self.encoding, self.network = encoding, network</li><li>self.grad_type = self.config.grad_type</li></ul></li><li>forward(self, points, with_grad=True, with_feature=True)<ul><li><code>with torch.inference_mode(torch.is_inference_mode_enabled() and not (with_grad and self.grad_type == &#39;analytic&#39;))</code>: 是否启用推理模式，当前为推理，并且没有grad，grad_type不是analytic<ul><li><code>with torch.set_grad_enabled(self.training or (with_grad and self.grad_type == &#39;analytic&#39;)):</code><ul><li>if with_grad and self.grad_type == ‘analytic’:</li><li>if not self.training:<ul><li>points = points.clone() # points may be in inference mode, get a copy to enable grad</li></ul></li><li>points.requires_grad_(True)</li></ul></li><li>points_ = points 初始位置</li><li>points = contract_to_unisphere(points, self.radius, self.contraction_type) <strong>points normalized to (0, 1)</strong></li><li><code>out = self.network(self.encoding(points.view(-1, 3))).view(*points.shape[:-1], self.n_output_dims).float()</code></li><li><code>sdf, feature = out[...,0], out</code></li><li>if ‘sdf_activation’ in self.config: sdf激活<ul><li>sdf = get_activation(self.config.sdf_activation)(sdf + float(self.config.sdf_bias))</li></ul></li><li>if ‘feature_activation’ in self.config:<ul><li>feature = get_activation(self.config.feature_activation)(feature)       </li></ul></li><li>if with_grad: 求梯度的两种方法：自动微分or有限差分法<ul><li><code>if self.grad_type == &#39;analytic&#39;:</code> <ul><li>grad = torch.autograd.grad(sdf, points_, grad_outputs=torch.ones_like(sdf),create_graph=True, retain_graph=True, only_inputs=True)[0]</li></ul></li><li><code>elif self.grad_type == &#39;finite_difference&#39;:</code><ul><li>有限差分得到$𝑓 ′ (𝑥) = (𝑓 (𝑥 + Δ𝑥) − (𝑓 𝑥 − Δ𝑥))/2Δ𝑥$，sdf对位置的梯度grad</li></ul></li></ul></li></ul></li><li><code>rv = [sdf]</code></li><li>if with_grad:<ul><li>rv.append(grad)</li></ul></li><li>if with_feature:<ul><li>rv.append(feature)</li></ul></li><li><code>rv = [v if self.training else v.detach() for v in rv]</code></li><li><code>return rv[0] if len(rv) == 1 else rv</code></li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">有限差分法</span><br><span class="line">eps = <span class="number">0.001</span></span><br><span class="line">points_d_ = torch.stack([</span><br><span class="line">    points_ + torch.as_tensor([eps, <span class="number">0.0</span>, <span class="number">0.0</span>]).to(points_), <span class="comment"># to(other): 返回一个与 Tensor other 具有相同 torch.dtype 和 torch.device 的 Tensor</span></span><br><span class="line">    points_ + torch.as_tensor([-eps, <span class="number">0.0</span>, <span class="number">0.0</span>]).to(points_),</span><br><span class="line">    points_ + torch.as_tensor([<span class="number">0.0</span>, eps, <span class="number">0.0</span>]).to(points_),</span><br><span class="line">    points_ + torch.as_tensor([<span class="number">0.0</span>, -eps, <span class="number">0.0</span>]).to(points_),</span><br><span class="line">    points_ + torch.as_tensor([<span class="number">0.0</span>, <span class="number">0.0</span>, eps]).to(points_),</span><br><span class="line">    points_ + torch.as_tensor([<span class="number">0.0</span>, <span class="number">0.0</span>, -eps]).to(points_)</span><br><span class="line">], dim=<span class="number">0</span>).clamp(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">points_d = scale_anything(points_d_, (-self.radius, self.radius), (<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">points_d_sdf = self.network(self.encoding(points_d.view(-<span class="number">1</span>, <span class="number">3</span>)))[...,<span class="number">0</span>].view(<span class="number">6</span>, *points.shape[:-<span class="number">1</span>]).<span class="built_in">float</span>()</span><br><span class="line">grad = torch.stack([</span><br><span class="line">    <span class="number">0.5</span> * (points_d_sdf[<span class="number">0</span>] - points_d_sdf[<span class="number">1</span>]) / eps,</span><br><span class="line">    <span class="number">0.5</span> * (points_d_sdf[<span class="number">2</span>] - points_d_sdf[<span class="number">3</span>]) / eps,</span><br><span class="line">    <span class="number">0.5</span> * (points_d_sdf[<span class="number">4</span>] - points_d_sdf[<span class="number">5</span>]) / eps,</span><br><span class="line">], dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li>VolumeSDF 接上<ul><li>forward_level(self, points)<ul><li>points = contract_to_unisphere(points, self.radius, self.contraction_type) # points normalized to (0, 1)</li><li><code>sdf = self.network(self.encoding(points.view(-1, 3))).view(*points.shape[:-1], self.n_output_dims)[...,0]</code></li><li>if ‘sdf_activation’ in self.config:<ul><li>sdf = get_activation(self.config.sdf_activation)(sdf + float(self.config.sdf_bias))</li></ul></li><li>return sdf</li></ul></li><li>update_step(self, epoch , global_step)<ul><li>update_module_step(self.encoding, epoch, global_step)</li><li>update_module_step(self.network, epoch, global_step)</li></ul></li></ul></li></ul><h2 id="texture"><a href="#texture" class="headerlink" title="texture"></a>texture</h2><div class="note info">            <p>根据feature、dirs得到背景颜色<br>根据feature、dirs，以及normal得到前景颜色</p>          </div><p>前背景颜色值<br>@models.register(‘volume-radiance’)</p><ul><li>VolumeRadiance，继承nn.Module<ul><li><code>__init__</code><ul><li>self.config = config</li><li>self.n_dir_dims = self.config.get(‘n_dir_dims’, 3)</li><li>self.n_output_dims = 3</li><li>encoding = get_encoding(self.n_dir_dims, self.config.dir_encoding_config)</li><li>self.n_input_dims = self.config.input_feature_dim + encoding.n_output_dims</li><li>network = get_mlp(self.n_input_dims, self.n_output_dims, self.config.mlp_network_config) </li><li>self.encoding = encoding</li><li>self.network = network</li></ul></li><li><code>forward(self, features, dirs, *args)</code><ul><li>dirs = (dirs + 1.) / 2. # (-1, 1) =&gt; (0, 1)</li><li>dirs_embd = self.encoding(dirs.view(-1, self.n_dir_dims))</li><li><code>network_inp = torch.cat([features.view(-1, features.shape[-1]), dirs_embd] + [arg.view(-1, arg.shape[-1]) for arg in args], dim=-1)</code></li><li><code>color = self.network(network_inp).view(*features.shape[:-1], self.n_output_dims).float()</code></li><li>if ‘color_activation’ in self.config:<ul><li>color = get_activation(self.config.color_activation)(color)</li></ul></li><li>return color</li></ul></li><li>update_step(self, epoch, global_step)<ul><li>update_module_step(self.encoding, epoch, global_step)</li></ul></li><li>regularizations(self, out)<ul><li>return {}</li></ul></li></ul></li></ul><p>@models.register(‘volume-color’) </p><ul><li>VolumeColor，不使用编码方法<ul><li><code>__init__</code><ul><li>self.config = config</li><li>self.n_output_dims = 3</li><li>self.n_input_dims = self.config.input_feature_dim</li><li>network = get_mlp(self.n_input_dims, self.n_output_dims, self.config.mlp_network_config)</li><li>self.network = network</li></ul></li><li><code>forward(self, features, *args)</code><ul><li><code>network_inp = features.view(-1, features.shape[-1])</code></li><li><code>color = self.network(network_inp).view(*features.shape[:-1], self.n_output_dims).float()</code></li><li>if ‘color_activation’ in self.config:<ul><li>color = get_activation(self.config.color_activation)(color)</li></ul></li><li>return color</li></ul></li><li>regularizations(self, out)<ul><li>return {}</li></ul></li></ul></li></ul><h2 id="ray-utils"><a href="#ray-utils" class="headerlink" title="ray_utils"></a>ray_utils</h2><ul><li>cast_rays</li></ul><p>获取光线的方向(相机坐标下)</p><ul><li>get_ray_directions(W, H, fx, fy, cx, cy, use_pixel_centers=True)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_ray_directions</span>(<span class="params">W, H, fx, fy, cx, cy, use_pixel_centers=<span class="literal">True</span></span>):</span><br><span class="line">    pixel_center = <span class="number">0.5</span> <span class="keyword">if</span> use_pixel_centers <span class="keyword">else</span> <span class="number">0</span> <span class="comment"># 是否使用像素中心</span></span><br><span class="line">    i, j = np.meshgrid(</span><br><span class="line">        np.arange(W, dtype=np.float32) + pixel_center,</span><br><span class="line">        np.arange(H, dtype=np.float32) + pixel_center,</span><br><span class="line">        indexing=<span class="string">&#x27;xy&#x27;</span></span><br><span class="line">    )</span><br><span class="line">    i, j = torch.from_numpy(i), torch.from_numpy(j)</span><br><span class="line"></span><br><span class="line">    directions = torch.stack([(i - cx) / fx, -(j - cy) / fy, -torch.ones_like(i)], -<span class="number">1</span>) <span class="comment"># (H, W, 3)</span></span><br><span class="line">    <span class="comment"># 计算方式与NeRF相同</span></span><br><span class="line">    <span class="keyword">return</span> directions</span><br></pre></td></tr></table></figure><p>获取光线</p><ul><li>get_rays(directions, c2w, keepdim=False):</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_rays</span>(<span class="params">directions, c2w, keepdim=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># Rotate ray directions from camera coordinate to the world coordinate</span></span><br><span class="line">    <span class="comment"># rays_d = directions @ c2w[:, :3].T # (H, W, 3) # slow?</span></span><br><span class="line">    <span class="keyword">assert</span> directions.shape[-<span class="number">1</span>] == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> directions.ndim == <span class="number">2</span>: <span class="comment"># (N_rays, 3)</span></span><br><span class="line">        <span class="keyword">assert</span> c2w.ndim == <span class="number">3</span> <span class="comment"># (N_rays, 4, 4) / (1, 4, 4)</span></span><br><span class="line">        rays_d = (directions[:,<span class="literal">None</span>,:] * c2w[:,:<span class="number">3</span>,:<span class="number">3</span>]).<span class="built_in">sum</span>(-<span class="number">1</span>) <span class="comment"># (N_rays, 3)</span></span><br><span class="line">        rays_o = c2w[:,:,<span class="number">3</span>].expand(rays_d.shape)</span><br><span class="line">    <span class="keyword">elif</span> directions.ndim == <span class="number">3</span>: <span class="comment"># (H, W, 3)</span></span><br><span class="line">        <span class="keyword">if</span> c2w.ndim == <span class="number">2</span>: <span class="comment"># (4, 4)</span></span><br><span class="line">            rays_d = (directions[:,:,<span class="literal">None</span>,:] * c2w[<span class="literal">None</span>,<span class="literal">None</span>,:<span class="number">3</span>,:<span class="number">3</span>]).<span class="built_in">sum</span>(-<span class="number">1</span>) <span class="comment"># (H, W, 3)</span></span><br><span class="line">            rays_o = c2w[<span class="literal">None</span>,<span class="literal">None</span>,:,<span class="number">3</span>].expand(rays_d.shape)</span><br><span class="line">        <span class="keyword">elif</span> c2w.ndim == <span class="number">3</span>: <span class="comment"># (B, 4, 4)</span></span><br><span class="line">            rays_d = (directions[<span class="literal">None</span>,:,:,<span class="literal">None</span>,:] * c2w[:,<span class="literal">None</span>,<span class="literal">None</span>,:<span class="number">3</span>,:<span class="number">3</span>]).<span class="built_in">sum</span>(-<span class="number">1</span>) <span class="comment"># (B, H, W, 3)</span></span><br><span class="line">            rays_o = c2w[:,<span class="literal">None</span>,<span class="literal">None</span>,:,<span class="number">3</span>].expand(rays_d.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> keepdim:</span><br><span class="line">        rays_o, rays_d = rays_o.reshape(-<span class="number">1</span>, <span class="number">3</span>), rays_d.reshape(-<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rays_o, rays_d</span><br></pre></td></tr></table></figure><h2 id="utils"><a href="#utils" class="headerlink" title="utils"></a>utils</h2><p>分批使用func处理数据，并决定是否移动到cpu</p><ul><li><code>chunk_batch(func, chunk_size, move_to_cpu, *args, **kwargs)</code><ul><li>B = None</li><li>for arg in args<ul><li>if isinstance(arg, torch.Tensor):<ul><li><code>B = arg.shape[0]</code></li><li>break</li></ul></li></ul></li><li>out = defaultdict(list)  将字典中同个key的多个value构成一个列表<ul><li>ref: <a href="https://blog.csdn.net/weixin_38145317/article/details/93175217">(21条消息) python 字典defaultdict(list)_wanghua609的博客-CSDN博客</a></li></ul></li><li>out_type = None</li><li>for i in range(0, B, chunk_size):<ul><li><code>out_chunk = func(*[arg[i:i+chunk_size] if isinstance(arg, torch.Tensor) else arg for arg in args], **kwargs)</code><ul><li>使用func函数得到一批输出</li></ul></li><li>if out_chunk is None:<ul><li>continue</li></ul></li><li>out_type = type(out_chunk)</li><li>if isinstance(out_chunk, torch.Tensor): 将out_chunk 变为字典<ul><li>out_chunk = {0: out_chunk}</li></ul></li><li>elif isinstance(out_chunk, tuple) or isinstance(out_chunk, list):<ul><li>chunk_length = len(out_chunk)</li><li>out_chunk = {i: chunk for i, chunk in enumerate(out_chunk)}</li></ul></li><li>elif isinstance(out_chunk, dict):<ul><li>pass</li></ul></li><li>else:<ul><li><code>print(f&#39;Return value of func must be in type [torch.Tensor, list, tuple, dict], get &#123;type(out_chunk)&#125;.&#39;)</code></li><li>exit(1)</li></ul></li><li>for k, v in out_chunk.items():<ul><li>v = v if torch.is_grad_enabled() else v.detach()</li><li>v = v.cpu() if move_to_cpu else v</li><li><code>out[k].append(v)</code></li></ul></li></ul></li><li>if out_type is None:<ul><li>return</li></ul></li><li>out = {k: torch.cat(v, dim=0) for k, v in out.items()}</li><li>if out_type is torch.Tensor:<ul><li>return out[0]</li></ul></li><li><code>elif out_type in [tuple, list]</code>:<ul><li><code>return out_type([out[i] for i in range(chunk_length)])</code></li></ul></li><li>elif out_type is dict:<ul><li>return out</li></ul></li></ul></li></ul><p>将dat从inp缩放到tgt</p><ul><li>scale_anything(dat, inp_scale, tgt_scale): <ul><li>if inp_scale is None:<ul><li><code>inp_scale = [dat.min(), dat.max()]</code></li></ul></li><li><code>dat = (dat  - inp_scale[0]) / (inp_scale[1] - inp_scale[0])</code></li><li><code>dat = dat * (tgt_scale[1] - tgt_scale[0]) + tgt_scale[0]</code></li><li>return dat</li></ul></li></ul><p>激活函数</p><ul><li>get_activation(name)<ul><li>if name is None:<ul><li>return lambda x: x</li></ul></li><li>name = name.lower() # lower： 将所有大写字符转换为小写</li><li>if name == ‘none’:  return lambda x: x</li><li>if name.startswith(‘scale’): <ul><li>scale_factor = float(name[5:])</li><li>return lambda x: x.clamp(0., scale_factor) / scale_factor</li></ul></li><li>elif name.startswith(‘clamp’):<ul><li>clamp_max = float(name[5:])</li><li>return lambda x: x.clamp(0., clamp_max)</li></ul></li><li>elif name.startswith(‘mul’):<ul><li>mul_factor = float(name[3:])</li><li>return lambda x: x * mul_factor</li></ul></li><li>elif name == ‘lin2srgb’:<code>return lambda x: torch.where(x &gt; 0.0031308, torch.pow(torch.clamp(x, min=0.0031308), 1.0/2.4)*1.055 - 0.055, 12.92*x).clamp(0., 1.)</code></li><li>elif name == ‘trunc_exp’: return trunc_exp</li><li>elif name.startswith(‘+’) or name.startswith(‘-‘): return lambda x: x + float(name)</li><li>elif name == ‘sigmoid’:return lambda x: torch.sigmoid(x)</li><li>elif name == ‘tanh’: return lambda x: torch.tanh(x)</li><li>else:  return getattr(F, name)</li></ul></li></ul><h1 id="systems"><a href="#systems" class="headerlink" title="systems"></a>systems</h1><h2 id="init-2"><a href="#init-2" class="headerlink" title="init"></a>init</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">systems = &#123;&#125;</span><br><span class="line"></span><br><span class="line">def register(name):</span><br><span class="line">    def decorator(cls):</span><br><span class="line">        systems[name] = cls</span><br><span class="line">        return cls</span><br><span class="line">    return decorator</span><br><span class="line"></span><br><span class="line">def make(name, config, load_from_checkpoint=None):</span><br><span class="line">    if load_from_checkpoint is None:</span><br><span class="line">        system = systems[name](config)</span><br><span class="line">    else:</span><br><span class="line">        system = systems[name].load_from_checkpoint(load_from_checkpoint, strict=False, config=config)</span><br><span class="line">    return system</span><br><span class="line"></span><br><span class="line">from . import nerf, neus</span><br></pre></td></tr></table></figure><h2 id="base-1"><a href="#base-1" class="headerlink" title="base"></a>base</h2><ul><li>BaseSystem，继承pl.LightningModule和SaverMixin<ul><li><code>__init__</code><ul><li>self.config = config</li><li>self.rank = get_rank()</li><li>self.prepare()</li><li>self.model = models.make(self.config.model.name, self.config.model)</li></ul></li><li>prepare<ul><li>pass</li></ul></li><li>forward(self, batch)<ul><li>raise NotImplementedError</li></ul></li><li>C(self, value)</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">C():</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(value, <span class="built_in">int</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(value, <span class="built_in">float</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    value = config_to_primitive(value)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(value, <span class="built_in">list</span>):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">&#x27;Scalar specification only supports list, got&#x27;</span>, <span class="built_in">type</span>(value))</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(value) == <span class="number">3</span>:</span><br><span class="line">        value = [<span class="number">0</span>] + value</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(value) == <span class="number">4</span></span><br><span class="line">    start_step, start_value, end_value, end_step = value</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(end_step, <span class="built_in">int</span>):</span><br><span class="line">        current_step = self.global_step</span><br><span class="line">        value = start_value + (end_value - start_value) * <span class="built_in">max</span>(<span class="built_in">min</span>(<span class="number">1.0</span>, (current_step - start_step) / (end_step - start_step)), <span class="number">0.0</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(end_step, <span class="built_in">float</span>):</span><br><span class="line">        current_step = self.current_epoch</span><br><span class="line">        value = start_value + (end_value - start_value) * <span class="built_in">max</span>(<span class="built_in">min</span>(<span class="number">1.0</span>, (current_step - start_step) / (end_step - start_step)), <span class="number">0.0</span>)</span><br><span class="line"><span class="keyword">return</span> value</span><br></pre></td></tr></table></figure><ul><li>BaseSystem接上<ul><li>preprocess_data(self, batch, stage)<ul><li>pass</li></ul></li><li>on_train_batch_start(self, batch, batch_idx, unused=0)等pl.LightningModule规定的方法</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Implementing on_after_batch_transfer of DataModule does the same.</span></span><br><span class="line"><span class="string">But on_after_batch_transfer does not support DP.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">on_train_batch_start</span>(<span class="params">self, batch, batch_idx, unused=<span class="number">0</span></span>):</span><br><span class="line">    self.dataset = self.trainer.datamodule.train_dataloader().dataset</span><br><span class="line">    self.preprocess_data(batch, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    update_module_step(self.model, self.current_epoch, self.global_step)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">on_validation_batch_start</span>(<span class="params">self, batch, batch_idx, dataloader_idx</span>):</span><br><span class="line">    self.dataset = self.trainer.datamodule.val_dataloader().dataset</span><br><span class="line">    self.preprocess_data(batch, <span class="string">&#x27;validation&#x27;</span>)</span><br><span class="line">    update_module_step(self.model, self.current_epoch, self.global_step)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">on_test_batch_start</span>(<span class="params">self, batch, batch_idx, dataloader_idx</span>):</span><br><span class="line">    self.dataset = self.trainer.datamodule.test_dataloader().dataset</span><br><span class="line">    self.preprocess_data(batch, <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">    update_module_step(self.model, self.current_epoch, self.global_step)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">on_predict_batch_start</span>(<span class="params">self, batch, batch_idx, dataloader_idx</span>):</span><br><span class="line">    self.dataset = self.trainer.datamodule.predict_dataloader().dataset</span><br><span class="line">    self.preprocess_data(batch, <span class="string">&#x27;predict&#x27;</span>)</span><br><span class="line">    update_module_step(self.model, self.current_epoch, self.global_step)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validation_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validation_epoch_end</span>(<span class="params">self, out</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Gather metrics from all devices, compute mean.</span></span><br><span class="line"><span class="string">    Purge repeated results using data index.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_step</span>(<span class="params">self, batch, batch_idx</span>):        </span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_epoch_end</span>(<span class="params">self, out</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Gather metrics from all devices, compute mean.</span></span><br><span class="line"><span class="string">    Purge repeated results using data index.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">export</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    optim = parse_optimizer(self.config.system.optimizer, self.model)</span><br><span class="line">    ret = &#123;</span><br><span class="line">        <span class="string">&#x27;optimizer&#x27;</span>: optim,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;scheduler&#x27;</span> <span class="keyword">in</span> self.config.system:</span><br><span class="line">        ret.update(&#123;</span><br><span class="line">            <span class="string">&#x27;lr_scheduler&#x27;</span>: parse_scheduler(self.config.system.scheduler, optim),</span><br><span class="line">        &#125;)</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">in</span> systems/utils.py</span><br><span class="line"></span><br><span class="line">得到优化器optim</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_optimizer</span>(<span class="params">config, model</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(config, <span class="string">&#x27;params&#x27;</span>):</span><br><span class="line">        params = [&#123;<span class="string">&#x27;params&#x27;</span>: get_parameters(model, name), <span class="string">&#x27;name&#x27;</span>: name, **args&#125; <span class="keyword">for</span> name, args <span class="keyword">in</span> config.params.items()]</span><br><span class="line">        rank_zero_debug(<span class="string">&#x27;Specify optimizer params:&#x27;</span>, config.params)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        params = model.parameters()</span><br><span class="line">    <span class="keyword">if</span> config.name <span class="keyword">in</span> [<span class="string">&#x27;FusedAdam&#x27;</span>]:</span><br><span class="line">        <span class="keyword">import</span> apex</span><br><span class="line">        optim = <span class="built_in">getattr</span>(apex.optimizers, config.name)(params, **config.args)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        optim = <span class="built_in">getattr</span>(torch.optim, config.name)(params, **config.args)</span><br><span class="line">    <span class="keyword">return</span> optim</span><br></pre></td></tr></table></figure><h2 id="neus-1"><a href="#neus-1" class="headerlink" title="neus"></a>neus</h2><p>有两种在console上输出信息的方式：</p><ul><li>self.print: correctly handle progress bar</li><li>rank_zero_info: use the logging module</li></ul><p>@systems.register(‘neus-system’)</p><ul><li>NeuSSystem，继承BaseSystem<ul><li>prepare<ul><li>self.criterions = { ‘psnr’: PSNR()}</li><li><code>self.train_num_samples = self.config.model.train_num_rays * (self.config.model.num_samples_per_ray + self.config.model.get(&#39;num_samples_per_ray_bg&#39;, 0))</code><ul><li>训练采样数= 训练光线数x(每条光线上采样数+背景中每条光线采样数)</li></ul></li><li>self.train_num_rays = self.config.model.train_num_rays<ul><li>训练光线数 = config中训练光线数</li></ul></li></ul></li><li>forward(self, batch)<ul><li><code>return self.model(batch[&#39;rays&#39;])</code></li></ul></li><li>preprocess_data(self, batch, stage)<ul><li>stage: train<ul><li>if batch_image_sampling 随机抽train_num_rays张图片, 索引为index（随机多张图片中，每张图片随机选取一个像素生成光线）<ul><li>directions :(n_images, H, W, 3) —&gt; (train_num_rays, 3)</li><li>c2w：(n_images, 3, 4)  —&gt; (train_num_rays, 3, 4)</li><li>rays_o, rays_d : (train_num_rays, 3)</li><li>rgb: (n_images, H, W, 3) —&gt; (train_num_rays, 3)</li><li>fg_mask: (n_images, H, W) —&gt; (train_num_rays,)</li></ul></li><li>else 随机抽取一张图片，索引index长度为1（一张图片，随机选取多个像素生成光线）<ul><li>directions :(n_images, H, W, 3) —&gt; (train_num_rays, 3)</li><li>c2w：(n_images, 3, 4)  —&gt; (1, 3, 4)</li><li>rays_o, rays_d : (train_num_rays, 3)</li><li>rgb: (n_images, H, W, 3) —&gt; (train_num_rays, 3)</li><li>fg_mask: (n_images, H, W) —&gt; (train_num_rays,)</li></ul></li></ul></li><li>stage: val<ul><li><code>index = batch[&#39;index&#39;]</code></li><li>c2w: (n_images, 3, 4)  —&gt; (3, 4)</li><li>directions: (n_images, H, W, 3) —&gt; ( H, W, 3)</li><li>rays_o, rays_d : (H, W, 3)</li><li>rgb: (n_images, H, W, 3) —&gt; (len(index)xHxW,3)</li><li>fg_mask: (n_images, H, W, 3) —&gt; (len(index)xHxW)</li></ul></li><li>stage: test<ul><li><code>index = batch[&#39;index&#39;]</code></li><li>c2w: (n_test_traj_steps ,3,4) —&gt; (3,4)</li><li>directions: (H,W,3) —&gt; (H,W,3)</li><li>rays_o, rays_d : (H,W,3)</li><li>rgb: (n_test_traj_steps, H, W, 3) —&gt; (HxW , 3)</li><li>fg_mask : (n_test_traj_steps, H, W) —&gt; (HxW)</li></ul></li><li>rays将rays_o和归一化后的rays_d，cat起来</li><li>stage: train<ul><li>if bg_color: white<ul><li>model.bg_color = torch.ones((3,))</li></ul></li><li>if bg_color: random<ul><li>model.bg_color = torch.rand((3,))</li></ul></li><li>else: raise NotImplementedError</li></ul></li><li>stage: val, test<ul><li>model.bg_color = torch.ones((3,))</li></ul></li><li>if apply_mask:<ul><li><code>rgb = rgb * fg_mask[...,None] + model.bg_color * (1 - fg_mask[...,None])</code></li></ul></li><li>batch.update({‘rays’: rays, ‘rgb’: rgb, ‘fg_mask’: fg_mask})</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_data</span>(<span class="params">self, batch, stage</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;index&#x27;</span> <span class="keyword">in</span> batch: <span class="comment"># validation / testing</span></span><br><span class="line">        index = batch[<span class="string">&#x27;index&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> self.config.model.batch_image_sampling:</span><br><span class="line">            index = torch.randint(<span class="number">0</span>, <span class="built_in">len</span>(self.dataset.all_images), size=(self.train_num_rays,), device=self.dataset.all_images.device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            index = torch.randint(<span class="number">0</span>, <span class="built_in">len</span>(self.dataset.all_images), size=(<span class="number">1</span>,), device=self.dataset.all_images.device)</span><br><span class="line">    <span class="keyword">if</span> stage <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>]:</span><br><span class="line">        c2w = self.dataset.all_c2w[index]</span><br><span class="line">        x = torch.randint(</span><br><span class="line">            <span class="number">0</span>, self.dataset.w, size=(self.train_num_rays,), device=self.dataset.all_images.device</span><br><span class="line">        )</span><br><span class="line">        y = torch.randint(</span><br><span class="line">            <span class="number">0</span>, self.dataset.h, size=(self.train_num_rays,), device=self.dataset.all_images.device</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> self.dataset.directions.ndim == <span class="number">3</span>: <span class="comment"># (H, W, 3)</span></span><br><span class="line">            directions = self.dataset.directions[y, x]</span><br><span class="line">        <span class="keyword">elif</span> self.dataset.directions.ndim == <span class="number">4</span>: <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">            directions = self.dataset.directions[index, y, x]</span><br><span class="line">        rays_o, rays_d = get_rays(directions, c2w)</span><br><span class="line">        rgb = self.dataset.all_images[index, y, x].view(-<span class="number">1</span>, self.dataset.all_images.shape[-<span class="number">1</span>]).to(self.rank)</span><br><span class="line">        fg_mask = self.dataset.all_fg_masks[index, y, x].view(-<span class="number">1</span>).to(self.rank)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c2w = self.dataset.all_c2w[index][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> self.dataset.directions.ndim == <span class="number">3</span>: <span class="comment"># (H, W, 3)</span></span><br><span class="line">            directions = self.dataset.directions</span><br><span class="line">        <span class="keyword">elif</span> self.dataset.directions.ndim == <span class="number">4</span>: <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">            directions = self.dataset.directions[index][<span class="number">0</span>] </span><br><span class="line">        rays_o, rays_d = get_rays(directions, c2w)</span><br><span class="line">        rgb = self.dataset.all_images[index].view(-<span class="number">1</span>, self.dataset.all_images.shape[-<span class="number">1</span>]).to(self.rank)</span><br><span class="line">        fg_mask = self.dataset.all_fg_masks[index].view(-<span class="number">1</span>).to(self.rank)</span><br><span class="line"></span><br><span class="line">    rays = torch.cat([rays_o, F.normalize(rays_d, p=<span class="number">2</span>, dim=-<span class="number">1</span>)], dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> stage <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> self.config.model.background_color == <span class="string">&#x27;white&#x27;</span>:</span><br><span class="line">            self.model.background_color = torch.ones((<span class="number">3</span>,), dtype=torch.float32, device=self.rank)</span><br><span class="line">        <span class="keyword">elif</span> self.config.model.background_color == <span class="string">&#x27;random&#x27;</span>:</span><br><span class="line">            self.model.background_color = torch.rand((<span class="number">3</span>,), dtype=torch.float32, device=self.rank)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.model.background_color = torch.ones((<span class="number">3</span>,), dtype=torch.float32, device=self.rank)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> self.dataset.apply_mask:</span><br><span class="line">        rgb = rgb * fg_mask[...,<span class="literal">None</span>] + self.model.background_color * (<span class="number">1</span> - fg_mask[...,<span class="literal">None</span>])</span><br><span class="line">    </span><br><span class="line">    batch.update(&#123;</span><br><span class="line">        <span class="string">&#x27;rays&#x27;</span>: rays,</span><br><span class="line">        <span class="string">&#x27;rgb&#x27;</span>: rgb,</span><br><span class="line">        <span class="string">&#x27;fg_mask&#x27;</span>: fg_mask</span><br><span class="line">    &#125;)      </span><br></pre></td></tr></table></figure><ul><li>NeuSSystem接上<ul><li>training_step(self, batch, batch_idx)<ul><li><code>out = self(batch) = self.model(batch[&#39;rays&#39;])</code>(self()相当于执行forward)</li><li>loss = 0</li><li>if dynamic_ray_sampling 动态更新训练光线数<ul><li><code>train_num_rays = int(train_num_rays*(train_num_samples/out[&#39;num_samples_full&#39;].sum().item()))</code><ul><li>如果采样得到的总点数多了，则减少光线数，如果总点数少了，则增加光线数</li></ul></li><li><code>self.train_num_rays = min(int(self.train_num_rays * 0.9 + train_num_rays * 0.1), self.config.model.max_train_num_rays)</code><ul><li>最后训练光线数为，两者取最小：<code>原来num*0.9 + 更新后num * 0.1</code>与config.model.max_train_num_rays</li></ul></li></ul></li><li>loss_rgb_mse = F.mse_loss，log(loss_rgb_mse)，<code>loss+= loss_rgb_mse *lambda_rgb_mse</code><ul><li>render_color: <code>out[&#39;comp_rgb_full&#39;][out[&#39;rays_valid_full&#39;][...,0]]</code> </li><li>gt_color: <code>batch[&#39;rgb&#39;][out[&#39;rays_valid_full&#39;][...,0]]</code></li></ul></li><li>loss_rgb_l1 = F.l1_loss，log(loss_rgb_l1)，<code>loss+= loss_rgb_l1 *lambda_rgb_l1</code><ul><li><code>out[&#39;comp_rgb_full&#39;][out[&#39;rays_valid_full&#39;][...,0]]</code></li><li><code>batch[&#39;rgb&#39;][out[&#39;rays_valid_full&#39;][...,0]]</code></li></ul></li><li>loss_eikonal，log(loss_eikonal)，<code>loss+= loss_eikonal *lambda_eikonal</code><ul><li><code>((torch.linalg.norm(out[&#39;sdf_grad_samples&#39;], ord=2, dim=-1) - 1.)**2).mean()</code></li></ul></li><li>loss_mask，log(loss_mask)，<code>loss+= loss_mask *lambda_mask</code><ul><li>opacity.clamp(1.e-3, 1.-1.e-3)</li><li><code>binary_cross_entropy(opacity, batch[&#39;fg_mask&#39;].float())</code></li></ul></li><li>loss_opaque，log(loss_opaque)，<code>loss+= loss_opaque *lambda_opaque</code><ul><li>binary_cross_entropy(opacity, opacity)</li></ul></li><li>loss_sparsity，log(loss_sparsity)，<code>loss+= loss_sparsity *lambda_sparsity</code><ul><li><code>torch.exp(-self.config.system.loss.sparsity_scale * out[&#39;sdf_samples&#39;].abs()).mean()</code></li><li>$\frac{1}{n.samples} \sum e^{-sparsity.scle \cdot |sdf|}$</li></ul></li><li>if lambda_distortion&gt;0<ul><li>loss_distortion，log(loss_distortion)，<code>loss+= loss_distortion *lambda_distortion</code><ul><li><code>flatten_eff_distloss(out[&#39;weights&#39;], out[&#39;points&#39;], out[&#39;intervals&#39;], out[&#39;ray_indices&#39;])</code></li></ul></li></ul></li><li>if learned_background and lambda_distortion_bg&gt;0<ul><li>loss_distortion_bg，log(loss_distortion_bg)，<code>loss+= loss_distortion_bg *lambda_distortion_bg</code><ul><li><code>flatten_eff_distloss(out[&#39;weights_bg&#39;], out[&#39;points_bg&#39;], out[&#39;intervals_bg&#39;], out[&#39;ray_indices_bg&#39;])</code></li></ul></li></ul></li><li>losses_model_reg = self.model.regularizations(out)</li><li>for name, value in losses_model_reg.items():<ul><li>self.log(f’train/loss_{name}’, value)</li><li>loss_ = value * self.C(self.config.system.loss[f”lambda_{name}”])</li><li>loss += loss_</li></ul></li><li>self.log(‘train/inv_s’, out[‘inv_s’], prog_bar=True)</li><li>for name, value in self.config.system.loss.items():</li><li>if name.startswith(‘lambda’):<ul><li>self.log(f’train_params/{name}’, self.C(value))</li></ul></li><li>self.log(‘train/num_rays’, float(self.train_num_rays), prog_bar=True)</li><li>return {‘loss’ : loss}</li></ul></li></ul></li></ul><p>loggers:</p><p><code>Epoch 0: : 29159it [30:20, 16.02it/s, loss=0.0265, train/inv_s=145.0, train/num_rays=1739.0, val/psnr=23.30]</code></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230714161558.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230714161756.png" alt="image.png"></p><ul><li>NeuSSystem接上<ul><li>validation_step(self, batch, batch_idx)<ul><li>out = self(batch)</li><li><code>psnr = self.criterions[&#39;psnr&#39;](out[&#39;comp_rgb_full&#39;].to(batch[&#39;rgb&#39;]), batch[&#39;rgb&#39;])</code></li><li>W, H = self.dataset.img_wh</li><li>self.save_image_grid</li><li>return {‘psnr’: psnr,’index’: batch[‘index’]}</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230714162151.png" alt="image.png"></p><ul><li>NeuSSystem接上<ul><li>validation_epoch_end(self,out)<ul><li>out = self.all_gather(out) 将所有数据类型的输出拼接起来<code>Union[Tensor, Dict, List, Tuple]</code></li><li>if self.trainer.is_global_zero: <ul><li>out_set = {}</li><li>for step_out in out:<ul><li>DP:<code>if step_out[&#39;index&#39;].ndim == 1: out_set[step_out[&#39;index&#39;].item()] = &#123;&#39;psnr&#39;: step_out[&#39;psnr&#39;]&#125;</code><ul><li>ref: 单机vs多机<a href="https://zhuanlan.zhihu.com/p/356967195">pytorch中的分布式训练之DP VS DDP - 知乎 (zhihu.com)</a></li></ul></li><li>DDP: <code>for oi, index in enumerate(step_out[&#39;index&#39;]):</code><ul><li><code>out_set[index[0].item()] = &#123;&#39;psnr&#39;: step_out[&#39;psnr&#39;][oi]&#125;</code></li></ul></li></ul></li><li>psnr = $\frac{1}{len(index)}\sum psnr_{i}$</li><li>self.log(psnr)</li></ul></li></ul></li><li>test_step(self, batch, batch_idx)<ul><li>out = self(batch)</li><li><code>psnr = self.criterions[&#39;psnr&#39;](out[&#39;comp_rgb_full&#39;].to(batch[&#39;rgb&#39;]), batch[&#39;rgb&#39;])</code></li><li>W, H = self.dataset.img_wh</li><li>self.save_image_grid<ul><li>由于测试时，采用的相机位姿是未知的新视点，因此在image生成时，<code>batch[&#39;rgb&#39;]</code>即gt图为zero(黑色)</li></ul></li><li>return {‘psnr’: psnr,’index’: batch[‘index’]}</li></ul></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230714212151.png" alt="image.png"></p><ul><li>NeuSSystem接上<ul><li>test_epoch_end(self,out)<ul><li>同validation<ul><li>psnr = $\frac{1}{len(index)}\sum psnr_{i}$</li><li>self.log(psnr)</li></ul></li><li>self.save_img_sequence()</li><li>self.export</li></ul></li><li>export<ul><li>mesh = self.model.export(self.config.export)</li><li>self.save_mesh() </li></ul></li></ul></li></ul><h2 id="criterions"><a href="#criterions" class="headerlink" title="criterions"></a>criterions</h2><ul><li>PSNR，继承nn.Module<ul><li>forward(self, inputs , targets, valid_mask= None, reduction= ‘mean’)<ul><li>assert reduction in [‘mean’, ‘none’]</li><li><code>value = (inputs - targets)**2</code>，即$v = (inputs - targets)^{2}$</li><li>if valid_mask is not None:<ul><li>value = value[valid_mask]</li></ul></li><li>if reduction == ‘mean’:<ul><li>return -10 * torch.log10(torch.mean(value))</li><li>$psnr = 10 \cdot log_{10}(\frac{1}{N} \sum v)$</li></ul></li><li>elif reduction == ‘none’:<ul><li>return -10 * torch.log10(torch.mean(value, dim=tuple(range(value.ndim)[1:])))</li></ul></li></ul></li></ul></li></ul><h2 id="utils-1"><a href="#utils-1" class="headerlink" title="utils"></a>utils</h2><ul><li>ChainedScheduler</li><li>SequentialLR</li><li>ConstantLR</li><li>LinearLR</li><li>get_scheduler</li><li>getattr_recursive</li><li>get_parameters</li><li>parse_optimizer</li><li>parse_scheduler</li><li>update_module_step(m,epoch,global_step)<ul><li>if hasattr(m,’update_step’) 如果m中有update_step这个属性or方法<ul><li>m.update_step(epoch,global_step) 则执行m.update_step(epoch,global_step)</li></ul></li><li>如果m中没有update_step，则不执行操作</li></ul></li></ul><h1 id="utils-2"><a href="#utils-2" class="headerlink" title="utils"></a>utils</h1><h2 id="mixins"><a href="#mixins" class="headerlink" title="mixins"></a>mixins</h2><p>class SaverMixin(): 被systems中的BaseSystem继承</p><ul><li>get_save_path(self,filename)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_save_path</span>(<span class="params">self, filename</span>):</span><br><span class="line">    save_path = os.path.join(self.save_dir, filename)</span><br><span class="line">    os.makedirs(os.path.dirname(save_path), exist_ok=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> save_path</span><br></pre></td></tr></table></figure><ul><li>save_image_grid(self, filename, imgs)<ul><li>img = self.get_image_grid_(imgs)</li><li>cv2.imwrite(self.get_save_path(filename),img)</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">in</span> val step:</span><br><span class="line">self.save_image_grid(<span class="string">f&quot;it<span class="subst">&#123;self.global_step&#125;</span>-<span class="subst">&#123;batch[<span class="string">&#x27;index&#x27;</span>][<span class="number">0</span>].item()&#125;</span>.png&quot;</span>, [</span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;rgb&#x27;</span>, <span class="string">&#x27;img&#x27;</span>: batch[<span class="string">&#x27;rgb&#x27;</span>].view(H, W, <span class="number">3</span>), <span class="string">&#x27;kwargs&#x27;</span>: &#123;<span class="string">&#x27;data_format&#x27;</span>: <span class="string">&#x27;HWC&#x27;</span>&#125;&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;rgb&#x27;</span>, <span class="string">&#x27;img&#x27;</span>: out[<span class="string">&#x27;comp_rgb_full&#x27;</span>].view(H, W, <span class="number">3</span>), <span class="string">&#x27;kwargs&#x27;</span>: &#123;<span class="string">&#x27;data_format&#x27;</span>: <span class="string">&#x27;HWC&#x27;</span>&#125;&#125;</span><br><span class="line">] + ([</span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;rgb&#x27;</span>, <span class="string">&#x27;img&#x27;</span>: out[<span class="string">&#x27;comp_rgb_bg&#x27;</span>].view(H, W, <span class="number">3</span>), <span class="string">&#x27;kwargs&#x27;</span>: &#123;<span class="string">&#x27;data_format&#x27;</span>: <span class="string">&#x27;HWC&#x27;</span>&#125;&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;rgb&#x27;</span>, <span class="string">&#x27;img&#x27;</span>: out[<span class="string">&#x27;comp_rgb&#x27;</span>].view(H, W, <span class="number">3</span>), <span class="string">&#x27;kwargs&#x27;</span>: &#123;<span class="string">&#x27;data_format&#x27;</span>: <span class="string">&#x27;HWC&#x27;</span>&#125;&#125;,</span><br><span class="line">] <span class="keyword">if</span> self.config.model.learned_background <span class="keyword">else</span> []) + [</span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;grayscale&#x27;</span>, <span class="string">&#x27;img&#x27;</span>: out[<span class="string">&#x27;depth&#x27;</span>].view(H, W), <span class="string">&#x27;kwargs&#x27;</span>: &#123;&#125;&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;rgb&#x27;</span>, <span class="string">&#x27;img&#x27;</span>: out[<span class="string">&#x27;comp_normal&#x27;</span>].view(H, W, <span class="number">3</span>), <span class="string">&#x27;kwargs&#x27;</span>: &#123;<span class="string">&#x27;data_format&#x27;</span>: <span class="string">&#x27;HWC&#x27;</span>, <span class="string">&#x27;data_range&#x27;</span>: (-<span class="number">1</span>, <span class="number">1</span>)&#125;&#125;</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">in</span> test_step:</span><br><span class="line">self.save_image_grid(<span class="string">f&quot;it<span class="subst">&#123;self.global_step&#125;</span>-test/<span class="subst">&#123;batch[<span class="string">&#x27;index&#x27;</span>][<span class="number">0</span>].item()&#125;</span>.png&quot;</span>, [</span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;rgb&#x27;</span>, <span class="string">&#x27;img&#x27;</span>: batch[<span class="string">&#x27;rgb&#x27;</span>].view(H, W, <span class="number">3</span>), <span class="string">&#x27;kwargs&#x27;</span>: &#123;<span class="string">&#x27;data_format&#x27;</span>: <span class="string">&#x27;HWC&#x27;</span>&#125;&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;rgb&#x27;</span>, <span class="string">&#x27;img&#x27;</span>: out[<span class="string">&#x27;comp_rgb_full&#x27;</span>].view(H, W, <span class="number">3</span>), <span class="string">&#x27;kwargs&#x27;</span>: &#123;<span class="string">&#x27;data_format&#x27;</span>: <span class="string">&#x27;HWC&#x27;</span>&#125;&#125;</span><br><span class="line">] + ([</span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;rgb&#x27;</span>, <span class="string">&#x27;img&#x27;</span>: out[<span class="string">&#x27;comp_rgb_bg&#x27;</span>].view(H, W, <span class="number">3</span>), <span class="string">&#x27;kwargs&#x27;</span>: &#123;<span class="string">&#x27;data_format&#x27;</span>: <span class="string">&#x27;HWC&#x27;</span>&#125;&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;rgb&#x27;</span>, <span class="string">&#x27;img&#x27;</span>: out[<span class="string">&#x27;comp_rgb&#x27;</span>].view(H, W, <span class="number">3</span>), <span class="string">&#x27;kwargs&#x27;</span>: &#123;<span class="string">&#x27;data_format&#x27;</span>: <span class="string">&#x27;HWC&#x27;</span>&#125;&#125;,</span><br><span class="line">] <span class="keyword">if</span> self.config.model.learned_background <span class="keyword">else</span> []) + [</span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;grayscale&#x27;</span>, <span class="string">&#x27;img&#x27;</span>: out[<span class="string">&#x27;depth&#x27;</span>].view(H, W), <span class="string">&#x27;kwargs&#x27;</span>: &#123;&#125;&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;rgb&#x27;</span>, <span class="string">&#x27;img&#x27;</span>: out[<span class="string">&#x27;comp_normal&#x27;</span>].view(H, W, <span class="number">3</span>), <span class="string">&#x27;kwargs&#x27;</span>: &#123;<span class="string">&#x27;data_format&#x27;</span>: <span class="string">&#x27;HWC&#x27;</span>, <span class="string">&#x27;data_range&#x27;</span>: (-<span class="number">1</span>, <span class="number">1</span>)&#125;&#125;</span><br><span class="line">])</span><br></pre></td></tr></table></figure><ul><li>get_image_grid_(self, imgs)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_image_grid_</span>(<span class="params">self, imgs</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(imgs[<span class="number">0</span>], <span class="built_in">list</span>):</span><br><span class="line">        <span class="keyword">return</span> np.concatenate([self.get_image_grid_(row) <span class="keyword">for</span> row <span class="keyword">in</span> imgs], axis=<span class="number">0</span>)</span><br><span class="line">    cols = []</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> imgs:</span><br><span class="line">        <span class="keyword">assert</span> col[<span class="string">&#x27;type&#x27;</span>] <span class="keyword">in</span> [<span class="string">&#x27;rgb&#x27;</span>, <span class="string">&#x27;uv&#x27;</span>, <span class="string">&#x27;grayscale&#x27;</span>]</span><br><span class="line">        <span class="keyword">if</span> col[<span class="string">&#x27;type&#x27;</span>] == <span class="string">&#x27;rgb&#x27;</span>:</span><br><span class="line">            rgb_kwargs = self.DEFAULT_RGB_KWARGS.copy()</span><br><span class="line">            rgb_kwargs.update(col[<span class="string">&#x27;kwargs&#x27;</span>])</span><br><span class="line">            cols.append(self.get_rgb_image_(col[<span class="string">&#x27;img&#x27;</span>], **rgb_kwargs))</span><br><span class="line">        <span class="keyword">elif</span> col[<span class="string">&#x27;type&#x27;</span>] == <span class="string">&#x27;uv&#x27;</span>:</span><br><span class="line">            uv_kwargs = self.DEFAULT_UV_KWARGS.copy()</span><br><span class="line">            uv_kwargs.update(col[<span class="string">&#x27;kwargs&#x27;</span>])</span><br><span class="line">            cols.append(self.get_uv_image_(col[<span class="string">&#x27;img&#x27;</span>], **uv_kwargs))</span><br><span class="line">        <span class="keyword">elif</span> col[<span class="string">&#x27;type&#x27;</span>] == <span class="string">&#x27;grayscale&#x27;</span>:</span><br><span class="line">            grayscale_kwargs = self.DEFAULT_GRAYSCALE_KWARGS.copy()</span><br><span class="line">            grayscale_kwargs.update(col[<span class="string">&#x27;kwargs&#x27;</span>])</span><br><span class="line">            cols.append(self.get_grayscale_image_(col[<span class="string">&#x27;img&#x27;</span>], **grayscale_kwargs))</span><br><span class="line">    <span class="keyword">return</span> np.concatenate(cols, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">DEFAULT_RGB_KWARGS = &#123;<span class="string">&#x27;data_format&#x27;</span>: <span class="string">&#x27;CHW&#x27;</span>, <span class="string">&#x27;data_range&#x27;</span>: (<span class="number">0</span>, <span class="number">1</span>)&#125;</span><br><span class="line">DEFAULT_UV_KWARGS = &#123;<span class="string">&#x27;data_format&#x27;</span>: <span class="string">&#x27;CHW&#x27;</span>, <span class="string">&#x27;data_range&#x27;</span>: (<span class="number">0</span>, <span class="number">1</span>), <span class="string">&#x27;cmap&#x27;</span>: <span class="string">&#x27;checkerboard&#x27;</span>&#125;</span><br><span class="line">DEFAULT_GRAYSCALE_KWARGS = &#123;<span class="string">&#x27;data_range&#x27;</span>: <span class="literal">None</span>, <span class="string">&#x27;cmap&#x27;</span>: <span class="string">&#x27;jet&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><ul><li>get_rgb_image_(self, img, data_format, data_range))</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_rgb_image_</span>(<span class="params">self, img, data_format, data_range</span>):</span><br><span class="line">    img = self.convert_data(img)</span><br><span class="line">    <span class="keyword">assert</span> data_format <span class="keyword">in</span> [<span class="string">&#x27;CHW&#x27;</span>, <span class="string">&#x27;HWC&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> data_format == <span class="string">&#x27;CHW&#x27;</span>:</span><br><span class="line">        img = img.transpose(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)</span><br><span class="line">    img = img.clip(<span class="built_in">min</span>=data_range[<span class="number">0</span>], <span class="built_in">max</span>=data_range[<span class="number">1</span>])</span><br><span class="line">    img = ((img - data_range[<span class="number">0</span>]) / (data_range[<span class="number">1</span>] - data_range[<span class="number">0</span>]) * <span class="number">255.</span>).astype(np.uint8)</span><br><span class="line">    imgs = [img[...,start:start+<span class="number">3</span>] <span class="keyword">for</span> start <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, img.shape[-<span class="number">1</span>], <span class="number">3</span>)]</span><br><span class="line">    imgs = [img_ <span class="keyword">if</span> img_.shape[-<span class="number">1</span>] == <span class="number">3</span> <span class="keyword">else</span> np.concatenate([img_, np.zeros((img_.shape[<span class="number">0</span>], img_.shape[<span class="number">1</span>], <span class="number">3</span> - img_.shape[<span class="number">2</span>]), dtype=img_.dtype)], axis=-<span class="number">1</span>) <span class="keyword">for</span> img_ <span class="keyword">in</span> imgs]</span><br><span class="line">    img = np.concatenate(imgs, axis=<span class="number">1</span>)</span><br><span class="line">    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure><ul><li>get_grayscale_image_(self, img, data_range , cmap)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_grayscale_image_</span>(<span class="params">self, img, data_range, cmap</span>):</span><br><span class="line">    img = self.convert_data(img)</span><br><span class="line">    img = np.nan_to_num(img)</span><br><span class="line">    <span class="keyword">if</span> data_range <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        img = (img - img.<span class="built_in">min</span>()) / (img.<span class="built_in">max</span>() - img.<span class="built_in">min</span>())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img = img.clip(data_range[<span class="number">0</span>], data_range[<span class="number">1</span>])</span><br><span class="line">        img = (img - data_range[<span class="number">0</span>]) / (data_range[<span class="number">1</span>] - data_range[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">assert</span> cmap <span class="keyword">in</span> [<span class="literal">None</span>, <span class="string">&#x27;jet&#x27;</span>, <span class="string">&#x27;magma&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> cmap == <span class="literal">None</span>:</span><br><span class="line">        img = (img * <span class="number">255.</span>).astype(np.uint8)</span><br><span class="line">        img = np.repeat(img[...,<span class="literal">None</span>], <span class="number">3</span>, axis=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">elif</span> cmap == <span class="string">&#x27;jet&#x27;</span>:</span><br><span class="line">        img = (img * <span class="number">255.</span>).astype(np.uint8)</span><br><span class="line">        img = cv2.applyColorMap(img, cv2.COLORMAP_JET)</span><br><span class="line">    <span class="keyword">elif</span> cmap == <span class="string">&#x27;magma&#x27;</span>:</span><br><span class="line">        img = <span class="number">1.</span> - img</span><br><span class="line">        base = cm.get_cmap(<span class="string">&#x27;magma&#x27;</span>)</span><br><span class="line">        num_bins = <span class="number">256</span></span><br><span class="line">        colormap = LinearSegmentedColormap.from_list(</span><br><span class="line">            <span class="string">f&quot;<span class="subst">&#123;base.name&#125;</span><span class="subst">&#123;num_bins&#125;</span>&quot;</span>,</span><br><span class="line">            base(np.linspace(<span class="number">0</span>, <span class="number">1</span>, num_bins)),</span><br><span class="line">            num_bins</span><br><span class="line">        )(np.linspace(<span class="number">0</span>, <span class="number">1</span>, num_bins))[:,:<span class="number">3</span>]</span><br><span class="line">        a = np.floor(img * <span class="number">255.</span>)</span><br><span class="line">        b = (a + <span class="number">1</span>).clip(<span class="built_in">max</span>=<span class="number">255.</span>)</span><br><span class="line">        f = img * <span class="number">255.</span> - a</span><br><span class="line">        a = a.astype(np.uint16).clip(<span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">        b = b.astype(np.uint16).clip(<span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">        img = colormap[a] + (colormap[b] - colormap[a]) * f[...,<span class="literal">None</span>]</span><br><span class="line">        img = (img * <span class="number">255.</span>).astype(np.uint8)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure><ul><li>convert_data(self, data)，将输入的数据转化成ndarry类型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convert_data</span>(<span class="params">self, data</span>): <span class="comment"># isinstance 判断一个对象是否是一个已知的类型</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, np.ndarray):</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, torch.Tensor):</span><br><span class="line">        <span class="keyword">return</span> data.cpu().numpy()</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, <span class="built_in">list</span>):</span><br><span class="line">        <span class="keyword">return</span> [self.convert_data(d) <span class="keyword">for</span> d <span class="keyword">in</span> data]</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, <span class="built_in">dict</span>):</span><br><span class="line">        <span class="keyword">return</span> &#123;k: self.convert_data(v) <span class="keyword">for</span> k, v <span class="keyword">in</span> data.items()&#125;</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">&#x27;Data must be in type numpy.ndarray, torch.Tensor, list or dict, getting&#x27;</span>, <span class="built_in">type</span>(data))</span><br></pre></td></tr></table></figure><ul><li>save_img_sequence(self, filename, img_dir, matcher, save_format=’gif’, fps=30)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">in</span> test step</span><br><span class="line">self.save_img_sequence(</span><br><span class="line">    <span class="string">f&quot;it<span class="subst">&#123;self.global_step&#125;</span>-test&quot;</span>, <span class="comment"># mp4 or gif文件名</span></span><br><span class="line">    <span class="string">f&quot;it<span class="subst">&#123;self.global_step&#125;</span>-test&quot;</span>, <span class="comment"># test生成的图片保存目录</span></span><br><span class="line">    <span class="string">&#x27;(\d+)\.png&#x27;</span>,</span><br><span class="line">    save_format=<span class="string">&#x27;mp4&#x27;</span>,</span><br><span class="line">    fps=<span class="number">30</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_img_sequence</span>(<span class="params">self, filename, img_dir, matcher, save_format=<span class="string">&#x27;gif&#x27;</span>, fps=<span class="number">30</span></span>):</span><br><span class="line">    <span class="keyword">assert</span> save_format <span class="keyword">in</span> [<span class="string">&#x27;gif&#x27;</span>, <span class="string">&#x27;mp4&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> filename.endswith(save_format):</span><br><span class="line">        filename += <span class="string">f&quot;.<span class="subst">&#123;save_format&#125;</span>&quot;</span></span><br><span class="line">    matcher = re.<span class="built_in">compile</span>(matcher)</span><br><span class="line">    img_dir = os.path.join(self.save_dir, img_dir)</span><br><span class="line">    imgs = []</span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(img_dir):</span><br><span class="line">        <span class="keyword">if</span> matcher.search(f):</span><br><span class="line">            imgs.append(f)</span><br><span class="line">    imgs = <span class="built_in">sorted</span>(imgs, key=<span class="keyword">lambda</span> f: <span class="built_in">int</span>(matcher.search(f).groups()[<span class="number">0</span>]))</span><br><span class="line">    imgs = [cv2.imread(os.path.join(img_dir, f)) <span class="keyword">for</span> f <span class="keyword">in</span> imgs]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> save_format == <span class="string">&#x27;gif&#x27;</span>:</span><br><span class="line">        imgs = [cv2.cvtColor(i, cv2.COLOR_BGR2RGB) <span class="keyword">for</span> i <span class="keyword">in</span> imgs]</span><br><span class="line">        imageio.mimsave(self.get_save_path(filename), imgs, fps=fps, palettesize=<span class="number">256</span>)</span><br><span class="line">    <span class="keyword">elif</span> save_format == <span class="string">&#x27;mp4&#x27;</span>:</span><br><span class="line">        imgs = [cv2.cvtColor(i, cv2.COLOR_BGR2RGB) <span class="keyword">for</span> i <span class="keyword">in</span> imgs]</span><br><span class="line">        imageio.mimsave(self.get_save_path(filename), imgs, fps=fps)</span><br></pre></td></tr></table></figure><ul><li>save_mesh()</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">in</span> export: </span><br><span class="line">self.save_mesh(</span><br><span class="line">    <span class="string">f&quot;it<span class="subst">&#123;self.global_step&#125;</span>-<span class="subst">&#123;self.config.model.geometry.isosurface.method&#125;</span><span class="subst">&#123;self.config.model.geometry.isosurface.resolution&#125;</span>.obj&quot;</span>,</span><br><span class="line">    **mesh</span><br><span class="line">)        </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_mesh</span>(<span class="params">self, filename, v_pos, t_pos_idx, v_tex=<span class="literal">None</span>, t_tex_idx=<span class="literal">None</span>, v_rgb=<span class="literal">None</span></span>):</span><br><span class="line">    v_pos, t_pos_idx = self.convert_data(v_pos), self.convert_data(t_pos_idx)</span><br><span class="line">    <span class="keyword">if</span> v_rgb <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        v_rgb = self.convert_data(v_rgb) <span class="comment"># 转为numpy</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> trimesh</span><br><span class="line">    mesh = trimesh.Trimesh(</span><br><span class="line">        vertices=v_pos,</span><br><span class="line">        faces=t_pos_idx,</span><br><span class="line">        vertex_colors=v_rgb</span><br><span class="line">    )</span><br><span class="line">    mesh.export(self.get_save_path(filename))</span><br></pre></td></tr></table></figure><p>obj文件：</p><div class="note info">            <p>可以看出最后生成的模型在一个半径为1的单位圆中</p>          </div><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 每个点的位置值和颜色值</span><br><span class="line">v -0.96953946 0.71037185 0.47863841 0.78431373 0.56470588 0.34117647</span><br><span class="line">v -0.96868885 0.70891666 0.47863841 0.97647059 0.86666667 0.66666667</span><br><span class="line">v -0.96868885 0.71037185 0.47713959 0.74901961 0.54509804 0.37647059</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"># 每个三角面的三个顶点的索引</span><br><span class="line">f 2370 2366 2270</span><br><span class="line">f 2366 2265 2270</span><br><span class="line">f 2374 2372 2373</span><br><span class="line">...</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Efficiency </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Neus </tag>
            
            <tag> Code </tag>
            
            <tag> Efficiency </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习基础</title>
      <link href="/Learn/Learn-DeepLearning/"/>
      <url>/Learn/Learn-DeepLearning/</url>
      
        <content type="html"><![CDATA[<p>学习深度学习过程中的一些基础知识</p><div class="table-container"><table><thead><tr><th>DL</th><th>修仙.炼丹</th><th>example</th></tr></thead><tbody><tr><td>框架</td><td>丹炉</td><td>PyTorch</td></tr><tr><td>网络</td><td>丹方.灵阵</td><td>CNN</td></tr><tr><td>数据集</td><td>灵材</td><td>MNIST</td></tr><tr><td>GPU</td><td>真火</td><td>NVIDIA</td></tr><tr><td>模型</td><td>成丹</td><td>.ckpt</td></tr></tbody></table></div><blockquote><p><a href="https://zhuanlan.zhihu.com/p/23781756">深度学习·炼丹入门 - 知乎 (zhihu.com)</a></p></blockquote><span id="more"></span><p>学习顺序MLP-RNN-seq2seq/编码器解码器架构-注意力机制-自注意力-transformer。</p><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h2><blockquote><p><a href="https://nn.labml.ai/unet/index.html">U-Net (labml.ai)</a></p></blockquote><p>图像分割</p><h1 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h1><p>损失函数在训练过程中，突然变得很大或者nan<br>添加 torch.cuda.amp.GradScaler() 解决 loss为nan或inf的问题</p><h1 id="神经网络MLP"><a href="#神经网络MLP" class="headerlink" title="神经网络MLP"></a>神经网络MLP</h1><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>根据每层的输入、权重weight和偏置bias，求出该层的输出，然后经过激活函数。按此一层一层传递，最终获得输出层的输出。</p><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><blockquote><p><a href="https://www.cnblogs.com/jsfantasy/p/12177275.html">神经网络之反向传播算法（BP）公式推导（超详细） - jsfantasy - 博客园 (cnblogs.com)</a></p></blockquote><p>假如激活函数为sigmoid函数：$\sigma(x) = \frac{1}{1+e^{-x}}$<br>sigmoid的导数为：$\frac{d}{dx}\sigma(x) = \frac{d}{dx} \left(\frac{1}{1+e^{-x}} \right)= \sigma(1-\sigma)$</p><p>因此当损失函数对权重求导，其结果与sigmoid的输出相关</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230702194201.png" alt="image.png"></p><ul><li>o代表输出，上标表示当前的层数，下标表示当前层数的第几号输出</li><li>z代表求和结果，即sigmoid的输入</li><li>权重$w^{J}_{ij}$的上标表示权值所属的层数，下标表示从I层的第i号节点到J层的第j号节点</li></ul><p>输出对J层的权重$w_{ij}$求导： </p><script type="math/tex; mode=display">\begin{align*}\frac{\partial L}{\partial w_{ij}} &=\frac{\partial}{\partial w_{ij}}\frac{1}{2}\sum_{k}(o_{k}-t_{k})^{2} \\&= \sum_k(o_k-t_k)\frac{\partial o_k}{\partial w_{ij}}\\&= \sum_k(o_k-t_k)\frac{\partial \sigma(z_k)}{\partial w_{ij}}\\&= \sum_k(o_k-t_k)o_k(1-o_k)\frac{\partial z_k}{\partial w_{ij}}\\&= \sum_k(o_k-t_k)o_k(1-o_k)w_{jk}\cdot\frac{\partial o_j}{\partial w_{ij}}\end{align*}</script><p>$\frac{\partial z_k}{\partial w_{ij}} = \frac{\partial z_k}{o_j}\cdot \frac{\partial o_j}{\partial w_{ij}} = w_{jk} \cdot \frac{\partial o_j}{\partial w_{ij}}$, because $z_k = o_j \cdot w_{jk} + b_k$</p><p>and $\frac{\partial z_j}{\partial w_{ij}} = o_i \left(z_j = o_i\cdot w_{ij} + b_j\right)$</p><script type="math/tex; mode=display">\begin{align*}\frac{\partial L}{\partial w_{ij}} &= \sum_k(o_k-t_k)o_k(1-o_k)w_{jk}\cdot\frac{\partial o_j}{\partial w_{ij}}\\&= \frac{\partial o_j}{\partial w_{ij}}\cdot\sum_k(o_k-t_k)o_k(1-o_k)w_{jk}\\&= o_j(1-o_j)\frac{\partial z_j}{\partial w_{ij}} \cdot\sum_k(o_k-t_k)o_k(1-o_k)w_{jk}\\&= o_j(1-o_j)o_i \cdot\sum_k(o_k-t_k)o_k(1-o_k)w_{jk}\\&= o_j(1-o_j)o_i \cdot\sum_k\delta _k^K\cdot w_{jk}\\&= \delta_j^J\cdot o_i^I\end{align*}</script><p>其中 $\delta_j^J = o_j(1-o_j) \cdot \sum_k \delta _k^K\cdot w_{jk}$</p><p>推广：</p><ul><li>输出层：$\frac{\partial L}{\partial w_{jk}} = \delta _k^K\cdot o_j$ ,其中$\delta _k^K = (o_k-t_k)o_k(1-o_k)$</li><li>倒二层：$\frac{\partial L}{\partial w_{ij}} = \delta _j^J\cdot o_i$ ,其中$\delta_j^J = o_j(1-o_j) \cdot \sum_k \delta _k^K\cdot w_{jk}$</li><li>倒三层：$\frac{\partial L}{\partial w_{ni}} = \delta _i^I\cdot o_n$ ,其中$\delta_i^I = o_i(1-o_i)\cdot \sum_j\delta_j^J\cdot w_{ij}$<ul><li>$o_n$ 为倒三层输入，即倒四层的输出</li></ul></li></ul><p>根据每一层的输入或输出，以及真实值，即可计算loss对每个权重参数的导数</p><h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h2><blockquote><p><a href="https://zh.d2l.ai/chapter_optimization/optimization-intro.html">11.1. 优化和深度学习 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p></blockquote><p>不同的算法有不同的参数更新方式</p><h3 id="优化的目标"><a href="#优化的目标" class="headerlink" title="优化的目标"></a>优化的目标</h3><p>训练数据集的最低经验风险可能与最低风险（泛化误差）不同</p><ul><li>经验风险是训练数据集的平均损失</li><li>风险则是整个数据群的预期损失</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230702201639.png" alt="image.png"></p><h3 id="优化的挑战"><a href="#优化的挑战" class="headerlink" title="优化的挑战"></a>优化的挑战</h3><table>  <tr>    <td style="text-align:center;">      <img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230702201744.png" alt="Image 1" style="width:500px;">      <p>局部最优</p>    </td>    <td style="text-align:center;">      <img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230702201754.png" alt="Image 2" style="width:500px;">      <p>鞍点</p>    </td>        <td style="text-align:center;">      <img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230702202522.png" alt="Image 2" style="width:500px;">      <p>梯度消失</p>    </td>  </tr></table><p>(鞍点 in 3D)saddle point be like:<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230702201813.png" alt="image.png"></p><h1 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h1><p>Position Embedding 与 Position encoding的区别</p><blockquote><p><a href="https://www.zhihu.com/question/402387099/answer/1366825959">两个PE的不同</a></p></blockquote><p>position embedding：随网络一起训练出来的位置向量，与前面说的一致，可以理解成动态的，即每次训练结果可能不一样。</p><p>position encoding：根据一定的编码规则计算出来位置表示，比如</p><script type="math/tex; mode=display">\gamma(p)=\left(\sin \left(2^{0} \pi p\right), \cos \left(2^{0} \pi p\right), \cdots, \sin \left(2^{L-1} \pi p\right), \cos \left(2^{L-1} \pi p\right)\right)</script>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neus代码理解</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Neus-code/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Neus-code/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/Totoro97/NeuS">Neus代码</a>的理解</p><p>NeRF与Neus相机坐标系的对比：</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230703144039.png" alt="image.png"></p><div class="table-container"><table><thead><tr><th>Method</th><th>Pixel to Camera coordinate</th></tr></thead><tbody><tr><td>NeRF</td><td>$\vec d = \begin{pmatrix} \frac{i-\frac{W}{2}}{f} \\ -\frac{j-\frac{H}{2}}{f} \\ -1 \\ \end{pmatrix}$ , $intrinsics = K = \begin{bmatrix} f &amp; 0 &amp; \frac{W}{2}  \\ 0 &amp; f &amp; \frac{H}{2}  \\ 0 &amp; 0 &amp; 1 \\ \end{bmatrix}$</td></tr><tr><td>Neus</td><td>$\vec d = intrinsics^{-1} \times  pixel = \begin{bmatrix} \frac{1}{f} &amp; 0 &amp; -\frac{W}{2 \cdot f}  \\ 0 &amp; \frac{1}{f} &amp; -\frac{H}{2 \cdot f} \\ 0 &amp; 0 &amp; 1 \\ \end{bmatrix} \begin{pmatrix} i \\ j \\ 1 \\ \end{pmatrix} = \begin{pmatrix} \frac{i-\frac{W}{2}}{f} \\ \frac{j-\frac{H}{2}}{f} \\ 1 \\ \end{pmatrix}$</td></tr></tbody></table></div><span id="more"></span><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><h2 id="Runner-train流程图"><a href="#Runner-train流程图" class="headerlink" title="Runner().train流程图"></a>Runner().train流程图</h2><iframe frameborder="0" style="width:100%;height:1153px;" src="https://viewer.diagrams.net/?highlight=0000ff&edit=_blank&layers=1&nav=1&title=Runner.drawio#R7V1Zk6M4Ev41RFd1RBPcx6Ndx8zG7vTudE3s7DwRlC3bTBlwA66jf%2F1KQtiA0gbbXG57ImraiEtk6stM5SEJ6p3%2F%2Fkvkrha%2FhVO0FBRp%2Bi6o94KiaJZq4H9Iy0faohiWlrbMI2%2BatsnbhifvB2KNEmtde1MUFy5MwnCZeKti4yQMAjRJCm1uFIVvxctm4bL41pU7R1zD08Rd8q1%2FetNkkbZairlt%2FxV580X2Ztmw0zO%2Bm13MviReuNPwLdekPgjqXRSGSfrLf79DS0K9jC7pfY87zm46FqEgqXPDaKV9cxdf3e9%2F%2BN%2FXc%2FlpFKhfvrCnvLrLNftgQTGW%2BHnjWYgfi3udfDBSGN%2FXYXbiS0wZNcIXyMbqfXsS%2F5qTf7%2BtMTui7Fm4U%2Bnj0pOMHpsnK1G4DqaI9FPGp98WXoKeVu6EnH3D4wq3LRJ%2FyU6TJ7FxIhubp%2BXpkH0UihL0nmtidPkFhT5Kog98SXY2YxobpXLGs7cty2WJtS1y7NZYm8tG2Xzz6C0j8A%2FGiwP4YnA0QlM8LtlhGCWLcB4G7vJh2zreUlHCR9tr%2FhWGK0a7v1GSfDDiueskLFIWvXvJ%2F8jtos6O%2FsqduX9nT6YHH9lBgD83dxM5%2FCt%2FbnsbPcruq83FOFxHE7SHVAqTCm40R8me65jcIXTcOyYitHQT77WI%2F8YZrHDAW0XhBMVxKstm3pxIKw8zdjdYpH7AosuY1Xm4KDoAF1Xn4aK3BRfzCpe6cNFqwkUfFFw0Di73buLGKLm5PQOAqJA%2BgQDSmj6RrStC6iJEr4kQ2RwURHQOIl9R8hZGL%2FE5IMTqW4VYAPm%2BPd58%2Fhyj5UwkKlnQx598MsMRsWk7%2ByTo98OTPapctGRVmaerqnZpyWYdyBH26f6RDU2YvPF05gTpBWdCZaN3KvMTuScvmC%2FRf93Ic4MJ2kvwV3bReVFdA8a2ZnRKdd6K%2F4YwjSJM%2Br0Ej7KrzoziwDjXuh3nKkdxjmrxwl2Rn5N1tPwYR%2B7khajxKvIVfRAtEFOxlbLSk3WOnApATdlqi5wb39TVLKw0CzNzr9ouZKAZiF2Y9TsHmSSMJgsxXCWeL46mrj84wcMbiJrOY6VbAxGQNFesVECgEivZE4eCFZvDyle0fkqVOorOwtmgq31PpZSrO66%2B9zoLalUiZWD%2Ba37CsQzdKW6ZLNDkZRV6NN5Dwz%2FPURb5IQ5t5xlbZOvVGSDJkHpHEm%2FueoGXcLRLyPxtTnHUPeG0UvDMNEVAV0sb67dAOb0lyqlXy7a%2BDKobE1CGZdkqfFQgonFnMYlcLzgLbW2YvcuYa6y5PlLqxgaUYQGFjw1ELEFDoodSmq9xFoiRew8VKHysgAqcQatlwzD7V8rXKXR9UVN3Cq0Oawqt8FPop7Xvu9HHnxGGwHnIGFPrW8ZkL7tCpRoqat05tKoOCioqP4der6ZugpwlcqOAhIcifDRExKiKmIViGGQsKMfFVEWzU9TYV9TURo1aFzXWsFDD%2B0Rw%2Fx3Pd%2BfIWaHIHyJeiIaRC3ixe3cuadecsPpgqesi0YaVFKbyLpI5CrBaCaahj%2F%2F5iB13kDmUmmaIuilt%2FlNKygZyMVqGaJldaht%2BHthM9YRPJ5MSm1RKS4Kt8ymlEKWSpIMm7J1WU2QPvjIqjzBbKrAJyKbqmEsKzyV9%2FOwmk4VDGaHcEUZIgn5P%2FVaELfgCKsRCcbJaEzlGLqEtr%2FmWSbgMo%2FSn78Yv%2BC56%2B136HJk8Ub%2FnWIQpmRT5ECdR%2BILu6NPU%2ByAMiEqcectlqcldevMAH04wfxBuHxO%2BeBN3OWInfG863ekXKgrfVkCqlXgPGO9gAqPSGvOBSJc%2Fd7zpO%2BER5VR%2BJFwMp3S9yCkD1HxQ1mN7rDJ4nD7Ygj0SrEfhwRDsO2E8Ii1Y7lu28KCTdtsQHjRhbJFG3DK%2BE0b0hz0Wxg%2FCg0muxMr74ZGcsu7oNbYwNvY%2B2RCssTCy6cBgjx%2BN6AtVeocpjDX6HlOwNcGmz7AswaZvtnT6DJM8cvx4QUNKMYtDyqw9pMy2hpR%2BdQzXL%2BCqm4eYTTgHMhXR%2BDzEALmRMyN%2FEZ6NxKsFigbp7aKTEaOoMWHUdD0F0Xhve2oP5Syh6QXJtrK6lHWQTXKX%2BlLnZx9k4KccmpEfF8MeQ1VFreg3ViSIQ9YWcJ3wyJA5LlwVUIVeqVRAmQnRvQKyvv%2FD%2BLf3I%2FDff3%2F%2BcL%2BPvX8%2BhH2npeY4vOV3szxulqcwEY1BGRVZv%2FP6jyZrD9KMMFUxyx3ZrIcD6iciJrUOzQidTxDK%2FFhT7zXzYW0si8ysuGN6bMa0WXpHvHKD7BaS6zunhHei%2BXPON5a%2FKNecexvQAegNkzB23AB3ZEmCpV544DsuRvXqWbR5s2gHNPK6dfoY1ySH%2BhLaqKt1h1UooPPeolx9N6n1vlkl8RCltaGLVtFW1TQwtV2zxE4DtzoQdyo6yqXP%2BC9wYtdfLVG88aBSR7di4FZMo%2BA5Xl2WCDRkTZSL%2BlcDixWwAhbVLqcfOj%2BNr%2BAoYad6YUELgH0qyD5pa2d1o8eu7svaeiybFFa7L4c10zB4Lw5d2CFwPH%2BFuUfWz6CClolXZl9mEdS8ZluE%2FvM67kmr6bqoFJNdVRBEJJCfza7yIGptLQKDjwB%2BIH7top9XvulS0XepyRBftE4N9Gs%2BZX3BVjdFzBhWPqXBp4itV8zQGKQXhdjlSgEpuqodYJdrbZW3GEBgPO%2FEALKMiDT7wgQTyWdZolnCpxnlQjq8A6Rs0G%2BcMxWekHzbDwf3OT7snvJ78Uyu%2BgFHuXhgPftIXkHObEarEydodeBXkNFPbWvl82fv6jvaYXlLhmiWIKdAykm1YMdlewqKDyrPSKaX5JE%2FkpMXucEc3UAD5ZZC7mKYqBiaqBR9gLoETp8MzG27HSaCMYUzjL0VTIytxdF4XCazFQbs9YNZeoY87SrWBvEUJOLA7ETe6zhxEyc1HQZrKBYduAaYbNC5oWjyvoRymnPR3CG2DmfmXIzmAhx%2FugGaH9p2atCJ5lJaMvj3BV8hK7Yrmz9Ab84x97U3N1i6MWHyjUfnBMThJgvpohaE6hJs9x3Tj8s1%2FhVNNIvJj7AYxc2i3pLbHUQftBZsI%2Bjjq06KsbQxJ6DZfBT%2FKItpEqKhdSZNP7S8ttp1oAIDFQzYEsNAaWmWCg5UYJ0qmqTkTMIIOeE6ib3pIH1thlF0SZsqFCjYauemrSdY6fYabct%2BH5bH18J8EaQNUBsA07C32oB93W7LcUqvSKuEyNgmkfO9dhR0%2FbGmy9F9T%2B0sZ4YQ0KGc6kj1BhMibXaIaaCphy0umHypoxZFs6sjFZanllqSp2CIT1UUEYi8Gm2pp16DfMOWp3VdNQPzvml9cnQw3reTOCr3tlbUvm7z2ScbwX8eqSdaDfnXZuoJzGveH3dZqSeAh80EA3xtZZ%2FARupPv0DR8WJMBlbv6qhgh946irCtnLuALvge5578n3QF%2BG12U2nJCXsTe3zcdctmIdJdt%2BAfaT%2B2Y2zzQSdIA96vlJuuD3GaDqTEWAelqrc2Vwc2nwnCCxKs5VFvdZrTB7MEqB540IXRozC2M1fU8%2FyWrqdxT1bGSFfPGFlkP0S6Xsa9YMv0x5isyPFgkHU3xgpdoONRsNW3kK6xczlMNnRee4KZFbuWB2hPg%2FY6ERj21C4jerUGHdZMQOGN1X6Cjk0kHvLVSN3mLIIureb7fXzaY65WrjPK0hdP0atH6mmDTl5%2FPIHo0m6dk6hU16zelwudh0arXP%2Fc5WrhFrucNg250ylSp86EmQY7T52eZsBVlGPLY3eR%2BTVVIG8IlaxdOElYB2Ygcns2EL9ERGv6EhrEn%2Bi4%2FJQ%2BqeYYLbapQnHxw7vTIlKsWwXYsO4BUNoF8io0Q52GEh42%2FsvyV1V0n4os1u2c%2BCp19%2BBe1H39Gx27MesAO6LPb%2FSVlypKtrv87PUUa9BCXq3FyYDd%2BFqTJDukhlQtDapgP51lYGdJecAzoWpzuf47iD2dQYP%2B3tf33GvqPn8euVMPD67sHZtjMUJ09%2B%2BbPS%2Bh9LotyuAqmpGpBnuXLEoCy8ySvODV2fltJ9IQ44%2BlWrIXbxsapuZuUVb9DokW27QiYSebgTqpZuttSyPMQVG0gWGxMf%2FGA%2FjqBYQMbLFL9uBC26mfcbEqQyqpDAusa9fgFY5ay1HtdR%2FSgXvggCWN4Asb33f0yBhWKR1SlljDzhAWd4csdxDBygi7fzm31MRwZl6A8nYGbWjK2MgpTvp733MPEKJYN%2BT7HaEk7bXO1MYm6bpZHenEa7%2BoJklLQ9%2FEXuG77%2BwVSRhNFiI%2Bvino5KnnY5aSokly9ILQKm35I1qjW0wBqeLrD%2BjSTjurJnUPHC0DNURAWyA%2F5MoXFOl%2Fom2Qf1HpfLoPxnFDfGcI%2B4jdSDLDgE5uGtHldklsbo5zily321Lk79LMeP8hza3o7e7Xl%2FHfz787Tr9Fyt3p8ZKKPVKxA3uXwtZRv5E0ftU10l2yj0%2FMZ3%2F1nuWhW2XrwwaXzpZEG0ieayLFAwSGegVGbWBAO5WCRFX6xAWwLymgrWj0LVwlno%2BZFIn4L3SIJry5ra3wCM5EEsl5c6ND7iu9mtTE7b57aDA2uH23cruk5pNFbAmsUtvufdc4lHtdjne4UN6N0BpI1jpC8r5O7gcyRkBE60qZkUyu2221Ng8nDjsAQ3bDyZbLcLIsEE6Q30dvD0u9bjYwLCxVYsQ4ESOw80WRTc75Im8Wv8yekwKd3Vri%2BWGumN3wr0JggoI4jJ5DrAXFeOIuyXL6Z4E%2Bzue6E31QimuL6LuQkrZGNBnghQWvs%2FrUZLU8mrH7ipzJAk1eqCu3S5vwNBgp5f04BwKjfpfxOjMcAZWFMFGlPoG0e8%2FqHELwWW%2FqJsjxfHeOzgdH2kCNQblXJ0mxkk2qiaNiJZvcHY4yHlYDqSsvyd5u1kSSj%2BLF%2BQBJN4uld4MB0oB2DBo%2BkOr6KORenRRZN%2FcDab2iMFoiNwq8YE7ysM9IMXH7kkoSH9Ha5Dx2BaZ%2BN0Q8Bkyy0Jd1l62pUbkSr9ovmGo5HPAXpKads0KRf0YwMkoV4QqAIr1bFPVbY9ndKgXNoKjuJCnNte4NRfwsaY4Ch1Y4ukk7SwCc6IgzVFEx7c1%2FZgEn%2BL%2BOtxmFmXohq4I3E2CqPQ3q1TG3pxw5rUsCqiIIAL7QZKU0pd5YvfNFEXt8EMWCp51QrJHK1Aj0ymEGDQSbAqilJkpVYK5U2NRNbEUjZQXf6XN%2FqpVoTxsR5eIlxaw9Ipoog4RHxICM%2FeHLXmB1JfjCU0OXp%2FGUL0gjO3I7M%2FIXhT7LFB2kvaLJoqFu7RW7ABgV3qfB3G5D1I29UrEt%2BtFSNN2jiMrPbFVXulnRD291Q1pvr0KVT%2FQorZ26WQ6wsPNrpyL1p98dsUmRWjfMrPRqzvZbr9XdZlQd8%2FTU2Ty99dB6rW10gomMrPZvV7mWImV287E3qMzMaqq8ay%2FZuQUKB6nordJuw%2BDOPVAtR2tqXVXOD%2BP9ye26HrvOajn2djOHiXCdkLWYWD1j2Z4iZ4Mw8t0lu2BwwNE3QMmkkQZlBAGL6bSHnLOZRDaIgNpFG2ojWg5QY6XsMEMu8TftGZfDerK%2BzCqOdqo%2FWd97QzvqDyhP8fz5DpgziOMLBojwXOFHRkAZnPhuW7uBedsL3eQduJ7%2FFmHS09uOXo%2BPOYu9MIjZQJDcYIr%2Fn%2FI%2FrnjUnoqjRr%2F1i0CK7IOJm6CA5C9sxy3xp85JUHZ%2B3PZ07fQ1j56DOnUOUMtiZHn%2FgQnYoWZbMNOu4bADtHBtO9Tu0w5Vd%2BfXNhQO4xIJhxUNU7loGJCi0WksTBtQ8u3wYQaU7sNU7TWLPetmDmZY0UXuJHGydwzRGaLqumjJB2ZpkD0bjA6tP21AObaDx0sm3arx0mtaoN6vDMxxtH4hz%2FB5qveyDJmiZuIgc6mydLWd0%2FTyDXqVW1srOwLUDub1Gdl%2FLpkOrtHSvUy%2FhioPwD%2BQ6g1f2GuoUuNTvTO8zDy0nMbnhxY4ONQ9WiqqjY%2F2szzTDQt8kutBHD7s0H1n%2BXQoDpdr4sRKj7%2BvUfThzNbB5JoOwpcClEo9VRUUtFBCiNVWKYCuXKVsfSlb16FzspV1mizgHTprfJiinG2SZqtkO7SRSTZRIy2WMB7lwKw%2F0PP2SLA0sksavsrKLhyNyClLEcb25hqyMPuDLliSML4bqARXCtjTwJWWd4ltozX4XSeu9eGXbbBbDb9%2BJ658sC9VRCgWHh4JQOzRV%2BeVjs%2FcYrNbRZlEnhvMl4XLk9zlpYjTGaBNx7g6wEiy2uLMdYWrA9BWd0qh95pQrvNTCowfEnEQ%2F0j%2FHSJC8MDfOaMwVBleSEeF0SK3t7giP6mghEXvK4yD4dFVV6RalIS24DmKjPgwCsl0aetUwh%2B6%2BC2cInLF%2FwE%3D"></iframe><p><a href="#数据集自定义">数据集自定义</a>：根据imgs2poses.py生成sparse_points.ply和poses.npy文件，若先前没有经过colmap，则会生成<code>sparse\0\*.bin</code><a href="#cameras文件">文件</a>,(cameras.bin, images.bin , points3D.bin)。然后根据gen_cameras.py文件，通过pose.npy读取第一个相机的c2w矩阵将第一个相机的单位坐标系保存为pose.ply文件，通过pose.npy和sparse_points_interest.ply文件生成preprocessed文件夹下的cameras_sphere.npz，并复制images生成image和mask文件夹下图片。</p><ul><li>imgs2poses.py<ul><li>sparse_points.ply：读取points3D文件中的所有点，生成的稀疏点云文件</li><li>poses.npy：通过cameras.bin和images.bin文件计算出的<a href="#images文件">pose数据</a>：大小num_images x 3 x 5，包括num_images x 3 x 4的c2w矩阵和num_images x 3的hwf数据</li></ul></li><li>gen_cameras.py<ul><li><a href="#pose文件">pose.ply</a>：读取第一个相机的pose，将该相机坐标系下的原点、xyz轴单位坐标转换到世界坐标系下，然后生成点云保存为pose.ply文件</li><li><a href="#两个矩阵">cameras_sphere.npz</a><ul><li>world_mat：通过pose.npz读取pose矩阵，分解为c2w和hwf，并将c2w求逆得到w2c，将hwf转化为intrinsic相机内参矩阵，最后得到<code>world_mat=intrinsic @ w2c</code></li><li>scale_mat：通过sparse_points_interest.ply文件，将其中的感兴趣区域，在世界坐标系下计算出scale_mat，<strong>该矩阵用于将世界坐标系原点缩放并平移到感兴趣区域的中心处，使得世界坐标系下的单位圆即为感兴趣的区域</strong>，这也是不需要mask的原因</li><li>image和mask：将images数据集文件夹下图片复制到preprocessed文件夹下的image下和并根据数据集图片生成同样大小的白色图片，放入mask文件夹</li></ul></li></ul></li></ul><p><a href="#dataset">数据处理</a>：</p><ul><li>读取cameras_sphere.npz文件、image和mask文件，获得相机的内外参矩阵intrinsics, pose，并对intrinsics求逆得到intrinsics_inv，在<a href="#光线生成">生成光线</a>时用于将图片像素的坐标转换为光线在世界坐标系下的原点o和方向向量d。</li><li>通过o和d，生成场景中的near和far，即在每条光线上采样时，采样点的最近坐标和最远坐标</li></ul><p><a href="#render">渲染</a>：</p><ul><li>根据o、d、near和far，以及其他参数，经过MLP网络，得到颜色值、sdf对输入pts_xyz的梯度等信息，然后计算loss，最后通过反向传播不断更新网络的参数，训练出最终的4个MLP网络</li><li>根据训练好的MLP网络，通过一个新相机点的位置，生成一系列光线，在光线上进行采样获得点云的坐标，然后将坐标输入MLP网络，获得三维空间中每个点云的颜色、SDF和梯度等信息。<ul><li>颜色跟观察方向有关、SDF与方向无关、梯度与方向无关，颜色可以用来生成新视点的图片、视频，SDF可以用来根据threshold选取零水平集来生成mesh模型表面，梯度可以做法向量图。</li></ul></li></ul><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p><code>self.dataset = Dataset(self.conf[&#39;dataset&#39;])</code></p><ul><li>相机内外参数矩阵</li><li>光线的生成以及坐标变换</li></ul><p>BlendedMVS/bmvs_bear/cameras_sphere</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">in gen_cameras : </span><br><span class="line">w2c = np.linalg.inv(pose)</span><br><span class="line"></span><br><span class="line">世界坐标系到像素坐标系转换矩阵</span><br><span class="line">(4, 4) world_mats_np0 = intrinsic @ w2c =w2pixel</span><br><span class="line">[[-1.0889766e+02  3.2340955e+02  6.2724188e+02 -1.6156446e+04] </span><br><span class="line">[-4.8021997e+02 -3.6971255e+02  2.8318774e+02 -8.9503633e+03]</span><br><span class="line">[ 2.4123600e-01 -4.2752099e-01  8.7122399e-01 -2.1731400e+01]</span><br><span class="line">[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]</span><br><span class="line"></span><br><span class="line">将世界坐标系平移缩放到感兴趣物体的中心</span><br><span class="line">(4, 4) scale_mats_np0 : sparse_points_interest中，以中心点为圆心，最远距离为半径的一个区域</span><br><span class="line">    scale_mat = np.diag([radius, radius, radius, 1.0]).astype(np.float32)</span><br><span class="line">    scale_mat[:3, 3] = center</span><br><span class="line">[[ 1.6737139  0.         0.        -2.702419 ]</span><br><span class="line">[ 0.         1.6737139  0.        -1.3968586]</span><br><span class="line">[ 0.         0.         1.6737139 27.347609 ]</span><br><span class="line">[ 0.         0.         0.         1.       ]]</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">P = world_mat @ scale_mat</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">[[-1.8226353e+02  5.4129504e+02  1.0498235e+03  8.3964941e+02]</span><br><span class="line"> [-8.0375085e+02 -6.1879303e+02  4.7397528e+02  6.0833594e+02]</span><br><span class="line"> [ 4.0376005e-01 -7.1554786e-01  1.4581797e+00  2.0397587e+00]</span><br><span class="line"> [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]</span><br><span class="line"></span><br><span class="line">[[-1.8226353e+02  5.4129504e+02  1.0498235e+03  8.3964941e+02]</span><br><span class="line"> [-8.0375085e+02 -6.1879303e+02  4.7397528e+02  6.0833594e+02]</span><br><span class="line"> [ 4.0376005e-01 -7.1554786e-01  1.4581797e+00  2.0397587e+00]]</span><br><span class="line"> &quot;&quot;&quot;</span><br><span class="line">P = P[:3, :4]</span><br></pre></td></tr></table></figure><p>将P分解为相机内参和外参矩阵，in dataset.py</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">out = cv.decomposeProjectionMatrix(P)</span><br><span class="line">K = out[0] # 3x3</span><br><span class="line">[[1.00980786e+03 1.61999036e-04 6.39247803e+02]</span><br><span class="line"> [0.00000000e+00 1.00980774e+03 4.83591949e+02]</span><br><span class="line"> [0.00000000e+00 0.00000000e+00 1.67371416e+00]]</span><br><span class="line"> </span><br><span class="line">R = out[1] # 3x3</span><br><span class="line">[[-0.33320493  0.8066752   0.48810825]</span><br><span class="line"> [-0.9114712  -0.40804535  0.05214698]</span><br><span class="line"> [ 0.24123597 -0.42752096  0.87122387]]</span><br><span class="line"></span><br><span class="line">t = out[2] # 4x1</span><br><span class="line">[[-0.16280915]</span><br><span class="line"> [ 0.30441687]</span><br><span class="line"> [-0.69216055]</span><br><span class="line"> [ 0.6338275 ]]</span><br><span class="line"> </span><br><span class="line">K = K / K[2, 2]</span><br><span class="line">[[6.0333350e+02 9.6790143e-05 3.8193369e+02]</span><br><span class="line"> [0.0000000e+00 6.0333344e+02 2.8893341e+02]</span><br><span class="line"> [0.0000000e+00 0.0000000e+00 1.0000000e+00]]</span><br><span class="line"></span><br><span class="line">intrinsics = np.eye(4)</span><br><span class="line">intrinsics[:3, :3] = K # intrinsics: 4x4 为相机内参矩阵</span><br><span class="line">[[6.03333496e+02 9.67901433e-05 3.81933685e+02 0.00000000e+00]</span><br><span class="line"> [0.00000000e+00 6.03333435e+02 2.88933411e+02 0.00000000e+00]</span><br><span class="line"> [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00]</span><br><span class="line"> [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00]]</span><br><span class="line"></span><br><span class="line">pose = np.eye(4, dtype=np.float32)</span><br><span class="line">pose[:3, :3] = R.transpose() # 正交矩阵 其转置等于逆 w2c --&gt; c2w</span><br><span class="line">pose[:3, 3] = (t[:3] / t[3])[:, 0] # pose: 4x4 为相机外参矩阵的逆</span><br><span class="line">[[-0.33320493 -0.9114712   0.24123597 -0.25686666]</span><br><span class="line"> [ 0.8066752  -0.40804535 -0.42752096  0.48028347]</span><br><span class="line"> [ 0.48810825  0.05214698  0.87122387 -1.092033  ]</span><br><span class="line"> [ 0.          0.          0.          1.        ]]</span><br><span class="line"> 单位向量经过pose变换到世界坐标系后仍然为单位向量</span><br><span class="line"></span><br><span class="line">世界坐标系下，光线的原点：</span><br><span class="line">[[-0.25686666]</span><br><span class="line"> [ 0.48028347]</span><br><span class="line"> [-1.092033  ]</span><br><span class="line"> [ 1.        ]]</span><br></pre></td></tr></table></figure><h3 id="光线生成"><a href="#光线生成" class="headerlink" title="光线生成"></a>光线生成</h3><p>gen_random_rays_at()随机生成光线<br>然后生成光线，in <code>dataset.py/gen_random_rays_at()</code> by img_idx ，batch_size, 并将rays的像素坐标转换到世界坐标系下</p><p>p_pixel —&gt; p_camera —&gt; p_world (or rays_d)</p><p><code>p_camera = intrinsics_inv @ p_pixel</code>:  <code>3x3 @ 3x1</code></p><p>$\begin{bmatrix} \frac{1}{f} &amp; 0 &amp; -\frac{W}{2 \cdot f}  \\ 0 &amp; \frac{1}{f} &amp; -\frac{H}{2 \cdot f} \\ 0 &amp; 0 &amp; 1 \\ \end{bmatrix} \begin{pmatrix} i \\ j \\ 1 \\ \end{pmatrix} = \begin{pmatrix} \frac{i-\frac{W}{2}}{f} \\ \frac{j-\frac{H}{2}}{f} \\ 1 \\ \end{pmatrix}$</p><p><code>p_world = pose @ p_camera</code>:  <code>3x3 @ 3x1</code></p><p>$\begin{bmatrix} r_{11}&amp;r_{12}&amp;r_{13}\\ r_{21}&amp;r_{22}&amp;r_{23}\\ r_{31}&amp;r_{32}&amp;r_{33} \end{bmatrix} \begin{pmatrix} x_{c} \\ y_{c} \\ z_{c} \\ \end{pmatrix} = \begin{pmatrix} x_{w} \\ y_{w} \\ z_{w} \\ \end{pmatrix} = rays_d$</p><p><code>rays_o = pose[:3, 3]</code> $= \begin{bmatrix} t_{x} \\ t_{y} \\ t_{z} \end{bmatrix}$，为相机坐标系原点在世界坐标系下位置</p><p>$pose = \begin{bmatrix}r_{11}&amp;r_{12}&amp;r_{13}&amp;t_x\\ r_{21}&amp;r_{22}&amp;r_{23}&amp;t_y\\ r_{31}&amp;r_{32}&amp;r_{33}&amp;t_z\\ 0&amp;0&amp;0&amp;1\end{bmatrix}$</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def gen_random_rays_at(self, img_idx, batch_size):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Generate random rays at world space from one camera.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    pixels_x = torch.randint(low=0, high=self.W, size=[batch_size]) </span><br><span class="line">    pixels_y = torch.randint(low=0, high=self.H, size=[batch_size])</span><br><span class="line">    color = self.images[img_idx][(pixels_y, pixels_x)]    # batch_size, 3</span><br><span class="line">    mask = self.masks[img_idx][(pixels_y, pixels_x)]      # batch_size, 3</span><br><span class="line">    # p : 像素坐标系下的坐标</span><br><span class="line">    p = torch.stack([pixels_x, pixels_y, torch.ones_like(pixels_y)], dim=-1).float()  # batch_size, 3</span><br><span class="line">    # 将p转换到相机坐标系下</span><br><span class="line">    # matmul : [1, 3, 3] x [batch_size, 3, 1] -&gt; [batch_size, 3, 1] -&gt; [batch_size, 3]</span><br><span class="line">    p = torch.matmul(self.intrinsics_all_inv[img_idx, None, :3, :3], p[:, :, None]).squeeze() # batch_size, 3</span><br><span class="line">    # rays_v ：将p归一化</span><br><span class="line">    rays_v = p / torch.linalg.norm(p, ord=2, dim=-1, keepdim=True)    # batch_size, 3</span><br><span class="line">    # rays_v ：将p转换到世界坐标系下</span><br><span class="line">    # matmul : [1, 3, 3] x [batch_size, 3, 1] -&gt; [batch_size, 3, 1] -&gt; [batch_size, 3]</span><br><span class="line">    rays_v = torch.matmul(self.pose_all[img_idx, None, :3, :3], rays_v[:, :, None]).squeeze()  # batch_size, 3</span><br><span class="line">    # [1,3].expand([batch_size, 3])</span><br><span class="line">    rays_o = self.pose_all[img_idx, None, :3, 3].expand(rays_v.shape) # batch_size, 3</span><br><span class="line">    return torch.cat([rays_o.cpu(), rays_v.cpu(), color, mask[:, :1]], dim=-1).cuda()    # batch_size, 10</span><br></pre></td></tr></table></figure><h3 id="计算near和far-from-o-d"><a href="#计算near和far-from-o-d" class="headerlink" title="计算near和far(from o,d)"></a>计算near和far(from o,d)</h3><p>根据rays_o 和rays_d 计算出near和far两个平面</p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230803193755.png" alt="image.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def near_far_from_sphere(self, rays_o, rays_d):</span><br><span class="line">    a = torch.sum(rays_d**2, dim=-1, keepdim=True)</span><br><span class="line">    b = 2.0 * torch.sum(rays_o * rays_d, dim=-1, keepdim=True)</span><br><span class="line">    mid = 0.5 * (-b) / a</span><br><span class="line">    # rays_o 在 rays_d 方向上的投影 / rays_d 在 rays_d 方向上的投影</span><br><span class="line">    near = mid - 1.0</span><br><span class="line">    far = mid + 1.0</span><br><span class="line">    return near, far</span><br></pre></td></tr></table></figure><h3 id="box的min和max-to生成mesh模型"><a href="#box的min和max-to生成mesh模型" class="headerlink" title="box的min和max(to生成mesh模型)"></a>box的min和max(to生成mesh模型)</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">(4, 4) scale_mats_np0</span><br><span class="line">[[ 1.6737139  0.         0.        -2.702419 ]</span><br><span class="line">[ 0.         1.6737139  0.        -1.3968586]</span><br><span class="line">[ 0.         0.         1.6737139 27.347609 ]</span><br><span class="line">[ 0.         0.         0.         1.       ]]</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">object_bbox_min = np.array([-1.01, -1.01, -1.01, 1.0])</span><br><span class="line">object_bbox_max = np.array([ 1.01,  1.01,  1.01, 1.0])</span><br><span class="line"># Object scale mat: region of interest to **extract mesh**</span><br><span class="line">object_scale_mat = np.load(os.path.join(self.data_dir, self.object_cameras_name))[&#x27;scale_mat_0&#x27;] # 4x4</span><br><span class="line"></span><br><span class="line"># object_bbox_? &gt; object_scale_mat缩放+平移 &gt; scale_mat缩放+平移</span><br><span class="line">object_bbox_min = np.linalg.inv(self.scale_mats_np[0]) @ object_scale_mat @ object_bbox_min[:, None] # 4x1</span><br><span class="line">object_bbox_max = np.linalg.inv(self.scale_mats_np[0]) @ object_scale_mat @ object_bbox_max[:, None] # 4x1</span><br><span class="line">self.object_bbox_min = object_bbox_min[:3, 0] # 3</span><br><span class="line">self.object_bbox_max = object_bbox_max[:3, 0] # 3</span><br><span class="line">如果</span><br><span class="line">render_cameras_name = cameras_sphere.npz</span><br><span class="line">object_cameras_name = cameras_sphere.npz</span><br><span class="line">两文件相同，则 </span><br><span class="line">np.linalg.inv(self.scale_mats_np[0]) @ object_scale_mat = </span><br><span class="line">[[ 1.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]</span><br><span class="line"> [ 0.0000000e+00  1.0000000e+00  0.0000000e+00 -5.9604645e-08]</span><br><span class="line"> [ 0.0000000e+00  0.0000000e+00  1.0000000e+00  0.0000000e+00]</span><br><span class="line"> [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]</span><br><span class="line">object_bbox_min , object_bbox_max 只平移，不缩放</span><br></pre></td></tr></table></figure><h2 id="神经网络结构Network"><a href="#神经网络结构Network" class="headerlink" title="神经网络结构Network"></a>神经网络结构Network</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Networks</span><br><span class="line">params_to_train = []</span><br><span class="line">self.nerf_outside = NeRF(**self.conf[&#x27;model.nerf&#x27;]).to(self.device) # 创建一个NeRF网络</span><br><span class="line">self.sdf_network = SDFNetwork(**self.conf[&#x27;model.sdf_network&#x27;]).to(self.device) # 创建一个SDF网络</span><br><span class="line">self.deviation_network = SingleVarianceNetwork(**self.conf[&#x27;model.variance_network&#x27;]).to(self.device)</span><br><span class="line">self.color_network = RenderingNetwork(**self.conf[&#x27;model.rendering_network&#x27;]).to(self.device)</span><br><span class="line">params_to_train += list(self.nerf_outside.parameters())</span><br><span class="line">params_to_train += list(self.sdf_network.parameters())</span><br><span class="line">params_to_train += list(self.deviation_network.parameters())</span><br><span class="line">params_to_train += list(self.color_network.parameters())</span><br><span class="line"></span><br><span class="line">self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)</span><br><span class="line"></span><br><span class="line">self.renderer = NeuSRenderer(self.nerf_outside,</span><br><span class="line">                             self.sdf_network,</span><br><span class="line">                             self.deviation_network,</span><br><span class="line">                             self.color_network,</span><br><span class="line">                             **self.conf[&#x27;model.neus_renderer&#x27;])</span><br></pre></td></tr></table></figure><p>Neus中共构建了4个network：</p><ul><li>NeRF：训练物体outside即背景的颜色</li><li>SDFNetwork：训练点云中的sdf值</li><li>RenderingNetwork：训练点云的RGB</li><li>SingleVarianceNetwork：训练一个单变量invs，用于计算$cdf = sigmoid(estimated.sdf  \cdot inv.s)$</li></ul><h3 id="NeRF"><a href="#NeRF" class="headerlink" title="NeRF"></a>NeRF</h3><p>同NeRF网络<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Pasted%20image%2020221206180113.png" alt="Pasted image 20221206180113.png|600"></p><ul><li>84—&gt;256—&gt;256—&gt;256—&gt;256—&gt;256+84—&gt;256—&gt;256—&gt;256+27—&gt;128—&gt;3</li><li>84—&gt;256—&gt;256—&gt;256—&gt;256—&gt;256+84—&gt;256—&gt;256—&gt;256—&gt;1</li></ul><h3 id="SDFNetwork"><a href="#SDFNetwork" class="headerlink" title="SDFNetwork"></a>SDFNetwork</h3><p>激活函数 $\text{Softplus}(x) = \frac{\log(1 + e^{\beta x})}{\beta}$</p><p>网络结构：<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/SDFNetwork_modify.png" alt="SDFNetwork"></p><ul><li>39—&gt;256—&gt;256—&gt;256—&gt;217—&gt;256—&gt;256—&gt;256—&gt;256—&gt;257<br>input: pts, 采样点的三维坐标 batch_size <em> n_samples x 3<br>output: 257个数 batch_size </em> n_samples x 257</li></ul><p><code>sdf(pts) = output[:, :1]</code>:  batch_size * n_samples x 1，采样点的sdf值</p><h3 id="RenderingNetwork"><a href="#RenderingNetwork" class="headerlink" title="RenderingNetwork"></a>RenderingNetwork</h3><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/RenderingNetwork.png" alt="RenderingNetwork.png"></p><p>input: rendering_input :<code>[batch_size * n_samples ,  3 + 27 + 3+ 256 = 289]</code><br><code>rendering_input = torch.cat([points, view_dirs, normals, feature_vectors], dim=-1)</code></p><ul><li>pts: batch_size * n_samples, 3</li><li>gradients: batch_size * n_samples, 3</li><li>dirs: batch_size * n_samples, 3<ul><li>位置编码 to view_dirs: batch_size * n_samples , 27</li></ul></li><li>feature_vector: batch_size * n_samples, 256</li></ul><p>output: sampled_color采样点的RGB颜色 batch_size * n_samples , 3</p><h3 id="SingleVarianceNetwork"><a href="#SingleVarianceNetwork" class="headerlink" title="SingleVarianceNetwork"></a>SingleVarianceNetwork</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class SingleVarianceNetwork(nn.Module):</span><br><span class="line">    def __init__(self, init_val):</span><br><span class="line">        super(SingleVarianceNetwork, self).__init__()</span><br><span class="line">        # variance 模型可以跟踪和优化这个参数，使其在训练过程中进行更新</span><br><span class="line">        self.register_parameter(&#x27;variance&#x27;, nn.Parameter(torch.tensor(init_val)))</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # torch.zeros([1, 3])</span><br><span class="line">        # 大小为 [len(x), 1] 的张量，每个元素都是 exp(variance * 10.0)</span><br><span class="line">        return torch.ones([len(x), 1]) * torch.exp(self.variance * 10.0)</span><br><span class="line"></span><br><span class="line">in Runner:</span><br><span class="line">self.deviation_network = SingleVarianceNetwork(**self.conf[&#x27;model.variance_network&#x27;]).to(self.device)</span><br></pre></td></tr></table></figure><p>render中<br><code>inv_s = deviation_network(torch.zeros([1, 3]))[:, :1].clip(1e-6, 1e6)</code></p><h2 id="render"><a href="#render" class="headerlink" title="render"></a>render</h2><p>input: </p><ul><li>rays_o, </li><li>rays_d, 单位向量</li><li>near, far : batch_sizex1,batch_sizex1</li><li>background_rgb=background_rgb,</li><li>cos_anneal_ratio=self.get_cos_anneal_ratio()</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">image_perm = self.get_image_perm()</span><br><span class="line">res_step = self.end_iter - self.iter_step</span><br><span class="line"></span><br><span class="line">for iter_i in tqdm(range(res_step)):</span><br><span class="line">    data = self.dataset.gen_random_rays_at(image_perm[self.iter_step % len(image_perm)], self.batch_size)</span><br><span class="line">    # data : [batch_size, 10] : [rays_o.cpu(), rays_v.cpu(), color, mask[:, :1]]</span><br><span class="line">    rays_o, rays_d, true_rgb, mask = data[:, :3], data[:, 3: 6], data[:, 6: 9], data[:, 9: 10]</span><br><span class="line">    </span><br><span class="line">    near, far = self.dataset.near_far_from_sphere(rays_o, rays_d)</span><br><span class="line">    </span><br><span class="line">    background_rgb = None</span><br><span class="line">    if self.use_white_bkgd:</span><br><span class="line">        background_rgb = torch.ones([1, 3])</span><br><span class="line"></span><br><span class="line">    render_out = self.renderer.render(rays_o, rays_d, near, far,</span><br><span class="line">                                      background_rgb=background_rgb,</span><br><span class="line">                                      cos_anneal_ratio=self.get_cos_anneal_ratio())</span><br></pre></td></tr></table></figure><p>output: render_out字典</p><ul><li>color_fine: render出来图片的RGB颜色值</li><li>s_val: $= \sum_{i}^{n.samples}(\frac{1.0}{invs_{i}})$<ul><li>inv_s: 一个可以更新的变量 $1 \times e^{10.0 \cdot var}$ ，并将其限制在$1 \times 10^{-6}$ ~ $1 \times 10^{6}$之间</li><li><code>ret_fine[&#39;s_val&#39;] = 1.0 / inv_s</code> # batch_size * n_samples, 1</li><li><code>s_val = ret_fine[&#39;s_val&#39;].reshape(batch_size, n_samples).mean(dim=-1, keepdim=True)</code> # batch_size, 1</li></ul></li><li>cdf_fine: $pre.cdf = {\Phi_s(f(\mathbf{p}(t_i)))}$<ul><li>batch_size, n_samples</li></ul></li><li>weight_sum: 一条光线上的权重之和(包括背景outside)<ul><li>batch_size, 1</li><li><code>weights_sum = weights.sum(dim=-1, keepdim=True)</code></li></ul></li><li>weight_max: 一条光线上权重的最大值<ul><li>batch_size, 1</li><li><code>torch.max(weights, dim=-1, keepdim=True)[0]</code></li></ul></li><li>gradients: 梯度,sdf对输入pts_xyz的梯度，与法向量的计算有关<ul><li>batch_size, n_samples, 3</li></ul></li><li>weights: 权重，每个采样点<ul><li>batch_size, n_samples or batch_size, n_samples + n_outside</li></ul></li><li>gradient_error: Eikonal损失值$\mathcal{L}_{r e g}=\frac{1}{n m}\sum_{k,i}(|\nabla f(\hat{\mathbf{p}}_{k,i})|_{2}-1)^{2}.$ 只计算在relax半径为1.2的圆内的采样点sdf的梯度<ul><li>$|\nabla f(\hat{\mathbf{p}}_{k,i})|_{2} = \sqrt{gx^{2}+gy^{2}+gz^{2}}$</li></ul></li><li>inside_sphere: 采样点是否在单位圆空间内<ul><li>batch_size, n_samples</li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#x27;color_fine&#x27;: color_fine, # batch_size, 3</span><br><span class="line">    &#x27;s_val&#x27;: s_val, # batch_size, 1</span><br><span class="line">    &#x27;cdf_fine&#x27;: ret_fine[&#x27;cdf&#x27;], # batch_size, n_samples</span><br><span class="line">    &#x27;weight_sum&#x27;: weights_sum, # batch_size, 1</span><br><span class="line">    &#x27;weight_max&#x27;: torch.max(weights, dim=-1, keepdim=True)[0], # batch_size, 1</span><br><span class="line">    &#x27;gradients&#x27;: gradients, # batch_size, n_samples, 3</span><br><span class="line">    &#x27;weights&#x27;: weights, # batch_size, n_samples or batch_size, n_samples + n_outside</span><br><span class="line">    &#x27;gradient_error&#x27;: ret_fine[&#x27;gradient_error&#x27;], # 1</span><br><span class="line">    &#x27;inside_sphere&#x27;: ret_fine[&#x27;inside_sphere&#x27;] # batch_size, n_samples</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">ret_fine = self.render_core(rays_o,</span><br><span class="line">                            rays_d,</span><br><span class="line">                            z_vals,</span><br><span class="line">                            sample_dist,</span><br><span class="line">                            self.sdf_network,</span><br><span class="line">                            self.deviation_network,</span><br><span class="line">                            self.color_network,</span><br><span class="line">                            background_rgb=background_rgb,</span><br><span class="line">                            background_alpha=background_alpha,</span><br><span class="line">                            background_sampled_color=background_sampled_color,</span><br><span class="line">                            cos_anneal_ratio=cos_anneal_ratio)</span><br><span class="line">                            </span><br><span class="line"># ret_fine:</span><br><span class="line">    # &#x27;color&#x27;: color, # batch_size, 3</span><br><span class="line">    # &#x27;sdf&#x27;: sdf, # batch_size * n_samples, 1</span><br><span class="line">    # &#x27;dists&#x27;: dists, # batch_size, n_samples</span><br><span class="line">    # &#x27;gradients&#x27;: gradients.reshape(batch_size, n_samples, 3),</span><br><span class="line">    # &#x27;s_val&#x27;: 1.0 / inv_s, # batch_size * n_samples, 1</span><br><span class="line">    # &#x27;mid_z_vals&#x27;: mid_z_vals, # batch_size, n_samples</span><br><span class="line">    # &#x27;weights&#x27;: weights, # batch_size, n_samples or batch_size, n_samples + n_outside</span><br><span class="line">    # &#x27;cdf&#x27;: c.reshape(batch_size, n_samples), # batch_size, n_samples</span><br><span class="line">    # &#x27;gradient_error&#x27;: gradient_error, # 1</span><br><span class="line">    # &#x27;inside_sphere&#x27;: inside_sphere # batch_size, n_samples</span><br><span class="line">color_fine = ret_fine[&#x27;color&#x27;]</span><br><span class="line">weights = ret_fine[&#x27;weights&#x27;]</span><br><span class="line">weights_sum = weights.sum(dim=-1, keepdim=True)</span><br><span class="line">gradients = ret_fine[&#x27;gradients&#x27;]</span><br><span class="line">s_val = ret_fine[&#x27;s_val&#x27;].reshape(batch_size, n_samples).mean(dim=-1, keepdim=True) # [batch_size, 1]</span><br></pre></td></tr></table></figure><p>function:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">render:</span><br><span class="line"></span><br><span class="line">batch_size = len(rays_o)</span><br><span class="line">sample_dist = 2.0 / self.n_samples   # Assuming the region of interest is a unit sphere </span><br><span class="line">z_vals = torch.linspace(0.0, 1.0, self.n_samples) # [n_samples]</span><br><span class="line">z_vals = near + (far - near) * z_vals[None, :]  # [batch_size, n_samples]</span><br><span class="line">拍照物体的采样点z方向坐标</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">物体外的z坐标(背景)</span><br><span class="line">z_vals_outside = None</span><br><span class="line">if self.n_outside &gt; 0:</span><br><span class="line">    z_vals_outside = torch.linspace(1e-3, 1.0 - 1.0 / (self.n_outside + 1.0), self.n_outside) # [n_outside]</span><br><span class="line"></span><br><span class="line">n_samples = self.n_samples</span><br><span class="line">perturb = self.perturb</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">添加扰动：</span><br><span class="line">if perturb_overwrite &gt;= 0:</span><br><span class="line">    perturb = perturb_overwrite</span><br><span class="line">if perturb &gt; 0:</span><br><span class="line">    t_rand = (torch.rand([batch_size, 1]) - 0.5) # [batch_size, 1]</span><br><span class="line">    z_vals = z_vals + t_rand * 2.0 / self.n_samples # [batch_size, n_samples]</span><br><span class="line"></span><br><span class="line">    if self.n_outside &gt; 0:</span><br><span class="line">        mids = .5 * (z_vals_outside[..., 1:] + z_vals_outside[..., :-1]) # [n_outside - 1]</span><br><span class="line">        upper = torch.cat([mids, z_vals_outside[..., -1:]], -1)     # [n_outside]</span><br><span class="line">        lower = torch.cat([z_vals_outside[..., :1], mids], -1)      # [n_outside]</span><br><span class="line">        t_rand = torch.rand([batch_size, z_vals_outside.shape[-1]]) # [batch_size, n_outside]</span><br><span class="line">        z_vals_outside = lower[None, :] + (upper - lower)[None, :] * t_rand</span><br><span class="line">        # Z_vals_outside:  1Xn_outside + 1Xn_outside * batch_sizeXn_outside = batch_sizeXn_outside</span><br><span class="line"></span><br><span class="line">if self.n_outside &gt; 0:</span><br><span class="line">    z_vals_outside = far / torch.flip(z_vals_outside, dims=[-1]) + 1.0 / self.n_samples # [batch_size, n_outside]</span><br><span class="line">    # filp: 将tensor的维度进行翻转，如[1,2,3] -&gt; [3,2,1] ，倒序排列</span><br><span class="line"></span><br><span class="line">背景outside:</span><br><span class="line">background_alpha = None</span><br><span class="line">background_sampled_color = None</span><br></pre></td></tr></table></figure><h3 id="get-cos-anneal-ratio"><a href="#get-cos-anneal-ratio" class="headerlink" title="get_cos_anneal_ratio"></a>get_cos_anneal_ratio</h3><p>output: </p><ul><li>数1或者比一小的数$\frac{iterstep}{anneal}, anneal=50000$</li><li>or 1 when anneal_end = 0</li></ul><h3 id="精采样n-importance"><a href="#精采样n-importance" class="headerlink" title="精采样n_importance"></a>精采样n_importance</h3><p>if self.n_importance &gt; 0: 精采样</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">with torch.no_grad(): # 不需要计算梯度</span><br><span class="line">    # pts : [batch_size, 1, 3] + [batch_size, 1, 3] * [batch_size, n_samples, 1] = [batch_size, n_samples, 3]</span><br><span class="line">    pts = rays_o[:, None, :] + rays_d[:, None, :] * z_vals[..., :, None] # [batch_size, n_samples, 3]</span><br><span class="line">    sdf = self.sdf_network.sdf(pts.reshape(-1, 3)).reshape(batch_size, self.n_samples)</span><br><span class="line">    # pts.reshape(-1, 3) : [batch_size * n_samples, 3]</span><br><span class="line">    # sdf : [batch_size * n_samples , 1] -&gt; [batch_size, n_samples]</span><br><span class="line"></span><br><span class="line">    for i in range(self.up_sample_steps):</span><br><span class="line">        # [batch_size, n_importance // up_sample_steps] per step</span><br><span class="line">        new_z_vals = self.up_sample(rays_o,</span><br><span class="line">                                    rays_d,</span><br><span class="line">                                    z_vals,</span><br><span class="line">                                    sdf,</span><br><span class="line">                                    self.n_importance // self.up_sample_steps,</span><br><span class="line">                                    64 * 2**i)</span><br><span class="line">        # # [batch_size, n_samples + n_importance // up_sample_steps], [batch_size, n_samples + n_importance // up_sample_steps]</span><br><span class="line">        z_vals, sdf = self.cat_z_vals(rays_o,</span><br><span class="line">                                    rays_d,</span><br><span class="line">                                    z_vals,</span><br><span class="line">                                    new_z_vals,</span><br><span class="line">                                    sdf,</span><br><span class="line">                                    last=(i + 1 == self.up_sample_steps))</span><br><span class="line">    # new_z_vals : [batch_size, n_importance]</span><br><span class="line">    # z_vals : [batch_size, n_samples + n_importance]</span><br><span class="line"></span><br><span class="line">n_samples = self.n_samples + self.n_importance</span><br></pre></td></tr></table></figure><h4 id="up-sample-self-rays-o-rays-d-z-vals-sdf-n-importance-inv-s"><a href="#up-sample-self-rays-o-rays-d-z-vals-sdf-n-importance-inv-s" class="headerlink" title="up_sample(self, rays_o, rays_d, z_vals, sdf, n_importance, inv_s):"></a>up_sample(self, rays_o, rays_d, z_vals, sdf, n_importance, inv_s):</h4><p>input:</p><ul><li>rays_o,</li><li>rays_d,</li><li>z_vals, batch_size X n_samples</li><li>sdf, batch_size X n_samples</li><li>self.n_importance // self.up_sample_steps, 每步处理$\frac{importance}{sampls.steps}$</li><li><code>64 * 2**i</code> , $64  \cdot  2^{i}$</li></ul><p>output:</p><ul><li>new_z_vals: batch_size X n_importance // up_sample_steps * steps_i</li></ul><p>function:</p><ul><li>pts: batch_size,n_samples,3</li><li>radius: pts的2-范数norm(ord=2)<ul><li>batch_size, n_samples</li></ul></li><li>inside_sphere: <code>inside_sphere = (radius[:, :-1] &lt; 1.0) | (radius[:, 1:] &lt; 1.0)</code><ul><li>point是否在单位圆的空间内</li><li>batch_size, n_samples - 1</li></ul></li><li>prev_sdf, next_sdf: 光线上sdf的前后 <code>prev_sdf[1] = next_sdf[0] = sdf[1]</code> <ul><li>batch_size, n_samples - 1</li></ul></li><li>prev_z_vals, next_z_vals:  光线上z坐标的前后 <code>prev_z_vals[1] = next_z_vals[0] = z_vals[1]</code> <ul><li>batch_size, n_samples - 1</li></ul></li><li>mid.sdf:  $mid.sdf = \frac{prev.sdf + next.sdf}{2} = \frac{f(p_{i})+f(p_{i+1})}{2}$<ul><li>batch_size, n_samples - 1</li></ul></li><li><p>cos_val: $cos.val = \frac{next.sdf - prev.sdf}{next.z.vals - prev.z.vals + 1e-5} = \frac{f(p_{i})-f(p_{i+1})}{z_{i}-z_{i+1}}$</p><ul><li>batch_size, n_samples - 1 </li></ul></li><li><p>prev_cos_val： 将cos_val堆叠，且最后一个删除，第一个插入0 <code>prev_cos_val[0] = 0, prev_cos_val[1] = cos_val[0]</code></p><ul><li>batch_size, n_samples - 1</li></ul></li><li>cos_val: stack prev_cos_val and cos_val<ul><li>batch_size, n_samples - 1, 2 </li></ul></li><li>cos_val: 在prev_cos_val和cos_val之间选择最小值，这一步的目的是当发生一条光线穿过物体两次时，具有更好的鲁棒性<ul><li>batch_size, n_samples - 1</li></ul></li><li>cos_val: 将cos_val限制在$-1 \times 10^{3}$和0之间，并将在单位圆空间外的值置False <code>cos_val.clip(-1e3, 0.0) * inside_sphere</code><ul><li>batch_size, n_samples - 1</li></ul></li><li>dist:  两点之间的距离 $dist = next.z.vals- prev.z.vals= z_{i+1}-z_{i}$<ul><li>batch_size, n_samples - 1 </li></ul></li></ul><p>batch_size, n_samples - 1: </p><ul><li>prev_esti_sdf: $\frac{mid.sdf - cos.val * dist}{2} \approx f(p_{i})$</li><li>next_esti_sdf: $\frac{mid.sdf + cos.val * dist}{2} \approx f(p_{i+1})$</li><li>prev_cdf: $prev.cdf = sigmoid(prev.esti.sdf \times inv.s) = sigmoid(\approx f(p_{i})\times 64  \cdot  2^{i})$</li><li>next_cdf: $next.cdf = sigmoid(next.esti.sdf \times inv.s) = sigmoid(\approx f(p_{i+1})\times 64  \cdot  2^{i})$</li><li>alpha: $\alpha = \frac{prev.cdf - next.cdf + 1 \times 10^{-5}}{prev.cdf + 1 \times 10^{-5}}$ is  $\alpha_i=\max\left(\frac{\Phi_s(f(\mathbf{p}(t_i))))-\Phi_s(f(\mathbf{p}(t_{i+1})))}{\Phi_s(f(\mathbf{p}(t_i)))},0\right).$</li><li>weights: $w_{i} = \alpha_{i} \cdot T_{i} =\alpha_{i} \cdot \prod_{j=1}^{i-1}(1-\alpha_j)$<ul><li>in code : <code>weights = alpha * torch.cumprod(torch.cat([torch.ones([batch_size, 1]), 1. - alpha + 1e-7], -1), -1)[:, :-1]</code></li></ul></li></ul><p><code>z_samples = sample_pdf(z_vals, weights, n_importance, det=True).detach()</code></p><h5 id="sample-pdf-z-vals-weights-n-importance-det-True"><a href="#sample-pdf-z-vals-weights-n-importance-det-True" class="headerlink" title="sample_pdf(z_vals, weights, n_importance, det=True)"></a>sample_pdf(z_vals, weights, n_importance, det=True)</h5><p>like NeRF</p><p>input:</p><ul><li>z_vals, batch_size X n_samples</li><li>weights, batch_size, n_samples - 1</li><li>n_importance, </li><li>det=True</li></ul><p>output:</p><ul><li>z_samples, batch_size X n_importance 经过逆变换采样得到的采样点的z坐标值</li></ul><h4 id="cat-z-vals-rays-o-rays-d-z-vals-new-z-vals-sdf-last-i-1-self-up-sample-steps"><a href="#cat-z-vals-rays-o-rays-d-z-vals-new-z-vals-sdf-last-i-1-self-up-sample-steps" class="headerlink" title="cat_z_vals(rays_o,rays_d,z_vals,new_z_vals,sdf,last=(i + 1 == self.up_sample_steps))"></a>cat_z_vals(rays_o,rays_d,z_vals,new_z_vals,sdf,last=(i + 1 == self.up_sample_steps))</h4><p>将原来的z_vals和经过逆变换采样得到的new_z_vals一起cat起来</p><p>input:</p><ul><li>rays_o,</li><li>rays_d,</li><li>z_vals, batch_size X n_samples</li><li>new_z_vals, <code>batch_size X n_importance // up_sample_steps * steps_i</code></li><li>sdf, batch_size X n_samples</li><li>last=(i + 1 == self.up_sample_steps): true(last step) or false</li></ul><p>output:</p><ul><li>z_vals, <code>batch_size X n_samples + n_importance // up_sample_steps * steps_i</code></li><li>sdf,  <code>batch_size X n_samples + n_importance // up_sample_steps * steps_i</code> when not last</li></ul><p><strong>last:</strong><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z_vals : batch_size X n_samples + n_importance </span><br><span class="line">n_samples = self.n_samples + self.n_important</span><br></pre></td></tr></table></figure></p><p><strong>then :</strong></p><ul><li>z_vals : batch_size X n_samples</li></ul><h3 id="render-core-outside-rays-o-rays-d-z-vals-feed-sample-dist-self-nerf"><a href="#render-core-outside-rays-o-rays-d-z-vals-feed-sample-dist-self-nerf" class="headerlink" title="render_core_outside(rays_o, rays_d, z_vals_feed, sample_dist, self.nerf)"></a>render_core_outside(rays_o, rays_d, z_vals_feed, sample_dist, self.nerf)</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">in render()</span><br><span class="line"># Background model</span><br><span class="line">if self.n_outside &gt; 0:</span><br><span class="line">    z_vals_feed = torch.cat([z_vals, z_vals_outside], dim=-1) # [batch_size, n_samples + n_outside]</span><br><span class="line">    z_vals_feed, _ = torch.sort(z_vals_feed, dim=-1)</span><br><span class="line">    ret_outside = self.render_core_outside(rays_o, rays_d, z_vals_feed, sample_dist, self.nerf)</span><br><span class="line"></span><br><span class="line">    background_sampled_color = ret_outside[&#x27;sampled_color&#x27;]</span><br><span class="line">    background_alpha = ret_outside[&#x27;alpha&#x27;]</span><br></pre></td></tr></table></figure><p>input: </p><ul><li>rays_o, <code>[batch_size,  3]</code></li><li>rays_d, <code>[batch_size,  3]</code></li><li>z_vals_feed, <code>batch_size, n_samples + n_outside</code> ,实际上此处为<code>[batch_size, n_samples + n_outside +n_importance]</code></li><li>sample_dist, $sample.dist = \frac{2.0}{n.samples}$</li><li>self.nerf, NeRF神经网络，使用nerf渲染函数进行color的计算<ul><li>如果使用了白色背景，color还需累加白背景<ul><li><code>background_rgb = torch.ones([1, 3])</code></li><li><code>color = color + background_rgb * (1.0 - weights_sum)</code></li></ul></li></ul></li></ul><p>output: ret_outside字典<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#x27;color&#x27;: color, # batch_size, 3</span><br><span class="line">    &#x27;sampled_color&#x27;: sampled_color, # batch_size, n_samples + n_outside, 3</span><br><span class="line">    &#x27;alpha&#x27;: alpha, # batch_size, n_samples + n_outside</span><br><span class="line">    &#x27;weights&#x27;: weights, # batch_size, n_samples + n_outside</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>function: like NeRF</p><ul><li>dis_to_center: 坐标的2范数，并限制在$1$ ~ $1 \times 10^{10}$<ul><li>batch_size, n_samples, 1 </li></ul></li><li>pts: <code>torch.cat([pts / dis_to_center, 1.0 / dis_to_center], dim=-1)</code><ul><li>batch_size, n_samples, 4</li><li>归一化pts, $\frac{x}{\sqrt{x^{2}+y^{2}+z^{2}}},\frac{y}{\sqrt{x^{2}+y^{2}+z^{2}}},\frac{z}{\sqrt{x^{2}+y^{2}+z^{2}}},\frac{1}{\sqrt{x^{2}+y^{2}+z^{2}}}$</li></ul></li></ul><h3 id="render-core"><a href="#render-core" class="headerlink" title="render_core()"></a>render_core()</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">render continue</span><br><span class="line">    background_sampled_color = ret_outside[&#x27;sampled_color&#x27;]</span><br><span class="line">    background_alpha = ret_outside[&#x27;alpha&#x27;]</span><br><span class="line"></span><br><span class="line"># Render core</span><br><span class="line">ret_fine = self.render_core(rays_o,</span><br><span class="line">                            rays_d,</span><br><span class="line">                            z_vals,</span><br><span class="line">                            sample_dist,</span><br><span class="line">                            self.sdf_network,</span><br><span class="line">                            self.deviation_network,</span><br><span class="line">                            self.color_network,</span><br><span class="line">                            background_rgb=background_rgb,</span><br><span class="line">                            background_alpha=background_alpha,</span><br><span class="line">                            background_sampled_color=background_sampled_color,</span><br><span class="line">                            cos_anneal_ratio=cos_anneal_ratio)</span><br></pre></td></tr></table></figure><p>input:</p><ul><li>rays_o, <code>[batch_size,  3]</code></li><li>rays_d, <code>[batch_size,  3]</code></li><li>z_vals, <code>batch_size, n_samples</code> ,实际上为<code>batch_size, n_samples + n_importance</code> </li><li>sample_dist, $sample.dist = \frac{2.0}{n.samples}$</li><li>self.sdf_network, sdf神经网络</li><li>self.deviation_network, inv_s参数神经网络</li><li>self.color_network, 采样点color神经网络</li><li>background_rgb=background_rgb, <code>batch_size, 3</code></li><li>background_alpha=background_alpha, <code>batch_size, n_samples + n_outside</code></li><li>background_sampled_color=background_sampled_color, <code>batch_size, n_samples + n_outside, 3</code></li><li>cos_anneal_ratio=cos_anneal_ratio ,数1或者比一小的数$\frac{iterstep}{anneal}, anneal=50000$</li></ul><p>output: ret_fine字典<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#x27;color&#x27;: color, # batch_size, 3</span><br><span class="line">    &#x27;sdf&#x27;: sdf, # batch_size * n_samples, 1</span><br><span class="line">    &#x27;dists&#x27;: dists, # batch_size, n_samples</span><br><span class="line">    &#x27;gradients&#x27;: gradients.reshape(batch_size, n_samples, 3),</span><br><span class="line">    &#x27;s_val&#x27;: 1.0 / inv_s, # batch_size * n_samples, 1</span><br><span class="line">    &#x27;mid_z_vals&#x27;: mid_z_vals, # batch_size, n_samples</span><br><span class="line">    &#x27;weights&#x27;: weights, # batch_size, n_samples or batch_size, n_samples + n_outside</span><br><span class="line">    &#x27;cdf&#x27;: c.reshape(batch_size, n_samples), # batch_size, n_samples</span><br><span class="line">    &#x27;gradient_error&#x27;: gradient_error, # 1</span><br><span class="line">    &#x27;inside_sphere&#x27;: inside_sphere # batch_size, n_samples</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>function:</p><ul><li>dists: 采样点间距离,$dists = z_{i+1} - z_{i}$<ul><li>batch_size, n_samples - 1 </li></ul></li><li>dists: 最后一行添加固定的粗采样点间距: $sample.dist = \frac{2.0}{n.samples}$<ul><li>batch_size, n_samples</li></ul></li><li>mid_z_vals:  $mid = z_{i} + \frac{dist_{i}}{2}$<ul><li>batch_size, n_samples</li></ul></li><li>pts:  $pts = \vec o + \vec d \cdot mid$<ul><li>batch_size, n_samples, 3 </li></ul></li><li>dirs: 方向向量扩展得到 <code>rays_d[:, None, :].expand(batch_size, n_samples, 3)</code><ul><li>batch_size, n_samples, 3 </li></ul></li><li>pts: reshape to batch_size * n_samples, 3 </li><li>dirs: reshape to batch_size * n_samples, 3 </li><li>sdf_nn_output:  =  sdf_network(pts)<ul><li>batch_size * n_samples, 257</li></ul></li><li>sdf: <code>sdf = sdf_nn_output[:, :1]</code><ul><li>batch_size * n_samples, 1</li></ul></li><li>feature_vector:  <code>feature_vector = sdf_nn_output[:, 1:]</code><ul><li>batch_size * n_samples, 256</li></ul></li><li>gradients:  梯度,sdf对输入pts_xyz的梯度，与法向量有关<ul><li>batch_size * n_samples, 3</li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def gradient(self, x):</span><br><span class="line">    # x : [batch_size * n_samples , 3]</span><br><span class="line">    x.requires_grad_(True) </span><br><span class="line">    y = self.sdf(x) # y : [batch_size * n_samples , 1]</span><br><span class="line">    d_output = torch.ones_like(y, requires_grad=False, device=y.device) # d_output : [batch_size * n_samples , 1]</span><br><span class="line">    # torch.autograd.grad : 计算梯度,返回一个元组，元组中的每个元素都是输入的梯度</span><br><span class="line">    gradients = torch.autograd.grad(</span><br><span class="line">        outputs=y,</span><br><span class="line">        inputs=x,</span><br><span class="line">        grad_outputs=d_output,</span><br><span class="line">        create_graph=True,</span><br><span class="line">        retain_graph=True,</span><br><span class="line">        only_inputs=True)[0]</span><br><span class="line">    return gradients.unsqueeze(1) # unsqueeze(1) : 在第1维增加一个维度</span><br><span class="line">    # return : [batch_size * n_samples , 1 , 3]</span><br></pre></td></tr></table></figure><ul><li>sampled_color: batch_size, n_samples, 3<ul><li><code>color_network(pts, gradients, dirs, feature_vector).reshape(batch_size, n_samples, 3)</code></li></ul></li><li>inv_s: <code>deviation_network(torch.zeros([1, 3]))[:, :1].clip(1e-6, 1e6)</code><ul><li>一个可以更新的变量 $1 \times e^{10.0 \cdot var}$ ，并将其限制在$1 \times 10^{-6}$ ~ $1 \times 10^{6}$之间</li><li>这个变量是用于sigmoid函数的输入，使其乘以s</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230630181909.png" alt="image.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class SingleVarianceNetwork(nn.Module):</span><br><span class="line">    def __init__(self, init_val):</span><br><span class="line">        super(SingleVarianceNetwork, self).__init__()</span><br><span class="line">        # variance 模型可以跟踪和优化这个参数，使其在训练过程中进行更新</span><br><span class="line">        self.register_parameter(&#x27;variance&#x27;, nn.Parameter(torch.tensor(init_val)))</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # torch.zeros([1, 3])</span><br><span class="line">        # 大小为 [len(x), 1] 的张量，每个元素都是 exp(variance * 10.0)</span><br><span class="line">        return torch.ones([len(x), 1]) * torch.exp(self.variance * 10.0)</span><br></pre></td></tr></table></figure><blockquote><p><a href="https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/mlp.html">4.1. 多层感知机 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230630132744.png" alt="image.png"></p><div style="display:flex; justify-content:space-between;"> <img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230630174602.png" alt="Image 1" style="width:50%;"><div style="width:10px;"></div> <img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230630174609.png" alt="Image 2" style="width:50%;"> </div><p>可以看出sigmoid函数的导数是一个偶函数，即$\phi(-x) = \phi(x)$</p><ul><li>inv_s: expand a num to <code>batch_size * n_samples, 1</code></li><li>true_cos: $true.cos = \frac{dx \cdot gx + dy \cdot gy + dz \cdot gz}{\sqrt{dx^{2}+dy^{2}+dz^{2}} \cdot \sqrt{gx^{2}+gy^{2}+gz^{2}}}$ 为sdf梯度方向，即物体表面的法线方向向量$\vec g$与光线方向向量$\vec d$的夹角<ul><li>batch_size * n_samples, 1 </li><li><code>true_cos = (dirs * gradients).sum(-1, keepdim=True)</code></li></ul></li></ul><div class="note info">            <p>why <code>true_cos = (dirs * gradients).sum(-1, keepdim=True)</code></p><ul><li>cdf对t的导数：$\frac{\mathrm{d}\Phi_s}{\mathrm{d}t}(f(\mathbf{p}(t)))= \nabla f(\mathbf{p}(t))\cdot\mathbf{v} \cdot \phi_s(f(\mathbf{p}(t)))$</li><li>sdf对t的导数：$\frac{\mathrm{d}f(\mathbf{p}(t))}{\mathrm{d}t}= \nabla f(\mathbf{p}(t))\cdot\mathbf{v}$，即为true_cos</li></ul>          </div><ul><li>iter_cos: $= -[relu(\frac{-true.cos+1}{2}) \cdot (1.0 - cos.anneal.ratio)+  relu(-true.cos) \cdot cos.anneal.ratio]$<ul><li>batch_size * n_samples, 1 </li><li>iter_cos 总是非正数</li><li>cos_anneal_ratio 数1或者比一小的数$\frac{iterstep}{anneal}, anneal=50000$ in womask cos_anneal_ratio is from 0 to 1, and always 1 after anneal steps<ul><li>anneal = 0 in wmask, then cos_anneal_ratio is always 1</li></ul></li></ul></li></ul><p>batch_size * n_samples, 1: </p><ul><li>estimated_next_sdf: $est.next.sdf = sdf + iter.cos \times dist \times 0.5$<ul><li><code>estimated_next_sdf = sdf + iter_cos * dists.reshape(-1, 1) * 0.5</code></li></ul></li><li>estimated_prev_sdf: $est.prev.sdf = sdf - iter.cos \times dist \times 0.5$<ul><li><code>estimated_prev_sdf = sdf - iter_cos * dists.reshape(-1, 1) * 0.5</code></li></ul></li><li>prev_cdf: $prev.cdf = sigmoid(est.prev.sdf \cdot inv.s)$<ul><li><code>prev_cdf = torch.sigmoid(estimated_prev_sdf * inv_s)</code></li></ul></li><li>next_cdf: $next.cdf = sigmoid(est.next.sdf \cdot inv.s)$<ul><li><code>next_cdf = torch.sigmoid(estimated_next_sdf * inv_s)</code></li></ul></li><li><p><code>p = prev_cdf - next_cdf ,  c = prev_cdf</code></p></li><li><p>alpha: $\alpha = \frac{p + 10^{-5}}{c + 10^{-5}} = \frac{prev.cdf - next.cdf}{prev.cdf}$ and in (0.0,1.0)</p><ul><li><script type="math/tex; mode=display">\alpha_i=\max\left(\frac{\Phi_s(f(\mathbf{p}(t_i))))-\Phi_s(f(\mathbf{p}(t_{i+1})))}{\Phi_s(f(\mathbf{p}(t_i)))},0\right).</script></li><li>batch_size, n_samples</li><li><code>alpha = ((p + 1e-5) / (c + 1e-5)).reshape(batch_size, n_samples).clip(0.0, 1.0)</code></li></ul></li><li>pts_norm: $\sqrt{x^{2}+y^{2}+z^{2}}$<ul><li>batch_size, n_samples</li><li><code>pts_norm = torch.linalg.norm(pts, ord=2, dim=-1, keepdim=True).reshape(batch_size, n_samples)</code></li></ul></li><li>inside_sphere, 在单位圆内的点置位 True，在外的为False<ul><li>batch_size, n_samples</li><li><code>inside_sphere = (pts_norm &lt; 1.0).float().detach()</code></li></ul></li><li>relax_inside_sphere，更放松一点的限制：在半径1.2的圆内的点<ul><li>batch_size, n_samples</li><li><code>relax_inside_sphere = (pts_norm &lt; 1.2).float().detach()</code></li></ul></li></ul><p>if background_alpha 不是 None，计算过背景的alpha值，将背景与物体前景的alpha和采样点颜色值cat起来</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if background_alpha is not None:</span><br><span class="line">    alpha = alpha * inside_sphere + background_alpha[:, :n_samples] * (1.0 - inside_sphere) # batch_size, n_samples</span><br><span class="line">    alpha = torch.cat([alpha, background_alpha[:, n_samples:]], dim=-1) # batch_size, n_samples + n_outside</span><br><span class="line">    sampled_color = sampled_color * inside_sphere[:, :, None] +\</span><br><span class="line">                    background_sampled_color[:, :n_samples] * (1.0 - inside_sphere)[:, :, None] # batch_size, n_samples, 3</span><br><span class="line">    sampled_color = torch.cat([sampled_color, background_sampled_color[:, n_samples:]], dim=1) # batch_size, n_samples + n_outside, 3</span><br></pre></td></tr></table></figure><ul><li>weights，计算每个采样点的权重 $w_{i} = \alpha_{i} \cdot T_{i} =\alpha_{i} \cdot \prod_{j=1}^{i-1}(1-\alpha_j)$<ul><li>batch_size, n_samples <strong>or</strong> batch_size, n_samples + n_outside</li><li><code>weights = alpha * torch.cumprod(torch.cat([torch.ones([batch_size, 1]), 1. - alpha + 1e-7], -1), -1)[:, :-1]</code></li></ul></li><li>weights_sum：权重的和，方便前景颜色与背景的颜色进行累加<ul><li>batch_size, 1</li><li><code>weights_sum = weights.sum(dim=-1, keepdim=True)</code></li></ul></li><li>color：$\hat{C}=\sum_{i=1}^n T_i\alpha_i c_i,$<ul><li>batch_size, 3</li><li>`color = (sampled_color * weights[:, :, None]).sum(dim=1)</li></ul></li></ul><p>累加背景的颜色值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if background_rgb is not None:    # Fixed background, usually black</span><br><span class="line">    color = color + background_rgb * (1.0 - weights_sum) # batch_size, 3</span><br></pre></td></tr></table></figure><p>计算loss<br>$\mathcal{L}_{r e g}=\frac{1}{n m}\sum_{k,i}(|\nabla f(\hat{\mathbf{p}}_{k,i})|_{2}-1)^{2}.$ 只计算在relax半径为1.2的圆内的采样点sdf的梯度</p><p>$|\nabla f(\hat{\mathbf{p}}_{k,i})|_{2} = \sqrt{gx^{2}+gy^{2}+gz^{2}}$</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Eikonal loss</span><br><span class="line">gradient_error = (torch.linalg.norm(gradients.reshape(batch_size, n_samples, 3), ord=2,</span><br><span class="line">                                    dim=-1) - 1.0) ** 2</span><br><span class="line"># gradient_error : batch_size, n_samples</span><br><span class="line"></span><br><span class="line">gradient_error = (relax_inside_sphere * gradient_error).sum() / (relax_inside_sphere.sum() + 1e-5)</span><br><span class="line"># gradient_error : 1</span><br></pre></td></tr></table></figure><h2 id="render后"><a href="#render后" class="headerlink" title="render后"></a>render后</h2><h3 id="get-loss"><a href="#get-loss" class="headerlink" title="get loss"></a>get loss</h3><ul><li>color_fine_loss: $\mathcal{L}_{color}=\frac{1}{m}\sum_k\mathcal{R}(\hat{C}_k,C_k).$</li><li>eikonal_loss: $\mathcal{L}_{r e g}=\frac{1}{n m}\sum_{k,i}(|\nabla f(\hat{\mathbf{p}}_{k,i})|_{2}-1)^{2}.$</li><li>mask_loss: $\mathcal{L}_{mask}=\mathrm{BCE}(M_k,\hat{O}_k)$</li></ul><p>total loss: $\mathcal L=\mathcal L_{color}+\lambda\mathcal L_{reg}+\beta\mathcal L_{mask}.$</p><ul><li>igr_weight = 0.1</li><li>mask_weight = 0.1 or 0.0 if womask</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">color_error = (color_fine - true_rgb) * mask</span><br><span class="line">color_fine_loss = F.l1_loss(color_error, torch.zeros_like(color_error), reduction=&#x27;sum&#x27;) / mask_sum</span><br><span class="line">psnr = 20.0 * torch.log10(1.0 / (((color_fine - true_rgb)**2 * mask).sum() / (mask_sum * 3.0)).sqrt())</span><br><span class="line"></span><br><span class="line">eikonal_loss = gradient_error</span><br><span class="line"></span><br><span class="line">mask_loss = F.binary_cross_entropy(weight_sum.clip(1e-3, 1.0 - 1e-3), mask)</span><br><span class="line"></span><br><span class="line">loss = color_fine_loss +\</span><br><span class="line">       eikonal_loss * self.igr_weight +\</span><br><span class="line">       mask_loss * self.mask_weight</span><br></pre></td></tr></table></figure><h3 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">self.optimizer.step()</span><br><span class="line"></span><br><span class="line">self.iter_step += 1</span><br></pre></td></tr></table></figure><h3 id="log-tensorboard-scalar"><a href="#log-tensorboard-scalar" class="headerlink" title="log(tensorboard.scalar)"></a>log(tensorboard.scalar)</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line">self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, &#x27;logs&#x27;))</span><br><span class="line"># if in autodl server , use: </span><br><span class="line"># self.writer = SummaryWriter(log_dir=os.path.join(&#x27;/root/tf-logs&#x27;))</span><br><span class="line"></span><br><span class="line">self.writer.add_scalar(&#x27;Loss/loss&#x27;, loss, self.iter_step)</span><br><span class="line">self.writer.add_scalar(&#x27;Loss/color_loss&#x27;, color_fine_loss, self.iter_step)</span><br><span class="line">self.writer.add_scalar(&#x27;Loss/eikonal_loss&#x27;, eikonal_loss, self.iter_step)</span><br><span class="line">self.writer.add_scalar(&#x27;Statistics/s_val&#x27;, s_val.mean(), self.iter_step)</span><br><span class="line">self.writer.add_scalar(&#x27;Statistics/cdf&#x27;, (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)</span><br><span class="line">self.writer.add_scalar(&#x27;Statistics/weight_max&#x27;, (weight_max * mask).sum() / mask_sum, self.iter_step)</span><br><span class="line">self.writer.add_scalar(&#x27;Statistics/psnr&#x27;, psnr, self.iter_step)</span><br></pre></td></tr></table></figure><h3 id="other-per-step"><a href="#other-per-step" class="headerlink" title="other per step"></a>other per step</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">if self.iter_step % self.report_freq == 0:</span><br><span class="line">    print(self.base_exp_dir)</span><br><span class="line">    print(&#x27;iter:&#123;:8&gt;d&#125; loss = &#123;&#125; lr=&#123;&#125;&#x27;.format(self.iter_step, loss, self.optimizer.param_groups[0][&#x27;lr&#x27;]))</span><br><span class="line"></span><br><span class="line">if self.iter_step % self.save_freq == 0:</span><br><span class="line">    self.save_checkpoint()</span><br><span class="line"></span><br><span class="line">if self.iter_step % self.val_freq == 0:</span><br><span class="line">    self.validate_image()</span><br><span class="line"></span><br><span class="line"># 每经过一定的迭代次数，就验证一次mesh， 5000步val一次，默认mesh的resolution=64</span><br><span class="line">if self.iter_step % self.val_mesh_freq == 0:</span><br><span class="line">    self.validate_mesh()</span><br><span class="line"></span><br><span class="line">self.update_learning_rate()</span><br><span class="line"></span><br><span class="line">if self.iter_step % len(image_perm) == 0:</span><br><span class="line">    image_perm = self.get_image_perm() # 重新随机一下image</span><br></pre></td></tr></table></figure><h4 id="validate-image"><a href="#validate-image" class="headerlink" title="validate_image"></a>validate_image</h4><p>将图片缩小resolution_level倍进行光线生成，然后分批次进行渲染，每批大小为batch_size</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def validate_image(self, idx=-1, resolution_level=-1):</span><br><span class="line">    if idx &lt; 0:</span><br><span class="line">        idx = np.random.randint(self.dataset.n_images)</span><br><span class="line"></span><br><span class="line">    print(&#x27;Validate: iter: &#123;&#125;, camera: &#123;&#125;&#x27;.format(self.iter_step, idx))</span><br><span class="line"></span><br><span class="line">    if resolution_level &lt; 0:</span><br><span class="line">        resolution_level = self.validate_resolution_level</span><br><span class="line">    rays_o, rays_d = self.dataset.gen_rays_at(idx, resolution_level=resolution_level)</span><br><span class="line">    H, W, _ = rays_o.shape</span><br><span class="line">    rays_o = rays_o.reshape(-1, 3).split(self.batch_size) # H*W / batch_size 个元组，每个元组中有batch_size个ray: (batch_size, 3)</span><br><span class="line">    rays_d = rays_d.reshape(-1, 3).split(self.batch_size) </span><br></pre></td></tr></table></figure><p>最终得到该图片每个像素的颜色值out_rgb_fine，以及inside_sphere内的法向量值out_normal_fine</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">for rays_o_batch, rays_d_batch in zip(rays_o, rays_d):</span><br><span class="line">    # rays_o_batch: (batch_size, 3) rays_d_batch: (batch_size, 3)</span><br><span class="line">    near, far = self.dataset.near_far_from_sphere(rays_o_batch, rays_d_batch)</span><br><span class="line">    background_rgb = torch.ones([1, 3]) if self.use_white_bkgd else None</span><br><span class="line"></span><br><span class="line">    render_out = self.renderer.render(rays_o_batch,</span><br><span class="line">                                      rays_d_batch,</span><br><span class="line">                                      near,</span><br><span class="line">                                      far,</span><br><span class="line">                                      cos_anneal_ratio=self.get_cos_anneal_ratio(),</span><br><span class="line">                                      background_rgb=background_rgb)</span><br><span class="line"></span><br><span class="line">    def feasible(key): return (key in render_out) and (render_out[key] is not None)</span><br><span class="line">    </span><br><span class="line">    if feasible(&#x27;color_fine&#x27;):</span><br><span class="line">        out_rgb_fine.append(render_out[&#x27;color_fine&#x27;].detach().cpu().numpy())</span><br><span class="line">    if feasible(&#x27;gradients&#x27;) and feasible(&#x27;weights&#x27;):</span><br><span class="line">        n_samples = self.renderer.n_samples + self.renderer.n_importance</span><br><span class="line">        # (batch_size, n_samples, 3) * (batch_size, n_samples, 1) -&gt; (batch_size, n_samples, 3)</span><br><span class="line">        normals = render_out[&#x27;gradients&#x27;] * render_out[&#x27;weights&#x27;][:, :n_samples, None] </span><br><span class="line">        if feasible(&#x27;inside_sphere&#x27;):</span><br><span class="line">            normals = normals * render_out[&#x27;inside_sphere&#x27;][..., None]</span><br><span class="line">        normals = normals.sum(dim=1).detach().cpu().numpy()</span><br><span class="line">        out_normal_fine.append(normals)</span><br><span class="line">    del render_out</span><br></pre></td></tr></table></figure><p>然后进行图片的拼接和保存</p><div style="display:flex; justify-content:space-between;"> <img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/00282500_0_22.png" alt="Image 1" style="width:10%;"><div style="width:10px;"></div> <img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/00282500_0_22%20(1).png" alt="Image 2" style="width:20%;"> </div><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">img_fine = None</span><br><span class="line">if len(out_rgb_fine) &gt; 0:</span><br><span class="line">    img_fine = (np.concatenate(out_rgb_fine, axis=0).reshape([H, W, 3, -1]) * 256).clip(0, 255)</span><br><span class="line"></span><br><span class="line">normal_img = None</span><br><span class="line">if len(out_normal_fine) &gt; 0:</span><br><span class="line">    normal_img = np.concatenate(out_normal_fine, axis=0)</span><br><span class="line">    # pose: c2w </span><br><span class="line">    # rot: w2c</span><br><span class="line">    rot = np.linalg.inv(self.dataset.pose_all[idx, :3, :3].detach().cpu().numpy())</span><br><span class="line">    normal_img = (np.matmul(rot[None, :, :], normal_img[:, :, None])</span><br><span class="line">                  .reshape([H, W, 3, -1]) * 128 + 128).clip(0, 255)</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(self.base_exp_dir, &#x27;validations_fine&#x27;), exist_ok=True)</span><br><span class="line">os.makedirs(os.path.join(self.base_exp_dir, &#x27;normals&#x27;), exist_ok=True)</span><br><span class="line"></span><br><span class="line">for i in range(img_fine.shape[-1]):  # img_fine.shape[-1] = 1</span><br><span class="line">    if len(out_rgb_fine) &gt; 0:</span><br><span class="line">        cv.imwrite(os.path.join(self.base_exp_dir,</span><br><span class="line">                                &#x27;validations_fine&#x27;,</span><br><span class="line">                                &#x27;&#123;:0&gt;8d&#125;_&#123;&#125;_&#123;&#125;.png&#x27;.format(self.iter_step, i, idx)),</span><br><span class="line">                   np.concatenate([img_fine[..., i],</span><br><span class="line">                                   self.dataset.image_at(idx, resolution_level=resolution_level)]))</span><br><span class="line">    if len(out_normal_fine) &gt; 0:</span><br><span class="line">        cv.imwrite(os.path.join(self.base_exp_dir,</span><br><span class="line">                                &#x27;normals&#x27;,</span><br><span class="line">                                &#x27;&#123;:0&gt;8d&#125;_&#123;&#125;_&#123;&#125;.png&#x27;.format(self.iter_step, i, idx)),</span><br><span class="line">                   normal_img[..., i])</span><br></pre></td></tr></table></figure><h4 id="validate-mesh生成mesh模型"><a href="#validate-mesh生成mesh模型" class="headerlink" title="validate_mesh生成mesh模型"></a>validate_mesh生成mesh模型</h4><p>根据一个$resolution^3$ 的sdf场，将阈值为0的点使用marching_cubes方法生成vertices和triangles，然后生成mesh的ply文件</p><h5 id="extract-geometry"><a href="#extract-geometry" class="headerlink" title="extract_geometry"></a>extract_geometry</h5><p><strong>extract_fields</strong><br>input:</p><ul><li>bound_min : 3 ; bound_max : 3 ; resolution : 64 </li><li>query_func : pts -&gt; sdf</li></ul><p>output: u<br>u : resolution x resolution x resolution, 为box 中每个点的sdf值</p><p><strong>extract_geometry</strong></p><p>根据体积数据和阈值重建出表面</p><blockquote><p><a href="https://github.com/pmneila/PyMCubes">pmneila/PyMCubes: Marching cubes (and related tools) for Python (github.com)</a></p></blockquote><p>input:</p><ul><li>bound_min, bound_max, resolution, </li><li>threshold, 用于<code>vertices, triangles = mcubes.marching_cubes(u, threshold)</code>，在等threshold面上，生成mesh的v和t</li><li>query_func，根据位置pts利用network计算出sdf<ul><li>query_func=lambda pts: -self.sdf_network.sdf(pts)<br>output:</li></ul></li><li>vertices：三角形网格点<ul><li>N_v , 3: 3为点的三维坐标</li></ul></li><li>triangles：三角形网格<ul><li>N_t , 3: 3为三角形网格顶点的索引index</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230630210635.png" alt="images.png"></p><p>根据v和t，<code>mesh = trimesh.Trimesh(vertices, triangles)</code>生成mesh，并导出ply：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mesh.export(os.path.join(self.base_exp_dir, &#x27;meshes&#x27;, &#x27;&#123;:0&gt;8d&#125;.ply&#x27;.format(self.iter_step)))</span><br></pre></td></tr></table></figure></p><h2 id="数据集自定义"><a href="#数据集自定义" class="headerlink" title="数据集自定义"></a>数据集自定义</h2><h3 id="custom-data流程图"><a href="#custom-data流程图" class="headerlink" title="custom_data流程图"></a>custom_data流程图</h3><p><iframe frameborder="0" style="width:100%;height:833px;" src="https://viewer.diagrams.net/?highlight=0000ff&edit=_blank&layers=1&nav=1&title=custom_data.drawio#R7Vxdc5s4FP01nmw7Ew%2BID5vHOIm7D7udzmRn2j55FCPbpICokB3TX78SiE8pMc0aQ9bOQ4IuEhb36BxdXSkeGbfB%2FhOB0eZv7CJ%2FBDR3PzLuRgDopq2zP9ySZBZ7AjLDmniuqFQaHrxfSBg1Yd16LoprFSnGPvWiunGJwxAtac0GCcHP9Wor7Nc%2FNYJrJBkeltCXrV89l27EWzhaaf8TeeuN%2BGRnKm4EMK8rDPEGuvi5YjLuR8YtwZhmV8H%2BFvncd7lbsnbzF%2B4W%2FSIopG0aOFMUff30z%2FWX%2B8l8PwnJLJnvrgUWMU3y90Uue31RxIRu8BqH0L8vrTOCt6GL%2BFM1Virr%2FIVxxIw6Mz4hShOBJdxSzEwbGvjiLtp79BtvPrZE6Xvlzt1ePDktJHkhpCSpNOLF79V7ZbO0lLfL3o%2B%2F1ItuE6YYb8lS1Hp6Mr651F2Es59m8FlPrPvJ7joffZCsEX2lHijAZaRAOECsP6wdQT6k3q7eDyhG57qoVyLILgSIakBf6%2BQO%2BlvxSV6wjkGEYxSPo0SCuwST%2B%2F9541H0EMHUD8%2BM0HXgVjikAlXdLpyr8OUOEYr2FZPsDnHXsrIWQh10W7DlueSanjNoU%2BGZpXXkQOv%2FzojWIB4kBGhJCFNJwr4YAiSGrFG4SAnyx4dX%2BKENgR%2FA7Jsf%2BbzcD0H0Cj1KshwiSI0eJVs6J4jZkiDOsAjSK8LvSwLbIjwZFMKmJIErz2dBNmvGfzEfxzQthdwHzC8xYp8jDwuywcHjNu5FGHMdEsJoAFkYizpVYZx2JYy25NSEu7ThMvZ2tO6bmBL8A91iHxNmCXHI6cTw8Bsm6HvrkBWXzGeI2WfcVx5bqNyIG4HnuikXVWDU%2BdkBHmZ9ojIVeJgKOEBXcIBLINdaxSYtVQwMK5KbSIzzMXQXS%2BwHMFq4kMIBBnTGZNxgylQR0uVh30lCuqnkxxCfj3AVEpTPJIbVr3I5EhxkG4pRPcABDSbWQf%2BdeIkiL%2FEeYYxcj4zAbQDpcrOgSYTOaIhPGxDpCoiMU45xfXqZnVvnHY2W07NuD2p6zvtdoWEmYsy2QpBuCVowxhG4pIxLwPY5%2BR751ZpfFXWrjH2lUhQx5h1VHHNW%2B2hFjxQkGw0igiIaqK5bNAUVbasrmCwJJv47uzgPfTScBixW34sXXXL%2BKeXxTUm23uTRbimPR8%2FBpE1vCIFJpUKEvZDGlSd%2F4YZK3kJvhP9i4VwOl%2ByJ5eApuvYfOC4nJ7iv6smd%2BcfxI8%2F42DDgrAsf4ygFpqm5I2t2tYQBIjC%2BYgEVe8qVF8A1KkqZC4y7q5Eli8jQFFmX1mOqTJJxyvUYeCV%2BPRdJ1p3Dq2RbAUp3kixHM%2BmeUTbmIzbeXXGJSHBOONVXz5aCPcpsRmdAGXKe%2BrK0OJDQO5z5G9gWlbyDEcMdGu4urj0GdT2zHAVPyrnoNPOMHBWUeRL%2Byhd9A3oju2Up5iHlPlN3E5Gc9s5jNNfb1WCxf2756bIZR%2BdaOPqG1UjDp%2BJuEdeJp7CYMMxtaB8xMSwCxUUW2o0jP8lrs3eoNqiY097UrcfvIKd9M2TNDjuF%2FLSTNrqfj2a3I%2BdmIP3NOMUbqcPsF7pXtYXbYJEF3OkwBZoh%2Flq%2F%2BY7nw2Fj3KCxbahorBcqfRL9NcAlUmkdqTgtIxVDH1akIu%2FmeCtWXqVZz%2BLIRYi5yH5O6VRIVbnkdtEKbv2iihTeDOcchq2Ia057DiMfABWPB17orZIBxoW2NraBU%2Fmpq9RUUwSJ2rQQs6o%2FzSP4U3lEfNKnSJ3ivJ9SL96mUoZiPaV0aq%2BaZMirJ34EVuT0hnhKXHdAI0unWj6pdjCPMXkrERzQ8SKtJS3qc7feCS1eHu0tWGGeiBXKD3cuOnd0QKd9AirHAQRBV%2BQ1suXh8cMBSdMUDn55H9JqI3NdnaVRO9HsVefeQgu9N1ooDmqonWr0yQt5I0PwopbjWXh8LY9imiV7BsYUEzSZovrPMeVBis6oIkdVRfYsVZwh%2BlFWHL13xek1K%2FKmibg%2FxZm2VJxeBUc%2BX%2FyMie8uAkjlVAcDBmhsKbJwvSUdHF8svc0Mrcp3dMcX%2B93N0D0Grq2naKvX0FWeo2P%2BlQTvkTKgcUJA%2BS%2Fep2WMfOIxIigieInimHkKzNPtFe5UnG4i5bst84%2By8xstAxj%2FOFhJZFoWcbRhIRZbivySt4OOiaLy2NRRQQWq%2FRRbAar9%2B6CyYvktGdmJuPKrRoz7fwE%3D"></iframe></p><h3 id="imgs2poses-py"><a href="#imgs2poses-py" class="headerlink" title="imgs2poses.py"></a>imgs2poses.py</h3><p>是否使用过colmap：</p><ul><li>如果已经使用colmap生成了<code>sparse/0/</code>下的<code>[&#39;cameras&#39;, &#39;images&#39;, &#39;points3D&#39;]</code>文件，将获得sparse_points.ply</li><li>若没有，则使用<code>run_colmap()</code>，即可生成sparse/0/下文件</li></ul><h4 id="run-colmap"><a href="#run-colmap" class="headerlink" title="run_colmap()"></a>run_colmap()</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">def run_colmap(basedir, match_type):</span><br><span class="line">    </span><br><span class="line">    logfile_name = os.path.join(basedir, &#x27;colmap_output.txt&#x27;)</span><br><span class="line">    logfile = open(logfile_name, &#x27;w&#x27;)</span><br><span class="line">    </span><br><span class="line">    feature_extractor_args = [</span><br><span class="line">        &#x27;colmap&#x27;, &#x27;feature_extractor&#x27;, </span><br><span class="line">            &#x27;--database_path&#x27;, os.path.join(basedir, &#x27;database.db&#x27;), </span><br><span class="line">            &#x27;--image_path&#x27;, os.path.join(basedir, &#x27;images&#x27;),</span><br><span class="line">            &#x27;--ImageReader.single_camera&#x27;, &#x27;1&#x27;,</span><br><span class="line">            # &#x27;--SiftExtraction.use_gpu&#x27;, &#x27;0&#x27;,</span><br><span class="line">    ]</span><br><span class="line">    # subprocess.check_output: 运行命令行程序，等待程序运行完成，然后返回输出结果</span><br><span class="line">    feat_output = ( subprocess.check_output(feature_extractor_args, universal_newlines=True) )</span><br><span class="line">    logfile.write(feat_output)</span><br><span class="line">    print(&#x27;Features extracted&#x27;)</span><br><span class="line"></span><br><span class="line">    exhaustive_matcher_args = [</span><br><span class="line">        &#x27;colmap&#x27;, match_type, </span><br><span class="line">            &#x27;--database_path&#x27;, os.path.join(basedir, &#x27;database.db&#x27;), </span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    match_output = ( subprocess.check_output(exhaustive_matcher_args, universal_newlines=True) )</span><br><span class="line">    logfile.write(match_output)</span><br><span class="line">    print(&#x27;Features matched&#x27;)</span><br><span class="line">    </span><br><span class="line">    p = os.path.join(basedir, &#x27;sparse&#x27;)</span><br><span class="line">    if not os.path.exists(p):</span><br><span class="line">        os.makedirs(p)</span><br><span class="line"></span><br><span class="line">    # mapper_args = [</span><br><span class="line">    #     &#x27;colmap&#x27;, &#x27;mapper&#x27;, </span><br><span class="line">    #         &#x27;--database_path&#x27;, os.path.join(basedir, &#x27;database.db&#x27;), </span><br><span class="line">    #         &#x27;--image_path&#x27;, os.path.join(basedir, &#x27;images&#x27;),</span><br><span class="line">    #         &#x27;--output_path&#x27;, os.path.join(basedir, &#x27;sparse&#x27;),</span><br><span class="line">    #         &#x27;--Mapper.num_threads&#x27;, &#x27;16&#x27;,</span><br><span class="line">    #         &#x27;--Mapper.init_min_tri_angle&#x27;, &#x27;4&#x27;,</span><br><span class="line">    # ]</span><br><span class="line">    mapper_args = [</span><br><span class="line">        &#x27;colmap&#x27;, &#x27;mapper&#x27;,</span><br><span class="line">            &#x27;--database_path&#x27;, os.path.join(basedir, &#x27;database.db&#x27;),</span><br><span class="line">            &#x27;--image_path&#x27;, os.path.join(basedir, &#x27;images&#x27;),</span><br><span class="line">            &#x27;--output_path&#x27;, os.path.join(basedir, &#x27;sparse&#x27;), # --export_path changed to --output_path in colmap 3.6</span><br><span class="line">            &#x27;--Mapper.num_threads&#x27;, &#x27;16&#x27;,</span><br><span class="line">            &#x27;--Mapper.init_min_tri_angle&#x27;, &#x27;4&#x27;,</span><br><span class="line">            &#x27;--Mapper.multiple_models&#x27;, &#x27;0&#x27;,</span><br><span class="line">            &#x27;--Mapper.extract_colors&#x27;, &#x27;0&#x27;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    map_output = ( subprocess.check_output(mapper_args, universal_newlines=True) )</span><br><span class="line">    logfile.write(map_output)</span><br><span class="line">    logfile.close()</span><br><span class="line">    print(&#x27;Sparse map created&#x27;)</span><br><span class="line">    </span><br><span class="line">    print( &#x27;Finished running COLMAP, see &#123;&#125; for logs&#x27;.format(logfile_name) )</span><br></pre></td></tr></table></figure><p>上述代码相当于分别运行:<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">colmap feature_extractor --database_path os.path.join(basedir, &#x27;database.db&#x27;) --image_path os.path.join(basedir, &#x27;images&#x27;) --ImageReader.single_camera 1</span><br><span class="line">colmap match_type --database_path os.path.join(basedir, &#x27;database.db&#x27;)</span><br><span class="line">match_type : exhaustive_matcher Or sequential_matcher</span><br><span class="line">colmap mapper --database_path os.path.join(basedir, &#x27;database.db&#x27;) --image_path os.path.join(basedir, &#x27;images&#x27;) --output_path os.path.join(basedir, &#x27;sparse&#x27;) --Mapper.num_threads 16 --Mapper.init_min_tri_angle 4 --Mapper.multiple_models 0 --Mapper.extract_colors 0</span><br></pre></td></tr></table></figure></p><ul><li>feature_extractor: Perform <strong>feature extraction or import features</strong> for a set of images.</li><li>exhaustive_matcher: Perform <strong>feature matching</strong> after performing feature extraction.</li><li>mapper: <strong>Sparse 3D reconstruction / mapping of the dataset</strong> using SfM after performing feature extraction and matching.</li></ul><p>然后将命令行的输出结果保存到logfile即<code>basedir/colmap_output.txt</code>中</p><blockquote><p>colmap命令行：<a href="https://colmap.github.io/cli.html">Command-line Interface — COLMAP 3.8-dev documentation</a><br>dense中深度图转换：<a href="https://zhuanlan.zhihu.com/p/584386128">COLMAP简明教程 导入指定参数 命令行 导出深度图 - 知乎 (zhihu.com)</a></p></blockquote><h4 id="load-colmap-data-to-colmap-read-model-py"><a href="#load-colmap-data-to-colmap-read-model-py" class="headerlink" title="load_colmap_data() to colmap_read_model.py"></a>load_colmap_data() to colmap_read_model.py</h4><p><code>python .\colmap_read_model.py E:\BaiduSyncdisk\NeRF_Proj\NeuS\video2bmvs\M590\sparse\0 .bin</code></p><p>读取<code>[&#39;cameras&#39;, &#39;images&#39;, &#39;points3D&#39;]</code>文件的数据</p><p>input:</p><ul><li>basedir</li></ul><p>output: </p><ul><li>poses, shape: 3 x 5 x num_images<ul><li>c2w: 3x4xn </li><li>hwf: 3x1xn</li></ul></li><li>pts3d, 一个长度为num_points字典，key为point3D_id，value为Point3D对象</li><li>perm, # 按照name排序，返回排序后的索引的列表：<code>[from 0 to num_images-1]</code></li></ul><h5 id="cameras-images-and-pts3d-be-like"><a href="#cameras-images-and-pts3d-be-like" class="headerlink" title="cameras images and pts3d be like:"></a>cameras images and pts3d be like:</h5><blockquote><p><a href="https://colmap.github.io/format.html#cameras-txt">Output Format — COLMAP 3.8-dev documentation</a></p></blockquote><div class="table-container"><table><thead><tr><th>var</th><th>example</th><th>info</th></tr></thead><tbody><tr><td>cameras</td><td><code>&#123;1: Camera(id=1, model=&#39;SIMPLE_RADIAL&#39;, width=960, height=544, params=array([ 5.07683492e+02,  4.80000000e+02,  2.72000000e+02, -5.37403479e-03])), ...&#125;</code></td><td>f, cx, cy, k=params</td></tr><tr><td>images</td><td><code>&#123;1: Image(id=1, qvec=array([ 0.8999159 , -0.29030237,  0.07162026,  0.31740581]), tvec=array([ 0.29762954, -2.81576928,  1.41888716]), camera_id=1, name=&#39;000.png&#39;, xys=xys, point3D_ids=point3D_ids, ...&#125;</code></td><td>perm = np.argsort(names),qvec,tvec to m=w2c_mats:4x4,</td></tr><tr><td>pts3D</td><td><code>&#123;1054: Point3D(id=1054, xyz=array([1.03491375, 1.65809594, 3.83718124]), rgb=array([147, 146, 137]), error=array(0.57352093), image_ids=array([115, 116, 117, 114, 113, 112]), point2D_idxs=array([998, 822, 912, 977, 889, 817])), ...&#125;</code></td></tr></tbody></table></div><p>xys and point3D_ids in images be like:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">xys=array([[ 83.70032501,   2.57579875],</span><br><span class="line">       [ 83.70032501,   2.57579875],</span><br><span class="line">       [469.29092407,   2.57086968],</span><br><span class="line">       ...,</span><br><span class="line">       [759.08764648, 164.65560913],</span><br><span class="line">       [533.28503418, 297.13980103],</span><br><span class="line">       [837.11437988, 342.07727051]]), </span><br><span class="line">point3D_ids=array([  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,       </span><br><span class="line"> -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1, 9109,   -1,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1,   -1, 8781,   -1,   -1, 8628,   -1,   -1,</span><br><span class="line"> -1, 2059,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1, 8791,   -1,   -1, 8683,   -1, 8387,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1,   -1, 9008, 9007,   -1, 9161, 8786,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1,   -1,   -1,   -1, 9175,   -1,   -1,   -1,</span><br><span class="line">9053,   -1,   -1,   -1,   -1, 8756,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line"> -1, 9024,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1, 9111,</span><br><span class="line"> -1,   -1, 9018,   -1, 9004,   -1,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1,   -1, 8992,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line">4701,   -1, 9067,   -1, 9166, 3880,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1,   -1,   -1,   -1, 8725,   -1, 9112,   -1,</span><br><span class="line"> -1,   -1,   -1, 8990,   -1, 8793, 9118, 8847, 9009, 9140, 9012,</span><br><span class="line"> -1,   -1,   -1, 7743, 9065, 8604, 3935,   -1,   -1,   -1,   -1,</span><br><span class="line">9075,   -1,   -1, 8966,   -1,   -1,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1,   -1,   19,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line">9017,   -1,   -1,   -1, 9020,   -1, 9005,   -1,   -1,   -1,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1, 8696,</span><br><span class="line"> -1,   -1, 8930,   -1,   -1, 8970,   -1,   -1,   -1,   -1, 9076,</span><br><span class="line"> -1, 9114, 8925,   -1, 8915,   -1, 9077, 8851, 8655, 5885, 4073,</span><br><span class="line"> -1, 3839,   -1,   -1,   -1,   -1, 9165, 9078,   -1,   -1,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1, 9055,   -1,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1, 9017,   -1,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line"> -1, 8682,   -1,   -1, 9170,   -1, 7562, 7556,   -1,   -1,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1,   -1, 8962, 9079,   -1,   -1,   -1, 8586,</span><br><span class="line">8224,   -1,   -1,   -1,   -1, 1399, 9168, 6439, 9121, 8255, 9169,</span><br><span class="line"> -1, 9151, 8971, 4698, 9171, 9172,   -1,   -1, 8898, 3916,   -1,</span><br><span class="line"> -1,   -1, 1788,   -1,   -1,   -1, 9080,   -1,   -1,   -1,   -1,</span><br><span class="line"> -1,   -1, 2097,   -1, 4103,   -1,   -1,   -1,   -1, 2073,   -1,</span><br><span class="line"> -1, 1771,   -1,   -1,   -1,   -1,   -1,   -1, 8813,   -1, 9030,</span><br><span class="line"> -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1, 8841, 9081,</span><br><span class="line"> -1,   -1,   -1, 8977,   -1, 8372, 9057, 6807, 9082, 5941, 4181,</span><br><span class="line">1675,   -1, 1683,   -1,   -1, 1503, 9083, 1973, 9071, 2679, 2412,</span><br><span class="line">3238,   -1, 9164, 1796, 9174,   -1,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line">9042, 9084,   -1,   -1,   -1,   -1,   -1, 9051, 9050,   -1, 9085,</span><br><span class="line"> -1, 9158, 9086,  853, 7671, 9128,   -1,   -1, 9058,   -1, 9087,</span><br><span class="line"> -1, 8502, 9102,   -1, 9106,   -1, 9039,   -1,   -1,   -1, 9069,</span><br><span class="line"> -1, 2261,   -1, 1793, 2643,   -1,   -1, 8810, 8945,   -1,   -1,</span><br><span class="line"> -1,   -1,   -1, 9043,   -1,   -1,   -1,   -1,   -1,   -1,   -1,</span><br><span class="line">9142,   -1,   -1, 9122, 9089, 9090, 8863, 9103, 2161, 2446,   -1,</span><br><span class="line"> -1,   -1,   -1,   -1, 9104,   -1, 9060, 9131,   -1,   -1,   -1,</span><br><span class="line"> -1, 8980, 8706,   -1, 9105, 9091, 9173,   -1,   -1, 2996,   -1,</span><br><span class="line"> -1, 9092,   -1,   -1,   -1,   -1, 9094, 9095, 9096, 9097, 9156,</span><br><span class="line"> -1,   -1,   -1,   -1, 8772, 8818,   -1,   -1, 9162, 9062, 9098,</span><br><span class="line"> -1,   -1, 8907, 9099, 8985, 4624,   -1, 3746, 8951,   -1,   -1,</span><br><span class="line">8908,   -1, 9135, 8986, 9101,   -1,   -1,   -1, 9137,   -1]))&#125;</span><br></pre></td></tr></table></figure><h5 id="cameras文件"><a href="#cameras文件" class="headerlink" title="cameras文件"></a>cameras文件</h5><p>input:</p><ul><li>path_to_model_file, <code>camerasfile = os.path.join(realdir, &#39;sparse/0/cameras.bin&#39;)</code><br>output:</li><li>cameras，一个长度为num_cameras字典，key为camera_id，value为Camera对象</li></ul><blockquote><p><a href="https://www.cnblogs.com/xiaohuidi/p/15767477.html">colmap 相机模型及参数 - 小小灰迪 - 博客园 (cnblogs.com)</a></p></blockquote><p>使用:<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">camerasfile = os.path.join(realdir, &#x27;sparse/0/cameras.bin&#x27;)</span><br><span class="line">camdata = read_model.read_cameras_binary(camerasfile)</span><br><span class="line"></span><br><span class="line">list_of_keys = list(camdata.keys()) # list from 1 to num_cameras</span><br><span class="line">cam = camdata[list_of_keys[0]] # Camera(id=1, model=&#x27;SIMPLE_RADIAL&#x27;, width=960, height=544, params=array([ 5.07683492e+02,  4.80000000e+02,  2.72000000e+02, -5.37403479e-03]))</span><br><span class="line">print( &#x27;Cameras&#x27;, len(cam)) # Cameras 5</span><br><span class="line"></span><br><span class="line">h, w, f = cam.height, cam.width, cam.params[0]</span><br><span class="line">hwf = np.array([h,w,f]).reshape([3,1])</span><br></pre></td></tr></table></figure></p><h5 id="images文件"><a href="#images文件" class="headerlink" title="images文件"></a>images文件</h5><p>input:</p><ul><li>path_to_model_file,<code>imagesfile = os.path.join(realdir, &#39;sparse/0/images.bin&#39;)</code><br>output:</li><li>images，一个长度为num_reg_images字典，key为image_id，value为Image对象</li></ul><p>使用:<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">imagesfile = os.path.join(realdir, &#x27;sparse/0/images.bin&#x27;)</span><br><span class="line">imdata = read_model.read_images_binary(imagesfile)</span><br><span class="line"></span><br><span class="line">w2c_mats = []</span><br><span class="line">bottom = np.array([0,0,0,1.]).reshape([1,4])</span><br><span class="line"></span><br><span class="line">names = [imdata[k].name for k in imdata] # 一个长度为num_images的list，每个元素为图片的名字</span><br><span class="line">print( &#x27;Images #&#x27;, len(names)) </span><br><span class="line">perm = np.argsort(names) # 按照name排序，返回排序后的索引的列表：[from 0 to num_images-1]</span><br><span class="line">for k in imdata:</span><br><span class="line">    im = imdata[k]</span><br><span class="line">    R = im.qvec2rotmat() # 将旋转向量转换成旋转矩阵 3x3</span><br><span class="line">    t = im.tvec.reshape([3,1]) # 平移向量 3x1</span><br><span class="line">    m = np.concatenate([np.concatenate([R, t], 1), bottom], 0) # 4x4</span><br><span class="line">    w2c_mats.append(m) # 一个长度为num_images的list，每个元素为4x4的矩阵</span><br><span class="line"></span><br><span class="line">w2c_mats = np.stack(w2c_mats, 0) # num_images x 4 x 4</span><br><span class="line">c2w_mats = np.linalg.inv(w2c_mats) # num_images x 4 x 4</span><br><span class="line"></span><br><span class="line">poses = c2w_mats[:, :3, :4].transpose([1,2,0]) # 3 x 4 x num_images</span><br><span class="line">poses = np.concatenate([poses, np.tile(hwf[..., np.newaxis], [1,1,poses.shape[-1]])], 1)</span><br><span class="line"># tile : 将hwf扩展成3 x 1 x 1 ，然后tile成3 x 1 x num_images，tile表示在某个维度上重复多少次</span><br><span class="line"># poses : 3 x 5 x num_images ，c2w：3 x 4 x num_images and hwf: 3 x 1 x num_images</span><br><span class="line"></span><br><span class="line"># must switch to [-u, r, -t] from [r, -u, t], NOT [r, u, -t]</span><br><span class="line">poses = np.concatenate([poses[:, 1:2, :], poses[:, 0:1, :], -poses[:, 2:3, :], poses[:, 3:4, :], poses[:, 4:5, :]], 1)</span><br></pre></td></tr></table></figure></p><p>其中<code>R = im.qvec2rotmat()</code>将旋转向量转换成旋转矩阵:</p><p>如果给定旋转向量为 [qw, qx, qy, qz]，其中 qw 是标量部分，qx, qy, qz 是向量部分，可以通过以下步骤将旋转向量转换为旋转矩阵：</p><p>构造单位四元数 q：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">q</span> = qw + qx * <span class="selector-tag">i</span> + qy * j + qz * k  其中 <span class="selector-tag">i</span>, j, k 是虚部的基本单位向量。</span><br></pre></td></tr></table></figure></p><p>计算旋转矩阵 R(w2c)：<br><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">R = | <span class="number">1</span> - <span class="number">2</span>*(qy^<span class="number">2</span> + qz^<span class="number">2</span>)   <span class="number">2</span>*(<span class="keyword">qx</span>*qy - <span class="keyword">qw</span>*qz)   <span class="number">2</span>*(<span class="keyword">qx</span>*qz + <span class="keyword">qw</span>*qy) |</span><br><span class="line">    | <span class="number">2</span>*(<span class="keyword">qx</span>*qy + <span class="keyword">qw</span>*qz)     <span class="number">1</span> - <span class="number">2</span>*(<span class="keyword">qx</span>^<span class="number">2</span> + qz^<span class="number">2</span>) <span class="number">2</span>*(qy*qz - <span class="keyword">qw</span>*<span class="keyword">qx</span>) |</span><br><span class="line">    | <span class="number">2</span>*(<span class="keyword">qx</span>*qz - <span class="keyword">qw</span>*qy)     <span class="number">2</span>*(qy*qz + <span class="keyword">qw</span>*<span class="keyword">qx</span>)   <span class="number">1</span> - <span class="number">2</span>*(<span class="keyword">qx</span>^<span class="number">2</span> + qy^<span class="number">2</span>) |</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">qvec2rotmat</span>(<span class="params">qvec</span>):</span><br><span class="line">    <span class="keyword">return</span> np.array([</span><br><span class="line">        [<span class="number">1</span> - <span class="number">2</span> * qvec[<span class="number">2</span>]**<span class="number">2</span> - <span class="number">2</span> * qvec[<span class="number">3</span>]**<span class="number">2</span>,</span><br><span class="line">         <span class="number">2</span> * qvec[<span class="number">1</span>] * qvec[<span class="number">2</span>] - <span class="number">2</span> * qvec[<span class="number">0</span>] * qvec[<span class="number">3</span>],</span><br><span class="line">         <span class="number">2</span> * qvec[<span class="number">3</span>] * qvec[<span class="number">1</span>] + <span class="number">2</span> * qvec[<span class="number">0</span>] * qvec[<span class="number">2</span>]],</span><br><span class="line">        [<span class="number">2</span> * qvec[<span class="number">1</span>] * qvec[<span class="number">2</span>] + <span class="number">2</span> * qvec[<span class="number">0</span>] * qvec[<span class="number">3</span>],</span><br><span class="line">         <span class="number">1</span> - <span class="number">2</span> * qvec[<span class="number">1</span>]**<span class="number">2</span> - <span class="number">2</span> * qvec[<span class="number">3</span>]**<span class="number">2</span>,</span><br><span class="line">         <span class="number">2</span> * qvec[<span class="number">2</span>] * qvec[<span class="number">3</span>] - <span class="number">2</span> * qvec[<span class="number">0</span>] * qvec[<span class="number">1</span>]],</span><br><span class="line">        [<span class="number">2</span> * qvec[<span class="number">3</span>] * qvec[<span class="number">1</span>] - <span class="number">2</span> * qvec[<span class="number">0</span>] * qvec[<span class="number">2</span>],</span><br><span class="line">         <span class="number">2</span> * qvec[<span class="number">2</span>] * qvec[<span class="number">3</span>] + <span class="number">2</span> * qvec[<span class="number">0</span>] * qvec[<span class="number">1</span>],</span><br><span class="line">         <span class="number">1</span> - <span class="number">2</span> * qvec[<span class="number">1</span>]**<span class="number">2</span> - <span class="number">2</span> * qvec[<span class="number">2</span>]**<span class="number">2</span>]])</span><br></pre></td></tr></table></figure><h5 id="points3D文件"><a href="#points3D文件" class="headerlink" title="points3D文件"></a>points3D文件</h5><p>input:</p><ul><li>path_to_model_file: <code>points3dfile = os.path.join(realdir, &#39;sparse/0/points3D.bin&#39;)</code><br>output:</li><li>pts3D, 一个长度为num_points字典，key为point3D_id，value为Point3D对象</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">points3dfile = os.path.join(realdir, &#x27;sparse/0/points3D.bin&#x27;)</span><br><span class="line">pts3d = read_model.read_points3d_binary(points3dfile)</span><br></pre></td></tr></table></figure><h4 id="save-poses-py"><a href="#save-poses-py" class="headerlink" title="save_poses.py"></a>save_poses.py</h4><p>input:</p><ul><li>basedir, </li><li>poses, shape: 3 x 5 x num_images<ul><li>c2w: 3x4xn </li><li>hwf: 3x1xn</li></ul></li><li>pts3d, 一个长度为num_points字典，key为point3D_id，value为Point3D对象<ul><li><code>&#123;1054: Point3D(id=1054, xyz=array([1.03491375, 1.65809594, 3.83718124]), rgb=array([147, 146, 137]), error=array(0.57352093), image_ids=array([115, 116, 117, 114, 113, 112]), point2D_idxs=array([998, 822, 912, 977, 889, 817])), ...&#125;</code></li></ul></li><li>perm, # 按照name排序，返回排序后的索引的列表：<code>[from 0 to num_images-1]</code></li></ul><p>save:</p><ul><li>sparse_points.ply : <ul><li>pcd = trimesh.PointCloud(pts) , pts: num_points x 3</li></ul></li><li>poses.npy : num_images x 3 x 5</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">save_poses</span>(<span class="params">basedir, poses, pts3d, perm</span>):</span><br><span class="line">    pts_arr = []</span><br><span class="line">    vis_arr = []</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> pts3d: <span class="comment"># k为空间点的id</span></span><br><span class="line">        pts_arr.append(pts3d[k].xyz) <span class="comment"># 每个空间点的三维坐标</span></span><br><span class="line">        cams = [<span class="number">0</span>] * poses.shape[-<span class="number">1</span>] <span class="comment"># 一个长度为num_images的list，每个元素为0</span></span><br><span class="line">        <span class="keyword">for</span> ind <span class="keyword">in</span> pts3d[k].image_ids: <span class="comment"># 每个空间点对应的图片index</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(cams) &lt; ind - <span class="number">1</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;ERROR: the correct camera poses for current points cannot be accessed&#x27;</span>)</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            cams[ind-<span class="number">1</span>] = <span class="number">1</span> <span class="comment"># 将第k个空间点对应图片的index 在cams列表中置为1</span></span><br><span class="line">        vis_arr.append(cams) </span><br><span class="line">    <span class="comment"># pts_arr shape： num_points x 3</span></span><br><span class="line">    <span class="comment">#vis_arr shape： num_points x num_images</span></span><br><span class="line"></span><br><span class="line">    pts = np.stack(pts_arr, axis=<span class="number">0</span>) <span class="comment"># num_points x 3</span></span><br><span class="line">    pcd = trimesh.PointCloud(pts)</span><br><span class="line">    pcd.export(os.path.join(basedir, <span class="string">&#x27;sparse_points.ply&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    pts_arr = np.array(pts_arr)</span><br><span class="line">    vis_arr = np.array(vis_arr)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Points&#x27;</span>, pts_arr.shape, <span class="string">&#x27;Visibility&#x27;</span>, vis_arr.shape )</span><br><span class="line">    <span class="comment"># pose: 3 x 5 x num_images</span></span><br><span class="line">    poses = np.moveaxis(poses, -<span class="number">1</span>, <span class="number">0</span>) <span class="comment"># 将最后一个维度移动到第一个维度 num_images x 3 x 5</span></span><br><span class="line">    poses = poses[perm] <span class="comment"># 按照perm排序 num_images x 3 x 5</span></span><br><span class="line">    np.save(os.path.join(basedir, <span class="string">&#x27;poses.npy&#x27;</span>), poses) <span class="comment"># num_images x 3 x 5</span></span><br></pre></td></tr></table></figure><h3 id="gen-cameras-py"><a href="#gen-cameras-py" class="headerlink" title="gen_cameras.py"></a>gen_cameras.py</h3><p>根据pose.npy文件和sparse_points_interest.ply文件来生成cameras_sphere.npz</p><ul><li>pose.npy主要保存每张图片的c2w矩阵和hwf</li><li>sparse_points_interest.ply用来生成相机缩放矩阵，将感兴趣的部位保存下来</li></ul><h4 id="pose文件"><a href="#pose文件" class="headerlink" title="pose文件"></a>pose文件</h4><p>pose.ply in Miku<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230702145000.png" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">work_dir = sys.argv[<span class="number">1</span>]</span><br><span class="line">poses_hwf = np.load(os.path.join(work_dir, <span class="string">&#x27;poses.npy&#x27;</span>)) <span class="comment"># n_images, 3, 5</span></span><br><span class="line">poses_raw = poses_hwf[:, :, :<span class="number">4</span>] <span class="comment"># n_images, 3, 4</span></span><br><span class="line">hwf = poses_hwf[:, :, <span class="number">4</span>] <span class="comment"># n_images, 3</span></span><br><span class="line">pose = np.diag([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]) <span class="comment"># 4, 4 对角线为1，其余为0</span></span><br><span class="line">pose[:<span class="number">3</span>, :<span class="number">4</span>] = poses_raw[<span class="number">0</span>] <span class="comment"># 3, 4 ， pose: 4, 4</span></span><br><span class="line">pts = []</span><br><span class="line"><span class="comment"># 下面四句是将相机坐标系的四个点转换到世界坐标系</span></span><br><span class="line"><span class="comment"># 世界坐标系下 原点，x轴，y轴，z轴</span></span><br><span class="line">pts.append((pose @ np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])[:, <span class="literal">None</span>]).squeeze()[:<span class="number">3</span>]) <span class="comment"># 4, 1 -&gt; 4 -&gt; 3</span></span><br><span class="line">pts.append((pose @ np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])[:, <span class="literal">None</span>]).squeeze()[:<span class="number">3</span>])</span><br><span class="line">pts.append((pose @ np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])[:, <span class="literal">None</span>]).squeeze()[:<span class="number">3</span>])</span><br><span class="line">pts.append((pose @ np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])[:, <span class="literal">None</span>]).squeeze()[:<span class="number">3</span>])</span><br><span class="line">pts = np.stack(pts, axis=<span class="number">0</span>)</span><br><span class="line">pcd = trimesh.PointCloud(pts)</span><br><span class="line">pcd.export(os.path.join(work_dir, <span class="string">&#x27;pose.ply&#x27;</span>))</span><br></pre></td></tr></table></figure><h4 id="两个矩阵"><a href="#两个矩阵" class="headerlink" title="两个矩阵"></a>两个矩阵</h4><p><strong>world_mat_{i}:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">h, w, f = hwf[i, <span class="number">0</span>], hwf[i, <span class="number">1</span>], hwf[i, <span class="number">2</span>]</span><br><span class="line">intrinsic = np.diag([f, f, <span class="number">1.0</span>, <span class="number">1.0</span>]).astype(np.float32)</span><br><span class="line">intrinsic[<span class="number">0</span>, <span class="number">2</span>] = (w - <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line">intrinsic[<span class="number">1</span>, <span class="number">2</span>] = (h - <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">intrinsic = </span><br><span class="line">[[ focal,  <span class="number">0.</span>       ,   (w-<span class="number">1</span>)/<span class="number">2</span>  , <span class="number">0</span> ]</span><br><span class="line">[ <span class="number">0.</span>  ,       focal ,   (h-<span class="number">1</span>)/<span class="number">2</span>   , <span class="number">0</span>]</span><br><span class="line">[ <span class="number">0.</span>  ,       <span class="number">0.</span>      ,   <span class="number">1.</span>            , <span class="number">0</span>]</span><br><span class="line">[ <span class="number">0.</span>  ,       <span class="number">0.</span>      ,   <span class="number">0.</span>            , <span class="number">1.</span> ]]</span><br><span class="line">np.float32</span><br><span class="line"></span><br><span class="line">convert_mat = np.zeros([<span class="number">4</span>, <span class="number">4</span>], dtype=np.float32)</span><br><span class="line">convert_mat[<span class="number">0</span>, <span class="number">1</span>] = <span class="number">1.0</span></span><br><span class="line">convert_mat[<span class="number">1</span>, <span class="number">0</span>] = <span class="number">1.0</span></span><br><span class="line">convert_mat[<span class="number">2</span>, <span class="number">2</span>] =-<span class="number">1.0</span></span><br><span class="line">convert_mat[<span class="number">3</span>, <span class="number">3</span>] = <span class="number">1.0</span></span><br><span class="line">pose = np.diag([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]).astype(np.float32)</span><br><span class="line">pose[:<span class="number">3</span>, :<span class="number">4</span>] = poses_raw[i]</span><br><span class="line"></span><br><span class="line">pose = </span><br><span class="line">[[r1,        r2       ,  r3            ,  tx]</span><br><span class="line">[ r1 ,       r2      ,   r3             , ty]</span><br><span class="line">[ r1 ,       r2      ,   r3             , tz]</span><br><span class="line">[ <span class="number">0.</span>  ,       <span class="number">0.</span>      ,   <span class="number">0.</span>            , <span class="number">1.</span> ]]</span><br><span class="line">convert_mat =</span><br><span class="line">[[<span class="number">0.</span>,      <span class="number">1.</span>      ,  <span class="number">0</span>           , <span class="number">0</span>]</span><br><span class="line">[<span class="number">1.</span>,       <span class="number">0.</span>       ,  <span class="number">0</span>            , <span class="number">0</span>]</span><br><span class="line">[<span class="number">0.</span>,       <span class="number">0.</span>       ,  -<span class="number">1.</span>          , <span class="number">0</span>]</span><br><span class="line">[<span class="number">0.</span>,       <span class="number">0.</span>       ,  <span class="number">0</span>            , <span class="number">1.</span>]]</span><br><span class="line">np.float32</span><br><span class="line"></span><br><span class="line">pose = pose @ convert_mat</span><br><span class="line"></span><br><span class="line">pose = </span><br><span class="line">[[r2,        r1       ,  -r3            ,  tx]</span><br><span class="line">[ r2 ,       r1      ,   -r3             , ty]</span><br><span class="line">[ r2 ,       r1      ,   -r3             , tz]</span><br><span class="line">[ <span class="number">0.</span>  ,       <span class="number">0.</span>      ,   <span class="number">0.</span>            , <span class="number">1.</span> ]]</span><br><span class="line"></span><br><span class="line">w2c = np.linalg.inv(pose)</span><br><span class="line"></span><br><span class="line"><span class="comment"># world_mat is w2pixel</span></span><br><span class="line">world_mat = intrinsic @ w2c</span><br><span class="line">world_mat = world_mat.astype(np.float32)</span><br></pre></td></tr></table></figure><p>pose要乘以covert_mat是因为在load_colmap_data时对pose进行了翻转</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># must switch to [-u, r, -t] from [r, -u, t], NOT [r, u, -t]</span></span><br><span class="line">poses = np.concatenate([poses[:, <span class="number">1</span>:<span class="number">2</span>, :], poses[:, <span class="number">0</span>:<span class="number">1</span>, :], -poses[:, <span class="number">2</span>:<span class="number">3</span>, :], poses[:, <span class="number">3</span>:<span class="number">4</span>, :], poses[:, <span class="number">4</span>:<span class="number">5</span>, :]], <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>scale_mat_{i}:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pcd = trimesh.load(os.path.join(work_dir, <span class="string">&#x27;sparse_points_interest.ply&#x27;</span>))</span><br><span class="line">vertices = pcd.vertices</span><br><span class="line">bbox_max = np.<span class="built_in">max</span>(vertices, axis=<span class="number">0</span>) </span><br><span class="line">bbox_min = np.<span class="built_in">min</span>(vertices, axis=<span class="number">0</span>)</span><br><span class="line">center = (bbox_max + bbox_min) * <span class="number">0.5</span></span><br><span class="line">radius = np.linalg.norm(vertices - center, <span class="built_in">ord</span>=<span class="number">2</span>, axis=-<span class="number">1</span>).<span class="built_in">max</span>()</span><br><span class="line">scale_mat = np.diag([radius, radius, radius, <span class="number">1.0</span>]).astype(np.float32)</span><br><span class="line">scale_mat[:<span class="number">3</span>, <span class="number">3</span>] = center</span><br></pre></td></tr></table></figure><h2 id="interpolate-view"><a href="#interpolate-view" class="headerlink" title="interpolate_view"></a>interpolate_view</h2><p>生成一个视频，从img_idx_0中间插值生成新视图的图片，过渡到img_idx_1，然后再回到img_idx_0，共2s，60frames</p><p>eg: 0 to 38 render video</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/00300000_0_38.gif" alt="00300000_0_38.gif"></p><p>插值：$ratio = \frac{\sin{\left(\frac{i}{frames}-0.5 \right)\cdot \pi}}{2}+\frac{1}{2} = 0.5 \rightarrow 1 \rightarrow 0.5$ </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">interpolate_view</span>(<span class="params">self, img_idx_0, img_idx_1</span>):</span><br><span class="line">    images = []</span><br><span class="line">    n_frames = <span class="number">60</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_frames):</span><br><span class="line">        <span class="built_in">print</span>(i)</span><br><span class="line">        images.append(self.render_novel_image(img_idx_0,</span><br><span class="line">                                              img_idx_1,</span><br><span class="line">                                              np.sin(((i / n_frames) - <span class="number">0.5</span>) * np.pi) * <span class="number">0.5</span> + <span class="number">0.5</span>,</span><br><span class="line">                      resolution_level=<span class="number">4</span>))</span><br><span class="line">    <span class="comment"># 做出了视频像是循环的效果</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_frames):</span><br><span class="line">        images.append(images[n_frames - i - <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    fourcc = cv.VideoWriter_fourcc(*<span class="string">&#x27;mp4v&#x27;</span>)</span><br><span class="line">    video_dir = os.path.join(self.base_exp_dir, <span class="string">&#x27;render&#x27;</span>)</span><br><span class="line">    os.makedirs(video_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    h, w, _ = images[<span class="number">0</span>].shape</span><br><span class="line">    writer = cv.VideoWriter(os.path.join(video_dir,</span><br><span class="line">                                         <span class="string">&#x27;&#123;:0&gt;8d&#125;_&#123;&#125;_&#123;&#125;.mp4&#x27;</span>.<span class="built_in">format</span>(self.iter_step, img_idx_0, img_idx_1)),</span><br><span class="line">                            fourcc, <span class="number">30</span>, (w, h))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> images:</span><br><span class="line">        writer.write(image)</span><br><span class="line"></span><br><span class="line">    writer.release()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">render_novel_image</span>(<span class="params">self, idx_0, idx_1, ratio, resolution_level</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Interpolate view between two cameras.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    rays_o, rays_d = self.dataset.gen_rays_between(idx_0, idx_1, ratio, resolution_level=resolution_level)</span><br><span class="line">    H, W, _ = rays_o.shape</span><br><span class="line">    rays_o = rays_o.reshape(-<span class="number">1</span>, <span class="number">3</span>).split(self.batch_size)</span><br><span class="line">    rays_d = rays_d.reshape(-<span class="number">1</span>, <span class="number">3</span>).split(self.batch_size)</span><br><span class="line"></span><br><span class="line">    out_rgb_fine = []</span><br><span class="line">    <span class="keyword">for</span> rays_o_batch, rays_d_batch <span class="keyword">in</span> <span class="built_in">zip</span>(rays_o, rays_d):</span><br><span class="line">        near, far = self.dataset.near_far_from_sphere(rays_o_batch, rays_d_batch)</span><br><span class="line">        background_rgb = torch.ones([<span class="number">1</span>, <span class="number">3</span>]) <span class="keyword">if</span> self.use_white_bkgd <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        render_out = self.renderer.render(rays_o_batch,</span><br><span class="line">                                          rays_d_batch,</span><br><span class="line">                                          near,</span><br><span class="line">                                          far,</span><br><span class="line">                                          cos_anneal_ratio=self.get_cos_anneal_ratio(),</span><br><span class="line">                                          background_rgb=background_rgb)</span><br><span class="line"></span><br><span class="line">        out_rgb_fine.append(render_out[<span class="string">&#x27;color_fine&#x27;</span>].detach().cpu().numpy())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">del</span> render_out</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gen_rays_between</span>(<span class="params">self, idx_0, idx_1, ratio, resolution_level=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Interpolate pose between two cameras.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    l = resolution_level <span class="comment"># 4</span></span><br><span class="line">    tx = torch.linspace(<span class="number">0</span>, self.W - <span class="number">1</span>, self.W // l)</span><br><span class="line">    ty = torch.linspace(<span class="number">0</span>, self.H - <span class="number">1</span>, self.H // l)</span><br><span class="line">    pixels_x, pixels_y = torch.meshgrid(tx, ty)</span><br><span class="line">    p = torch.stack([pixels_x, pixels_y, torch.ones_like(pixels_y)], dim=-<span class="number">1</span>)  <span class="comment"># W, H, 3</span></span><br><span class="line">    p = torch.matmul(self.intrinsics_all_inv[<span class="number">0</span>, <span class="literal">None</span>, <span class="literal">None</span>, :<span class="number">3</span>, :<span class="number">3</span>], p[:, :, :, <span class="literal">None</span>]).squeeze()  <span class="comment"># W, H, 3</span></span><br><span class="line">    rays_v = p / torch.linalg.norm(p, <span class="built_in">ord</span>=<span class="number">2</span>, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># W, H, 3</span></span><br><span class="line">    trans = self.pose_all[idx_0, :<span class="number">3</span>, <span class="number">3</span>] * (<span class="number">1.0</span> - ratio) + self.pose_all[idx_1, :<span class="number">3</span>, <span class="number">3</span>] * ratio</span><br><span class="line">    pose_0 = self.pose_all[idx_0].detach().cpu().numpy()</span><br><span class="line">    pose_1 = self.pose_all[idx_1].detach().cpu().numpy()</span><br><span class="line">    pose_0 = np.linalg.inv(pose_0)</span><br><span class="line">    pose_1 = np.linalg.inv(pose_1)</span><br><span class="line">    rot_0 = pose_0[:<span class="number">3</span>, :<span class="number">3</span>]</span><br><span class="line">    rot_1 = pose_1[:<span class="number">3</span>, :<span class="number">3</span>]</span><br><span class="line">    rots = Rot.from_matrix(np.stack([rot_0, rot_1]))</span><br><span class="line">    key_times = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    slerp = Slerp(key_times, rots)</span><br><span class="line">    rot = slerp(ratio)</span><br><span class="line">    pose = np.diag([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>])</span><br><span class="line">    pose = pose.astype(np.float32)</span><br><span class="line">    pose[:<span class="number">3</span>, :<span class="number">3</span>] = rot.as_matrix()</span><br><span class="line">    pose[:<span class="number">3</span>, <span class="number">3</span>] = ((<span class="number">1.0</span> - ratio) * pose_0 + ratio * pose_1)[:<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line">    pose = np.linalg.inv(pose)</span><br><span class="line">    rot = torch.from_numpy(pose[:<span class="number">3</span>, :<span class="number">3</span>]).cuda()</span><br><span class="line">    trans = torch.from_numpy(pose[:<span class="number">3</span>, <span class="number">3</span>]).cuda()</span><br><span class="line">    rays_v = torch.matmul(rot[<span class="literal">None</span>, <span class="literal">None</span>, :<span class="number">3</span>, :<span class="number">3</span>], rays_v[:, :, :, <span class="literal">None</span>]).squeeze()  <span class="comment"># W, H, 3</span></span><br><span class="line">    rays_o = trans[<span class="literal">None</span>, <span class="literal">None</span>, :<span class="number">3</span>].expand(rays_v.shape)  <span class="comment"># W, H, 3</span></span><br><span class="line">    <span class="keyword">return</span> rays_o.transpose(<span class="number">0</span>, <span class="number">1</span>), rays_v.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> Neus </tag>
            
            <tag> Code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>InstantNGP</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/Efficiency/NeRF-InstantNGP/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/Efficiency/NeRF-InstantNGP/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://tom94.net/">Thomas Müller</a>    <a href="https://research.nvidia.com/person/alex-evans">Alex Evans</a> <a href="https://research.nvidia.com/person/christoph-schied">Christoph Schied</a> <a href="https://research.nvidia.com/person/alex-keller">Alexander Keller</a></td></tr><tr><td>Conf/Jour</td><td>ACM Transactions on Graphics (SIGGRAPH 2022)</td></tr><tr><td>Year</td><td>2022</td></tr><tr><td>Project</td><td><a href="https://nvlabs.github.io/instant-ngp/">Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (nvlabs.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4667039735068246017&amp;noteId=1702708284370548992">Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (readpaper.com)</a></td></tr></tbody></table></div><p><strong>哈希函数在cuda(cuda c++)中进行编程</strong>，不需深挖具体代码，初学只需理解多分辨率哈希编码思想。i.e.目前只需要学会使用tiny-cuda-nn即可：<a href="https://github.com/nvlabs/tiny-cuda-nn#pytorch-extension">NVlabs/tiny-cuda-nn: Lightning fast C++/CUDA neural network framework (github.com)</a></p><p><strong>哈希编码思想：</strong><br>哈希编码后的输出值的数量与L(分辨率数量)、F(特征向量维度)有关，eg: L=16,F=2，则输入一个坐标xyz，根据多分辨率体素网格，插值出来L个特征值，每个特征值维度为2，因此输出值的维度为32</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230703160333.png" alt="image.png"></p><script type="math/tex; mode=display">\begin{aligned}N_{l} & :=\left\lfloor N_{\mathrm{min}}\cdot b^{l}\right\rfloor,  \\&b:=\exp\left(\frac{\ln N_{\mathrm{max}}-\ln N_{\mathrm{min}}}{L-1}\right).\end{aligned}</script><p>L为分辨率数量，l为分辨率序号。示例中L=2，$N_{0}= N_{min} =2$ , $N_{1}= N_{max}= 3$ , $b = \frac{3}{2}$</p><ul><li>L：多分辨率</li><li>T：每个分辨率下有T个特征向量</li><li>F：特征向量的维度</li><li>最小和最大分辨率：$N_{min} , N_{max}$</li><li>b：每个level的缩放per_level_scale $b= e^{\frac{ln(\frac{N_{max}}{N_{min}})}{L-1}}$</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230703160145.png" alt="image.png"></p><span id="more"></span><h1 id="改进之处"><a href="#改进之处" class="headerlink" title="改进之处"></a>改进之处</h1><p>InstantNGP通过<a href="#多分辨率哈希编码">多分辨率哈希编码</a>加速了对特征值的查找，减少了对空白空间的访问</p><div class="table-container"><table><thead><tr><th>对比</th><th>NeRF</th><th>InstantNGP</th></tr></thead><tbody><tr><td>编码</td><td>频率编码</td><td>多分辨率哈希编码</td></tr><tr><td>资源</td><td>全连接神经网络，MLP，训练和评估花费大</td><td>采用多功能的新输入编码和在不牺牲质量的前提下更小的网络，减少了资源花费</td></tr><tr><td>速度</td><td>很慢</td><td>实现了几个数量级的提速</td></tr></tbody></table></div><p>编码可视化(nerfstudio)：<a href="https://docs.nerf.studio/en/latest/nerfology/model_components/visualize_encoders.html">Field Encoders - nerfstudio</a></p><ul><li>我们通过一种通用的新输入编码来降低这种成本，这种编码允许在不牺牲质量的情况下使用更小的网络，从而显著减少浮点数和内存访问操作的数量:一个小的神经网络通过一个多分辨率的可训练特征向量哈希表来增强，这些特征向量的值通过随机梯度下降进行优化。多分辨率结构允许网络消除哈希冲突的歧义，使得一个简单的架构在现代gpu上并行化是微不足道的，使用完全融合的CUDA内核实现整个系统来利用这种并行性，最大限度地减少浪费的带宽和计算操作</li><li><p>我们实现了几个数量级的综合加速，能够在几秒钟内训练高质量的神经图形原语neural graphics primitives，并在几十毫秒内以1920×1080的分辨率进行渲染。</p></li><li><p>关键词：大规模并行算法;矢量/流算法;神经网络。(Massively parallel algorithms; Vector / streaming algorithms; Neural networks.)</p><ul><li>其他关键词和短语:图像合成，神经网络，编码，哈希，gpu，并行计算，函数逼近。(Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation.)</li></ul></li></ul><h1 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h1><p>计算机图形原语基本上是由参数化外观的数学函数表示的。数学表示的质量和性能特征对于视觉保真度至关重要:我们希望表示在捕获高频局部细节的同时保持快速和紧凑。由多层感知器(mlp)表示的函数，用作神经图形原语，已被证明符合这些标准(在不同程度上)</p><p>这些方法的重要共同点是编码将神经网络输入映射到高维空间，这是从紧凑模型中提取高近似质量的关键。这些编码中最成功的是可训练的、特定于任务的数据结构：<a href="https://readpaper.com/paper/3044538714">Neural Sparse Voxel Fields</a>，<a href="https://readpaper.com/paper/3121736960">Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes</a>承担了很大一部分学习任务。这使得使用更小、更高效的mlp成为可能。然而，这种数据结构依赖于启发式方法和结构修改(如剪枝、分割或合并)，这可能会使训练过程复杂化，将方法限制在特定的任务上，或者限制gpu上的性能，因为控制流和指针追踪是昂贵的。</p><p>我们通过多分辨率哈希编码来解决这些问题，该编码具有自适应性和高效性，并且与任务无关。它仅通过两个值进行配置——参数数量𝑇和所需的最高分辨率𝑁max，在经过几秒钟的训练后，在各种任务中能够达到最先进的质量：</p><ul><li>Gigapixel image: MLP学习从2D坐标到高分辨率图像的RGB颜色的映射。</li><li>SDF: MLP学习从三维坐标到表面距离的映射。</li><li>NRC: MLP从蒙特卡罗路径跟踪器中学习给定场景的5D光场。</li><li>NeRF: MLP从图像观测和相应的透视变换中学习给定场景的3D密度和5D光场</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230703152512.png" alt="image.png"></p><p>同时实现任务无关的适应性和效率的关键是多分辨率哈希表层次结构：</p><ul><li>适应性:我们将一系列的网格映射到对应的固定大小的特征向量数组。在粗分辨率下，网格点与数组条目之间存在一对一的映射关系。在细分辨率下，数组被视为哈希表，并使用空间哈希函数进行索引，多个网格点会与同一数组条目重叠。这种哈希碰撞导致碰撞的训练梯度取平均值，意味着最大的梯度——与损失函数最相关的梯度将占主导地位。因此，哈希表会自动优先考虑具有最重要细节的稀疏区域。与以前的方法不同，训练过程中不需要对数据结构进行任何结构更新。</li><li>效率:我们的哈希表查找是O(1)，不需要控制流。这很好地映射到现代gpu，避免了执行分歧和树遍历中固有的串行指针追逐。所有分辨率的哈希表都可以并行查询。</li></ul><h1 id="先前的编码方式-BACKGROUND-AND-RELATED-WORK"><a href="#先前的编码方式-BACKGROUND-AND-RELATED-WORK" class="headerlink" title="先前的编码方式(BACKGROUND AND RELATED WORK)"></a>先前的编码方式(BACKGROUND AND RELATED WORK)</h1><p>编码的好处：高频的输入可以重建出图片的高频信息</p><ul><li><p>将机器学习模型的输入编码到高维空间的早期示例包括独热编码one-hot encoding(Harris和Harris 2013)和核技巧kernel trick(Theodoridis 2008)，通过这些方法可以使数据的复杂排列线性可分。</p></li><li><p>频率编码：对于神经网络而言，在循环结构的注意力组件(Gehring等人，2017)和随后的变换器(Vaswani等人，2017)中，输入编码被证明在帮助神经网络识别当前正在处理的位置方面非常有用。 Vaswani等人(2017)将标量位置𝑥 ∈ R编码为𝐿 ∈ N正弦和余弦函数的多分辨率序列。</p></li></ul><script type="math/tex; mode=display">\begin{aligned}\operatorname{enc}(x)=\left(\begin{matrix}\sin(2^0x),\sin(2^1x),\ldots,\sin(2^{L-1}x),\\\cos(2^0x),\cos(2^1x),\ldots,\cos(2^{L-1}x)\end{matrix}\right).\end{aligned}</script><p>这已经被应用于计算机图形学中，用于在NeRF算法中对空间方向变化的光场和体密度进行编码(Mildenhall等人，2020)。这个光场的五个维度使用上述公式独立地进行编码；后来将其扩展到了随机定向的平行波前(Tancik等人，2020)和细节级别过滤(Barron等人，2021a)。我们将这个编码家族称为频率编码。值得注意的是，频率编码后跟线性变换已经在其他计算机图形学任务中使用，比如逼近可见性函数(Annen等人，2007；Jansen和Bavoil，2010)。Müller等人(2019；2020)提出了一种基于光栅化核心的连续变体的单热编码，即单斑点编码，它在有界域中可以实现比频率编码更准确的结果，但代价是单尺度的。</p><ul><li><p>参数化编码: 最近，参数化编码取得了最先进的结果，它们模糊了经典数据结构和神经方法之间的界限。其思想是在辅助数据结构中排列额外的可训练参数（除了权重和偏置），例如一个网格(Chabra等人，2020年；Jiang等人，2020年；Liu等人，2020年；Mehta等人，2021年；Peng等人，2020a年；Sun等人，2021年；Tang等人，2018年；Yu等人，2021a年)或一棵树(Takikawa等人，2021年)，并根据输入向量x ∈ R𝑑查找并（可选地）插值这些参数。<strong>这种安排以更大的内存占用为代价来换取较小的计算成本</strong>：<strong><em>在通过网络向后传播的每个梯度中，全连接MLP网络中的每个权重都必须更新，而对于可训练的输入编码参数（”特征向量”），只有很少数量受到影响</em></strong>。例如，对于一个三线性插值的三维网格特征向量，每个样本反向传播到编码时只需要更新8个这样的网格点。通过这种方式，尽管参数化编码的总参数数量比固定输入编码要高得多，但在训练期间更新所需的FLOP数和内存访问并没有显著增加。通过减小MLP的大小，这样的参数化模型通常可以更快地收敛训练，而不会牺牲逼近质量。</p><ul><li>另一种参数化方法使用了对域R𝑑的树细分，其中一个名为大型辅助坐标编码器神经网络（ACORN）(Martel等人，2021)被训练用于在围绕x的叶节点处输出密集的特征网格。这些密集的特征网格大约有10,000个条目，然后进行线性插值，类似于Liu等人2020的方法。与先前的参数化编码相比，这种方法往往能够产生更高程度的适应性，但计算成本更高，只有当足够多的输入x落入每个叶节点时，才能摊销这些成本。</li></ul></li><li><p>稀疏参数编码。尽管现有的参数编码比非参数编码的前身具有更高的准确性，但它们也具有效率和多功能性的缺点。<strong>密集的可训练特征网格所占用的内存比神经网络权重要多</strong>。为了说明权衡并激励我们的方法，图2展示了几种不同编码方式对神经辐射场重建质量的影响。没有任何输入编码（a）时，网络只能学习到一个相当平滑的位置函数，导致对光场的逼近较差。频率编码（b）允许相同中等大小的网络（8个隐藏层，每个层宽度为256）更准确地表示场景。中间图像（c）将一个较小的网络与1283个三线性插值、16维特征向量的密集网格配对，总共有3360万个可训练参数。由于每个样本仅影响8个网格点，大量的可训练参数可以有效地更新。</p><ul><li>然而，密集的网格在两个方面是浪费的。<ul><li>首先，它将同样多的特征分配给空白区域和靠近表面的区域。参数数量呈O(𝑁^3)增长，而感兴趣的可见表面的面积仅呈O(𝑁^2)增长。在这个例子中，网格的分辨率为1283，但只有53,807个（2.57%）单元与可见表面接触。</li><li>其次，自然场景呈现出平滑性，促使人们使用多分辨率分解的方法(Chibane等，2020年; Hadadan等，2021年)。图2（d）显示了使用编码的结果，其中插值特征存储在分辨率从163到1733的八个共位网格中，每个网格包含2维特征向量。这些向量被连接在一起形成一个16维（与（c）相同）的输入网络。尽管参数数量不到（c）的一半，但重构质量相似。</li></ul></li><li>如果已经知道感兴趣的表面，可以使用诸如八叉树(Takikawa等，2021年)或稀疏网格(Chabra等，2020年; Chibane等，2020年; Hadadan等，2021年; Jiang等，2020年; Liu等，2020年; Peng等，2020a年)这样的数据结构来剔除稠密网格中未使用的特征。然而，在NeRF设置中，表面只在训练期间出现。NSVF (Liu等，2020年)和其他一些同时进行的工作(Sun等，2021年; Yu等，2021a年)采用了多阶段的粗到细的策略，逐渐对特征网格的区域进行细化和剔除。虽然这种方法有效，但会导致更复杂的训练过程，其中稀疏数据结构必须定期更新。</li></ul></li><li><p>多分辨率哈希编码: 我们的方法——图2（e，f）——结合了这两个想法来减少浪费。我们将可训练的特征向量存储在一个紧凑的空间哈希表中，其大小是一个超参数𝑇，可以通过调整来在参数数量和重建质量之间进行权衡。<strong>它既不依赖于训练过程中的渐进修剪，也不依赖于场景几何的先验知识</strong>。类似于（d）中的多分辨率网格，我们使用多个独立的哈希表在不同分辨率上进行索引，将它们的插值输出连接在一起，然后通过MLP传递。<strong>尽管参数数量减少了20倍，但重建质量与密集网格编码相当</strong>。与以前在3D重建中使用空间哈希（Teschner等，2003）(Nießner等，2013)的工作不同，我们不通过典型的探测、存储或链式处理等手段来明确处理哈希函数的冲突。<strong>相反，我们依靠神经网络学习自行消除哈希冲突，避免了控制流分歧，减少了实现复杂性，提高了性能</strong>。<strong>另一个性能优势是哈希表的可预测内存布局，它独立于所表示的数据</strong>。尽管在树状数据结构中很难实现良好的缓存行为，但我们的哈希表可以针对低级架构细节（如缓存大小）进行微调。</p></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230703154529.png" alt="image.png"></p><p>可训练参数的数量（MLP权重+编码参数）、训练时间和重建精度（PSNR）显示在每个图像下方。<br>由于参数更新的稀疏性和较小的MLP，我们的编码（e）具有与频率编码配置（b）相似的可训练参数总数，训练速度超过8倍。增加参数（f）的数量进一步提高了重建精度，而不显著增加训练时间。</p><ul><li>C: 单分辨率，将单位正方体划分为多个小正方体，每个小正方体8个顶点，每个顶点有一个特征值，当输入一个三维坐标xyz时，从8个顶点特征值<strong>三线性插值出</strong>输入三维坐标的特征值</li><li>D: 多分辨率，将多个分辨率插值出的特征值concatenate起来</li><li>E: 计算出每个网格的每个顶点的哈希值，并将其存入哈希表中，用哈希值做索引，特征值为内容，输入的每个坐标，通过每个顶点所在网格的位置，通过特征值三线性插值出输入坐标的特征值，然后作为MLP的输入</li></ul><h1 id="多分辨率哈希编码"><a href="#多分辨率哈希编码" class="headerlink" title="多分辨率哈希编码"></a>多分辨率哈希编码</h1><p>MULTIRESOLUTION HASH ENCODING</p><p>给定一个全连接神经网络𝑚(y; Φ)，我们对其输入𝑦 = enc(x; 𝜃)的编码感兴趣，该编码在各种应用中提高了近似质量和训练速度，同时没有明显的性能开销。我们的神经网络不仅具有可训练的权重参数Φ，还具有可训练的编码参数𝜃。这些参数被分成𝐿个层级，每个层级包含多达𝑇个特征向量，每个特征向量的维度为𝐹。表1显示了这些超参数的典型值。图3说明了我们多分辨率哈希编码中执行的步骤。每个层级（图中以红色和蓝色显示的两个层级）是独立的，并在概念上将特征向量存储在一个网格的顶点上，该网格的分辨率选择为最粗和最细分辨率之间的几何级数$[𝑁_{min}, 𝑁_{max}]$。</p><ul><li>L：多分辨率</li><li>T：每个分辨率下有T个特征向量</li><li>F：特征向量的维度</li><li>最小和最大分辨率：$N_{min} , N_{max}$</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230703160145.png" alt="image.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230703160333.png" alt="image.png"></p><p>2D中多分辨率哈希编码的示意图。(1) 对于给定的输入坐标 x，我们在 𝐿 个分辨率级别上找到周围的体素，并通过哈希其整数坐标为它们的角点分配索引。(2) 对于所有得到的角点索引，我们从哈希表 𝜃𝑙 中查找相应的 𝐹 维特征向量。(3) 根据 x 在相应的第 𝑙 个体素中的相对位置进行线性插值。(4) 我们将每个级别的结果以及辅助输入 𝜉 ∈ R𝐸 进行串联，生成编码的 MLP 输入 𝑦 ∈ R𝐿𝐹 +𝐸，这是最后进行评估的。(5) 为了训练编码，损失梯度通过 MLP (5)、串联 (4)、线性插值 (3) 并累积在查找到的特征向量中进行反向传播。</p><script type="math/tex; mode=display">\begin{aligned}N_{l} & :=\left\lfloor N_{\mathrm{min}}\cdot b^{l}\right\rfloor,  \\&b:=\exp\left(\frac{\ln N_{\mathrm{max}}-\ln N_{\mathrm{min}}}{L-1}\right).\end{aligned}</script><p>L为分辨率数量，l为分辨率序号。示例中L=2，$N_{0}= N_{min} =2$ , $N_{1}= N_{max}= 3$ , $b = \frac{3}{2}$</p><p>⌊x𝑙 ⌋ 和 ⌈x𝑙 ⌉ 在 $Z_{𝑑}$ (整数)中跨越一个具有 2𝑑 个整数顶点的体素。我们将每个角落映射到级别的相应特征向量数组中的一个条目，该数组的固定大小最多为𝑇 。对于需要少于 𝑇 个参数的密集网格的粗略级别，即$(𝑁_{𝑙} + 1)𝑑 ≤ 𝑇$ ，此映射是一对一的。在更细的级别上，我们使用一个哈希函数 ℎ :$Z^{𝑑} → Z_{𝑇}$ 来索引数组，实际上将其视为哈希表，尽管没有显式的冲突处理。相反，我们依赖基于梯度的优化来在数组中存储适当的稀疏细节，并使用后续的神经网络 𝑚(y; Φ) 进行冲突解决。可训练的编码参数 𝜃 的数量因此为 O (𝑇 )，并且受到 𝑇 · 𝐿 · 𝐹 的限制，我们的情况始终为 𝑇 · 16 · 2 (表 1)。</p><div class="note info">            <p>哈希运算(杂凑函数):把任意长度的输入通过散列算法变换成固定长度的输出<br>Hash table样式为——hash value : grid index</p>          </div><p>我们使用的空间哈希函数(Teschner et al . 2003)的形式为：$h(\mathbf{x})=\left(\bigoplus\limits_{i=1}^dx_i\pi_i\right)\mod T,$⊕ 表示按位异或运算,异或的运算法则为：0⊕0=0，1⊕0=1，0⊕1=1，1⊕1=0（同为0，异为1），𝜋𝑖 是唯一的、很大的素数。实际上，这个公式对每个维度的线性同余（伪随机）排列结果进行了异或操作 (Lehmer 1951)，从而减弱了维度对散列值的影响。值得注意的是，为了实现（伪）独立性，只需要对 𝑑 - 1 个维度进行排列，因此我们选择 𝜋1 := 1，以获得更好的缓存一致性，𝜋2 = 2 654 435 761，𝜋3 = 805 459 861。<br>最后，每个角落的特征向量根据 x 在其超立方体内的相对位置进行𝑑线性插值，即插值权重为 w𝑙 := x𝑙 − ⌊x𝑙 ⌋。请记住，这个过程对每个级别都是独立进行的𝐿 levels. 每个级别的插值特征向量以及辅助输入 𝜉 ∈ R𝐸（如编码的视角和神经辐射缓存中的纹理）被连接起来产生 y ∈ R𝐿𝐹 +𝐸，它是输入编码 enc(x; 𝜃) 到 MLP 𝑚(y; Φ) 的。</p><ul><li><p>性能与质量: 特征向量的个数T越多，质量越好，但同时性能会更低。内存占用与 𝑇 成线性关系，而质量和性能往往以次线性的方式进行缩放。我们在图 4 中分析了 𝑇 的影响，报告了三种神经图形原语在各种 𝑇 值下的测试误差与训练时间的关系。我们建议从业者使用 𝑇 来调整编码以获得所需的性能特征。超参数 𝐿（层级数量）和 𝐹（特征维度数量）也在质量和性能之间进行权衡，我们在图 5 中对可训练编码参数 𝜃 的数量近似恒定条件下进行了分析。在这个分析中，<strong>我们发现 (𝐹 = 2, 𝐿 = 16) 是我们所有应用中有利的 Pareto 最优解</strong>，因此我们在所有其他结果中使用这些值，并将其推荐为默认值。</p></li><li><p>隐式哈希冲突解决(多个相同的grid可能具有相同的哈希值)。这种编码能够在哈希冲突存在的情况下忠实地重建场景，这可能看起来违反直觉。其成功的关键在于不同的分辨率级别具有互补的不同优势。<strong>低分辨率只有少量网格，哈希冲突较少，高分辨率网格很多，可能发生哈希冲突，但是由于当多个网格的哈希值冲突时，可能只有其中的几个网格是有价值的，其他是空气等与所建物体无关的网格</strong>。(<em>粗略的级别，以及整个编码，是一对一映射的——也就是说，它们根本不会发生冲突。然而，它们只能表示场景的低分辨率版本，因为它们提供的特征是从一个间隔较大的点网格进行线性插值的。相反，细节级别可以捕捉到小的特征，因为它们具有细密的网格分辨率，但会遇到许多冲突——也就是说，不同的点会哈希到相同的表项。</em>)<strong>附近具有相同整数坐标⌊x𝑙⌋的输入不被视为碰撞；只有当不同的整数坐标散列到相同的索引时才会发生碰撞</strong>。幸运的是，这种碰撞在空间中是伪随机分布的，并且统计上不太可能在给定一对点的每个级别同时发生。</p><ul><li>当训练样本以这种方式发生碰撞时，它们的梯度会取平均值。需要考虑到这些样本对最终重构的重要性很少相等。例如，辐射场中可见表面上的一个点会对重构图像产生很强的贡献（具有高能见度和高密度，两者都对梯度的幅值产生乘法效应），从而导致其表项发生很大变化，而指向同一表项的空间中的一个点将具有更小的权重。因此，更重要样本的梯度主导了碰撞平均值，并且通过优化，使别名的表项自然地反映了更高权重点的需求。哈希编码的多分辨率方面涵盖了从粗分辨率𝑁min（可以保证无碰撞）到任务所需的最细分辨率𝑁max的全范围。因此，它保证了无论稀疏性如何，都包括了可能发生有意义学习的所有尺度。几何缩放允许只使用O(log(𝑁max/𝑁min))个级别来覆盖这些尺度，这允许选择一个保守地较大的𝑁max值。</li></ul></li><li><p>在线适应性。请注意，如果输入x的分布在训练过程中随时间变化，例如如果它们变得集中在一个小区域内，那么更精细的网格级别将会遇到较少的冲突，从而可以学习到更准确的函数。换句话说，多分辨率哈希编码会自动适应训练数据的分布，继承了树状编码的好处(Takikawa等人，2021)，而不需要进行特定任务的数据结构维护，这可能导致训练过程中出现离散跳跃。我们的一个应用程序，第5.3节中的神经辐射缓存，不断适应动画视点和3D内容，极大地受益于这一特性。</p></li><li><p>𝑑-线性插值。对查询的哈希表条目进行插值，确保编码 enc(x; 𝜃) 及其与神经网络 𝑚(enc(x; 𝜃); Φ) 的组合是连续的。如果没有插值，网络输出中将存在与网格对齐的不连续性，这会导致不理想的块状外观。当近似偏微分方程时，可能希望更高阶的平滑性。计算机图形学中的一个具体示例是有符号距离函数，此时梯度 𝜕𝑚(enc(x; 𝜃); Φ)/𝜕x，即表面法线，理想情况下也应连续。<strong>如果必须保证更高阶的平滑性，我们在附录 A 中描述了一种低成本的方法，但在我们的任何结果中都没有使用，因为会导致重建质量略微降低。</strong></p></li></ul><h1 id="IMPLEMENTATION"><a href="#IMPLEMENTATION" class="headerlink" title="IMPLEMENTATION"></a>IMPLEMENTATION</h1><p>为了展示多分辨率哈希编码的速度，我们在CUDA中实现了它，并将其与tiny-cuda-nn框架(Müller 2021)中的快速全融合MLP集成在一起。我们将多分辨率哈希编码的源代码作为Müller 2021的更新版本发布，并发布了与神经图形原语相关的源代码，网址为<a href="https://github.com/nvlabs/instant-ngp%E3%80%82">https://github.com/nvlabs/instant-ngp</a></p><ul><li>性能考虑。为了优化推理和反向传播的性能，我们以半精度（每个条目2字节）存储哈希表条目。此外，我们还以完全精度维护参数的主副本，以进行稳定的混合精度参数更新，参考Micikevicius等人2018的方法。<ul><li>为了最佳地利用GPU的缓存，我们逐层评估哈希表：<strong>在处理一批输入位置时，我们安排计算来查找所有输入的多分辨率哈希编码的第一级，然后是所有输入的第二级，依此类推</strong>。因此，在任何给定时间，只有少量连续的哈希表需要驻留在缓存中，这取决于GPU上可用的并行性。重要的是，这种计算结构自动地充分利用了可用的缓存和并行性，适用于各种哈希表大小𝑇。</li><li>在我们的硬件上，只要哈希表大小保持在$T ≤ 2^{19}$以下，编码的性能基本保持不变。超过这个阈值后，性能开始显著下降；请参见图4。这是由于我们的NVIDIA RTX 3090 GPU的6 MB L2缓存对于单个级别而言变得太小，当2 · T · F &gt; 6 · 220时，其中2是半精度条目的大小。</li><li>特征维度𝐹的最佳数量取决于GPU架构。一方面，较小的数量有利于先前提到的流式处理方法中的缓存本地性，但另一方面，较大的𝐹通过允许𝐹宽度的向量加载指令来促进内存一致性。𝐹 = 2给出了我们在所有实验中使用的最佳成本质量权衡（参见图5）。</li></ul></li><li>体系结构/神经网络：在除了稍后我们将描述的NeRF之外的所有任务中，我们使用具有两个隐藏层的多层感知器（MLP），每个隐藏层具有64个神经元，隐藏层上使用修正线性单元（ReLU）激活函数，以及一个线性输出层。对于NeRF和有符号距离函数，最大分辨率𝑁max设置为2048×场景尺寸，对于half of the gigapixel image 设置为width的一半，以及辐射缓存中的$2^{19}$（为了支持大场景中的近距离物体而设置的较大值）。</li><li>初始化。我们按照Glorot和Bengio 2010的方法初始化神经网络权重，以便在神经网络的各个层中提供合理的激活和梯度缩放。我们使用均匀分布U（−10−4，10−4）初始化哈希表条目，以提供一定的随机性，同时鼓励初始预测接近零。我们还尝试了各种不同的分布，包括零初始化，但结果都导致初始收敛速度稍微变慢。哈希表似乎对初始化方案具有鲁棒性。</li><li>训练。我们通过应用Adam (Kingma和Ba 2014)来共同训练神经网络权重和哈希表条目，其中我们设定 $𝛽1 = 0.9，𝛽2 = 0.99，𝜖 = 10^{−15}$。𝛽1和𝛽2的选择只会产生很小的差异，但𝜖 = 10−15的小值在哈希表条目的梯度稀疏和弱时能够显著加快收敛速度。为了防止长时间训练后的发散，我们对神经网络权重应用了弱的L2正则化（因子为10−6），但不适用于哈希表条目。</li><li>当拟合gigapixel图像或NeRFs时，我们使用L2损失。对于有符号距离函数，我们使用平均绝对百分比误差（MAPE），定义为|预测值-目标值|/|目标值|+0.01，并且对于神经辐射缓存，我们使用亮度相对L2损失(Müller et al. 2021)。我们观察到对于有符号距离函数，学习率为$10^-4$时收敛最快，对于其他情况，学习率为$10^-2$。对于神经辐射缓存，我们使用批量大小为$2^{14}$，对于其他情况，使用批量大小为$2^{18}$。最后，我们在哈希表条目的梯度完全为0时跳过Adam步骤。这在梯度稀疏的情况下可以节省约10%的性能，这在𝑇 ≫ BatchSize的情况下是常见的。尽管这种启发式方法违反了Adam背后的一些假设，但我们观察到收敛没有降级。</li><li>非空间输入维度 $𝜉 ∈ R^𝐸$。多分辨率哈希编码以相对低维度目标处理空间坐标。我们所有的实验都是在2D或3D中进行的。然而，在学习光场时，将辅助维度$𝜉 ∈ R^𝐸$ 输入神经网络往往是非常有用的，例如视角和材料参数。在这种情况下，可以使用已建立的技术对辅助维度进行编码，其成本不会随着维度的增加而超线性地增长；我们在神经辐射缓存中使用单blob编码(Müller等，2019)，在NeRF中使用球谐基函数，与同时进行的工作相似(Verbin等，2021；Yu等，2021a)。</li></ul><h1 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h1><p>突出编码的多功能性和高质量，我们将其与以前的编码进行比较，在四个不同的计算机图形基元中，这些基元受益于编码空间坐标。</p><h2 id="Gigapixel-Image-Approximation"><a href="#Gigapixel-Image-Approximation" class="headerlink" title="Gigapixel Image Approximation"></a>Gigapixel Image Approximation</h2><h2 id="Signed-Distance-Functions"><a href="#Signed-Distance-Functions" class="headerlink" title="Signed Distance Functions"></a>Signed Distance Functions</h2><h2 id="Neural-Radiance-Caching"><a href="#Neural-Radiance-Caching" class="headerlink" title="Neural Radiance Caching"></a>Neural Radiance Caching</h2><h2 id="Neural-Radiance-and-Density-Fields-NeRF"><a href="#Neural-Radiance-and-Density-Fields-NeRF" class="headerlink" title="Neural Radiance and Density Fields (NeRF)"></a>Neural Radiance and Density Fields (NeRF)</h2>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF/Efficiency </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> InstantNGP </tag>
            
            <tag> Efficiency </tag>
            
            <tag> Encoding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git学习</title>
      <link href="/Learn/Learn-Git/"/>
      <url>/Learn/Learn-Git/</url>
      
        <content type="html"><![CDATA[<p>不要乱用 git reset —hard commit_id 回退 git commit 版本</p><span id="more"></span><h1 id="Clone"><a href="#Clone" class="headerlink" title="Clone"></a>Clone</h1><h2 id="克隆子目录"><a href="#克隆子目录" class="headerlink" title="克隆子目录"></a>克隆子目录</h2><ol><li>初始化：</li></ol><p>Git init</p><ol><li>连接远端库：</li></ol><p>Git remote add origin url</p><ol><li>启用”Sparse Checkout”功能：</li></ol><p>Git config core.Sparsecheckout true</p><ol><li>添加想要 clone 的目录：</li></ol><p>Echo “子目录路径” &gt;&gt; .git/info/sparse-checkout<br>注意：子目录路径不包含 clone 的一级文件夹名称：<br>例如库路径是：<br><a href="https://A/B/C/example.git">https://A/B/C/example.git</a><br>我们想 clone example 下的 D/E/F 目录，则：<br><code>echo “D/E/F” &gt;&gt; .git/info/sparse-checkout</code></p><ol><li>Pull 代码：</li></ol><p>Git pull origin master<br>或者不包含历史版本的 clone：<br>Git pull —depth 1 origin master</p><blockquote><p>版权声明：本文为 CSDN 博主「luo870604851」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。<br>原文链接： <a href="https://blog.csdn.net/luo870604851/article/details/119748749">https://blog.csdn.net/luo870604851/article/details/119748749</a></p></blockquote><h2 id="克隆私有仓库"><a href="#克隆私有仓库" class="headerlink" title="克隆私有仓库"></a>克隆私有仓库</h2><blockquote><p><a href="https://blog.csdn.net/weixin_45508265/article/details/124340158">(21条消息) Git clone 克隆私有项目_git clone 项目_风信子的猫Redamancy的博客-CSDN博客</a><br><a href="https://collabnix.com/how-to-fix-support-for-password-authentication-was-removed-error-in-github/">How to Fix “Support for password authentication was removed” error in GitHub – Collabnix</a></p></blockquote><p><code>git clone http://tokens-name:tokens@github.com/YOUR-USERNAME/YOUR-REPOSITORY</code></p><h1 id="Git-教程"><a href="#Git-教程" class="headerlink" title="Git 教程"></a>Git 教程</h1><p>平时使用：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m shuoming</span><br><span class="line">git push</span><br></pre></td></tr></table></figure><blockquote><p><a href="https://www.liaoxuefeng.com/wiki/896043488029600/897271968352576">工作区和暂存区 - 廖雪峰的官方网站 (liaoxuefeng.com)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230626160832.png" alt="image.png"></p><p><code>git status</code> 查看暂存区 stage 状态</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230626161153.png" alt="image.png"></p><h1 id="Git-reset-—hard-commit-id-导致本地文件丢失"><a href="#Git-reset-—hard-commit-id-导致本地文件丢失" class="headerlink" title="Git reset —hard commit_id 导致本地文件丢失"></a>Git reset —hard commit_id 导致本地文件丢失</h1><blockquote><p><a href="https://blog.csdn.net/qq_56098414/article/details/121291539">(19条消息) 恢复因git reset —hard 但未提交全部文件到仓库导致的文件丢失问题_git reset —hard 把未提交的文件搞丢了_数祖的博客-CSDN博客</a><br><a href="https://blog.csdn.net/chailihua0826/article/details/94619904?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-94619904-blog-121291539.235^v38^pc_relevant_anti_vip_base&amp;spm=1001.2101.3001.4242.2&amp;utm_relevant_index=4">(19条消息) git add 后git reset —hard xxx的代码丢失，代码如何找回_小小花111111的博客-CSDN博客</a></p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">git fsck --lost-found</span><br><span class="line"></span><br><span class="line">git show hash_id</span><br><span class="line">git ls-tree 文件名</span><br><span class="line"></span><br><span class="line">git read-tree --prefix=lib 726d5644919729dd97b7dd23f4a733e2daabab85</span><br><span class="line"></span><br><span class="line">git restore lib</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="删除远程仓库文件，保留本地文件："><a href="#删除远程仓库文件，保留本地文件：" class="headerlink" title="删除远程仓库文件，保留本地文件："></a>删除远程仓库文件，保留本地文件：</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git rm --cached 文件 //本地中该文件不会被删除</span><br><span class="line"></span><br><span class="line">git rm -r --cached 文件夹 //删除文件夹</span><br><span class="line"></span><br><span class="line">git commit -m &#x27;删除某个文件&#x27;</span><br><span class="line"></span><br><span class="line">git push</span><br></pre></td></tr></table></figure><h1 id="新建仓库"><a href="#新建仓库" class="headerlink" title="新建仓库"></a>新建仓库</h1><p>使用 token 链接仓库信息<br><code>git remote set-url origin https://&lt;your_token&gt;@github.com/&lt;USERNAME&gt;/&lt;REPO&gt;.git</code></p><h1 id="用-token-克隆私有仓库"><a href="#用-token-克隆私有仓库" class="headerlink" title="用 token 克隆私有仓库"></a>用 token 克隆私有仓库</h1><p>Git clone from token:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://NeRF-Mine:xxxtokenxxx@github.com/qiyun71/NeRF-Mine.git</span><br><span class="line">tensorboard --port 6007 --logdir /root/tf-logs</span><br></pre></td></tr></table></figure><p>Git remote remove origin<br>Git remote add origin path_repo</p><p>配置全局账户</p><ul><li>Git config —global user.Name …</li><li>Git config —global user.Email …</li></ul><p>Git push 需要配置 token(2023.11.13)</p><ul><li>生成之后，如果之前执行了下面的命令，设置了存储登录凭据（默认存储在~/.Git-credentials）<ul><li>Git config —global credential.Helper store</li><li>Git config —global —unset credential.Helper 清理之前保存的账密信息</li></ul></li><li>Git config —global credential.Helper store</li></ul><p>测试没有百度网盘同步方法</p>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF目前进展</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF-review/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/NeRF-review/</url>
      
        <content type="html"><![CDATA[<p>NeRF相关的论文 at CVPR/ICCV/ECCV/NIPS/ICML/ICLR/SIGGRAPH<br><a href="https://blog.csdn.net/weixin_43962054/article/details/121762182">计算机视觉顶会2022截稿时间及会议时间_ijcai2024截稿日期-CSDN博客</a></p><div class="table-container"><table><thead><tr><th>My post</th><th>Brief description</th><th>status</th></tr></thead><tbody><tr><td><a href="NeRF-Principle.md">NeRF</a> + <a href="NeRF-code.md">Code</a></td><td>NeRF 原理 + 代码理解</td><td>Completed</td></tr><tr><td><a href="NeuS.md">NeuS</a> + <a href="Neus-code.md">Code</a></td><td>表面重建方法 SDFNetwork</td><td>Completed</td></tr><tr><td><a href="NeRF-InstantNGP.md">InstantNGP</a> + <a href="NeRF-InstantNGP-code.md">Tiny-cuda-nn</a></td><td>加速 NeRF 的训练和推理</td><td>Completed（Tcnn）</td></tr><tr><td><a href="Neus-Instant-nsr-pl.md">Instant-nsr-pl</a> + <a href="Neus-Instant-nsr-pl-code.md">Code</a></td><td>Neus+Tcnn+NSR+pl</td><td>Completed</td></tr><tr><td><a href="Instant-NSR.md">Instant-NSR</a> + <a href="Instant-NSR-code.md">Code</a></td><td>快速表面重建</td><td>Completed</td></tr><tr><td><a href="NeRO.md">NeRO</a> + <a href="NeRO-code.md">Code</a></td><td>考虑镜面和漫反射的体渲染函数</td><td>In Processing</td></tr><tr><td><a href="NeRF-Mine.md">NeRF-Mine</a></td><td>基于 Instant-nsr-pl 创建的项目</td><td>Completed</td></tr></tbody></table></div><p>Related link : <a href="https://paperswithcode.com/task/3d-reconstruction">3D Reconstruction</a> | <a href="https://github.com/lif314/awesome-NeRF-papers">awesome-NeRF-papers</a></p><span id="more"></span><h1 id="NeRF"><a href="#NeRF" class="headerlink" title="NeRF"></a>NeRF</h1><p>ECCV 2020 Oral - Best Paper Honorable Mention</p><div class="table-container"><table><thead><tr><th>Year</th><th style="text-align:center">Title&amp;Project Page</th><th style="text-align:center">Brief Description</th><th style="text-align:center">Conf/Jour</th></tr></thead><tbody><tr><td><a href="NeRF-Principle.md">2020</a></td><td style="text-align:center"><a href="https://www.matthewtancik.com/nerf">NeRF:Representing Scenes as Neural Radiance Fields for View Synthesis</a></td><td style="text-align:center">初始文</td><td style="text-align:center">ECCV</td></tr></tbody></table></div><h2 id="原理改进"><a href="#原理改进" class="headerlink" title="原理改进"></a>原理改进</h2><p>Volume Rendering Function</p><h3 id="PL-NeRF"><a href="#PL-NeRF" class="headerlink" title="PL-NeRF"></a>PL-NeRF</h3><p><a href="https://github.com/mikacuy/PL-NeRF">mikacuy/PL-NeRF: NeRF Revisited: Fixing Quadrature Instability in Volume Rendering, Neurips 2023 (github.com)</a></p><p>NeRF 分段常数积分 —&gt; PL-NeRF 分段线性积分<br><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231101151141.png" alt="image.png|666"></p><ul><li>StEik：基于 SDF 的隐式场优化问题(Neural SDF)，提出一个新的约束项<ul><li><a href="https://zhuanlan.zhihu.com/p/649921965">NeurIPS 2023 | 三维重建中的Neural SDF(Neural Implicit Surface) - 知乎 (zhihu.com)</a></li><li><a href="https://github.com/sunyx523/StEik">sunyx523/StEik (github.com)</a></li></ul></li><li>Rethinking Directional Integration in Neural Radiance Fields，修改了 <strong>NeRF 的渲染方程</strong><ul><li><a href="https://arxiv.org/abs/2311.16504">Rethinking Directional Integration in Neural Radiance Fields (arxiv.org)</a></li></ul></li></ul><h2 id="Camera-Pose"><a href="#Camera-Pose" class="headerlink" title="Camera Pose"></a>Camera Pose</h2><div class="table-container"><table><thead><tr><th>Year</th><th>Title&amp;Project Page</th><th>Brief Description</th><th style="text-align:center">Conf/Jour</th></tr></thead><tbody><tr><td>2023</td><td><a href="https://arxiv.org/abs/2311.17119">[2311.17119] Continuous Pose for Monocular Cameras in Neural Implicit Representation (arxiv.org)</a></td><td>将单目相机姿势优化为时间连续函数</td><td style="text-align:center"></td></tr><tr><td>2023</td><td><a href="https://zhiwenfan.github.io/PF-GRT/">Pose-Free Generalizable Rendering Transformer (zhiwenfan.github.io)</a></td><td></td><td style="text-align:center"></td></tr><tr><td>2023</td><td><a href="https://arxiv.org/abs/2310.02687">[2310.02687] USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields (arxiv.org)</a></td><td></td><td style="text-align:center"></td></tr><tr><td>2023</td><td><a href="https://arxiv.org/abs/2309.11326">[2309.11326] How to turn your camera into a perfect pinhole model (arxiv.org)</a></td><td></td><td style="text-align:center"></td></tr><tr><td>2023</td><td><a href="https://arxiv.org/abs/2312.08760">[2312.08760] CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning (arxiv.org)</a></td><td></td><td style="text-align:center">AAAI</td></tr></tbody></table></div><h3 id="Other-paper-about-camera-pose"><a href="#Other-paper-about-camera-pose" class="headerlink" title="Other paper about camera pose"></a>Other paper about camera pose</h3><ul><li>How to turn your camera into a perfect pinhole model</li></ul><h2 id="Efficiency"><a href="#Efficiency" class="headerlink" title="Efficiency"></a>Efficiency</h2><h3 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h3><div class="table-container"><table><thead><tr><th>Year</th><th style="text-align:center">Title&amp;Project Page</th><th style="text-align:center">Brief Description</th><th style="text-align:center">Conf/Jour</th></tr></thead><tbody><tr><td><a href="NeRF-InstantNGP.md">2022</a></td><td style="text-align:center"><a href="https://nvlabs.github.io/instant-ngp/">Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</a></td><td style="text-align:center">多分辨率哈希编码</td><td style="text-align:center">ACM Transactions on Graphics (SIGGRAPH)</td></tr><tr><td><a href="Neus-Instant-nsr-pl.md">2023</a></td><td style="text-align:center"><a href="https://github.com/bennyguo/instant-nsr-pl">bennyguo/instant-nsr-pl</a></td><td style="text-align:center">Neus+NeRF+Nerfacc+tcnn</td><td style="text-align:center">None</td></tr><tr><td><a href="Neuralangelo.md">2023</a></td><td style="text-align:center"><a href="https://research.nvidia.com/labs/dir/neuralangelo/">Neuralangelo: High-Fidelity Neural Surface Reconstruction</a></td><td style="text-align:center">NGP_but数值梯度+Neus_SDF</td><td style="text-align:center">IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</td></tr><tr><td><a href="PermutoSDF.md">2023</a></td><td style="text-align:center"><a href="https://radualexandru.github.io/permuto_sdf/">PermutoSDF</a></td><td style="text-align:center">NGP_butPermutohedral lattice + Neus_SDF，曲率损失和颜色MLP正则解决镜面+无纹理区域，更光滑</td><td style="text-align:center">IEEE/CVF Conference on CVPR</td></tr><tr><td><a href="NeuDA.md">2023</a></td><td style="text-align:center"><a href="https://3d-front-future.github.io/neuda/">NeuDA</a></td><td style="text-align:center">NGP_butDeformable Anchors+HPE + Neus</td><td style="text-align:center">CVPR</td></tr><tr><td><a href="Zip-NeRF.md">2023</a></td><td style="text-align:center"><a href="https://jonbarron.info/zipnerf/">Zip-NeRF (jonbarron.info)</a></td><td style="text-align:center">NGP + Mip-NeRF360</td><td style="text-align:center">ICCV</td></tr><tr><td><a href="Tri-MipRF.md">2023</a></td><td style="text-align:center"><a href="https://wbhu.github.io/projects/Tri-MipRF/">Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields (wbhu.github.io)</a></td><td style="text-align:center">Tri-MipRF encoding(TensoRF + NGP)+ Cone Sampling(Mip-NeRF)</td><td style="text-align:center">ICCV</td></tr><tr><td><a href="TensoRF.md">2022</a></td><td style="text-align:center"><a href="https://apchenstu.github.io/TensoRF/">TensoRF: Tensorial Radiance Fields (apchenstu.github.io)</a></td><td style="text-align:center">TensoRF引入VM分解，提高重建的质量和速度，并减小了内存占用</td><td style="text-align:center">ECCV</td></tr><tr><td><a href="Strivec.md">2023</a></td><td style="text-align:center"><a href="https://github.com/Zerg-Overmind/Strivec">Zerg-Overmind/Strivec (github.com)</a></td><td style="text-align:center">局部张量CP分解为三向量+多尺度网格+占用网格采样方式</td><td style="text-align:center">ICCV</td></tr><tr><td><a href="3D%20Gaussian%20Splatting.md">2023</a></td><td style="text-align:center"><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">3D Gaussian Splatting for Real-Time Radiance Field Rendering (inria.fr)</a></td><td style="text-align:center">优化3D高斯点云，并使用splatting渲染方式，实现实时高分辨率训练+渲染</td><td style="text-align:center">SIGGRAPH</td></tr><tr><td><a href="BakedSDF.md">2023</a></td><td style="text-align:center"><a href="https://bakedsdf.github.io/">BakedSDF</a></td><td style="text-align:center">VolSDF+MarchingCube得到mesh，漫反射+高斯叶来渲染颜色</td><td style="text-align:center">SIGGRAPH</td></tr><tr><td></td><td style="text-align:center"></td><td style="text-align:center"></td></tr></tbody></table></div><h2 id="Large-Scale-Scene"><a href="#Large-Scale-Scene" class="headerlink" title="Large Scale Scene"></a>Large Scale Scene</h2><div class="table-container"><table><thead><tr><th>Year</th><th>Title&amp;Project Page</th><th>Brief Description</th><th style="text-align:center">Conf/Jour</th></tr></thead><tbody><tr><td><a href="NeRF++.md">2020</a></td><td><a href="https://github.com/Kai-46/nerfplusplus">nerfplusplus: improves over nerf in 360 capture of unbounded scenes</a></td><td>将背景的采样点表示为四维向量，与前景分别使用不同的MLP进行训练</td><td style="text-align:center">arXiv</td></tr><tr><td><a href="Mip-NeRF%20360.md">2022</a></td><td><a href="https://jonbarron.info/mipnerf360/">mip-NeRF 360</a></td><td>将单位球外的背景参数化，小型提议网络进行精采样，正则化dist消除小区域云雾</td><td style="text-align:center">CVPR</td></tr></tbody></table></div><h2 id="PointCloud"><a href="#PointCloud" class="headerlink" title="PointCloud"></a>PointCloud</h2><div class="table-container"><table><thead><tr><th>Year</th><th>Title&amp;Project Page</th><th>Brief Description</th><th style="text-align:center">Conf/Jour</th></tr></thead><tbody><tr><td><a href="3D%20Gaussian%20Splatting.md">2023</a></td><td><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">3D Gaussian Splatting for Real-Time Radiance Field Rendering (inria.fr)</a></td><td>优化3D高斯点云，并使用splatting渲染方式，实现实时高分辨率训练+渲染</td><td style="text-align:center">SIGGRAPH</td></tr><tr><td>2023</td><td><a href="https://xrvitd.github.io/Projects/GCNO/index.html">Globally Consistent Normal Orientation for Point Clouds by Regularizing the Winding-Number Field</a></td><td>使得稀疏点云和薄壁点云法向量一致</td><td style="text-align:center">ACM Transactions on Graphics(SIGGRAPH)</td></tr><tr><td>2023</td><td><a href="https://research.nvidia.com/labs/toronto-ai/NKSR/">Neural Kernel Surface Reconstruction</a></td><td>从点云中进行表面重建</td><td style="text-align:center">CVPR 2023 Highlight</td></tr><tr><td><a href="Point-NeRF.md">2022</a></td><td><a href="https://xharlie.github.io/projects/project_sites/pointnerf/">Point-NeRF: Point-based Neural Radiance Fields</a></td><td>生成初始化点云，基于点云进行增删和体渲染</td><td style="text-align:center">CVPR 2022 Oral</td></tr></tbody></table></div><h2 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h2><div class="table-container"><table><thead><tr><th>Year</th><th style="text-align:center">Title&amp;Project Page</th><th style="text-align:center">Brief Description</th><th style="text-align:center">Conf/Jour</th></tr></thead><tbody><tr><td><a href="A%20Critical%20Analysis%20of%20NeRF-Based%203D%20Reconstruction.md">2023</a></td><td style="text-align:center"><a href="https://www.mdpi.com/2072-4292/15/14/3585">A Critical Analysis of NeRF-Based 3D Reconstruction</a></td><td style="text-align:center">对比了Colmap摄影测量法和NeRF-based方法在3D Reconstruction中的优劣</td><td style="text-align:center">MDPI remote sensing</td></tr><tr><td><a href="NeRF%20in%20the%20industrial%20and%20robotics%20domain.md">2023</a></td><td style="text-align:center"><a href="https://github.com/Maftej/iisnerf">Maftej/iisnerf (github.com)</a></td><td style="text-align:center">探索了NeRF在工业和机器人领域的应用</td><td style="text-align:center">None</td></tr><tr><td><a href="2023%20Conf%20about%20NeRF.md">2023</a></td><td style="text-align:center">None</td><td style="text-align:center">2023年NeRF Review ref others</td><td style="text-align:center">None</td></tr><tr><td><a href="A%20Review%20of%20Deep%20Learning-Powered%20Mesh%20Reconstruction%20Methods.md">2023</a></td><td style="text-align:center">A Review of Deep Learning-Powered Mesh Reconstruction Methods</td><td style="text-align:center">介绍了几种3D模型表示方法+回顾了将DL应用到Mesh重建中的方法</td><td style="text-align:center">None</td></tr></tbody></table></div><h2 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h2><p>加速训练和渲染过程、提高新视图质量</p><div class="table-container"><table><thead><tr><th>Year</th><th>Title&amp;Project Page</th><th>Brief Description</th><th style="text-align:center">Conf/Jour</th></tr></thead><tbody><tr><td><a href="NerfAcc.md">2023</a></td><td><a href="https://www.nerfacc.com/en/latest/">NerfAcc Documentation — nerfacc 0.5.3</a></td><td>一种新的采样方法可以加速NeRF</td><td style="text-align:center">arXiv</td></tr><tr><td><a href="Mip-NeRF.md">2021</a></td><td><a href="https://jonbarron.info/mipnerf/">mip-NeRF </a></td><td>截头圆锥体采样方法+IPEncoding</td><td style="text-align:center">ICCV</td></tr><tr><td><a href="Floaters%20No%20More.md">2023</a></td><td><a href="https://gradient-scaling.github.io/#Code">Floaters No More: Radiance Field Gradient Scaling for Improved Near-Camera Training</a></td><td>通过梯度缩放，解决基于NeRF重建场景中的背景塌陷和镜前漂浮物</td><td style="text-align:center">The Eurographics Association</td></tr><tr><td>2023</td><td><a href="https://kaist-viclab.github.io/pronerf-site/">ProNeRF: Learning Efficient Projection-Aware Ray Sampling for Fine-Grained Implicit Neural Radiance Fields (kaist-viclab.github.io)</a></td><td>投影感知采样（PAS）</td></tr></tbody></table></div><ul><li><a href="https://arxiv.org/pdf/2310.04152.pdf">Improving Neural Radiance Fields Using Near-Surface Sampling With Point Cloud Generation (arxiv.org)</a></li><li><a href="https://arxiv.org/abs/2311.07044">L0-Sampler: An L0 Model Guided Volume Sampling for NeRF</a> 分段常数采样改为分段指数采样</li></ul><h2 id="Sparse-images-Generalization"><a href="#Sparse-images-Generalization" class="headerlink" title="Sparse images/Generalization"></a>Sparse images/Generalization</h2><div class="table-container"><table><thead><tr><th>Year</th><th>Title&amp;Project Page</th><th>Brief Description</th><th style="text-align:center">Conf/Jour</th></tr></thead><tbody><tr><td><a href="SparseNeuS.md">2022</a></td><td><a href="https://www.xxlong.site/SparseNeuS/">SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse Views</a></td><td>多层几何推理框架+多尺度颜色混合+一致性感知的微调</td><td style="text-align:center">ECCV</td></tr><tr><td>2023</td><td><a href="https://sparsenerf.github.io/">SparseNeRF</a></td><td>利用来自现实世界不准确观测的深度先验知识</td><td style="text-align:center">Technical Report</td></tr><tr><td>2021</td><td><a href="https://alexyu.net/pixelnerf/">pixelNeRF: Neural Radiance Fields from One or Few Images (alexyu.net)</a></td><td></td><td style="text-align:center">CVPR</td></tr><tr><td><a href="FORGE.md">2022</a></td><td><a href="https://ut-austin-rpl.github.io/FORGE/">FORGE (ut-austin-rpl.github.io)</a></td><td>voxel特征提取2D—&gt;3D+相机姿态估计+特征共享和融合+神经隐式重建</td><td style="text-align:center">ArXiv</td></tr><tr><td><a href="FreeNeRF.md">2023</a></td><td><a href="https://jiawei-yang.github.io/FreeNeRF/">FreeNeRF: Frequency-regularized NeRF (jiawei-yang.github.io)</a></td><td>稀疏视图训练时，逐步开放高频分量可以获得更好的效果，遮挡正则消除floaters</td><td style="text-align:center">CVPR</td></tr><tr><td>2023</td><td><a href="https://sarahweiii.github.io/zerorf/">ZeroRF (sarahweiii.github.io)</a></td><td></td><td style="text-align:center"></td></tr><tr><td>2023</td><td><a href="https://github.com/eezkni/ColNeRF">eezkni/ColNeRF: [AAAI2024] Pytorch implementation of “ColNeRF: Collaboration for Generalizable Sparse Input Neural Radiance Field” (github.com)</a></td><td></td><td style="text-align:center">AAAI2024</td></tr></tbody></table></div><h2 id="Surface-Reconstruction"><a href="#Surface-Reconstruction" class="headerlink" title="Surface Reconstruction"></a>Surface Reconstruction</h2><div class="table-container"><table><thead><tr><th>Year</th><th style="text-align:center">Title&amp;Project Page</th><th style="text-align:center">Brief Description</th><th style="text-align:center">Conf/Jour</th></tr></thead><tbody><tr><td><a href="NeuS.md">2021</a></td><td style="text-align:center"><a href="https://lingjie0206.github.io/papers/NeuS/">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction</a></td><td style="text-align:center">Neus: SDF表面重建方法</td><td style="text-align:center">NeurIPS</td></tr><tr><td><a href="Instant-NSR.md">2022</a></td><td style="text-align:center"><a href="https://zhaofuq.github.io/NeuralAM/">Human Performance Modeling and Rendering via Neural Animated Mesh</a></td><td style="text-align:center">NSR: Neus_TSDF + NGP，但是依赖mask</td><td style="text-align:center">SIGGRAPH Asia</td></tr><tr><td><a href="Neus-Instant-nsr-pl.md">2023</a></td><td style="text-align:center"><a href="https://github.com/bennyguo/instant-nsr-pl">bennyguo/instant-nsr-pl</a></td><td style="text-align:center">Neus+NeRF+Nerfacc+tcnn</td><td style="text-align:center">None</td></tr><tr><td><a href="Neuralangelo.md">2023</a></td><td style="text-align:center"><a href="https://research.nvidia.com/labs/dir/neuralangelo/">Neuralangelo: High-Fidelity Neural Surface Reconstruction</a></td><td style="text-align:center">NGP_but数值梯度+Neus_SDF</td><td style="text-align:center">IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</td></tr><tr><td><a href="PermutoSDF.md">2023</a></td><td style="text-align:center"><a href="https://radualexandru.github.io/permuto_sdf/">PermutoSDF</a></td><td style="text-align:center">NGP_butPermutohedral lattice + Neus_SDF，曲率损失和颜色MLP正则解决镜面+无纹理区域，更光滑</td><td style="text-align:center">IEEE/CVF Conference on CVPR</td></tr><tr><td><a href="NeuDA.md">2023</a></td><td style="text-align:center"><a href="https://3d-front-future.github.io/neuda/">NeuDA</a></td><td style="text-align:center">NGP_butDeformable Anchors+HPE + Neus</td><td style="text-align:center">CVPR</td></tr><tr><td><a href="NeRO.md">2023</a></td><td style="text-align:center"><a href="https://liuyuan-pal.github.io/NeRO/">NeRO: Neural Geometry and BRDF Reconstruction of Reflective Objects from Multiview Images</a></td><td style="text-align:center">Neus_SDF 新的光表示方法可以重建准确的几何和BRDF，但是细节处由于太光滑而忽略，反射颜色也依赖准确的法线</td><td style="text-align:center">SIGGRAPH (ACM TOG)</td></tr><tr><td><a href="UNISURF.md">2021</a></td><td style="text-align:center"><a href="https://moechsle.github.io/unisurf/">UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction </a></td><td style="text-align:center">UNISURF用占用值来表示表面，代替NeRF中的$\alpha$</td><td style="text-align:center">ICCV (oral)</td></tr><tr><td><a href="PlankAssembly.md">2023</a></td><td style="text-align:center"><a href="https://manycore-research.github.io/PlankAssembly/">PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs</a></td><td style="text-align:center">基于Transform的自注意力提出模型,将2D三视图转化成3D模型的代码形式DSL</td><td style="text-align:center">ICCV</td></tr><tr><td><a href="NeUDF.md">2023</a></td><td style="text-align:center"><a href="http://geometrylearning.com/neudf/">NeUDF</a></td><td style="text-align:center">使用UDF，可以重建具有任意拓扑的表面，例如非水密表面</td><td style="text-align:center">CVPR</td></tr><tr><td><a href="NeuS2.md">2023</a></td><td style="text-align:center"><a href="https://vcai.mpi-inf.mpg.de/projects/NeuS2/">NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction</a></td><td style="text-align:center">基于Neus、NGP和NSR，实现高质量快速的静态和动态建模</td><td style="text-align:center">ICCV</td></tr><tr><td><a href="Color-NeuS.md">2023</a></td><td style="text-align:center"><a href="https://colmar-zlicheng.github.io/color_neus/">Color-NeuS (colmar-zlicheng.github.io)</a></td><td style="text-align:center">解决了类Neus方法推理时表面颜色提取困难和不正确的问题</td><td style="text-align:center">arXiv</td></tr><tr><td><a href="HF-NeuS.md">2022</a></td><td style="text-align:center"><a href="https://github.com/yiqun-wang/HFS">HF-NeuS: Improved Surface Reconstruction Using High-Frequency Details</a></td><td style="text-align:center">新的SDF与透明度$\alpha$关系函数,将SDF分解为基和位移两个独立隐函数的组合</td><td style="text-align:center">NeurIPS</td></tr><tr><td><a href="Geo-Neus.md">2022</a></td><td style="text-align:center"><a href="https://github.com/GhiXu/Geo-Neus">Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for Multi-view Reconstruction</a></td><td style="text-align:center">使用COLMAP产生的稀疏点来作为SDF的显示监督,具有多视图立体约束的隐式曲面上的几何一致监督</td><td style="text-align:center">NeurIPS</td></tr><tr><td><a href="IDR.md">2020</a></td><td style="text-align:center"><a href="https://lioryariv.github.io/idr/">Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance (lioryariv.github.io)</a></td><td style="text-align:center">端到端的IDR：可以从masked的2D图像中学习3D几何、外观，<em>允许粗略的相机估计</em></td><td style="text-align:center">NeurIPS</td></tr><tr><td><a href="FlexiCubes.md">2023</a></td><td style="text-align:center"><a href="https://research.nvidia.com/labs/toronto-ai/flexicubes/">Flexible Isosurface Extraction for Gradient-Based Mesh Optimization (FlexiCubes) (nvidia.com)</a></td><td style="text-align:center">一种新的Marching Cube的方法</td><td style="text-align:center">ACM Trans. on Graph. (SIGGRAPH 2023)</td></tr><tr><td></td><td style="text-align:center"></td><td style="text-align:center"></td></tr></tbody></table></div><h2 id="Shadow-amp-Highlight"><a href="#Shadow-amp-Highlight" class="headerlink" title="Shadow&amp;Highlight"></a>Shadow&amp;Highlight</h2><div class="table-container"><table><thead><tr><th>Year</th><th>Title&amp;Project Page</th><th>Brief Description</th><th style="text-align:center">Conf/Jour</th></tr></thead><tbody><tr><td>2023</td><td><a href="https://nrhints.github.io/">Relighting Neural Radiance Fields with Shadow and Highlight Hints</a></td><td>数据集使用相机位姿和灯源位姿,训练集大约500张,Shadow and Highlight hints</td><td style="text-align:center">SIGGRAPH</td></tr><tr><td><a href="NeRO.md">2023</a></td><td><a href="https://liuyuan-pal.github.io/NeRO/">NeRO: Neural Geometry and BRDF Reconstruction of Reflective Objects from Multiview Images</a></td><td>Neus_SDF 新的光表示方法可以重建准确的几何和BRDF，但是细节处由于太光滑而忽略，反射颜色也依赖准确的法线</td><td style="text-align:center">SIGGRAPH (ACM TOG)</td></tr><tr><td><a href="ShadowNeuS.md">2023</a></td><td><a href="https://gerwang.github.io/shadowneus/">ShadowNeuS (gerwang.github.io)</a></td><td>多光照下单视图重建SDF+RGB图像重建外观+BRDF</td><td style="text-align:center">CVPR</td></tr><tr><td><a href="Ref-NeRF.md">2022</a></td><td><a href="https://dorverbin.github.io/refnerf/">Ref-NeRF (dorverbin.github.io)</a></td><td>基于球面谐波的IDE编码+预测表面法向+BRDF</td><td style="text-align:center">CVPR</td></tr><tr><td><a href="NeRFactor.md">2021</a></td><td><a href="https://xiuming.info/projects/nerfactor/">NeRFactor (xiuming.info)</a></td><td>NeRFactor在未知光照条件下从图像中恢复物体形状和反射率</td><td style="text-align:center">SIGGRAPH</td></tr><tr><td><a href="Ref-NeuS.md">2023</a></td><td><a href="https://g3956.github.io/">Ref-NeuS (g3956.github.io)</a></td><td>Anomaly Detection for Reflection Score + Visibility Identification for Reflection Score+反射感知的光度损失</td><td style="text-align:center">ICCV Oral</td></tr><tr><td><a href="NeuFace.md">2023</a></td><td><a href="https://github.com/aejion/NeuFace">NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images</a></td><td><strong>BRDF+SDF+PBR</strong>框架，端到端训练，重建人脸的几何+外观</td><td style="text-align:center">CVPR</td></tr><tr><td>2024</td><td><a href="https://github.com/cuiziteng/Aleth-NeRF">cuiziteng/Aleth-NeRF: [AAAI 2024] Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption (github.com)</a></td><td></td><td style="text-align:center">AAAI</td></tr><tr><td>2023</td><td><a href="https://arxiv.org/abs/2312.08118">[2312.08118] Neural Radiance Fields for Transparent Object Using Visual Hull (arxiv.org)</a></td><td></td></tr></tbody></table></div><h2 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h2><div class="table-container"><table><thead><tr><th>Year</th><th>Title&amp;Project Page</th><th>Brief Description</th><th style="text-align:center">Conf/Jour</th></tr></thead><tbody><tr><td><a href="NeRF-Studio.md">2023</a></td><td><a href="https://docs.nerf.studio/en/latest/">nerfstudio</a></td><td>集成现有的NeRF方法</td><td style="text-align:center">ACM SIGGRAPH</td></tr><tr><td>2022</td><td><a href="https://autonomousvision.github.io/sdfstudio/">SDFStudio</a></td><td>集成基于SDF的NeRF方法</td><td style="text-align:center">None</td></tr></tbody></table></div><h1 id="Dalao"><a href="#Dalao" class="headerlink" title="Dalao"></a>Dalao</h1><div class="table-container"><table><thead><tr><th>PhD.School</th><th style="text-align:center">Homepage</th><th style="text-align:center">Paper</th></tr></thead><tbody><tr><td>Zhejiang University</td><td style="text-align:center"><a href="http://www.cad.zju.edu.cn/home/zhpcui/">Zhaopeng Cui (zju.edu.cn)</a></td><td style="text-align:center">Global Structure-from-Motion by Similarity Averaging</td></tr><tr><td>HKU</td><td style="text-align:center"><a href="https://liuyuan-pal.github.io/">刘缘Yuan Liu - Homepage (liuyuan-pal.github.io)</a></td><td style="text-align:center">Neus,NeRO</td></tr><tr><td>CUHK</td><td style="text-align:center"><a href="https://wbhu.github.io/">胡文博HU, Wenbo’s Homepage (wbhu.github.io)</a></td><td style="text-align:center">Tri-MipRF</td></tr><tr><td>HKU</td><td style="text-align:center"><a href="https://quartz-khaan-c6f.notion.site/Peng-Wang-0ab0a2521ecf40f5836581770c14219c">Peng Wang (王鹏) (notion.site)</a></td><td style="text-align:center">Neus</td></tr><tr><td>UC Berkeley</td><td style="text-align:center"><a href="https://jonbarron.info/">Jon Barron</a></td><td style="text-align:center">Mip-NeRF,Mip-NeRF360,Zip-NeRF,Ref-NeRF</td></tr><tr><td>UC Berkeley</td><td style="text-align:center"><a href="https://www.matthewtancik.com/">Matthew Tancik</a></td><td style="text-align:center">NerfStudio,NerfAcc,Plenoxels,Mip-NeRF,</td></tr><tr><td>UC Berkeley</td><td style="text-align:center"><a href="https://bmild.github.io/">Ben Mildenhall (bmild.github.io)</a></td><td style="text-align:center">NeRF</td></tr><tr><td>ShanghaiTech University</td><td style="text-align:center"><a href="https://apchenstu.github.io/">陈安沛Anpei Chen (apchenstu.github.io)</a></td><td style="text-align:center">TensoRF</td></tr><tr><td>University of Pennsylvania</td><td style="text-align:center"><a href="https://lingjie0206.github.io/">Lingjie Liu (lingjie0206.github.io)</a></td><td style="text-align:center">NeuS,NeuS2,NeuralUDF,Drag Your GAN</td></tr></tbody></table></div><h1 id="有趣的应用"><a href="#有趣的应用" class="headerlink" title="有趣的应用"></a>有趣的应用</h1><div class="table-container"><table><thead><tr><th>Year</th><th style="text-align:center">Title&amp;Project Page</th><th style="text-align:center">Brief Description</th><th>Conf/Jour</th></tr></thead><tbody><tr><td>2023</td><td style="text-align:center"><a href="https://world-from-eyes.github.io/">Seeing the World through Your Eyes</a></td><td style="text-align:center">从人眼的倒影中重建物体</td><td>None</td></tr><tr><td><a href="PAniC-3D.md">2023</a></td><td style="text-align:center"><a href="https://github.com/shuhongchen/panic3d-anime-reconstruction">PAniC-3D Stylized Single-view 3D Reconstruction from Portraits of Anime Characters</a></td><td style="text-align:center">从插画风格角色肖像中重建3D</td><td>CVPR</td></tr><tr><td>2023</td><td style="text-align:center"><a href="https://www.lerf.io/">LERF: Language Embedded Radiance Fields</a></td><td style="text-align:center">用语言查询空间中的3D物体，并高亮显示</td><td>ICCV 2023 (Oral)</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Review </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch代码学习笔记</title>
      <link href="/Learn/Learn-PyTorch/"/>
      <url>/Learn/Learn-PyTorch/</url>
      
        <content type="html"><![CDATA[<p>基于Pytorch学习DL时，学习到的一些技巧/code</p><span id="more"></span><h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><h2 id="windows"><a href="#windows" class="headerlink" title="windows"></a>windows</h2><blockquote><p><a href="https://dashen.wang/1283.html">关于国内conda安装cuda11.6+pytorch的那些事。 – 王大神 (dashen.wang)</a></p></blockquote><p>使用miniconda创建虚拟环境</p><ul><li>conda create -n mine python=3.8</li><li>conda activate mine</li></ul><p>安装cuda</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">换源：</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge</span><br><span class="line">conda config --set show_channel_urls true</span><br><span class="line"></span><br><span class="line">安装：</span><br><span class="line">conda install pytorch torchvision torchaudio pytorch-cuda=11.6</span><br><span class="line"></span><br><span class="line">Found conflicts:</span><br><span class="line">Package pytorch conflicts for:  </span><br><span class="line">torchaudio -&gt; pytorch[version=&#x27;1.10.0|1.10.1|1.10.2|1.11.0|1.12.0|1.12.1|1.13.0|1.13.1|2.0  </span><br><span class="line">.0|2.0.1|1.9.1|1.9.0|1.8.1|1.8.0|1.7.1|1.7.0|1.6.0&#x27;]  </span><br><span class="line">torchvision -&gt; pytorch[version=&#x27;1.10.0|1.10.1|1.10.2|2.0.1|2.0.0|1.13.1|1.13.0|1.12.1|1.12  </span><br><span class="line">.0|1.11.0|1.9.1|1.9.0|1.8.1|1.8.0|1.7.1|1.7.0|1.6.0|1.5.1&#x27;]</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">使用以下命令安装</span><br><span class="line">&gt; conda install -c gpytorch gpytorch</span><br><span class="line"></span><br><span class="line">安装带cuda的torch</span><br><span class="line">pip install torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118 --user</span><br></pre></td></tr></table></figure><h1 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Neus: </span><br><span class="line">torch.set_default_tensor_type(&#x27;torch.cuda.FloatTensor&#x27;)</span><br><span class="line">parser.add_argument(&#x27;--gpu&#x27;, type=int, default=0)</span><br><span class="line">torch.cuda.set_device(args.gpu)</span><br><span class="line"></span><br><span class="line">self.device = torch.device(&#x27;cuda&#x27;)</span><br><span class="line">network = Network(**self.conf[&#x27;model.nerf&#x27;]).to(self.device)</span><br><span class="line"></span><br><span class="line">#################################################################</span><br><span class="line">NeRF:</span><br><span class="line">device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line">model = NeRF().to(device)</span><br><span class="line">render_poses = torch.Tensor(render_poses).to(device)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.device(&#x27;cpu&#x27;), torch.device(&#x27;cuda&#x27;), torch.device(&#x27;cuda:1&#x27;)</span><br><span class="line">如果有多个GPU，我们使用`torch.device(f&#x27;cuda:&#123;i&#125;&#x27;)` 来表示第i块GPU（i从0开始）。 另外，`cuda:0`和`cuda`是等价的。</span><br><span class="line"></span><br><span class="line">查询gpu数量</span><br><span class="line">torch.cuda.device_count()</span><br><span class="line"></span><br><span class="line">查询张量所在设备</span><br><span class="line">x = torch.tensor([1, 2, 3])</span><br><span class="line">x.device   #device(type=&#x27;cpu&#x27;) 默认为gpu，也可为cpu</span><br></pre></td></tr></table></figure><p>两张量相互运算需要在同一台设备上<code>Z = X.cuda(1)</code></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230702204507.png" alt="image.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给网络指定设备</span><br><span class="line">net = nn.Sequential(nn.Linear(3, 1))</span><br><span class="line">net = net.to(device=try_gpu())</span><br></pre></td></tr></table></figure><p>==只要所有的数据和参数都在同一个设备上， 我们就可以有效地学习模型==</p><h1 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h1><p><a href="https://zhuanlan.zhihu.com/p/69294347">PyTorch 的 Autograd - 知乎 (zhihu.com)</a></p><h1 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h1><h2 id="Adam多个model参数，然后更新lr"><a href="#Adam多个model参数，然后更新lr" class="headerlink" title="Adam多个model参数，然后更新lr"></a>Adam多个model参数，然后更新lr</h2><p>Adam_in_Neus: params_to_train is a list</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">params_to_train = []</span><br><span class="line">self.nerf_outside = NeRF(**self.conf[&#x27;model.nerf&#x27;]).to(self.device) # 创建一个NeRF网络</span><br><span class="line">self.sdf_network = SDFNetwork(**self.conf[&#x27;model.sdf_network&#x27;]).to(self.device) # 创建一个SDF网络</span><br><span class="line">self.deviation_network = SingleVarianceNetwork(**self.conf[&#x27;model.variance_network&#x27;]).to(self.device)</span><br><span class="line">self.color_network = RenderingNetwork(**self.conf[&#x27;model.rendering_network&#x27;]).to(self.device)</span><br><span class="line">params_to_train += list(self.nerf_outside.parameters())</span><br><span class="line">params_to_train += list(self.sdf_network.parameters())</span><br><span class="line">params_to_train += list(self.deviation_network.parameters())</span><br><span class="line">params_to_train += list(self.color_network.parameters())</span><br><span class="line"></span><br><span class="line">self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)</span><br></pre></td></tr></table></figure><p>然后更新学习率</p><p><code>g = self.optimizer.param_groups[index]</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for g in self.optimizer.param_groups:</span><br><span class="line">    g[&#x27;lr&#x27;] = self.learning_rate * learning_factor</span><br></pre></td></tr></table></figure><p><strong>from</strong><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def update_learning_rate(self):</span><br><span class="line">    if self.iter_step &lt; self.warm_up_end:</span><br><span class="line">        learning_factor = self.iter_step / self.warm_up_end</span><br><span class="line">    else:</span><br><span class="line">        alpha = self.learning_rate_alpha</span><br><span class="line">        progress = (self.iter_step - self.warm_up_end) / (self.end_iter - self.warm_up_end)</span><br><span class="line">        learning_factor = (np.cos(np.pi * progress) + 1.0) * 0.5 * (1 - alpha) + alpha</span><br><span class="line"></span><br><span class="line">    for g in self.optimizer.param_groups:</span><br><span class="line">        g[&#x27;lr&#x27;] = self.learning_rate * learning_factor</span><br></pre></td></tr></table></figure></p><h2 id="lr学习率"><a href="#lr学习率" class="headerlink" title="lr学习率"></a>lr学习率</h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/611364321">lr scheduler介绍和可视化 - 知乎 (zhihu.com)</a></p></blockquote><p>lr_scheduler.nameLR</p><div class="table-container"><table><thead><tr><th>nameLR</th><th>Brief</th></tr></thead><tbody><tr><td>ConstantLR</td><td>init_lr乘以factor持续total_iters</td></tr><tr><td>CosineAnnealingLR</td><td>构造一个cos函数，周期为<code>2T_max</code>,学习率区间为<code>[init_lr,eta_min]</code>，cos向左平移last_epoch个iter</td></tr><tr><td>CyclicLR</td><td>三种mode：triangular三角波amplitude不变，triangular2每个cycle的amplitude减半，exp_range每个cycle iteration将amplitude缩放为$gamma^{iteration}$</td></tr><tr><td>ExponentialLR</td><td>指数减小lr：$gamma^{iter}$</td></tr><tr><td>LambdaLR</td><td>使用自定义的lambda来处理lr</td></tr><tr><td>StepLR</td><td>阶梯每step_size步将lr乘以gamma</td></tr><tr><td>MultiStepLR</td><td>在<code>milestones = [30,80]</code>处将lr乘以gamma</td></tr><tr><td>OneCycleLR</td><td>not chainable，lr先上升到max_lr,然后减小。最大值处的step为<code>total_step * pct_start = epochs * steps_per_epoch * pct_start</code></td></tr><tr><td>ConstantLR</td><td>前total_iters的lr为<code>init_lr * factor</code></td></tr><tr><td>LinearLR</td><td>从<code>init_lr * start_factor</code>开始线性增长total_iters步到 <code>init_lr * end_factor</code></td></tr><tr><td>MultiplicativeLR</td><td>学习率从init_lr 根据<code>lr_lambda = lambda step: factor</code>非线性衰减：$lr = factor^{step}$</td></tr></tbody></table></div><p>连接多个lr</p><div class="table-container"><table><thead><tr><th>nameLR</th><th>Brief</th></tr></thead><tbody><tr><td>SequentialLR</td><td><strong>milestones</strong>前为scheduler1，后为scheduler2</td></tr><tr><td>ChainedScheduler</td><td>多个scheduler叠加</td></tr><tr><td></td></tr></tbody></table></div><h1 id="私有成员"><a href="#私有成员" class="headerlink" title="私有成员"></a>私有成员</h1><p>带双下划线函数</p><div class="table-container"><table><thead><tr><th>function</th><th>brief description</th></tr></thead><tbody><tr><td><code>nn.module.__repr__</code></td><td>当print(model)时会运行该函数</td></tr><tr><td><code>__del__</code></td><td>当<code>del object</code>时运行该函数</td></tr></tbody></table></div><h1 id="torch-cuda"><a href="#torch-cuda" class="headerlink" title="torch.cuda"></a>torch.cuda</h1><h2 id="cuda事件计算程序运行时间"><a href="#cuda事件计算程序运行时间" class="headerlink" title="cuda事件计算程序运行时间"></a>cuda事件计算程序运行时间</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">iter_start = torch.cuda.Event(enable_timing = <span class="literal">True</span>)</span><br><span class="line">iter_end = torch.cuda.Event(enable_timing = <span class="literal">True</span>)</span><br><span class="line">iter_start.record()</span><br><span class="line"><span class="comment"># iter 1 code</span></span><br><span class="line">iter_end.record()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;iter time: <span class="subst">&#123;iter_start.elapsed_time(iter_end)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>eg:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">iter_start = torch.cuda.Event(enable_timing = <span class="literal">True</span>)</span><br><span class="line">iter_end = torch.cuda.Event(enable_timing = <span class="literal">True</span>)</span><br><span class="line">iter_start.record()</span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]).cuda()</span><br><span class="line"></span><br><span class="line">iter_end.record()</span><br><span class="line"></span><br><span class="line">timestamp = iter_start.elapsed_time(iter_end)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;iter time: <span class="subst">&#123;timestamp:03f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Python的工具</title>
      <link href="/Python%20Practise/Make-Py-Tools/"/>
      <url>/Python%20Practise/Make-Py-Tools/</url>
      
        <content type="html"><![CDATA[<p>Python写的一些小工具</p><span id="more"></span><h1 id="video2image"><a href="#video2image" class="headerlink" title="video2image"></a>video2image</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_video_to_images</span>(<span class="params">video_path, output_folder, frame_interval=<span class="number">1</span></span>):</span><br><span class="line">    <span class="comment"># 创建输出文件夹</span></span><br><span class="line">    os.makedirs(output_folder, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 打开视频文件</span></span><br><span class="line">    video_capture = cv2.VideoCapture(video_path)</span><br><span class="line">    <span class="comment"># 读取视频的帧</span></span><br><span class="line">    frame_count = <span class="number">0</span></span><br><span class="line">    saved_frame_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 读取一帧</span></span><br><span class="line">        ret, frame = video_capture.read()</span><br><span class="line">        <span class="comment"># 如果视频帧读取失败，则退出循环</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 判断是否保存该帧</span></span><br><span class="line">        <span class="keyword">if</span> frame_count % frame_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 生成输出文件名</span></span><br><span class="line">            <span class="comment"># output_filename = os.path.join(output_folder, f&quot;frame_&#123;saved_frame_count:05d&#125;.jpg&quot;)</span></span><br><span class="line">            output_filename = os.path.join(output_folder, <span class="string">f&quot;<span class="subst">&#123;frame_count:03d&#125;</span>.png&quot;</span>)  <span class="comment"># like BlendedMVS</span></span><br><span class="line">             <span class="comment"># 保存帧为图像文件</span></span><br><span class="line">            cv2.imwrite(output_filename, frame)</span><br><span class="line">            saved_frame_count += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 增加帧计数</span></span><br><span class="line">        frame_count += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 释放资源</span></span><br><span class="line">    video_capture.release()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;视频分割完成，总共分割得到 <span class="subst">&#123;saved_frame_count&#125;</span> 张图片。&quot;</span>)</span><br><span class="line"><span class="comment"># 示例用法</span></span><br><span class="line">video_path = <span class="string">&quot;wsz.mp4&quot;</span></span><br><span class="line">output_folder = <span class="string">&quot;output_images&quot;</span></span><br><span class="line">frame_interval = <span class="number">1</span>  <span class="comment"># 保存每隔一帧的图像</span></span><br><span class="line">split_video_to_images(video_path, output_folder, frame_interval)</span><br></pre></td></tr></table></figure><h1 id="get-mask-from-image"><a href="#get-mask-from-image" class="headerlink" title="get_mask_from_image"></a>get_mask_from_image</h1><p>To get images dataset like BlendedMVS</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_mask</span>(<span class="params"><span class="built_in">dir</span></span>):</span><br><span class="line">    dir_mask = os.path.join(<span class="built_in">dir</span>,<span class="string">&#x27;mask&#x27;</span>)</span><br><span class="line">    <span class="comment"># print(dir_mask)</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dir_mask):</span><br><span class="line">        os.mkdir(dir_mask)</span><br><span class="line">    img_dir = os.path.join(<span class="built_in">dir</span>,<span class="string">&#x27;image&#x27;</span>)</span><br><span class="line">    img_list = os.listdir(img_dir)</span><br><span class="line">    <span class="keyword">for</span> img_file <span class="keyword">in</span> img_list:</span><br><span class="line">        (filename, extension) = os.path.splitext(img_file)</span><br><span class="line">        <span class="keyword">if</span> extension == <span class="string">&#x27;.jpg&#x27;</span> :</span><br><span class="line">            img_file_png = filename + <span class="string">&#x27;.png&#x27;</span></span><br><span class="line">            img_file_png = os.path.join(img_dir, img_file_png)</span><br><span class="line">            img_file_mask = <span class="built_in">dir</span> + <span class="string">&#x27;/mask/&#x27;</span> + filename + <span class="string">&#x27;.png&#x27;</span></span><br><span class="line">            img = cv2.imread(os.path.join(img_dir,img_file))</span><br><span class="line">            img_mask = np.zeros(img.shape, np.uint8)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(img.shape[<span class="number">0</span>]):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(img.shape[<span class="number">1</span>]):</span><br><span class="line">                    img_mask[i,j,:] = [<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>]</span><br><span class="line">            cv2.imwrite(img_file_png, img)  </span><br><span class="line">            cv2.imwrite(img_file_mask, img_mask)  </span><br><span class="line">            os.remove(os.path.join(img_dir,img_file))  <span class="comment"># 删除原文件</span></span><br><span class="line">        <span class="keyword">elif</span> extension == <span class="string">&#x27;.png&#x27;</span> :</span><br><span class="line">            img_file_mask = <span class="built_in">dir</span> + <span class="string">&#x27;/mask/&#x27;</span> + filename + <span class="string">&#x27;.png&#x27;</span></span><br><span class="line">            img = cv2.imread(os.path.join(img_dir,img_file))</span><br><span class="line">            img_mask = np.ones(img.shape, np.uint8)</span><br><span class="line">            img_mask = img_mask * <span class="number">255</span></span><br><span class="line">            cv2.imwrite(img_file_mask, img_mask)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;get mask done!&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="利用ffmpeg的图片转视频"><a href="#利用ffmpeg的图片转视频" class="headerlink" title="利用ffmpeg的图片转视频"></a>利用ffmpeg的图片转视频</h1><p>NeRO的Relight后生成图片，需要生成一个可循环的视频</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">just_video = <span class="literal">True</span></span><br><span class="line"><span class="comment"># 输入文件夹路径和输出文件夹路径</span></span><br><span class="line">input_folder = <span class="string">&quot;E:\\BaiduSyncdisk\\NeRF_Proj\\NeRO\\data\\relight\\bear-neon\\&quot;</span></span><br><span class="line">image_file_suffix = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> just_video:</span><br><span class="line">    <span class="comment"># 获取输入文件夹中的所有图片文件</span></span><br><span class="line">    image_files = [f <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(input_folder) <span class="keyword">if</span> f.lower().endswith((<span class="string">&#x27;.jpg&#x27;</span>, <span class="string">&#x27;.jpeg&#x27;</span>, <span class="string">&#x27;.png&#x27;</span>, <span class="string">&#x27;.gif&#x27;</span>))]</span><br><span class="line">    image_files.sort(key=<span class="keyword">lambda</span> x:<span class="built_in">int</span>(x.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]))</span><br><span class="line">    <span class="comment"># print(image_files)</span></span><br><span class="line">    image_num =  <span class="built_in">len</span>(image_files)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;==&gt;图片数量: &#x27;</span> + <span class="built_in">str</span>(image_num))</span><br><span class="line"></span><br><span class="line">    out_image_th = <span class="number">0</span></span><br><span class="line">    image_file_suffix = image_files[<span class="number">0</span>].split(<span class="string">&#x27;.&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 倒序复制图片</span></span><br><span class="line">    <span class="keyword">for</span> image_file <span class="keyword">in</span> image_files:</span><br><span class="line">        out_image_name = image_num * <span class="number">2</span> - <span class="number">1</span> - out_image_th</span><br><span class="line">        out_image_name = <span class="built_in">str</span>(out_image_name) + <span class="string">&#x27;.&#x27;</span> + image_file_suffix</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;==&gt;正在处理图片：&quot;</span>, image_file , <span class="string">&#x27;--&gt;&#x27;</span> , out_image_name)</span><br><span class="line">        out_image_th += <span class="number">1</span></span><br><span class="line">        input_path = os.path.join(input_folder, image_file)</span><br><span class="line">        output_path = os.path.join(input_folder, out_image_name)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打开图片并倒序保存</span></span><br><span class="line">        image = Image.<span class="built_in">open</span>(input_path)</span><br><span class="line">        image.save(output_path)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;==&gt;处理完成&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="comment"># ffmpeg -r 15 -i %3d.jpg video.avi -vf  &quot;scale=ih*16/9:ih:force_original_aspect_ratio=decrease,pad=ih*16/9:ih:(ow-iw)/2:(oh-ih)/2&quot;</span></span><br><span class="line">frame = <span class="number">120</span></span><br><span class="line">video_name = <span class="string">&#x27;nero_relight.mp4&#x27;</span></span><br><span class="line"><span class="keyword">if</span> image_file_suffix <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    image_file_suffix = <span class="string">&#x27;png&#x27;</span></span><br><span class="line">cmds=[</span><br><span class="line">    <span class="string">&#x27;ffmpeg&#x27;</span>, <span class="string">&#x27;-r&#x27;</span> , <span class="built_in">str</span>(frame) , <span class="string">&#x27;-i&#x27;</span> , input_folder + <span class="string">&#x27;%d.&#x27;</span>+image_file_suffix ,</span><br><span class="line">    <span class="string">&#x27;-vf&#x27;</span> , <span class="string">&quot;scale=ih*16/9:ih:force_original_aspect_ratio=decrease,pad=ih*16/9:ih:(ow-iw)/2:(oh-ih)/2&quot;</span> , video_name</span><br><span class="line">]</span><br><span class="line">subprocess.run(cmds)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python Practise </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF代码理解</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF-code/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/NeRF-code/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/yenchenlin/nerf-pytorch">NeRF代码</a>(基于Pytorch)。流程图：<a href="#train流程">train流程图</a> and <a href="#渲染流程图">渲染流程图</a> (基于Drawio)</p><span id="more"></span><h1 id="train流程"><a href="#train流程" class="headerlink" title="train流程"></a>train流程</h1><iframe frameborder="0" style="width:100%;height:1724px;" src="https://viewer.diagrams.net/?highlight=0000ff&edit=_blank&layers=1&nav=1&title=train%E6%B5%81%E7%A8%8B.drawio#R7V1bk6O4Ff41rkoe4gKJ6%2BP0TM%2Fmsj1JZVJJ9sklG9kmDYgVcru9vz6SARss2aOYxoIUVVM9Rki2ONJ37jrM4Of0%2FSeK8u0LiXAyA1b0PoNfZgBA13f5f6LlULbYrmeVLRsaR1XbueF7%2FBuuGutuuzjCRasjIyRhcd5uXJEswyvWakOUkn2725ok7V%2FN0QZLDd9XKJFb%2FxVHbFu2BsA%2Ft%2F8Rx5tt%2Fcu2F5Z3UlR3rp6k2KKI7BtN8HkGP1NCWPkpff%2BME0G9mi7luK9X7p4mRnHGdAZ8%2B4ePf8qfQ7b6y28L9mf89PLPT3%2BoJluwQ%2F3AOOLPX10SyrZkQzKUPJ9bnyjZZREW32rxq3OfnwnJeaPNG%2F%2BDGTtUi4l2jPCmLUuT6i5%2Bj9m%2FxfC5W1390rjz5b365uPFob7IGD00BonLX5r3zsOOV%2FW4NclYNRHb49fl84qHvErGqqkgO7rCN2hXb0dEN5jd6AdOi81hgkmK%2Bfz4OIoTxOK39jxQtV03p37nFeUfqkX9Hxa4%2Bt43lOyqX2IUxdnvfi8t%2FHlZxUrstzHD33N0pMCeg7u9hGqyvmHK8PttwsqEqAc4YF6zi4pbBBV09mfk2TVj2DZQ51p9Uc%2BSyDThQxcfQBMf0CQ%2BgIQPLknW8WbBn7jA9CZOLDM4gf4lTmxPARSgAIrXG1CCCSh3AwVqAsW%2BsjEegxQoISUhKFrM4Nfy3yJCDI0DLyeBYQwvRuFiN8Byho4aLj1ue0dz2%2Fsmd72j3vXLBPO1oEPd89Cy5xpb3lNsedjXlvckSkpk4yZSLj6udjQ5PFG0ehUb40f0ayuuPVAT%2BJJmerLqGuRUMZATV%2FlwcvoSOXNS4EWRbzGNuf06wF3pBMPblTYcCyc%2BKy4tteWsxRiwgHVNYNszycNPzzMpp32ucWhyjW3Z0bGiGDG8yDBdD5AZKtRS6JtWS213Qsr9SNF1eNiOUaTILo9v%2BO9fBwgRaEkQcaCrB5HeFAYwuQQ7QETX5jMsTGSrL8NsT%2Bjr4tcdpofFOuN3jzcsussW1c0BQgiEkuniqLzq7kO96v4Eofsh5OlCCBiFkGzu8%2BkucLrEUTRIv7pC2sDAuLQZoXk6HKiEmlABRqECZUfYxA4%2FfI2h0eBJPc0GO4zX4ofoppjT0pdMsoQT7NMMeCgV7C5bFvmJWE1GuSXpclcYYZKh02KQriMzSMD5qCuzyKA3FjlpE%2FfDB%2BjarMA1CZ96mg34VKDJEdsOUZkAkt7tAePKhGzWHHAh0Y4%2FNWsTqGCUvOLPJCGUt2QkExBax0ly0YSSeJPxyxWnEubtT4KGIhjxqbqRxlF0xJ9qRdpr1seiQGlR3FA2hhzFmvQWxgGjSZzokwu5ulzIbBzBvcKFBAP6kdAeIENSOQIeypCgUV%2FayINvQNcRAIz60oDsCGAkWI5DZvvQNESAbDhc8po0TveUE0vsBSK6xhEmg6OuIydR%2BLoakdMbAzIT7%2FogO%2BAeKf6BDCjQzVuEXeNd1dC%2FkZjP8ezbtqzLHRXaFzulfIhq4MVmOc2kQxqfMxbtbYC2J9TNLAiMZojX07zwZFN0KBZZPgLVT5FpoM15e8s0gHI6cSYLrf9bU9R2ZXVcoWs81BSFo0lKHqA2DnUj29BoOnN9OnESWH2usRcYFViym%2B%2Fsphi%2BbAp1DzP1lp%2FgTPmiHUDia4LEsdQ7o6NVAAOvtZ1s61JgljPrzSaAcmZ%2BrS8OEIAQtMkV6uag9ubxcCab6n701Qrjj9Fn1Kaqp9nU%2FqPVCSOlHytJ1uvB48W2jLsIXaPiauR6u6PrxHKMBsA9b2KK%2Fa%2BxZ3SNHdklskRstY3Xh7FoD7ZlPKboTkjpgBTdQLxj9lD2tUD8UHHiSEaJeZxM3r4OONE2co3G3r3JlHrAGntdHRndeKHsbhj2kSSJF9owNM0Lp2OvHXCim4HvGj32Wk9ToV0PECTepWJtHiQemEByN0hc7dDRh3jFP1GuCDc65MLZXTS%2B%2BcJp7l3yZOBfFBO9GOA6twfwD%2BUcPtS17sqRrcZJ3PqIbqMpLQvSDh3cwNK0BnpLhPOMnjMcObg9Xae7Z%2FQMmic73SnaA7Jj%2BY6NwmoGitMaDxaC02GzDjjRTfjr7Ie9SwhKMg0Gt4WgFGq%2BGNCPEKyp2IBxgdI8wYs8GmKhIwnF0DGN4jrGPaH4HhTr%2BojNZkF5so%2BYEbrazleIDQ4jUhzF0dUI%2B8PIdDCx3sI6NWqN%2BjbqaTb2OkqSBcVCWIlcCi8RadRLyj9txCd%2BZ5HEhbg9A0%2Fir3tsi%2BIVm7lfBocPG7hz0E4TtB1VppKjqDzQW56gL0viY3FgkbgyosrAtoKOD63B6gM1HX%2F%2B07fnl79%2BGREpVfbJY0l5pUp79DYeKkJVRtVDqRgY1Q9HHhv1teu6G80hCCZ39gPWODDq8fJV1fuLYla7h%2BN0A9ICD5ArqhQeV2U3P1bhkW2qYxWrYyBNDBaed7bFFEsEZTRG2ebqgbt%2BqelC6c1SKqWnDrE9pGyVbRlNzriztB8YDAfSzc4Iu0qZ%2B3yJtXFyAm%2Flj7gaULsc4F%2B%2Bzu8yZBfeHtCP89GXs004AwV5kQ3xdBkE8quyzHPRYDSelQFqHoF20QCz2qXsGqi9MFH8VrthhDIyX6LV6x7R6FhJ4NiD%2F2Kjk2IcyVmccvrSecFwfn3gwOCoUmo8lSB%2BLBxlk7l5Xm98xR0C3SN8vWUT2JbZhLr7dJvBsDjdAIvZuiiBbAy8oKMFcFyw60CxjQAFeHL9KVV574e%2BXzac0m464ETXBAiMJuEHssqsEOkZ3i8Sik5V3DQ1gV0elWNOKsGYdQHVoZiH6gLhCG3yFhpNOn4D3UTw0KhTsJ7mbTwW6E3AarXFq9e8tLw1Ickfnw9cx3xvjBeKvurFjI%2BFotFznCOHYqhrJYdGTzyHWlZyLoSaFS%2BquqaWsHmLSkxajeKn1f1RYA6EsmdqAJibzoR2wJyu2dbZI91tjWWz7QbmGC5YcUwjaqGuPG0trvAxiShO0Ua8SWAU2FNUIlZjT5Vz0N9L0SyjSQdtW9DSBF%2FbFrQNgk87HGTUFgy1bMEafDktIz4t6LFfo3ReibxRwM1zwkHCzWjVpI%2BAm0HXS6hr6nV2Ud4VfT2V7T8dOfBuB1P9Oghw74DAud3frlUrdf9%2BgrWhlqW7ScgSJQvBZGZlOrDoKeAxWvbi9che%2BCUlhDVXij%2Fm9uV40hQ%2B%2Fxc%3D"></iframe><h2 id="if-use-batching"><a href="#if-use-batching" class="headerlink" title="if use_batching"></a>if use_batching</h2><iframe frameborder="0" style="width:100%;height:1113px;" src="https://viewer.diagrams.net/?highlight=0000ff&edit=_blank&layers=1&nav=1&title=train_batch.drawio#R7Vxbb6M4GP01SO1DK%2B6Qx5CkU612q5VmdjvzFJHgADsEZ4C0yf76tcHmZichbQmwjTSagm3AnO%2Fq7zgIymS9%2BxLZG%2B8P6IBAkEVnJyhTQZYlTZPRH9yyz1pMSc8a3Mh3yKCi4av%2FLyCNImnd%2Bg6IKwMTCIPE31QblzAMwTKptNlRBF%2Brw1YwqD51Y7uAafi6tAO29dl3Eo%2B8hWwU7Y%2FAdz36ZEkfZT1rmw4mbxJ7tgNfS03KTFAmEYRJdrTeTUCAwaO4ZNc9HOjNJxaBMGlygbN2539%2Fc9X5t%2BeV%2BO3fx1dt8nynZXd5sYMteWEy2WRPEXAjuN0IirWCYULkI5nonH0%2BmdILiBKw40nHXtCbFgAgzQFwDZJoj8aRqxSRYEaUxiDTfC0kIBlkiFdGnzbaROpufusCGHRAsDkDJ%2Bk0Tq%2Ben4CvG3uJz1%2BRNSCMvGQdYLzQoR1vMv1c%2BTvgNMLzqMTqILNg7imYLHgqDzy1LfDkAYJHe6uKKHeOpTJcLOUqllLnWKoMlo%2B7550yGEC7t2y9l%2BFj1LfoYfTGaPUDGPc4epgDBK%2Bv0WM0XCx7Fz3oDGrhQx0Mot2btsRLq%2FUAPdeKN3ZYgVH%2FtcUrJWsJAxgJyhh1Ru7iBk0N%2FUOPF0tHt%2FgQAydiLO9W9toP9tk16Eb2epN2KoqKJQGCF5D4S5vpqd4kTiWCbyGZm12tL5sl7gxhtLaDavcrARP3q9k8084AJAmI7tCrLv3Q5V6PRJnc2YHvhln3EukAiKrdfuikmoH7xdLU0s4kssN4hW5Kbx%2BCfMArjJzq08uXL%2BzlTxy8Q%2BeuhrmsmjnWsjoqjrUS8o4fbwKboO6HgV968CqAdlKeEBUuOnLx3yck8hSpTBmQcmX6kPUyFobfFLWfMLS3ZyCHHbyiVbwSz6RkjTUppTWLYrNX4LjgKzmFUeJBF4Z2MCtarVTI2PVMsXyKMb9DuCHY%2FYNUdU%2FAs7cJrCILdn7yvXT8A9%2FqXiNn0x25c3qypychet%2Fv5ZPSVfi0uCw9o9c1lmIMt9ESNHDgiR254JhnJS4dA3lUKSIQ2In%2FUq0cfbyI2WXL1WleneZwnKai9s1pml06TeoZz3OahZ%2F8UXGTrTtNvaHTlLReeU1N7FLGbwuM75NxjESUjDEFglpSj0HaHnwM3VS8jB6oZq%2F0QGJLVtsYzBd2svSwR68rSeTB9WIbd%2BIoZbNWPzBYTynzVmxma57S6NRT5pbzo9TTV0%2BpNnWUer8MhK0R7UF8KHsoSSlOIvgTTLKshzqcFfI1tSaSmE1JVqZY2CBQKhmMScfad5xUcXgWV1WmNoxOqRndiDU6ns3JbdmcfM33r%2Fn%2BkPN9tWpQitZ1vi833vXQ%2BOXP5qhqoV3ROaFdNO45sOSl74%2FHpT%2Fksvwh2xxMjp%2BWWkuOKFrDgq85VXVhNPuz5eZ8NBuQVZdFU2JdXmTv4zmKVDR%2BLCIaOm4iKMhW5OD%2F3MXtrtNdEW8AP3eb3eFNeYHrQun0Qol67pMrJaVfhXg6717b1PmUVpF1EO%2Bli527L%2BVKFF%2FXQLw1kD9H8%2FbD9IIphnwA66E6aazzkq2LLogUNrXqabSqi%2BOt0evt0YpmUqejlfjR0Ypc%2Bif00aRzddJFvjbRO2QTJRfVFCWfxTs20R7fcUCKfoNObSpMCkQxSKgyKXgWjh176ftJ78uFmlaNVblfuRBbpLxBvhinObe709kQO2IguZHZ%2FUJZabAJuwfYyWoT7HiV9fZ%2BS9FgXzYIHWr5iwAuf9a8UuiU2NT8F17cSPUGur%2BJ43mHr2lKPJXko3HkQ9veGceUWtHE1Gtyz16ICWTsjWoFZ7OuQC1HRF0fSjY1jK0Gitk04xr1Kyayv1yw8DYDdPg0R6stJ1vYWLsbNMqSbjn1gP8vyzqS69GAQwvlK56LEK1UfUrySreFzHFVJ19hyjsLC6qer2TKOS%2FGWZ9bnCNOzZsrzvZ%2B3Sez0X2mC%2BMHwXwQZqpgmcJ4DDYQGeRME0aiYM5ib7taIRSzXpSxkQPriZYZagItgJW6KSXI9aRKEjlMIvdnHa0lpBpbrAvh57EFrW8bSLRPt2mr3XxEbZqPaP3iK1Q2HyGvwESz3rITGrMCV7TOyT76jZP3sROEmiC8BJ%2BUKDiIg3zDMbLhMNNwimY4xjEcJhiOswtHqIUmvEJOKuSMwkE6gcslpCRCjU4ombTor20XxIJmhZt7nK3D9f3Sg%2F4S3BDS4VbQpgfJhsKirsK%2FmPDr4jyUdFw%2BUdNrxQ2FU%2FvK94BdJD3W2Iptyl6jTG2SHjiDDgs9IK21BtsMB4OdfNGMdRglbc0Qe4ZbJ9%2FKePcKyeDs3bwobtTx9hw3naFQDF6V5aLIdbJL83zkdAY57obrSyLH%2B1lHfZneKvl0dCneLvVE1aYn1JOhKffGAcs6l30y6s7twvsxdE7WMRsJo3Fa9tSF0USwxrgF5T3mCJc%2BUftIJ4yELOalUSxaZJEBmwbSPFHMkvL8UrwljFMap9nkgeGfpS5o1JYAOqcuyK2Qa625oE6%2BS3a%2B8659zy3ncztz3WyaxSOKiIqfMo8Kc3Q1EOHo%2FpDWDGQ33f71FHm%2F1o%2BKJv4mBbH3hf%2FV10x8%2BPUFXmmnXpXR8UbZel0iBNFqHu%2FDxMP1nKIQg5yxNRHG6cHIEqyZMDOwkzYN6pXRkyZSaWRGS%2BXDi2pUNsHjZZCOVeiAfnC06KDKqLUVGOdTj4qscH9Fd77OoNPia81ZuC6%2Bea3M%2FgM%3D"></iframe><h1 id="config-parser"><a href="#config-parser" class="headerlink" title="config_parser()"></a>config_parser()</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def config_parser():</span><br><span class="line">    import configargparse</span><br><span class="line">    parser = configargparse.ArgumentParser()</span><br><span class="line">    # 此处命令行更改的参数为 args.config</span><br><span class="line">    parser.add_argument(&#x27;--config&#x27;, is_config_file=True,</span><br><span class="line">                        help=&#x27;config file path&#x27;)</span><br><span class="line">    parser.add_argument(&quot;--expname&quot;, type=str,</span><br><span class="line">                        help=&#x27;experiment name&#x27;)</span><br><span class="line">    parser.add_argument(&quot;--basedir&quot;, type=str, default=&#x27;./logs/&#x27;,</span><br><span class="line">                        help=&#x27;where to store ckpts and logs&#x27;)</span><br><span class="line">    parser.add_argument(&quot;--datadir&quot;, type=str, default=&#x27;./data/llff/fern&#x27;,</span><br><span class="line">                        help=&#x27;input data directory&#x27;)</span><br><span class="line">    # training options</span><br><span class="line">    parser.add_argument(&quot;--netdepth&quot;, type=int, default=8,</span><br><span class="line">                        help=&#x27;layers in network&#x27;)</span><br><span class="line">    parser.add_argument(&quot;--netwidth&quot;, type=int, default=256,</span><br><span class="line">                        help=&#x27;channels per layer&#x27;)</span><br><span class="line">    parser.add_argument(&quot;--lrate&quot;, type=float, default=5e-4,</span><br><span class="line">                        help=&#x27;learning rate&#x27;)</span><br><span class="line">    parser.add_argument(&quot;--no_batching&quot;, action=&#x27;store_true&#x27;,</span><br><span class="line">                        help=&#x27;only take random rays from 1 image at a time&#x27;)</span><br><span class="line">    parser.add_argument(&quot;--no_reload&quot;, action=&#x27;store_true&#x27;,</span><br><span class="line">                        help=&#x27;do not reload weights from saved ckpt&#x27;)</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">    return parser</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">use：    </span><br><span class="line">parser = config_parser()</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">basedir = args.basedir</span><br></pre></td></tr></table></figure><h1 id="load-data"><a href="#load-data" class="headerlink" title="load_???_data()"></a>load_???_data()</h1><h2 id="load-llff-data"><a href="#load-llff-data" class="headerlink" title="load_llff_data()"></a>load_llff_data()</h2><p>输入：</p><ul><li>args.datadir ：’./data/llff/fern’</li><li>args.factor,</li><li>recenter=True, </li><li>bd_factor=.75,</li><li>spherify=args.spherify</li><li>path_zflat=False</li></ul><p>输出：</p><h2 id="load-blender-data-basedir-half-res-False-testskip-1"><a href="#load-blender-data-basedir-half-res-False-testskip-1" class="headerlink" title="load_blender_data(basedir, half_res=False, testskip=1)"></a>load_blender_data(basedir, half_res=False, testskip=1)</h2><p>输入：</p><ul><li>basedir，数据集路径’E:\\3\\Work\dataset\\nerf_synthetic\\chair’<blockquote><p>[!info]- chair 文件夹</p><ul><li>chair：<ul><li>test<ul><li>200张png 800x800x4</li></ul></li><li>train<ul><li>100张png</li></ul></li><li>val<ul><li>100张png</li></ul></li><li>.DS_Store</li><li>transforms_test.json</li><li>transforms_train.json</li><li>transforms_val.json</li></ul></li></ul></blockquote></li><li>half_res，是否将图像缩小一倍 （下采样）</li><li>testskip，测试集跳着读取图像<br>输出： </li><li>imgs：train、val、test，三个集的图像数据   imgs.shape : (400, 800, 800, 4)</li><li>poses：相机外参矩阵，相机位姿 poses.shape : (400, 4, 4)  400张图片的4x4相机外参矩阵</li><li>render_poses：渲染位姿，生成视频的相机位姿（torch.Size([40, 4, 4])：40帧）</li><li>[H, W, focal] 图片数据，高、宽、焦距</li><li>i_split，三个array数组<ol><li>0,1,2,…,99 （100张train图像）</li><li>100,101,…,199 （100张val图像）</li><li>200,201,…399 （200张test图像）</li></ol></li></ul><h1 id="create-nerf-args"><a href="#create-nerf-args" class="headerlink" title="create_nerf(args)"></a>create_nerf(args)</h1><p>输入：</p><ul><li>args，由命令行和默认设置的arguments共同组成的字典<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">parser = config_parser()</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure></li></ul><p>输出：</p><ul><li>render_kwargs_train</li><li>render_kwargs_test<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">render_kwargs_train = &#123;</span><br><span class="line">    &#x27;network_query_fn&#x27; : network_query_fn,</span><br><span class="line">    &#x27;perturb&#x27; : args.perturb, # 默认为1 --perturb：在训练时对输入进行扰动</span><br><span class="line">    &#x27;N_importance&#x27; : args.N_importance, # --N_importance：每条射线的附加精细采样数</span><br><span class="line">    &#x27;network_fine&#x27; : model_fine,</span><br><span class="line">    &#x27;N_samples&#x27; : args.N_samples, # --N_samples：每条射线的粗略采样数</span><br><span class="line">    &#x27;network_fn&#x27; : model,</span><br><span class="line">    &#x27;use_viewdirs&#x27; : args.use_viewdirs, # --use_viewdirs：使用全5D的输入代替3D的输入</span><br><span class="line">    &#x27;white_bkgd&#x27; : args.white_bkgd,</span><br><span class="line">    &#x27;raw_noise_std&#x27; : args.raw_noise_std, # 默认0 --raw_noise_std：添加到输入的噪声标准差</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># NDC only good for LLFF-style forward facing data</span><br><span class="line"># NDC 全称是 Normalized Device Coordinates，即归一化设备坐标</span><br><span class="line">if args.dataset_type != &#x27;llff&#x27; or args.no_ndc:</span><br><span class="line">    print(&#x27;Not ndc!&#x27;)</span><br><span class="line">    render_kwargs_train[&#x27;ndc&#x27;] = False</span><br><span class="line">    render_kwargs_train[&#x27;lindisp&#x27;] = args.lindisp</span><br><span class="line">    </span><br><span class="line">render_kwargs_test = &#123;k : render_kwargs_train[k] for k in render_kwargs_train&#125;</span><br><span class="line">render_kwargs_test[&#x27;perturb&#x27;] = False</span><br><span class="line">render_kwargs_test[&#x27;raw_noise_std&#x27;] = 0.</span><br></pre></td></tr></table></figure></li><li>start ：global step</li><li>grad_vars：model的参数列表，包括权重和偏置<ul><li><code>grad_vars = list(model.parameters())</code></li><li>if args.N_importance &gt; 0: <code>grad_vars += list(model_fine.parameters())</code></li></ul></li><li>optimizer<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(params=grad_vars, lr=args.lrate, betas=(0.9, 0.999))</span><br><span class="line"></span><br><span class="line">if 加载了ckpt ：  optimizer.load_state_dict(ckpt[&#x27;optimizer_state_dict&#x27;])</span><br></pre></td></tr></table></figure></li></ul><h2 id="get-embedder-args-multires-args-i-embed"><a href="#get-embedder-args-multires-args-i-embed" class="headerlink" title="get_embedder(args.multires, args.i_embed)"></a>get_embedder(args.multires, args.i_embed)</h2><p>输入：</p><ul><li>args.multires, 输入的L</li><li>args.i_embed，默认为0，使用位置编码，-1为无位置编码</li></ul><p>输出：</p><ul><li>embed, 位置编码函数，<code>将(1024 * 32 * 64) * 3处理为 (1024 * 32 * 64) * 63</code><ul><li>input_ch = 3 , L = 10 并且包括输入维度</li></ul></li><li>embedder_obj.out_dim : 输出的维度</li></ul><h3 id="Embedder"><a href="#Embedder" class="headerlink" title="Embedder()"></a>Embedder()</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class Embedder:</span><br><span class="line">    def __init__(self, **kwargs):</span><br><span class="line">        self.kwargs = kwargs</span><br><span class="line">        self.create_embedding_fn()</span><br><span class="line">    def create_embedding_fn(self):  </span><br><span class="line">    </span><br></pre></td></tr></table></figure><h2 id="NeRF-nn-Module"><a href="#NeRF-nn-Module" class="headerlink" title="NeRF(nn.Module)"></a>NeRF(nn.Module)</h2><p>继承nn.Module构建的类</p><p>def <strong>init</strong>(self, D=8, W=256, input_ch=3, input_ch_views=3, output_ch=4, skips=[4], use_viewdirs=False)</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/357021687">Pytorch 中的 forward理解 - 知乎 (zhihu.com)</a></p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = NeRF(...) 实例化</span><br><span class="line">model(input) 相当于 model.forward(input)</span><br></pre></td></tr></table></figure><ul><li>D=args.netdepth, W=args.netwidth</li><li>input_ch=input_ch, output_ch=output_ch</li><li>skips=skips,</li><li>input_ch_views=input_ch_views</li><li>use_viewdirs=args.use_viewdirs</li></ul><h2 id="network-query-fn"><a href="#network-query-fn" class="headerlink" title="network_query_fn"></a>network_query_fn</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,</span><br><span class="line">    embed_fn=embed_fn,</span><br><span class="line">    embeddirs_fn=embeddirs_fn,</span><br><span class="line">    netchunk=args.netchunk)</span><br></pre></td></tr></table></figure><h1 id="if-render-only"><a href="#if-render-only" class="headerlink" title="if render_only:"></a>if render_only:</h1><h2 id="render-path"><a href="#render-path" class="headerlink" title="render_path()"></a>render_path()</h2><p>输入：</p><ul><li>render_poses, 测试集的渲染相机位姿  200 <em> 4 </em> 4 </li><li>hwf, </li><li>K, 相机内参矩阵</li><li>chunk, <code>args.chunk=1024*32</code></li><li>render_kwargs = render_kwargs_test</li><li>gt_imgs=None, </li><li>savedir=None, </li><li>render_factor=0</li></ul><p>输出：</p><ul><li>rgbs, 200 x W x H x 3</li><li>disps，200xWxH</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">render_path(render_poses, hwf, K, args.chunk, render_kwargs_test, gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor)</span><br></pre></td></tr></table></figure><h3 id="render"><a href="#render" class="headerlink" title="render()"></a>render()</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rgb, disp, acc, _ = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], **render_kwargs)</span><br></pre></td></tr></table></figure><h1 id="get-rays-np-H-W-K-c2w-numpy版本-narray"><a href="#get-rays-np-H-W-K-c2w-numpy版本-narray" class="headerlink" title="get_rays_np(H, W, K, c2w) numpy版本(narray)"></a>get_rays_np(H, W, K, c2w) numpy版本(narray)</h1><p>通过输入的图片大小和相机参数，得到从相机原点到图片每个像素的光线（方向向量d和相机原点o）</p><p>输入：</p><ul><li>H：图片的高</li><li>W：图片的宽</li><li>K：相机内参矩阵<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">K = np.array([</span><br><span class="line">    [focal, 0, 0.5*W],</span><br><span class="line">    [0, focal, 0.5*H],</span><br><span class="line">    [0, 0, 1]</span><br><span class="line">])</span><br></pre></td></tr></table></figure></li><li>c2w：相机外参矩阵<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c2w = np.array([</span><br><span class="line">            [ -0.9980267286300659,  0.04609514772891998,  -0.042636688798666, -0.17187398672103882],</span><br><span class="line">            [ -0.06279052048921585,  -0.7326614260673523, 0.6776907444000244, 2.731858730316162],</span><br><span class="line">            [-3.7252898543727042e-09, 0.6790306568145752,0.7341099381446838, 2.959291696548462],</span><br><span class="line">            [ 0.0,0.0,0.0,1.0 ]])</span><br></pre></td></tr></table></figure></li></ul><p>输出：从相机原点到800x800图片中每个像素生成的光线 $r(t)=\textbf{o}+t\textbf{d}$</p><ul><li>rays_o：光线原点（世界坐标系下）(800, 800, 3)</li><li>rays_d：光线的方向向量（世界坐标系下）(800, 800, 3)</li></ul><p>需要对render输入的光线做batch: <code>rays = batch_rays</code></p><h1 id="render-1"><a href="#render-1" class="headerlink" title="render()"></a>render()</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(H, W, K, chunk=1024*32, rays=None, c2w=None, ndc=True,near=0., far=1.,use_viewdirs=False, c2w_staticcam=None, **kwargs)</span><br></pre></td></tr></table></figure><p>输入</p><ul><li>H</li><li>W</li><li>K</li><li>chunk=args.chunk 同时处理光线的最大数量</li><li>rays=batch_rays  <code>[2, batch_size, 3]</code> </li><li>verbose=i &lt; 10   </li><li>retraw=True</li><li>render_kwargs_train</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">render_kwargs_train = &#123;</span><br><span class="line">    &#x27;network_query_fn&#x27; : network_query_fn,</span><br><span class="line">    &#x27;perturb&#x27; : args.perturb, # 默认为1 --perturb：在训练时对输入进行扰动，是否分层采样</span><br><span class="line">    &#x27;N_importance&#x27; : args.N_importance, # --N_importance：每条射线的附加精细采样数 = 128</span><br><span class="line">    &#x27;network_fine&#x27; : model_fine,</span><br><span class="line">    &#x27;N_samples&#x27; : args.N_samples, # --N_samples：每条射线的粗略采样数 64</span><br><span class="line">    &#x27;network_fn&#x27; : model,</span><br><span class="line">    &#x27;use_viewdirs&#x27; : args.use_viewdirs, # --use_viewdirs：使用全5D的输入代替3D的输入</span><br><span class="line">    &#x27;white_bkgd&#x27; : args.white_bkgd,</span><br><span class="line">    &#x27;raw_noise_std&#x27; : args.raw_noise_std, # 默认0 --raw_noise_std：添加到输入的噪声标准差</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出：<code>ret_list + [ret_dict]</code></p><ul><li>rgb: <code>all_ret[&#39;rgb_map&#39;]</code> </li><li>disp: <code>all_ret[&#39;disp_map&#39;]</code></li><li>acc: <code>all_ret[&#39;acc_map&#39;]</code></li><li>extras: <code>[&#123;&#39;raw&#39;: raw , &#39;...&#39;: ...&#125;]</code><br>eg:  <code>all_ret[&#39;raw&#39;] : W * H * N_samples +N_importance * 4</code><br>eg: <code>all_ret[&#39;rgb_map&#39;] : W * H * 3</code></li></ul><h2 id="渲染流程图"><a href="#渲染流程图" class="headerlink" title="渲染流程图"></a>渲染流程图</h2><iframe frameborder="0" style="width:100%;height:1583px;" src="https://viewer.diagrams.net/?highlight=0000ff&edit=_blank&layers=1&nav=1&title=render%E6%B5%81%E7%A8%8B.drawio#R7R1bc6O2%2Btcwm3QmHu7gR1%2BS3Zme7mm7Z7qnTx5sY5uzGFzAcbK%2F%2FkhCAiR9tnECNkncmW6MJJD47jcJxRitnz4n3mb1Wzz3Q0VX50%2BKMVZ0XTMNFf3BLc95i62becMyCeZ0UNnwLfjp00Z633IbzP2UG5jFcZgFG75xFkeRP8u4Ni9J4h0%2FbBGH%2FKwbb%2BlLDd9mXii3fg%2Fm2SpvdXWnbP%2FiB8sVm1mz%2B3nP2mOD6ZukK28e7ypNxr1ijJI4zvJf66eRH2LgMbjk9z3s6S0WlvhRVucG77P%2FFD5Gz9%2F%2BUhePm1%2FT2V9L444uNs2e2Qv7c%2FT%2B9DJOslW8jCMvvC9bh0m8jeY%2BfqqKrsox%2F4rjDWrUUOP%2F%2FCx7psj0tlmMmlbZOqS9%2FlOQ%2FRff3rPo1d%2BVnvETfTK5eGYXUZY8V27Cl39X%2B8rbyBW7bxFHGV2IZqBrGWoUkGm8TWb%2BAVAx6vOSpZ8dGKfn4zAcKxNQnHz247WP1ocGJH7oZcEjT2ceJddlMa7EKPpBkXoCgulzH71wS2dae0GEWkKMLXQz%2Fp0lpE2ghBLPGDW7VZD53zYegdEOMTuP09pwfvSTzH86CBnaa1mUc54Zb9HrXcmIRduqwoSm2hIwzUtyi1bhlZJzjnELxysl67TOLXpNbjE6xS26xC3ozed%2B0n3ecC%2FNG%2FZVk9TlDaMmb1id4g1D4g20%2BkniPaed4w5HFbnDkrjDAZjDaIs5GCdeiDtepDkuxR1WTe5wO8UdlsQd0Xz2Nrij0AkX4w7jqjvqcodbkzs07VLssXncjf%2F8t7P68vB898fU%2FPEzjfw747KO5tkRPPfSVcHijWIbBK9pNo1tcusgQQKsMmATB1GWVp78O24oJYvLCxZDjEgcHq6Z%2FcPjbdU6eAP6ka%2B4pNPi1V8unVxJsk%2B9bLYKFs%2FdFO%2Bia6Bf3DXQLuo3vy35rtUNM2kXizOBEkh337eA5wR6u%2BiG4eu0IuElGWsYYtBNE8RCvlR6W0k1p%2BoKXZjIOCL8xfHFwmprC%2BGGdrQFQ6gUQnobysKwLq4snKuyqC096kZZtW6FkswLO3x6%2B0huS1%2FUxXhLHsFRKevq7agLy3G4eUz3sPQ3hCDHqeMdxzpJuwjjW1IuuqRcIj%2FbxcmPyT9bP3meLCJFt0NEF8NpwvGX%2Fc8W55kJJd6lhBQHaIBmbJ4IPbJ%2B9GtJ%2FuKb1GQbTegEnddcpn5xzXVZE%2Fhtaa66UV7NbkWOnSp%2BHINnd8s8zfgUxrckHuQgtL%2Be%2BnMiFjrGvnZfAJAqs68LcG9rQWj9GoSuz712Te7VLxakOLhuIJaHWpV7S3FdxbWVe1tx%2B8qwr9ybyhC1qLhlMFIG2iK6ma220Y9f0HjbuMW3DExl4OAfQ1VxHzrHaEzyFIxmA3rSADjNao3TNAlIXS%2Bj0Tg%2BO1pG05Lxzyyct%2BbusXULpQNEN3WxuMZ2nR6vnUxNLiBgCqxp7QTnz67lNfXZ5HI%2B8uvYRPbusOGmkp8qc%2FQqTWtabZ1fffX%2FfLjp9Xq3neMny%2BS5yTZkbtL0s6qgy7LTG1ZBdf22jll%2BuuwaYX7pHKuIbpFtyqxyXrfo3Qc12uIUpy6ndMxYcyROmSW%2Bl%2FmTyE8WnWcYC6h2OCvDGLKxe%2FMdXWOP8cst%2FYGYSo1xhFbuwgnERkK4AUKWuk39yWPg7%2BZBIicfEYgzHkFplsQ%2F%2FFEcosUZ4yiOMDcvgjAUmrwwWEbocoYQhwx4Y4gRFsy8cEA71sF8TkQBRBG8eGiBKAyhskiHspo2QBVi8qI5qpDTwnkgQaYIrlmTfeUPgzUwFw0VpraHNbnmnQSJLGVo09jQYIwxRy%2BGmCFxnn%2BC9NnYwB36iDXN86YRseW9hOCODViw6w%2BDa0fg0D6A6%2F5ZcW1KuA6izRYnBdRc0vKc%2BXWSeutN6KN%2BG%2F1AYImmKf6DOw2xkWD6ZLEuyfRSlNM1kSXRFckB7PdLPjWSfzqU%2FGuPfGQHAyQfkXpuS%2BS9njyIypdo5OjECCbDj0U%2Fpi3QDyB%2BikTxeehHTk3Q%2BOi8JiLtRkjoRqShbOUjfA1um3g4%2F0aHX4dh5COQo1h8YgGWjw7laNojR7nqfQ85vor8IL%2BlSQqEng8R4YckOiExaOtAVPa8JlhfIro7dHmE8MzOiz35JaQ3eBNEh6%2FZdAoWWv2%2BqjZEjEJ1tw1tWmmLGOEtVRJSup6kLi7OfdYDy6Md3cRwscMe4NVcK82bR3Hz%2B1ReV2kue%2FWJt9PjbZb7Zh2LZotFra4D6OTzHnkj%2ByVmruuQQhwt0f9THMIaPSjuB%2FIfLcFgdw1ZXZ3VdDJlex3RuQKFiQ4Grhoxpn5O0DLSwzO%2FehIaU70GwgSBAXiO6jkFhiVnOrD9myynkzWCCQl3K7aqWEOKshHBmDXGjbguw9BUajPPg3RDbsIYluOplYdAd3uzWXHz0cE7AppUHkyWVxIuuEx%2Fk60OTgVT44nqpU3rW4jG96G9FJA8c9uyvuXEy9dJsN4ge8uLELwkvb2K19NtehGd7aj8ziTXAnQ2yySdBXjX81RAwxaEFVDPBMNUv5Rde2jZFf7IJdRkM%2B9ejYajuUI9bf%2BcZ1KAAJTN2r%2F9Q%2F6AykMJYAWQZURo%2BtF8gE%2BlRdfTMJ79yJseArx4SufsEFy3sFLYwbPEbmGnyOLBoTf1w6E3%2B7EkCxVMl%2BM8f06WfTmLAsflHeKJoxxKqVLtaRorr6BkeccOJ37pplg2JF4sUp%2Fb7yrvKBN1rkjqOVykjbPHH%2BSqPVXVLNNy%2B4aj9V3%2BsXv2475gDxqIBNkPyb2BCbKTQWO9mbQVs9%2BsYa%2FXQ0YbouHBnYbtLzonbz4KqQu9iSUcNFDerQPiuJJsB6q7dR2wf2yrJdkOpxF4T%2FTN4apF87%2FYs89OSlGBzZit1cvB5pZ1NWHr6kfoTAh44MX2uxxc9%2FU0A8hE1lTgWM7z2sja2wqeOxrviCNgXRyCspuxXw3RMqxKlPSjRTNdTVREQFz9zIpI3g5x2JjYg8Vrjh%2BZGbKMg8LVJmArtoffN%2FMFlAbPn6%2B7j5wN7IjBYF6NwtpGIbT97ExHB7%2FuHFFZ3mZxMlv1Zl7GDtkgB3QMHvBpG3QTxiCXx%2FjcjqEyHJEf98rAUu5dZYhucnBLf4xbuma1SF%2FyADcrndVqAbawJTiapeZ2tOIMP9F03qcipMPl9z6xzF3ZX%2BTy8gE0OVf2s2ydM27K5qePmQePr3oOKQxEr4%2FT6SSQFOa9D%2BTBzc4F7xoRmzAuLIQCb%2FeJBbiIe4Pz%2FeLgQ6tNN14ELhdbJnfUyCDhOmZnKPgDas2AX5wj9BfZy2bY%2Bxr1H2Qw8B0ulyhMO87e48omEKTz9fBrhBGwZ3ADVNQMYAjpS9YtJTE6laryXFH%2F7c%2FLLMupynNLLq8maocWiWUkv0omNTu1TCSq%2BVVS2d2pRf6cpNmcW2VuRqDWm58lS48IkNdoBP5SHr7aRtPAS4lB8OCFqX8LTkt2TVnDr%2FmpztZ435vDPt%2F5rQxN9KyL81YqVoYJ7Xgq3O3mzQy5yIPU7YThpNvmxl6J%2B6p6NvpG%2BQ6H7798aXRnYCMrlO2O6lpfpHaOMnljKrgJ6J1qMenqfpOmEYO1FTyfaLQ0IuJaDDtp4mdmoL3mJrS7rt%2Ba5JMD63JQ8Ro%2FFDftCp6yA50ZYAF4bC14CGz6PqXWtQk392V1sk3NfLTGtqmJjtXnihF4uWK3MWC%2F%2BWrf4uPlhS0IbTeGDk1qrWLVBL6astcfr%2BJ6CMjJRlC9fw%2FDvukbmfa6qyGnM9FbATK5hq71gCNYW4uKQlvJ%2FGwSBimp%2FIsw1nDDPJhl3Y8yW1Bm9axRZmBn2VE7e5aTMfUAb%2BjpSezPbcXaXnjrIHzOh6788NHH5L3fGq905JPinihO1l5Y6dtRuOBOk4pjNfQzxEN3aM2zIFrKd%2B6PdJKeANFFRJ%2BpspWQnizxonSBnsSeSVgXoyxO5vx8xY3Tohj4ToCVjreIYTDpZp%2F%2BsBjEsAERehRaQRQGbKZFGHuZML0osgp%2FvU3%2FoAnzUfQDoE8OQLUFrR3qbF0492zatlLJaPZUU1fa3mluHbBt8GS%2F%2B0mAoIuV0ivLxs26GVDrYl%2BSPrhuQC42lRnA4zF3ezP%2BOaWgrNz6SnsZyZ7CUmaiwqpE9Dj7HAEsX9uZEghnhwaWtBA4ygjmx4IHciAhcBTx2o8FDaQaE49zcknwGwdccz2e%2B0R5TLvX69FmvOXBGfNZiDqA6rqjaji2WDAGmq3QztTWshbATnuBVF4Ydv2S4%2FV7%2FufX%2FM9M3%2B1F6qvRZzWDJvbpuwJJ0BdmINeiNcPKlHeANIMjotDy813vH3ChUX9wQxFG0WdwJ0YJd82P39UOlltkUnH7mQZF16ETCJr4uANsz8nlS21w6CKeIRevPM%2B3POk35k79fWvsqwMiVofC6u35RXI0sHH2PcSkUp8YVb%2F5Oh7hYsT%2BWOnn3wZTFddR7h1laCjDIfts2PD9cDVYlNjWJ1tgopDDb0fDRVIkqEAkApdhClYc36PsS96amLK4PilixHWLQSP9eNyIDTmSVgbjR1wnFEJiA%2BAoEus9EkhSK7EktRJOYrdDEaUCKkJQ6ZjpPStIvBxo4P8WC%2FneMiCfIxzzpK70EXP2MYu6DmXXIWHX%2FkBxTVIbTCqL6ZiHI3Y0JxhesdJKJqG62OE9tNhW10hFh4wJXHHtKgMb3XhHV0hhxg68x%2Bfho2WPKyssxrj4XfoGGzM4ec3NBxObUpoarzSh8yygTxU4rYlH4PzxeuWbUsb4o2SZbGHbswvpuLPWE1j7fcsTUiJlVqRMjHAaqWmNaFxOI5JkynvXh%2F9ZBTi9Tf7xwjRWyPHE6N9iJ64a57sU0LsnN7ftSc%2FGjk80xMCODXkdDQUN0GUSY4gWfZ%2FRW65%2BI59PNO7%2FDw%3D%3D"></iframe><h2 id="get-rays-H-W-K-c2w-torch版本-tensor"><a href="#get-rays-H-W-K-c2w-torch版本-tensor" class="headerlink" title="get_rays(H, W, K, c2w) torch版本(tensor)"></a>get_rays(H, W, K, c2w) torch版本(tensor)</h2><p>通过输入的图片大小和相机参数，得到从相机原点到图片每个像素的光线（方向向量d和相机原点o）</p><p>输入：</p><ul><li>H：图片的高</li><li>W：图片的宽</li><li>K：相机内参矩阵<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">K = np.array([</span><br><span class="line">    [focal, 0, 0.5*W],</span><br><span class="line">    [0, focal, 0.5*H],</span><br><span class="line">    [0, 0, 1]</span><br><span class="line">])</span><br></pre></td></tr></table></figure></li><li>c2w：相机外参矩阵<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c2w = np.array([</span><br><span class="line">            [ -0.9980267286300659,  0.04609514772891998,  -0.042636688798666, -0.17187398672103882],</span><br><span class="line">            [ -0.06279052048921585,  -0.7326614260673523, 0.6776907444000244, 2.731858730316162],</span><br><span class="line">            [-3.7252898543727042e-09, 0.6790306568145752,0.7341099381446838, 2.959291696548462],</span><br><span class="line">            [ 0.0,0.0,0.0,1.0 ]])</span><br></pre></td></tr></table></figure></li></ul><p>输出：从相机原点到800x800图片中每个像素生成的光线 $r(t)=\textbf{o}+t\textbf{d}$</p><ul><li>rays_o：光线原点（世界坐标系下）(800, 800, 3)</li><li>rays_d：光线的方向向量（世界坐标系下）(800, 800, 3)</li></ul><h2 id="ndc-rays-H-W-focal-near-rays-o-rays-d"><a href="#ndc-rays-H-W-focal-near-rays-o-rays-d" class="headerlink" title="ndc_rays(H, W, focal, near, rays_o, rays_d)"></a>ndc_rays(H, W, focal, near, rays_o, rays_d)</h2><p>仅需要对LLFF做Projection 变换到NDC坐标系下</p><p>输入：</p><ul><li>H</li><li>W</li><li>focal</li><li>near</li><li>rays_o</li><li>rays_d</li></ul><p>输出：NDC坐标系下</p><ul><li>rays_o</li><li>rays_d</li></ul><h2 id="batchify-rays-rays-flat-chunk-1024-32-kwargs"><a href="#batchify-rays-rays-flat-chunk-1024-32-kwargs" class="headerlink" title="batchify_rays(rays_flat, chunk=1024*32, **kwargs)"></a>batchify_rays(rays_flat, chunk=1024*32, **kwargs)</h2><p>输入：</p><ul><li>rays_flat: (WxH)x8 or (WxH)x11</li><li>chunk=1024*32</li><li>**kwargs</li></ul><p>输出：</p><ul><li>all_ret: 将ret = {‘rgb_map’ : rgb_map, ‘disp_map’ : disp_map, ‘acc_map’ : acc_map} 拼接起来</li></ul><p><strong>ret：chunk(1024 * 32) —&gt; all_ret：800x800</strong></p><h3 id="render-rays"><a href="#render-rays" class="headerlink" title="render_rays()"></a>render_rays()</h3><p>输入：</p><ul><li>ray_batch</li><li>继承来自render()输入的参数：</li><li>network_fn,</li><li>network_query_fn,</li><li>N_samples,</li><li>retraw=False,</li><li>lindisp=False,</li><li>perturb=0.,</li><li>N_importance=0, 每条光线增加的采样数</li><li>network_fine=None,</li><li>white_bkgd=False,</li><li>raw_noise_std=0.,</li><li>verbose=False,</li><li>pytest=False</li></ul><p>输出：<br>ret = {‘rgb_map’ : rgb_map, ‘disp_map’ : disp_map, ‘acc_map’ : acc_map}<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">if retraw:</span><br><span class="line">    ret[&#x27;raw&#x27;] = raw</span><br><span class="line"></span><br><span class="line">if N_importance &gt; 0:</span><br><span class="line">    ret[&#x27;rgb0&#x27;] = rgb_map_0</span><br><span class="line">    ret[&#x27;disp0&#x27;] = disp_map_0</span><br><span class="line">    ret[&#x27;acc0&#x27;] = acc_map_0</span><br><span class="line">    ret[&#x27;z_std&#x27;] = torch.std(z_samples, dim=-1, unbiased=False)  # [N_rays]</span><br></pre></td></tr></table></figure></p><h4 id="run-network-inputs-viewdirs-fn-embed-fn-embeddirs-fn-netchunk-1024-64"><a href="#run-network-inputs-viewdirs-fn-embed-fn-embeddirs-fn-netchunk-1024-64" class="headerlink" title="run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024*64)"></a>run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024*64)</h4><p>输入：</p><ul><li>inputs </li><li>viewdirs</li><li>fn：network_fn = model ，经过一次MLP训练</li><li>embed_fn</li><li>embeddirs_fn</li><li>netchunk=1024*64</li></ul><p>输出：</p><ul><li>outputs: chunk <em> N_samples </em> 4 (每条光线，每个采样点的RGBσ)</li></ul><p>在这里调用了run_network函数<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,</span><br><span class="line">                                                                embed_fn=embed_fn,</span><br><span class="line">                                                                embeddirs_fn=embeddirs_fn,</span><br><span class="line">                                                                netchunk=args.netchunk)</span><br></pre></td></tr></table></figure></p><h5 id="batchify-fn-chunk"><a href="#batchify-fn-chunk" class="headerlink" title="batchify(fn, chunk)"></a>batchify(fn, chunk)</h5><p>输入：</p><ul><li>fn : model</li><li>chunk : 1024* 32 </li></ul><p>如果chunk没有数据，则返回fn，否则：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def ret(inputs):</span><br><span class="line">    return torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)</span><br><span class="line">            # 每个chunk的大小为1024*32，即每次fn处理1024*32个点，最后返回结果</span><br></pre></td></tr></table></figure></p><h4 id="raw2outputs-raw-z-vals-rays-d-raw-noise-std-0-white-bkgd-False-pytest-False"><a href="#raw2outputs-raw-z-vals-rays-d-raw-noise-std-0-white-bkgd-False-pytest-False" class="headerlink" title="raw2outputs(raw, z_vals, rays_d, raw_noise_std=0, white_bkgd=False, pytest=False)"></a>raw2outputs(raw, z_vals, rays_d, raw_noise_std=0, white_bkgd=False, pytest=False)</h4><p>体渲染在其中<br>chunk = N_rays，射线数<br>输入：</p><ul><li>raw: chunk <em> N_samples </em> 4</li><li>z_vals: chunk * N_samples</li><li>rays_d: chunk * 3</li><li>raw_noise_std=0, </li><li>white_bkgd=False, </li><li>pytest=False)</li></ul><p>输出：</p><ul><li>rgb_map, <code>[chunk, 3]</code></li><li>disp_map:  <code>[chunk]</code></li><li>acc_map: <code>[chunk]</code></li><li>weights: <code>[chunk, N_samples]</code></li><li>depth_map: <code>[chunk]</code></li></ul><h4 id="sample-pdf-bins-weights-N-samples-det-False-pytest-False"><a href="#sample-pdf-bins-weights-N-samples-det-False-pytest-False" class="headerlink" title="sample_pdf(bins, weights, N_samples, det=False, pytest=False)"></a>sample_pdf(bins, weights, N_samples, det=False, pytest=False)</h4><p>输入：</p><ul><li>bins, chunk * 63</li><li>weights, chunk * 62</li><li>N_samples = N_importance</li><li>det=False  = (perturb\==0.)</li><li>pytest=False</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230618145142.png" alt="image.png|555"></p><p>输出：</p><ul><li>samples:  chunk * N_importance</li></ul><h1 id="img2mse-and-mse2psnr"><a href="#img2mse-and-mse2psnr" class="headerlink" title="img2mse() and mse2psnr()"></a>img2mse() and mse2psnr()</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">img2mse = lambda x, y : torch.mean((x - y) ** 2)</span><br><span class="line">mse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.]))</span><br></pre></td></tr></table></figure><p> 数学公式： </p><p> $\text{MSE}(x, y) = \frac{1}{N}\sum_{i=1}^{N}(x_i - y_i)^2$</p><p> $\text{PSNR}(x) = -10 \cdot \frac{\ln(x)}{\ln(10)}$</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> NeRF </tag>
            
            <tag> Code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nerfstudio——简化NeRF流程</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF-Studio/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/NeRF-Studio/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>Nerfstudio: A Modular Framework for Neural Radiance Field Development</th></tr></thead><tbody><tr><td>Author</td><td>Tancik, Matthew and Weber, Ethan and Ng, Evonne and Li, Ruilong and Yi, Brentand Kerr, Justin and Wang, Terrance and Kristoffersen, Alexander and Austin,Jake and Salahi, Kamyar and Ahuja, Abhik and McAllister, David and Kanazawa,Angjoo</td></tr><tr><td>Conf/Jour</td><td>ACM SIGGRAPH 2023 Conference Proceedings</td></tr><tr><td>Year</td><td>2023</td></tr><tr><td>Project</td><td><a href="https://github.com/nerfstudio-project/nerfstudio/">nerfstudio-project/nerfstudio: A collaboration friendly studio for NeRFs (github.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4762351665164582913&amp;noteId=1908666225137730048">Nerfstudio: A Modular Framework for Neural Radiance Field Development (readpaper.com)</a></td></tr></tbody></table></div><p><a href="https://github.com/nerfstudio-project/nerfstudio/">Nerfstudio</a>提供了一个简单的API，可以简化创建、训练和测试NeRF的端到端过程。该库通过将每个组件模块化，支持更易于理解的NeRF实现。通过更模块化的NeRF，我们希望为探索这项技术提供更用户友好的体验。</p><span id="more"></span><h1 id="Autodl使用"><a href="#Autodl使用" class="headerlink" title="Autodl使用"></a>Autodl使用</h1><p>选择实例，pytorch2.0.0，python3.8，cuda11.8</p><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p><a href="https://docs.nerf.studio/en/latest/quickstart/installation.html">Installation - nerfstudio</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create --name nerfstudio -y python=3.8</span><br><span class="line">conda activate nerfstudio</span><br><span class="line">python -m pip install --upgrade pip</span><br></pre></td></tr></table></figure><p>for cuda11.8，需要很长时间<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118</span><br><span class="line">pip install ninja git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch</span><br></pre></td></tr></table></figure></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install nerfstudio</span><br><span class="line"></span><br><span class="line">默认源不好用，使用清华源</span><br><span class="line">pip install nerfstudio -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><h3 id="安装FFmpeg"><a href="#安装FFmpeg" class="headerlink" title="安装FFmpeg"></a>安装FFmpeg</h3><p><a href="https://www.onitroad.com/jc/linux/ubuntu/faq/how-to-install-ffmpeg-on-ubuntu-20-04.html">在Ubuntu 20.04 中安装FFMPEG-之路教程 (onitroad.com)</a><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update </span><br><span class="line">sudo apt install ffmpeg</span><br><span class="line"></span><br><span class="line">ffmpeg -version</span><br></pre></td></tr></table></figure></p><h3 id="安装Colmap"><a href="#安装Colmap" class="headerlink" title="安装Colmap"></a>安装Colmap</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt upgrade</span><br><span class="line">sudo apt-get install \</span><br><span class="line">    git \</span><br><span class="line">    cmake \</span><br><span class="line">    build-essential \</span><br><span class="line">    libboost-program-options-dev \</span><br><span class="line">    libboost-filesystem-dev \</span><br><span class="line">    libboost-graph-dev \</span><br><span class="line">    libboost-system-dev \</span><br><span class="line">    libboost-test-dev \</span><br><span class="line">    libeigen3-dev \</span><br><span class="line">    libsuitesparse-dev \</span><br><span class="line">    libfreeimage-dev \</span><br><span class="line">    libmetis-dev \</span><br><span class="line">    libgoogle-glog-dev \</span><br><span class="line">    libgflags-dev \</span><br><span class="line">    libglew-dev \</span><br><span class="line">    qtbase5-dev \</span><br><span class="line">    libqt5opengl5-dev \</span><br><span class="line">    libcgal-dev</span><br><span class="line">如果一次安装不上，可以继续下步，缺什么装什么</span><br></pre></td></tr></table></figure><p><code>sudo apt install colmap</code></p><h2 id="加载数据-amp-训练model"><a href="#加载数据-amp-训练model" class="headerlink" title="加载数据&amp;训练model"></a>加载数据&amp;训练model</h2><p><code>ns-train nerfacto --data data/nerfstudio/poster</code></p><h3 id="Download-some-test-data"><a href="#Download-some-test-data" class="headerlink" title="Download some test data:"></a>Download some test data:</h3><p><code>ns-download-data nerfstudio --capture-name=poster</code></p><blockquote><p>[!error]<br>AutoDL连接不了google drive，只能使用自己的数据集or：<br>    使用google的colab下载数据集并将其打包成zip，然后再上传到autodl</p><h3 id="Use-Own-Data"><a href="#Use-Own-Data" class="headerlink" title="Use Own Data"></a>Use Own Data</h3></blockquote><div class="note primary">            <p>配好环境后，可以在任意地址创建文件夹，放入需要训练的数据集 </p>          </div><p><code>ns-process-data &#123;video,images,polycam,record3d&#125; --data &#123;DATA_PATH&#125; --output-dir &#123;PROCESSED_DATA_DIR&#125;</code></p><p><code>ns-process-data &#123;images, video&#125; --data &#123;DATA_PATH&#125; --output-dir &#123;PROCESSED_DATA_DIR&#125;</code></p><h4 id="eg-Miku"><a href="#eg-Miku" class="headerlink" title="eg: Miku"></a>eg: Miku</h4><p>cd autodl-tmp<br><code>ns-process-data images --data data/images --output-dir data/nerfstudio/images_name</code></p><p>跳过图像处理：复制和缩放<br><code>ns-process-data images --data data/Miku/image/ --output-dir data/nerfstudio/Miku --skip-image-processing</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">06.29:</span><br><span class="line">(nerfstudio) root@autodl-container-7092458c99-5f01fa1c:~/autodl-tmp# ns-process-data images  --data data/Miku/image/ --output-dir data/nerfstudio/Miku --skip-image-processing --skip-colmap  </span><br><span class="line">[15:37:47] Only single camera shared for all images is supported.</span><br><span class="line">数据集必须是单个相机去拍照物体？？？</span><br><span class="line">无所谓：无卡开机用cpu算</span><br><span class="line">ns-process-data images  --data data/Miku/image/ --output-dir data/nerfstudio/Miku --skip-image-processing --no-gpu</span><br><span class="line">依然不行</span><br><span class="line"></span><br><span class="line">问题&amp;原因：</span><br><span class="line">qt.qpa.xcb: could not connect to display qt.qpa.plugin: Could not load the Qt platform plugin &quot;xcb&quot; in &quot;&quot; even though it was found.  This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.</span><br><span class="line">最大的可能就是 --SiftExtraction.use_gpu 1  必须要求GPU带一个显示器</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">06.30:</span><br><span class="line">使用3090开机但是use no gpu</span><br><span class="line">ns-process-data images  --data data/Miku/image/ --output-dir data/nerfstudio/Miku --skip-image-processing --no-gpu</span><br><span class="line"></span><br><span class="line">[15:32:40] 🎉 Done extracting COLMAP features.                                                       colmap_utils.py:131</span><br><span class="line">[15:49:59] 🎉 Done matching COLMAP features.                                                         colmap_utils.py:145</span><br><span class="line">[15:53:28] 🎉 Done COLMAP bundle adjustment.                                                         colmap_utils.py:167</span><br><span class="line">[15:53:56] 🎉 Done refining intrinsics.                                                              colmap_utils.py:176</span><br><span class="line">           🎉 🎉 🎉 All DONE 🎉 🎉 🎉                                                images_to_nerfstudio_dataset.py:100</span><br><span class="line">           Starting with 178 images                                                  images_to_nerfstudio_dataset.py:103</span><br><span class="line">           Colmap matched 178 images                                                 images_to_nerfstudio_dataset.py:103</span><br><span class="line">           COLMAP found poses for all images, CONGRATS!                              images_to_nerfstudio_dataset.py:103</span><br><span class="line">train：</span><br><span class="line">ns-train nerfacto --data data/nerfstudio/Miku</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>in viewer:  it is easy to view results and process</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230630161446.png" alt="image.png"></p><h3 id="Train-model"><a href="#Train-model" class="headerlink" title="Train model"></a>Train model</h3><p><code>ns-train nerfacto --data data/nerfstudio/poster</code></p><h2 id="export"><a href="#export" class="headerlink" title="export"></a>export</h2><h3 id="mesh"><a href="#mesh" class="headerlink" title="mesh"></a>mesh</h3><p>手动调整参数得到命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ns-export poisson --load-config outputs/Miku/nerfacto/2023-06-30_155708/config.yml --output-dir exports/mesh/ --target-num-faces 50000 --num-pixels-per-side 2048 --normal-method open3d --num-points 1000000 --remove-outliers True --use-bounding-box True --bounding-box-min -0.5 -0.5 -1 --bounding-box-max 0.5 0.5 0</span><br><span class="line"></span><br><span class="line">output: </span><br><span class="line">Loading latest checkpoint from load_dir  </span><br><span class="line">✅ Done loading checkpoint from outputs/Miku/nerfacto/2023-06-30_155708/nerfstudio_models/step-000029999.ckpt  </span><br><span class="line">☁ Computing Point Cloud ☁ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 00:05  </span><br><span class="line">✅ Cleaning Point Cloud  </span><br><span class="line">✅ Estimating Point Cloud Normals  </span><br><span class="line">✅ Generated PointCloud with 1008679 points.  </span><br><span class="line">Computing Mesh... this may take a while.</span><br><span class="line"></span><br><span class="line">CPU生成mesh的速度很慢 大约用了1个小时多，效果也不是很好，因为使用的是nerfacto的方法，零水平集有很多坑洞</span><br></pre></td></tr></table></figure><h2 id="使用viewer"><a href="#使用viewer" class="headerlink" title="使用viewer"></a>使用viewer</h2><p><a href="https://viewer.nerf.studio/">nerfstudio viewer</a></p><h3 id="autodl"><a href="#autodl" class="headerlink" title="autodl"></a>autodl</h3><p>本地运行：<code>ssh -CNg -L 6006:127.0.0.1:6006 root@123.125.240.150 -p 42151</code></p><blockquote><p>[!important]<br>本地端口:localhost:远程端口</p></blockquote><p>一般本地进入服务器（ssh）<br><code>ssh -p 23394 root@connect.beijinga.seetacloud.com</code></p><p>将服务器6006端口映射到本地的6006端口上<br><code>ssh -CNg -L 6006:127.0.0.1:6006 root@connect.beijinga.seetacloud.com -p 23394</code></p><h3 id="viewer"><a href="#viewer" class="headerlink" title="viewer"></a>viewer</h3><p>一般nerfstudio的viewer运行在本地的7007端口上<br><code>ssh -L 7007:localhost:7007 &lt;username&gt;@&lt;remote-machine-ip&gt;</code></p><p>需要在本地再开一个终端，并运行，将本地的6006端口与远程的7007进行绑定</p><ul><li>eg: <code>ssh -L 7007:localhost:7007 root@connect.beijinga.seetacloud.com -p 23394</code></li><li>ssh -L 7007:localhost:7007 root@<remote-machine-ip> -p port</li></ul><p>此时打开<a href="https://viewer.nerf.studio/">nerfstudio viewer</a>，在Getting started中输入ws://localhost:7007，即可在viewer中查看</p><h4 id="更换服务器的端口"><a href="#更换服务器的端口" class="headerlink" title="更换服务器的端口"></a>更换服务器的端口</h4><ul><li>当服务器的7007被占用时：<br>  默认为7007，修改端口7007为6006 并训练<br>  <code>ns-train nerfacto --data data/nerfstudio/poster --viewer.websocket-port 6006</code></li><li>此时在本地需运行<br>  <code>ssh -L 7007:localhost:6006 root@connect.beijinga.seetacloud.com -p 23394</code></li></ul>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Framework </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Instant-nsr-pl</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/Neus-Instant-nsr-pl/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/Efficiency/Neus-Instant-nsr-pl/</url>
      
        <content type="html"><![CDATA[<p>使用<a href="https://github.com/NVlabs/instant-ngp">Instant-ngp</a>中的编码技术，使Neus可以更快的进行inference，大概只需要5~10min生成一个模型</p><blockquote><p><a href="https://github.com/NVlabs/instant-ngp">NVlabs/instant-ngp: Instant neural graphics primitives: lightning fast NeRF and more (github.com)</a><br><a href="https://github.com/zhaofuq/Instant-NSR">zhaofuq/Instant-NSR: Pytorch implementation of fast surface resconstructor (github.com)</a><br><a href="https://github.com/kwea123/ngp_pl">kwea123/ngp_pl: Instant-ngp in pytorch+cuda trained with pytorch-lightning (high quality with high speed, with only few lines of legible code) (github.com)</a></p></blockquote><p><strong><em>neus：对无纹理的区域处理的很差</em></strong></p><span id="more"></span><p>无论文，来源Neus的issue：<a href="https://github.com/Totoro97/NeuS/issues/78">Train NeuS in 10min using Instant-NGP acceleration techniques · Issue #78 · Totoro97/NeuS (github.com)</a><br><a href="https://github.com/bennyguo/instant-nsr-pl">bennyguo/instant-nsr-pl: Neural Surface reconstruction based on Instant-NGP. Efficient and customizable boilerplate for your research projects. Train NeuS in 10min! (github.com)</a></p><h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><p>autodl镜像：<br>    GPU 3090<br>    PyTorch  1.10.0<br>    Python  3.8(ubuntu20.04)<br>    Cuda  11.3</p><ul><li>安装tiny-cuda-nn扩展之前需要编译好环境<ul><li>安装tiny-cuda-nn<code>pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch</code><ul><li>这一步可以先通过clone tiny-cuda-nn，然后在bindings/torch文件夹下运行<code>python setup.py</code></li></ul></li><li>需要提前准备好cuda、cmake、gcc等环境<a href="https://github.com/NVlabs/tiny-cuda-nn#requirements">NVlabs/tiny-cuda-nn: Lightning fast C++/CUDA neural network framework (github.com)</a></li><li>auto-dl生成实例时可以选择pytorch&gt;=1.10.0，cuda11.3or higher，cmake需要升级版本<ul><li><a href="https://zhuanlan.zhihu.com/p/588741279">带你复现nerf之instant-ngp（从0开始搭环境） - 知乎 (zhihu.com)</a><ul><li><code>wget https://cmake.org/files/v3.21/cmake-3.21.0.tar.gz</code></li><li><code>tar -zxvf cmake-3.21.0.tar.gz</code></li><li><code>cd cmake-3.21.0</code></li><li><code>./bootstrap &amp;&amp; make &amp;&amp; sudo make install</code></li><li><code>cmake --version</code></li></ul></li><li>cmake编译完成后，如果cmake -version失败需要进行环境变量的配置 <code>vim 保存并退出 :wq</code><ul><li><a href="https://blog.csdn.net/xiaobumi123/article/details/109578993">(18条消息) Cmake安装遇到问题_snap cmake_小布米的博客-CSDN博客</a></li></ul></li></ul></li><li>sudo apt-get install build-essential git</li><li>然后对tiny-cuda-nn进行编译，此处需要用到cuda，因此需要带GPU模式开机<ul><li><code>$ git clone --recursive https://github.com/nvlabs/tiny-cuda-nn</code></li><li><code>$ cd tiny-cuda-nn</code></li><li><code>tiny-cuda-nn$ cmake . -B build</code></li><li><code>tiny-cuda-nn$ cmake --build build --config RelWithDebInfo -j</code></li></ul></li><li>最后运行setup.py，如果不在全局环境安装，亦可在conda虚拟环境中安装<ul><li><code>tiny-cuda-nn$ cd bindings/torch</code></li><li><code>tiny-cuda-nn/bindings/torch$ python setup.py install</code></li></ul></li></ul></li><li>编译完成cmake和tiny-cuda-nn后，创建虚拟环境并利用pip安装python库<ul><li><code>conda create -n inneus python=3.8</code></li><li><code>conda init bash</code> 初始化bash终端</li><li><code>conda activate inneus</code></li><li><code>pip install -r requirements.txt</code><ul><li>torch=2.0.1 , torchvision=0.15.2 ,pytorch-lightning= 1.9.5</li></ul></li><li><code>pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch</code></li><li><code>pip install trimesh -i https://mirrors.ustc.edu.cn/pypi/web/simple/</code></li></ul></li></ul><h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><h2 id="NeRF-Synthetic，解压并放在-load文件夹下，The-file-structure-should-be-like-load-nerf-synthetic-lego"><a href="#NeRF-Synthetic，解压并放在-load文件夹下，The-file-structure-should-be-like-load-nerf-synthetic-lego" class="headerlink" title="NeRF-Synthetic，解压并放在/load文件夹下，The file structure should be like load/nerf_synthetic/lego."></a>NeRF-Synthetic，解压并放在/load文件夹下，The file structure should be like <code>load/nerf_synthetic/lego</code>.</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># train NeRF</span><br><span class="line">python launch.py --config configs/nerf-blender.yaml --gpu 0 --train dataset.scene=lego tag=example</span><br><span class="line"></span><br><span class="line"># train NeuS with mask</span><br><span class="line">python launch.py --config configs/neus-blender.yaml --gpu 0 --train dataset.scene=lego tag=example</span><br><span class="line"># train NeuS without mask</span><br><span class="line">python launch.py --config configs/neus-blender.yaml --gpu 0 --train dataset.scene=lego tag=example system.loss.lambda_mask=0.0</span><br></pre></td></tr></table></figure><p>代码快照、ckpt和实验输出保存在 exp/[name]/[tag]@[timestamp]，而TensorBoard日志可以在 runs/[name]/[tag]@[timestamp] 找到。您可以通过在YAML文件中指定参数而无需添加—来更改任何配置，例如：<br><code>python launch.py --config configs/nerf-blender.yaml --gpu 0 --train dataset.scene=lego tag=iter50k seed=0 trainer.max_steps=50000</code></p><p>test，生成mp4和mesh的obj文件<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">python launch.py --config path/to/your/exp/config/parsed.yaml --resume path/to/your/exp/ckpt/epoch=0-step=20000.ckpt --gpu 0 --test</span><br><span class="line"></span><br><span class="line">- eg:</span><br><span class="line">python launch.py --config exp/neus-blender-lego/example@20230601-185640/config/parsed.yaml --resume exp/neus-blender-lego/example@20230601-185640/ckpt/epoch=0-step=20000.ckpt --gpu 0 --test</span><br><span class="line"></span><br><span class="line">or：</span><br><span class="line">### neus-blender-chair/example@20230624-191845/</span><br><span class="line">python launch.py --config exp/neus-blender-chair/example@20230624-191845/config/parsed.yaml --resume exp/neus-blender-chair/example@20230624-191845/ckpt/epoch=0-step=20000.ckpt --gpu 0 --test</span><br></pre></td></tr></table></figure></p><h3 id="eg-neus-chair"><a href="#eg-neus-chair" class="headerlink" title="eg: neus_chair"></a>eg: neus_chair</h3><p>neus_chair_wmask<br><code>python launch.py --config configs/neus-blender.yaml --gpu 0 --train dataset.scene=chair tag=example</code><br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230601205022.png" alt="Pasted image 20230601205022.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230601205055.png" alt="Pasted image 20230601205055.png"></p><p>nerf生成mesh的obj文件很小（nerf_chair）：<br><code>python launch.py --config configs/nerf-blender.yaml --gpu 0 --train dataset.scene=chair tag=example</code><br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230601202815.png" alt="Pasted image 20230601202815.png|300"></p><p>neus_chair_womask<br><code>python launch.py --config configs/neus-blender.yaml --gpu 0 --train dataset.scene=chair tag=chair system.loss.lambda_mask=0.0</code></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230601212157.png" alt="Pasted image 20230601212157.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230601212246.png" alt="Pasted image 20230601212246.png"></p><h3 id="eg-neus-lego"><a href="#eg-neus-lego" class="headerlink" title="eg: neus_lego"></a>eg: neus_lego</h3><p>neus_lego_wmask<br><code>python launch.py --config configs/neus-blender.yaml --gpu 0 --train dataset.scene=lego tag=lego</code><br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230601213938.png" alt="Pasted image 20230601213938.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230601214004.png" alt="Pasted image 20230601214004.png"></p><p>neus_lego &amp; w/o mask<br><code>python launch.py --config configs/neus-blender.yaml --gpu 0 --train dataset.scene=lego tag=example system.loss.lambda_mask=0.0</code><br>迭代20000步，大致需要18分钟，生成的模型细节还是不够精细<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230601201131.png" alt="Pasted image 20230601201131.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230601201258.png" alt="Pasted image 20230601201258.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230601201510.png" alt="Pasted image 20230601201510.png"></p><h2 id="Blended-MVS，DTU"><a href="#Blended-MVS，DTU" class="headerlink" title="Blended_MVS，DTU"></a>Blended_MVS，DTU</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># train NeuS on DTU without mask</span><br><span class="line">python launch.py --config configs/neus-dtu.yaml --gpu 0 --train</span><br><span class="line"># train NeuS on DTU with mask</span><br><span class="line">python launch.py --config configs/neus-dtu.yaml --gpu 0 --train system.loss.lambda_mask=0.1</span><br></pre></td></tr></table></figure><blockquote><p>[!note]<br>作者只提供了DTU数据集的加载方式，但是DTU和Bmvs相差不大，因此只需要作微小修改即可完成bmvs数据集的处理：</p></blockquote><p>修改config/neus-dtu.yaml文件，dtu保持不变(需要用到dtu.py数据集加载文件)，修改数据集的文件路径（dtu与bmvs数据集差别不大，都是由image、mask和cameras_sphere.npz组成</p><p>在/datasets/dtu.py中修改数据集的文件名，</p><ul><li>dtu的是前面补0到6位数.png</li><li>bmvs是前面补0到3位数.png</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">灵活调整dtu.py 文件</span><br><span class="line"># # DTU 数据集</span><br><span class="line"># img_sample = cv2.imread(os.path.join(self.config.root_dir, &#x27;image&#x27;, &#x27;000000.png&#x27;))</span><br><span class="line"># BMVS数据集</span><br><span class="line">img_sample = cv2.imread(os.path.join(self.config.root_dir, &#x27;image&#x27;, &#x27;000.png&#x27;))</span><br><span class="line"></span><br><span class="line"># bmvs</span><br><span class="line">img_path = os.path.join(self.config.root_dir, &#x27;image&#x27;, f&#x27;&#123;i:03d&#125;.png&#x27;)</span><br><span class="line"># DTU</span><br><span class="line"># img_path = os.path.join(self.config.root_dir, &#x27;image&#x27;, f&#x27;&#123;i:06d&#125;.png&#x27;)</span><br></pre></td></tr></table></figure><h3 id="eg-neus-bmvs-clock-womask"><a href="#eg-neus-bmvs-clock-womask" class="headerlink" title="eg: neus_bmvs_clock_womask"></a>eg: neus_bmvs_clock_womask</h3><p><code>python launch.py --config configs/neus-dtu.yaml --gpu 0 --train</code><br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230602160929.png" alt="Pasted image 20230602160929.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230602160959.png" alt="Pasted image 20230602160959.png"></p><p>也有噪声<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230602161034.png" alt="Pasted image 20230602161034.png"></p><h3 id="eg-neus-bmvs-clock-wmask"><a href="#eg-neus-bmvs-clock-wmask" class="headerlink" title="eg: neus_bmvs_clock_wmask"></a>eg: neus_bmvs_clock_wmask</h3><p><code>python launch.py --config configs/neus-dtu.yaml --gpu 0 --train system.loss.lambda_mask=0.1</code></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230602172423.png" alt="Pasted image 20230602172423.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230602172441.png" alt="Pasted image 20230602172441.png"></p><h3 id="eg-neus-bmvs-bear-womask"><a href="#eg-neus-bmvs-bear-womask" class="headerlink" title="eg: neus_bmvs_bear_womask"></a>eg: neus_bmvs_bear_womask</h3><p><code>python launch.py --config configs/neus-dtu.yaml --gpu 0 --train</code></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230602163439.png" alt="Pasted image 20230602163439.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230602163553.png" alt="Pasted image 20230602163553.png"></p><h3 id="eg-neus-bmvs-bear-womask-给毛绒物体添加缺陷"><a href="#eg-neus-bmvs-bear-womask-给毛绒物体添加缺陷" class="headerlink" title="eg: neus_bmvs_bear_womask 给毛绒物体添加缺陷"></a>eg: neus_bmvs_bear_womask 给毛绒物体添加缺陷</h3><ul><li>修改images中的照片数据or自己做一个照片数据（毛绒玩具）</li></ul><h2 id="自定义数据集"><a href="#自定义数据集" class="headerlink" title="自定义数据集"></a>自定义数据集</h2><h3 id="服务器环境配置colmap-未完成"><a href="#服务器环境配置colmap-未完成" class="headerlink" title="服务器环境配置colmap (未完成)"></a>服务器环境配置colmap (未完成)</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git clone https://ceres-solver.googlesource.com/ceres-solver</span><br><span class="line">cd ceres-solver</span><br><span class="line">git checkout $(git describe --tags) # Checkout the latest release</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake .. -DBUILD_TESTING=OFF -DBUILD_EXAMPLES=OFF</span><br><span class="line">make -j</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure><h3 id="本地处理数据集-同neus类似BMVS数据集形式"><a href="#本地处理数据集-同neus类似BMVS数据集形式" class="headerlink" title="本地处理数据集(同neus类似BMVS数据集形式)"></a>本地处理数据集(同neus类似BMVS数据集形式)</h3><p>同neus使用自定义数据集<a href="NeuS.md#Neus使用自制数据集">NeuS</a></p><h4 id="eg-使用neus自定义数据集对M590三维重建"><a href="#eg-使用neus自定义数据集对M590三维重建" class="headerlink" title="eg: 使用neus自定义数据集对M590三维重建"></a>eg: 使用neus自定义数据集对M590三维重建</h4><ul><li>拍M590视频</li><li>video2img.py，将mp4按帧拆分成png，并生成mask文件夹</li><li>使用colmap对image下图片进行处理，得到相机位姿和点云ply等文件</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/%E5%BD%95%E5%88%B6_2023_06_25_16_27_37_917.gif" alt="录制_2023_06_25_16_27_37_917.gif"></p><ul><li>使用img2poses.py得到sparse_points.ply<ul><li><code>cd colmap_preprocess       ##neus/preprocess_custom_data/colmap_preprocess/</code></li><li><code>python imgs2poses.py $&#123;data_dir&#125;</code></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230625163416.png" alt="image.png"></p><ul><li>根据sparse_points.ply在meshlab中进行clean操作，只保留interest区域的点云，并保存为${data_dir}/sparse_points_interest.ply</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230625163527.png" alt="image.png"></p><ul><li>使用gen_cameras.py生成npz文件<ul><li><code>python gen_cameras.py $&#123;data_dir&#125;</code></li><li>在 ${data_dir}下生成preprocessed，包括image、mask和cameras_sphere.npz，但image和mask下无文件，需要自行拷贝</li></ul></li><li>打包成zip，上传到百度云盘，然后在AutoDL的AutoPanel中下载到服务器的/root/autodl-tmp文件夹下</li><li>解压到/instant-nsr-pl/load文件夹下，修改config/neus-dtu.yaml文件中的数据集路径</li><li>运行<code>python launch.py --config configs/neus-dtu.yaml --gpu 0 --train</code></li><li>train结束后得到obj文件，下载到本地使用meshlab打开</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/GIF%202023-6-25%2016-44-37.gif" alt="GIF 2023-6-25 16-44-37.gif"></p><h5 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h5><ul><li>数据集使用xiaomi13录制成视频然后跳帧选取图片得到，会有一些反光和阴影</li><li>M590的右键部分有一块大的缺陷，这是由于数据集在右键部分信息太少(反光，阴影)</li><li>M590的下部分没有建模出来，这是由于数据集未录制其下部分的信息<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/M590.gif" alt="M590.gif"></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230625165030.png" alt="image.png"></p><h4 id="eg-对Miku进行重建"><a href="#eg-对Miku进行重建" class="headerlink" title="eg: 对Miku进行重建"></a>eg: 对Miku进行重建</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python launch.py --config configs/neus-dtu.yaml --gpu 0 --train</span><br><span class="line"></span><br><span class="line">test: 设置 neus-dtu.yaml 中的 resolution 为1024</span><br><span class="line">python launch.py --config configs/neus-dtu.yaml --resume /root/autodl-tmp/instant-nsr-pl/exp/neus-dtu-Miku/@20230629-132903/ckpt/epoch=0-step=20000.ckpt --gpu 0 --test</span><br></pre></td></tr></table></figure><p>效果不太好</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230630210349.png" alt="image.png|300"></p><p>更改resolution=1024 后，点更多更细致，但是由于训练出来的sdf网络相同，在相近的两个位置，sdf值相同，因此大体还是一样的模型，只是细节处有所不同(面数多了)</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230630212921.png" alt="images"></p><h5 id="使用更高质量的数据集"><a href="#使用更高质量的数据集" class="headerlink" title="使用更高质量的数据集"></a>使用更高质量的数据集</h5><p>当使用更高质量的数据集(178x1080x1920)时，会爆显存(RTX3090-24G)，下采样到(540x960)…</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">dtu</span></span><br><span class="line"><span class="attr">root_dir:</span> <span class="string">./load/miku</span> </span><br><span class="line"><span class="attr">cameras_file:</span> <span class="string">cameras_sphere.npz</span></span><br><span class="line"><span class="attr">img_downscale:</span> <span class="number">2</span> <span class="comment"># specify training image size by either img_wh or img_downscale</span></span><br><span class="line"><span class="attr">n_test_traj_steps:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">apply_mask:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>当使用原来的cameras_sphere.npz文件，即高质量数据集使用低质量生成的相机内参矩阵和c2w矩阵时，对应不上，会导致生成的隐式模型错位(左2,4)，生成的背景也很杂乱(3)</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230712135714.png" alt="image.png"></p><p>且FFMPEG会报错<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (3240, 960) to (3248, 960) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or <span class="built_in">set</span> the macro_block_size to 1 (risking incompatibility).</span><br><span class="line">[swscaler @ 0x5d9be80] Warning: data is not aligned! This can lead to a speed loss</span><br></pre></td></tr></table></figure></p><p>重新使用colmap和img2poses.py及gen_cameras.py生成一下cameras_sphere.npz：</p><p>cd to colmap_preprocess</p><p><code>python imgs2poses.py ..\Miku</code></p><p><code>python gen_cameras.py ..\Miku</code></p><p>cd to instant-nsr-pl</p><p><code>python launch.py --config configs/neus-dtu.yaml --gpu 0 --train</code></p><p><code>python launch.py --config configs/neus-dtu.yaml --resume exp/neus-dtu-miku_l/@20230712-162601/ckpt/epoch=0-step=20000.ckpt --gpu 0 --test</code></p><p>依然会error: This can lead to a speed loss，数据未对齐<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230712165423.png" alt="image.png"></p><h5 id="使用更高质量的数据集，将step增加一倍"><a href="#使用更高质量的数据集，将step增加一倍" class="headerlink" title="使用更高质量的数据集，将step增加一倍"></a>使用更高质量的数据集，将step增加一倍</h5><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230714165754.png" alt="image.png|999"></p><p><img src="https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/it40000-test2.gif" alt="it40000-test2.gif"></p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based/Efficiency </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Neus </tag>
            
            <tag> Efficiency </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeuS</title>
      <link href="/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/NeuS/"/>
      <url>/3DReconstruction/Multi-view/Implicit%20Function/NeRF-based/NeuS/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://totoro97.github.io/about.html">Peng Wang</a>    <a href="https://lingjie0206.github.io/">Lingjie Liu</a>    <a href="https://liuyuan-pal.github.io/">Yuan Liu</a>    <a href="http://people.mpi-inf.mpg.de/~theobalt/">Christian Theobalt</a>    <a href="https://www.cs.hku.hk/index.php/people/academic-staff/taku">Taku Komura</a>    <a href="https://www.cs.hku.hk/people/academic-staff/wenping">Wenping Wang</a></td></tr><tr><td>Conf/Jour</td><td>NeurIPS 2021 (Spotlight)</td></tr><tr><td>Year</td><td>2021</td></tr><tr><td>Project</td><td><a href="https://lingjie0206.github.io/papers/NeuS/">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction (lingjie0206.github.io)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4718711588576575489&amp;noteId=1791151226962648064">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction (readpaper.com)</a></td></tr></tbody></table></div><p>实现了三维重建：从多视角图片中重建出了 mesh 模型</p><span id="more"></span><p><a href="https://github.com/Totoro97/NeuS">Code: Totoro97/NeuS: Code release for NeuS (github.com)</a><br><a href="https://lingjie0206.github.io/papers/NeuS/">Project Page</a></p><p>Neus 的总目标是实现从 2D 图像输入中以高保真度重建对象和场景。<br>现有的神经表面重建方法，如 DVR[Niemeyer 等人，2020]和 IDR[Yariv 等人，2020]，需要前景掩码作为监督，很容易陷入局部最小值，因此在对具有严重自遮挡或薄结构的对象进行重建时面临困难。<br>最近的新视角合成神经方法，如 NeRF[Mildenhall 等人，2020]及其变体，使用体积渲染来产生具有优化鲁棒性的神经场景表示，即使对于非常复杂的对象也是如此。<strong>然而，从这种学习的隐式表示中提取高质量的表面是困难的，因为在表示中没有足够的表面约束。</strong><br>在 NeuS 中，我们提出将表面表示为有符号距离函数（SDF）的零水平集，并开发了一种新的体积渲染方法来训练神经 SDF 表示。我们观察到，传统的体积渲染方法会导致固有的几何误差（即偏差）对于表面重建，因此提出了一个新的公式，它在一阶近似中没有偏差，从而即使在没有掩码监督的情况下也能实现更准确的表面重建。<br>在 DTU 数据集和 BlendedMVS 数据集上的实验证明，NeuS 在高质量表面重建方面优于现有技术，尤其是对于具有复杂结构和自遮挡的对象和场景。</p><h1 id="优点-amp-不足之处"><a href="#优点-amp-不足之处" class="headerlink" title="优点&amp;不足之处"></a>优点&amp;不足之处</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul><li>通过手动在 meshlab 中 clean 稀疏点云 ply 中其他噪音位置的点云，构建了一个精确的 bounds，可以将模型不包括背景且几乎没有噪声的生成出来</li><li>通过构建 SDF 场，其零水平集相比 NeRF 的密度场水平集(Threshold = 25)，生成的 mesh 更加精细，或者说更加平滑<ul><li>对图片中深度突然变化的部分，sdf 也可以很好的重建出来</li></ul></li></ul><h2 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h2><ul><li>对于无纹理物体(例如反光和阴影区域)的重建效果并不理想</li><li>需要手动在 meshlab 中 clean 稀疏点云 ply 中其他噪音位置的点云，这也是本文所说不需 mask 监督的方法</li><li>(in paper:)一个有趣的未来研究方向是根据不同的局部几何特征，对不同空间位置具有不同方差的概率以及场景表示的优化进行建模</li></ul><p><a href="#Neus与NeRF对比">Neus 与 NeRF 对比</a></p><h1 id="引言-相关工作"><a href="#引言-相关工作" class="headerlink" title="引言+相关工作"></a>引言+相关工作</h1><h2 id="SDF-简单理解"><a href="#SDF-简单理解" class="headerlink" title="SDF 简单理解"></a>SDF 简单理解</h2><p>SDF：输入一个空间中的点，输出为该点到某个表面（可以是曲面）最近的距离，符号在表面外部为正，内部为负。<br>给定一个物体的平面，我们定义 SDF 为空间某点到该平面距离为 0 的位置的点的集合（也就是物体表面的点）。如果空间中某点在物体表面之外，那么 SDF&gt;0；反之如果空间中某点在物体表面之内，那么 SDF&lt;0。这样子就可以找到物体表面在三维空间中的位置，自然而然的生成三维表面。</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230629135016.png" alt="image.png"></p><ol><li>mesh 是一种由图表示的数据结构，基于顶点、边、面共同组成的多面体。它可以十分灵活的表示复杂物体的表面，在计算机图形学中有着广泛的应用。从 nerf 输出的物理意义就可以想到，density 可以用来表示空间中沿光线照射方向的密度，<strong>那么我们可以通过基于密度的阈值来控制得到的 mesh</strong>。这种方法的好处是，训练好一个 nerf 的模型就可以得到一个 mesh 了。但是这种方式也有很大的缺点：一是训练结果会有很多噪音而且生成的 mesh 会有很多的空洞，二是很难控制一个合理的阈值。</li><li>这里我们可以考虑使用有向距离场（Signed distance function 简称 SDF）来取代 nerf 建模。使用 SDF 的一大好处是，SDF 函数本身在空间是连续的，这样子就不需要考虑离散化的问题。我们之后使用 Marching cubes 方法来生成 mesh。</li><li>NeRF 生成一个带有密度和颜色信息的模型，通过使用 SDF 来代替密度，在密度大的地方表示为物体的表面，就可以生成一个 mesh 模型。</li></ol><blockquote><p><a href="https://zhuanlan.zhihu.com/p/553474332">旷视 3d CV master 系列训练营二： 基于 NeRF 和 SDF 的三维重建与实践 - 知乎 (zhihu.com)</a></p></blockquote><h2 id="相较于以往的工作："><a href="#相较于以往的工作：" class="headerlink" title="相较于以往的工作："></a>相较于以往的工作：</h2><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Pasted%20image%2020230531185214.png" alt="Pasted image 20230531185214.png"></p><p>IDR 无法应对图片中深度突然变化的部分，这是因为它对每条光线只进行一次表面碰撞（上图（a）上），于是梯度也到此为止，这对于反向传播来说过于局部了，于是困于局部最小值，如下图 IDR 无法处理好突然加深的坑。</p><p>NeRF 的体积渲染方法提出沿着每条光线进行多次采样（上图（a）下）然后进行 α 合并，可以应对突然的深度变化但 NeRF 是专注于生成新视点图像而不是表面重建所以有明显噪声。</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/496752239">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction - 知乎 (zhihu.com)</a></p></blockquote><p>新的体积渲染方案：Neus，使用 SDF 进行表面表示，并使用一种新的体积渲染方案来学习神经 SDF 表示。</p><h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><p>本文集中在通过经典的绘制技术从 2D 图像中学习编码 3D 空间中的几何和外观的隐式神经表示，限制在此范围内，相关工作可以大致分为基于表面渲染的方法和基于体积渲染的方法</p><ul><li>基于表面渲染：假设光线的颜色仅依赖于光线与场景几何的交点的颜色，这使得梯度仅反向传播到交点附近的局部区域，很难重建具有严重自遮挡和突然深度变化的复杂物体，因此需要物体 mask 来进行监督</li><li>基于体积渲染：eg_NeRF，通过沿每条射线的采样点的 α-合成颜色来渲染图像。正如在介绍中所解释的，它可以处理突然的深度变化并合成高质量的图像。提取学习到的隐式场的高保真表面是困难的，因为基于密度的场景表示对其等值集缺乏足够的约束。<br>相比之下，我们的方法结合了基于表面渲染和基于体积渲染的方法的优点，通过将场景空间约束为带符号距离函数 SDF，但使用体积渲染来训练具有鲁棒性的 representation。</li><li>同时 UNISURF 也通过体积渲染学习隐式曲面。在优化过程中，通过缩小体积渲染的样本区域来提高重建质量。<br><strong>UNISURF 用占用值来表示表面，而我们的方法用 SDF 来表示场景</strong>，(因此可以自然地提取表面作为场景的零水平集，产生比 UNISURF 更好的重建精度。)</li></ul><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><div class="note info">            <p>构建了一个无偏，且 collusion-aware 的权重函数 w(t) = T(t)ρ(t) </p>          </div><p>给定一组三维物体的 pose 图像，目标是重建该物体的表面，该表面由神经隐式 SDF 的零水平集表示。<br>为了学习神经网络的权重，开发了一种<strong>新的体积渲染方法</strong>来渲染隐式 SDF 的图像，并最小化渲染图像与输入图像之间的差异。确保了 Neus 在重建复杂结构物体时的鲁棒性优化。</p><h2 id="渲染步骤"><a href="#渲染步骤" class="headerlink" title="渲染步骤"></a>渲染步骤</h2><h3 id="场景表示"><a href="#场景表示" class="headerlink" title="场景表示"></a>场景表示</h3><p>被重构物体的场景表示为两个函数：</p><ul><li>f：将空间点的空间坐标映射为该点到物体的符号距离</li><li>c：编码与点 x 和观察方向 v 相关联的颜色<br>这两个函数都被 MLP 编码，物体的表面有 SDF 的零水平集表示 $\mathcal{S}=\left\{\mathbf{x}\in\mathbb{R}^3|f(\mathbf{x})=0\right\}.$</li></ul><p>定义概率密度函数 $\phi_s(x) =\frac{se^{-sx}}{(1+e^{-sx})^{2}}$</p><p>其为 sigmoid 函数的导数 $\Phi_s(x)=(1+e^{-sx})^{-1},\text{i.e.,}\phi_s(x)=\Phi_s’(x)$</p><h3 id="Neus-与-NeRF-对比"><a href="#Neus-与-NeRF-对比" class="headerlink" title="Neus 与 NeRF 对比"></a>Neus 与 NeRF 对比</h3><p>相同点：</p><ul><li>使用 NeRF 提出的频率编码方式进行位置编码</li><li>使用了从像素坐标到世界坐标系转换的方式来生成光线(o,d)</li></ul><p>不同点之一——使用了不同的相机坐标变换：</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230703144039.png" alt="image.png"></p><div class="table-container"><table><thead><tr><th>Method</th><th>Pixel to Camera coordinate</th></tr></thead><tbody><tr><td>NeRF</td><td>$\vec d = \begin{pmatrix} \frac{i-\frac{W}{2}}{f} \\ -\frac{j-\frac{H}{2}}{f} \\ -1 \\ \end{pmatrix}$ , $intrinsics = K = \begin{bmatrix} f &amp; 0 &amp; \frac{W}{2}  \\ 0 &amp; f &amp; \frac{H}{2}  \\ 0 &amp; 0 &amp; 1 \\ \end{bmatrix}$</td></tr><tr><td>Neus</td><td>$\vec d = intrinsics^{-1} \times  pixel = \begin{bmatrix} \frac{1}{f} &amp; 0 &amp; -\frac{W}{2 \cdot f}  \\ 0 &amp; \frac{1}{f} &amp; -\frac{H}{2 \cdot f} \\ 0 &amp; 0 &amp; 1 \\ \end{bmatrix} \begin{pmatrix} i \\ j \\ 1 \\ \end{pmatrix} = \begin{pmatrix} \frac{i-\frac{W}{2}}{f} \\ \frac{j-\frac{H}{2}}{f} \\ 1 \\ \end{pmatrix}$</td></tr></tbody></table></div><p>不同点：</p><p>体渲染、采样方式、训练出来的网络模型以及 near、far 计算方式</p><div class="table-container"><table><thead><tr><th>Project</th><th>Neus</th><th>NeRF</th></tr></thead><tbody><tr><td>渲染函数</td><td>$C(\mathbf{o},\mathbf{v})=\int_{0}^{+\infty}w(t)c(\mathbf{p}(t),\mathbf{v})\mathrm{d}t$</td><td>$\mathrm{C}(r)=\int_{\mathrm{t}_{\mathrm{n}}}^{\mathrm{t}_{\mathrm{f}}} \mathrm{T}(\mathrm{t}) \sigma(\mathrm{r}(\mathrm{t})) \mathrm{c}(\mathrm{r}(\mathrm{t}), \mathrm{d}) \mathrm{dt}$</td></tr><tr><td>权重</td><td>$\omega(t)=T(t)\rho(t),\text{where}T(t)=\exp\left(-\int_0^t\rho(u)\mathrm{d}u\right)$ <strong>无偏、且遮挡</strong></td><td>$w(t)=T(t)\sigma(t) , \text { where } \mathrm{T}(\mathrm{t})=\exp \left(-\int_{\mathrm{t}_{\mathrm{n}}}^{\mathrm{t}} \sigma(\mathrm{r}(\mathrm{s})) \mathrm{ds}\right)$ <strong>遮挡但有偏</strong></td></tr><tr><td>不透明度密度函数</td><td>$\rho(t)=\max\left(\frac{-\frac{\mathrm{d}\Phi_s}{\mathrm{d}t}(f(\mathbf{p}(t)))}{\Phi_s(f(\mathbf{p}(t)))},0\right)$</td><td>$\sigma(t) = \sigma_{i}=raw2alpha(raw[…,3] + noise, dists)$</td></tr><tr><td>离散化</td><td>$\hat{C}=\sum_{i=1}^n T_i\alpha_i c_i$ $T_i=\prod_{j=1}^{i-1}(1-\alpha_j)$ $\alpha_i=\max\left(\frac{\Phi_s(f(\mathbf{p}(t_i))))-\Phi_s(f(\mathbf{p}(t_{i+1})))}{\Phi_s(f(\mathbf{p}(t_i)))},0\right)$</td><td>$\hat{C}(\mathbf{r})=\sum_{i=1}^{N} T_{i}\alpha_{i}\mathbf{c}_{i}$ $\hat{C}(\mathbf{r})=\sum_{i=1}^{N} T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right) \mathbf{c}_{i}$ $T_{i}=\exp \left(-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}\right)$</td></tr><tr><td>精采样</td><td>使用 sdf_network 得到的 sdf 求出 cos，并得到估计的 sdf，求出$\alpha$和 weight，用权重逆变换采样</td><td>经过 MLP 得到$\sigma$，求出$\alpha$和 weight，用权重逆变换采样</td></tr><tr><td>网络模型</td><td>隐式 SDF 场</td><td>隐式密度点云场</td></tr><tr><td>near_far</td><td>根据光线的原点和方向向量计算</td><td>不同的数据集有不同的计算方式</td></tr></tbody></table></div><p>权重函数，Neus(右)，NeRF(左)<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Pasted%20image%2020230606154119.png" alt="Pasted image 20230606154119.png"></p><h2 id="训练损失函数"><a href="#训练损失函数" class="headerlink" title="训练损失函数"></a>训练损失函数</h2><p>loss 函数：</p><script type="math/tex; mode=display">\mathcal L=\mathcal L_{color}+\lambda\mathcal L_{reg}+\beta\mathcal L_{mask}.</script><p>颜色损失：$\mathcal{L}_{color}=\frac{1}{m}\sum_k\mathcal{R}(\hat{C}_k,C_k).$<br>Eikonal term,类似<a href="https://lioryariv.github.io/idr/">IDR</a>:$\mathcal{L}_{r e g}=\frac{1}{n m}\sum_{k,i}(|\nabla f(\hat{\mathbf{p}}_{k,i})|_{2}-1)^{2}.$<br><a href="https://github.com/amosgropp/IGR">IGR</a>：$\mathcal{L}_{mask}=\mathrm{BCE}(M_k,\hat{O}_k)$</p><ul><li>沿着相机 ray 的权重之和：$\hat{O}_k=\sum_{i=1}^n T_{k,i}\alpha_{k,i}$</li><li>是否使用 mask 监督: (BCE 是二值交叉熵损失)：$M_{k} ∈ {0, 1}$</li></ul><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="Dataset：DTU-amp-BlendedMVS"><a href="#Dataset：DTU-amp-BlendedMVS" class="headerlink" title="Dataset：DTU&amp;BlendedMVS"></a>Dataset：DTU&amp;BlendedMVS</h2><p>DTU: 15 个场景、1600 × 1200 像素<br>BlendedMVS：7 个场景、768 × 576 像素</p><h2 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h2><p>表面重建方法：</p><ul><li>IDR：高质量表面重建，但需要前景 mask 监督</li><li>DVR，没有比较<br>体积渲染方法：</li><li>NeRF：我们使用 25 的 threshold 从学习的密度场中提取网格（在补充材料中验证了为什么用 25 做阈值）<br>广泛使用的经典 MVS 方法：（MVS：Multi-View Stereo Reconstruction 多视点立体重建）</li><li>colmap：从 colmap 的输出点云中，使用 Screened Poisson Surface Reconstruction 重建 mesh</li></ul><p>UNISURF：将表面渲染与以占用场做场景表示的体积渲染统一起来</p><h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>设感兴趣的物体在单位球体内，每批采样 512 条光线，单个 RTX2080Ti：14h（with mask），16h（without mask）<br>对于 w/o mask ，使用 NeRF++对背景进行建模，网络架构和初始化方案与 IDR 相似</p><h2 id="环境配置："><a href="#环境配置：" class="headerlink" title="环境配置："></a>环境配置：</h2><p>autodl 镜像：<br>Miniconda  conda3<br>Python  3.8(ubuntu20.04)<br>Cuda  11.8<br><code>conda create -n neus python=3.8</code><br><code>pip install -r requirements.txt</code><br><code>pip --default-timeout=1000 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html</code></p><h2 id="命令行运行程序："><a href="#命令行运行程序：" class="headerlink" title="命令行运行程序："></a>命令行运行程序：</h2><p><code>python exp_runner.py --mode train --conf ./confs/wmask.conf --case &lt;case_name&gt;</code> # 带 mask 监督<br><code>python exp_runner.py --mode train --conf ./confs/womask.conf --case &lt;case_name&gt;</code> # 不带 mask 监督<br>case_name 是所选物体数据集文件夹的名称<br>如果训练过程中中断，可以设置<code>--is_continue</code> 来进行加载最后的 ckpt 继续训练：<br><code>python exp_runner.py --mode train --conf ./confs/wmask.conf --case &lt;case_name&gt; --is_continue</code></p><p>训练生成的结果会保存在根目录下 exp 文件夹下，可以看到 meshes 文件夹中保存了训练过程中的 mesh 模型，但面片较少。需要比较精细的 mesh 模型需要运行<br><code>python exp_runner.py --mode validate_mesh --conf &lt;config_file&gt; --case &lt;case_name&gt; --is_continue # use latest checkpoint</code><br>多视角渲染<br><code>python exp_runner.py --mode interpolate_&lt;img_idx_0&gt;_&lt;img_idx_1&gt; --conf &lt;config_file&gt; --case &lt;case_name&gt; --is_continue # use latest checkpoint</code></p><h3 id="eg1-clock-wmask"><a href="#eg1-clock-wmask" class="headerlink" title="eg1: clock_wmask"></a>eg1: clock_wmask</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python exp_runner.py --mode train --conf ./confs/wmask.conf --case bmvs_clock #带mask</span><br><span class="line">中断后继续训练</span><br><span class="line">python exp_runner.py --mode train --conf ./confs/wmask.conf --case bmvs_clock --is_continue</span><br><span class="line">将mesh模型精细化</span><br><span class="line">python exp_runner.py --mode validate_mesh --conf ./confs/wmask.conf --case bmvs_clock --is_continue</span><br><span class="line">插值多视角渲染成mp4(0~1之间新视角生成)</span><br><span class="line">python exp_runner.py --mode interpolate_000_001 --conf ./confs/wmask.conf --case bmvs_clock --is_continue</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Pasted%20image%2020230601130943.png" alt="Pasted image 20230601130943.png"></p><p>除了主体模型外还有一些噪音<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Pasted%20image%2020230601131053.png" alt="Pasted image 20230601131053.png"></p><p>Neus： (结果与 resolution 无关)</p><ul><li>使用 Neus 时，精细化模型参数设置为 <code>resolution=512</code> 可能与此有关</li><li>改为<code>resolution=1024</code> 运行一下 validate_mesh<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Pasted%20image%2020230602165940.png" alt="Pasted image 20230602165940.png"></li></ul><p>虽然运行后，面数更多了，但是细节处依然不清楚，可能需要继续增加训练的步数</p><ul><li>看代码后：resolution 可以增加 object 的分辨率，再缩放到物体实际大小<ul><li>可以看到 vertices 和 faces 都增加了，但由于 sdf 生成的相同，因此表面变得更精细，但细节处不清楚的地方依然不清楚</li></ul></li></ul><h3 id="eg2-bear-womask"><a href="#eg2-bear-womask" class="headerlink" title="eg2: bear_womask"></a>eg2: bear_womask</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python exp_runner.py --mode train --conf ./confs/womask.conf --case bmvs_bear  #没有mask</span><br><span class="line">python exp_runner.py --mode train --conf ./confs/womask.conf --case bmvs_bear  --is_continue</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Pasted%20image%2020230601211543.png" alt="Pasted image 20230601211543.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Pasted%20image%2020230601211823.png" alt="Pasted image 20230601211823.png"></p><p>运行后，mesh 模型的面很少</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">将mesh模型精细化</span><br><span class="line">python exp_runner.py --mode validate_mesh --conf ./confs/womask.conf --case bmvs_bear --is_continue</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Pasted%20image%2020230601211901.png" alt="Pasted image 20230601211901.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230601211955.png" alt="Pasted image 20230601211955.png"></p><h3 id="eg3-Miku"><a href="#eg3-Miku" class="headerlink" title="eg3: Miku"></a>eg3: Miku</h3><p>模型不够理想</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python exp_runner.py --mode train --conf ./confs/womask.conf --case Miku</span><br><span class="line">python exp_runner.py --mode validate_mesh --conf ./confs/womask.conf --case Miku --is_continue</span><br><span class="line"></span><br><span class="line">python exp_runner.py --mode interpolate_0_38 --conf ./confs/womask.conf --case Miku --is_continue</span><br></pre></td></tr></table></figure><p>效果：</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230630141311.png" alt="image.png"></p><ul><li>底下的圆盘在 sparse_points.ply 中没有剔除干净，所以会出现额外的突出</li><li>面部表情、裙子装饰等细小部位不够细致，腿部也不够细致</li><li>头发部位 and 鞋子也由于在点云中没有完全去除噪点，因此会有额外的噪声被建模出来</li></ul><p>0 to 38 render video：</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/00300000_0_38.gif" alt="00300000_0_38.gif"></p><h2 id="Neus-如何给模型加纹理："><a href="#Neus-如何给模型加纹理：" class="headerlink" title="Neus 如何给模型加纹理："></a>Neus 如何给模型加纹理：</h2><blockquote><p><a href="https://github.com/Totoro97/NeuS/issues/48">How to reconstruct texture after generating mesh ? · Issue #48 · Totoro97/NeuS (github.com)</a></p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"># get texture or say color</span><br><span class="line">def validate_mesh_vertex_color(self, world_space=False, resolution=64, threshold=0.0, name=None):</span><br><span class="line">    print(&#x27;Start exporting textured mesh&#x27;)</span><br><span class="line"></span><br><span class="line">    bound_min = torch.tensor(self.dataset.object_bbox_min, dtype=torch.float32)</span><br><span class="line">    bound_max = torch.tensor(self.dataset.object_bbox_max, dtype=torch.float32)</span><br><span class="line">    vertices, triangles = self.renderer.extract_geometry(bound_min, bound_max, resolution=resolution,</span><br><span class="line">                                                        threshold=threshold)</span><br><span class="line">    print(f&#x27;Vertices count: &#123;vertices.shape[0]&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">    vertices = torch.tensor(vertices, dtype=torch.float32) # n_t x 3</span><br><span class="line">    vertices_batch = vertices.split(self.batch_size) # n_t / batch_size x batch_size x 3</span><br><span class="line">    render_iter = len(vertices_batch) # n_t / batch_size</span><br><span class="line"></span><br><span class="line">    vertex_colors = []</span><br><span class="line">    for iter in tqdm(range(render_iter)):</span><br><span class="line">        feature_vector = self.sdf_network.sdf_hidden_appearance(vertices_batch[iter])[ : ,1:]</span><br><span class="line">        gradients = self.sdf_network.gradient(vertices_batch[iter]).squeeze()</span><br><span class="line">        dirs = -gradients</span><br><span class="line">        vertex_color = self.color_network(vertices_batch[iter], gradients, dirs,</span><br><span class="line">                                            feature_vector).detach().cpu().numpy()[..., ::-1]  # BGR to RGB</span><br><span class="line">        # .detach() ：将变量从网络中隔离开，不参与参数更新</span><br><span class="line">        vertex_colors.append(vertex_color)</span><br><span class="line">    vertex_colors = np.concatenate(vertex_colors)</span><br><span class="line">    print(f&#x27;validate point count: &#123;vertex_colors.shape[0]&#125;&#x27;)</span><br><span class="line">    vertices = vertices.detach().cpu().numpy()</span><br><span class="line"></span><br><span class="line">    if world_space:</span><br><span class="line">        vertices = vertices * self.dataset.scale_mats_np[0][0, 0] + self.dataset.scale_mats_np[0][:3, 3][None]</span><br><span class="line"></span><br><span class="line">    os.makedirs(os.path.join(self.base_exp_dir, &#x27;meshes&#x27;), exist_ok=True)</span><br><span class="line">    mesh = trimesh.Trimesh(vertices, triangles, vertex_colors=vertex_colors)</span><br><span class="line">    if name is not None:</span><br><span class="line">        mesh.export(os.path.join(self.base_exp_dir, &#x27;meshes&#x27;, f&#x27;&#123;name&#125;.ply&#x27;))</span><br><span class="line">    else:</span><br><span class="line">        mesh.export(os.path.join(self.base_exp_dir, &#x27;meshes&#x27;, &#x27;&#123;:0&gt;8d&#125;_vertex_color.ply&#x27;.format(self.iter_step)))</span><br><span class="line"></span><br><span class="line">    logging.info(&#x27;End&#x27;)</span><br></pre></td></tr></table></figure><ul><li>这里采用了<code>dirs = -gradients</code> 来做渲染网络输入的观察方向，渲染网络输出的 color 值不是很准确<ul><li>由于 Miku 的表面较为复杂，训练时其表面法向与观察方向并不在一条直线上，因此在推理过程中这样设置的话生成的 color 值不准</li></ul></li></ul><h2 id="渲染自定义视角的视频"><a href="#渲染自定义视角的视频" class="headerlink" title="渲染自定义视角的视频"></a>渲染自定义视角的视频</h2><p>根据中间两个 img 进行插值，中间插入生成的新视图图片</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python exp_runner.py --mode interpolate_&lt;img_idx_0&gt;_&lt;img_idx_1&gt; --conf &lt;config_file&gt; --case &lt;case_name&gt; --is_continue # use latest checkpoint</span><br><span class="line"></span><br><span class="line">eg:</span><br><span class="line">python exp_runner.py --mode train --conf ./confs/womask.conf --case bmvs_bear</span><br><span class="line">python exp_runner.py --mode interpolate_1_2 --conf ./confs/womask.conf --case bmvs_bear --is_continue # use latest checkpoint</span><br></pre></td></tr></table></figure><p>eg: 1 to 2<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/00300000_1_2.gif" alt="00300000_1_2.gif"></p><div style="display: flex; justify-content: center;"> <img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/001.png" alt="Image 1" style="width: 50%; height: auto; margin: 10px;"> to <img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/002.png" alt="Image 2" style="width: 50%; height: auto; margin: 10px;"> </div><h1 id="Neus-使用自制数据集"><a href="#Neus-使用自制数据集" class="headerlink" title="Neus 使用自制数据集"></a>Neus 使用自制数据集</h1><h2 id="自定义数据集-colmap-操作"><a href="#自定义数据集-colmap-操作" class="headerlink" title="自定义数据集 colmap 操作"></a>自定义数据集 colmap 操作</h2><p>自己拍一组照片: <strong>手机或者相机 绕 物体拍一周，每张的角度不要超过 30°（保证有 overlap 区域）</strong></p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/397760339">colmap 简介及入门级使用 - 知乎 (zhihu.com)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230531192515.png" alt="Pasted image 20230531192515.png"></p><p>打开 colmap.bat 进入 gui 界面，点击<strong>Reconstruction</strong>，再点击<strong>Automatic reconstruction</strong></p><p><strong>workspace folder：选择 workspace 文件夹，注意不支持中文路径</strong><br><strong>Image folder：选择存放多视角图像的数据文件夹，注意不支持中文路径</strong><br><strong>Data type：选择 Individual images</strong><br><strong>Quality：看需要选择，选择 High 重建花费的时间最长，重建的质量不一定最好；</strong></p><p>在 COLMAP 中看生成的稀疏点云<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230531191938.png" alt="Pasted image 20230531191938.png"></p><p>在 workspace folder 文件夹-&gt;dense-&gt;0 文件夹下找到 fused.ply 数据，用 meshlab 中打开可以看到稠密的三维重建的结果。<br>在 Meshlab 中查看生成的稠密点云</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/Pasted%20image%2020230531192204.png" alt="Pasted image 20230531192204.png"></p><h2 id="Neus-命令操作"><a href="#Neus-命令操作" class="headerlink" title="Neus 命令操作"></a>Neus 命令操作</h2><blockquote><p><a href="https://blog.csdn.net/mockbird123/article/details/129934066">(18 条消息) 基于 Nerf 的三维重建算法 Neus 初探_Alpha 狗蛋的博客-CSDN 博客</a></p></blockquote><p>两个文件夹：image 和 mask，一个文件<br>image 文件夹就是 rgb 图片数据，算法默认支持 png 格式。<br>mask 文件夹包含的是模型的前景图像，前景和后景以黑色和白色区分，如果配置文件选择 withou mask，其实这个文件夹的数据是没有意义的。但必须有文件，且名称、图像像素要和 image 的图像一一对应。<br>最后是 cameras_sphere.npz 文件，它包括了相机的属性和图像的位姿信息等，这个是需要我们自己计算的。官方给出了两种计算方案，第二种是用 colmap 计算 npz 文件。</p><h3 id="使用-Colmap-生成-npz-文件"><a href="#使用-Colmap-生成-npz-文件" class="headerlink" title="使用 Colmap 生成 npz 文件"></a>使用 Colmap 生成 npz 文件</h3><p>可以提前通过 colmap 运行得到 sparse/0/中的文件，或者通过 img2poses 中的 run_colmap()生成，然后再得到 sparse_points.ply</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd colmap_preprocess</span><br><span class="line">python imgs2poses.py $&#123;data_dir&#125;</span><br></pre></td></tr></table></figure><p>将会生成：<code>$&#123;data_dir&#125;/sparse_points.ply</code>，在 meshlab 中选择多余部分的 Vertices，并删除，然后保存为<code>$&#123;data_dir&#125;/sparse_points_interest.ply</code>.</p><p>然后</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python gen_cameras.py $&#123;data_dir&#125;</span><br></pre></td></tr></table></figure><p>就会在 ${data_dir}下生成 preprocessed，包括 image、mask 和 cameras_sphere.npz</p>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Multi-view/Implicit Function/NeRF-based </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SurfaceReconstruction </tag>
            
            <tag> NeRF </tag>
            
            <tag> NeuS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF原理</title>
      <link href="/3DReconstruction/Basic%20Knowledge/NeRF/NeRF-Principle/"/>
      <url>/3DReconstruction/Basic%20Knowledge/NeRF/NeRF-Principle/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>Title</th><th>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</th></tr></thead><tbody><tr><td>Author</td><td><a href="https://bmild.github.io/">Ben Mildenhall*</a><a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan*</a><a href="https://www.matthewtancik.com/">Matthew Tancik*</a><a href="https://jonbarron.info/">Jonathan T. Barron</a><a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a><a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a></td></tr><tr><td>Conf/Jour</td><td>ECCV 2020 Oral - Best Paper Honorable Mention</td></tr><tr><td>Year</td><td>2020</td></tr><tr><td>Project</td><td><a href="https://www.matthewtancik.com/nerf">NeRF: Neural Radiance Fields (matthewtancik.com)</a></td></tr><tr><td>Paper</td><td><a href="https://readpaper.com/pdf-annotate/note?pdfId=4544709973778259969&amp;noteId=752432474097127424">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (readpaper.com)</a></td></tr></tbody></table></div><p>NeRF（Neural Radiance Fields）是一种用于生成逼真三维场景的计算机图形学方法。通过神经网络对场景中的每个空间点进行建模，NeRF可以估计每个点的颜色和密度信息。利用渲染方程，NeRF能够合成高质量的逼真图像。相较于传统的渲染方法，NeRF能够处理复杂的光照和反射效果，广泛应用于虚拟现实、增强现实、电影制作和游戏开发等领域。然而，NeRF方法仍面临一些挑战，如计算复杂度和对训练数据的依赖性。研究人员正在不断改进NeRF，以提高其效率和扩展性。</p><span id="more"></span><blockquote><p>大佬的公式推导+代码分析<a href="https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html#prerequisites">NeRF: A Volume Rendering Perspective | Will (yconquesty.github.io)</a></p></blockquote><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Network.png" alt="Network.png|666"></p><p>输入多张不同视角的图片，通过训练出的MLP模型$F_{\theta}$，输出一个隐式表达的模型，该模型包含粒子的坐标、颜色和不透明度等信息。通过体渲染函数生成新视角的图片。</p><p>原理：</p><ol><li>从相机原点出发，经过图片上的某点像素，生成一条<a href="#光线生成">光线</a>  $r(t)=\textbf{o}+t \textbf{d}$ ，$\textbf{o}$为相机原点，$\textbf{d}$为光线的方向向量。</li><li>在光线上采样N个点，并得到这些点(粒子)的空间坐标xyz，同一条光线上，通过d也可得出粒子的方向坐标$(\theta,\phi)$。</li><li>对粒子坐标$(x,y,z,\theta,\phi)$做<a href="#位置编码">位置编码</a>，将低维信息转化为高维的信息。(神经网络在表示颜色和几何形状的高频变化方面表现不佳。他人的研究也表明深度神经网络倾向于学习低频信息，可以通过在输入之前将高频函数映射到高维空间的方法来增强高频信息的拟合能力。)</li><li>构建<a href="#神经网络">MLP网络</a>，输入为粒子坐标的高维信息，输出为粒子的RGB颜色和不透明度，然后根据<a href="#体渲染函数">体渲染函数</a>，由粒子的RGB和不透明度计算出图片像素的颜色值。loss为该图片像素颜色和ground truth的均方差损失。</li><li>根据粗采样后得到的网络，进行<a href="#分层采样">精采样</a>。由粗网络得到的点云模型，根据体渲染函数，计算出权重，对权重大的地方，采样的点多一点，根据精采样+粗采样得到的粒子，重复3、4步，最后训练出一个更精细的网络模型。</li><li>多张不同视角图片—&gt;MLP网络—&gt;隐式点云模型，确定的相机位姿—&gt;生成光线得到点云的坐标—&gt;隐式点云模型—&gt;点云的密度和颜色—&gt;新视角图片</li></ol><p>隐式表达模型：输入的是点云的位置坐标信息，输出时该点的不透明度和颜色值。根据某个确定的相机位姿以及图片的大小，每个像素生成一条光线，并在光线上进行采样得到点云，根据点云中每个点的位置坐标进行编码并输入进网络得到该点的密度值和颜色值，然后通过体渲染函数，计算出每个像素点处的颜色值，最终得到一张新视角出的图片。</p><h1 id="光线生成"><a href="#光线生成" class="headerlink" title="光线生成"></a>光线生成</h1><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Pasted%20image%2020230531151815.png" alt="Pasted image 20230531151815.png|666"></p><p>$r(t)=\textbf{o}+t\textbf{d}$  ，$\textbf{o}$是相机原点，$\textbf{d}$为方向向量</p><p>t通过 $t_{i} \sim \mathcal{U}\left[t_{n}+\frac{i-1}{N}\left(t_{f}-t_{n}\right), t_{n}+\frac{i}{N}\left(t_{f}-t_{n}\right)\right]$ ，在 $t_{near}$  和 $t_{far}$ 之间，等间距获取N个点</p><p>根据相机原点和图片上随机一点像素，生成一条光线，并将该光线的坐标转换到世界坐标系。</p><p>已知：图片大小，所选图片上的像素位置，相机参数(焦距、相机位姿)。光线经过坐标变换：图片二维的像素坐标构建—&gt;相机三维坐标—&gt;世界坐标，得到光线在世界坐标系下的原点和方向向量, 然后对ray_o和ray_d进行NDC坐标变换，将锥形区域变换为一个2x2x2的正方体区域。(LLFF数据集)</p><blockquote><p><a href="https://blog.csdn.net/weixin_38842821/article/details/125933604?spm=1001.2014.3001.5506">世界坐标系、相机坐标系和图像坐标系的转换_相机坐标系到图像坐标系_滴滴滴’cv的博客-CSDN博客</a></p></blockquote><p>从世界坐标系到相机坐标系的投影我们称为相机外参矩阵（反之为相机姿态矩阵），而从相机坐标系到像素坐标系的投影我们称为相机内参矩阵（用K来表示，由相机焦距与中心点决定）。<a href="https://zhuanlan.zhihu.com/p/553665958">旷视3d CV master系列训练营三：NeRF在实际场景中的应用 - 知乎 (zhihu.com)</a></p><h2 id="图片二维坐标—-gt-相机三维坐标-X-c-Y-c-Z-c"><a href="#图片二维坐标—-gt-相机三维坐标-X-c-Y-c-Z-c" class="headerlink" title="图片二维坐标—&gt;相机三维坐标 $X_{c}, Y_{c},Z_{c}$"></a>图片二维坐标—&gt;相机三维坐标 $X_{c}, Y_{c},Z_{c}$</h2><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/v2-f5e8824a9163b874e71166425d3e654c_720w.webp" alt="v2-f5e8824a9163b874e71166425d3e654c_720w.webp"></p><script type="math/tex; mode=display">\begin{bmatrix}X_c\\ Y_c\\ Z_c\end{bmatrix}=\mathbf{\begin{bmatrix}f_x&0&c_x\\ 0&f_y&c_y\\ 0&0&1\end{bmatrix}}^{-1} \begin{bmatrix}x\\ y\\ 1\end{bmatrix}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">相机内参矩阵</span><br><span class="line">K = np.array([</span><br><span class="line">    [focal, <span class="number">0</span>, <span class="number">0.5</span>*W],</span><br><span class="line">    [<span class="number">0</span>, focal, <span class="number">0.5</span>*H],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">])</span><br><span class="line"><span class="comment"># focal为焦距</span></span><br><span class="line">focal = <span class="number">.5</span> * W / np.tan(<span class="number">.5</span> * camera_angle_x)</span><br><span class="line"><span class="comment">#camera_angle_x在数据集的json文件中</span></span><br></pre></td></tr></table></figure><h2 id="相机三维坐标—-gt-世界三维坐标XYZ"><a href="#相机三维坐标—-gt-世界三维坐标XYZ" class="headerlink" title="相机三维坐标—&gt;世界三维坐标XYZ"></a>相机三维坐标—&gt;世界三维坐标XYZ</h2><script type="math/tex; mode=display">外参矩阵^{-1} = \begin{bmatrix}r_{11}&r_{12}&r_{13}&t_x\\ r_{21}&r_{22}&r_{23}&t_y\\ r_{31}&r_{32}&r_{33}&t_z\\ 0&0&0&1\end{bmatrix}</script><script type="math/tex; mode=display">\begin{bmatrix}X\\ Y\\ Z\\ 1\end{bmatrix}=\begin{bmatrix}r_{11}&r_{12}&r_{13}&t_x\\ r_{21}&r_{22}&r_{23}&t_y\\ r_{31}&r_{32}&r_{33}&t_z\\ 0&0&0&1\end{bmatrix}\begin{bmatrix}X_c\\ Y_c\\ Z_c\\ 1\end{bmatrix}</script><blockquote><p><a href="https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#background">NeRF: How NDC Works | Will (yconquesty.github.io)</a><br><a href="https://zhuanlan.zhihu.com/p/144329075">计算机图形学二：视图变换(坐标系转化，正交投影，透视投影，视口变换) - 知乎 (zhihu.com)</a></p></blockquote><h2 id="NDC-标准化设备坐标系"><a href="#NDC-标准化设备坐标系" class="headerlink" title="NDC  标准化设备坐标系"></a>NDC  标准化设备坐标系</h2><p>相机坐标系中坐标—&gt;投影变换—&gt;NDC中坐标</p><div class="note info">            <p>NeRF中是直接从世界坐标系转换到的NDC中的坐标</p>          </div><p>Projection transformation 分为 <strong>透视变换</strong>和<strong>正交变换</strong></p><h3 id="透视投影变换"><a href="#透视投影变换" class="headerlink" title="透视投影变换"></a>透视投影变换</h3><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230615160229.png" alt="image.png|555"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Pasted%20image%2020230612205212.png" alt="Pasted image 20230612205212.png|666"></p><h3 id="正交投影变换"><a href="#正交投影变换" class="headerlink" title="正交投影变换"></a>正交投影变换</h3><p>光线平行，将原空间中物体，变换到一个2x2x2的立方体中</p><h3 id="投影变换矩阵"><a href="#投影变换矩阵" class="headerlink" title="投影变换矩阵"></a>投影变换矩阵</h3><script type="math/tex; mode=display">\mathbf{M}_{\text{per}}=\begin{bmatrix}\frac{n}{r}&0&0&0\\ 0&\frac{n}{t}&0&0\\ 0&0&-\frac{f+n}{f-n}&-\frac{2nf}{f-n}\\ 0&0&-1&0\end{bmatrix}</script><p>将齐次坐标左乘投影矩阵，即可得到在NDC下的坐标：</p><script type="math/tex; mode=display">\begin{aligned}\begin{pmatrix}\frac{n}{r}&0&0\\ 0&\frac{n}{t}&0&0\\ 0&0&\frac{-(f+n)}{f-n}&\frac{-2fn}{f-n}\\ 0&0&-1&0\end{pmatrix}\begin{pmatrix}x\\ y\\ z\\ 1\end{pmatrix}& =\begin{pmatrix}\frac{n}{r}x\\ \frac{n}{t}y\\ \frac{-(f+n)}{f-n}z-\frac{-2fn}{f-n}\\ -z\end{pmatrix} \\\mathrm{project}& \to\begin{pmatrix}\frac{n}{r}\frac{x}{-z}\\ \frac{n}{t}\frac{y}{-z}\\ \frac{(f+n)}{f-n}-\frac{2fn}{f-n}\frac{1}{-z}\end{pmatrix} \end{aligned}</script><p>因此对于光线：</p><p>$\begin{pmatrix}a_x\frac{o_x+td_x}{o_z+td_z}\\ a_y\frac{o_y+td_y}{o_z+td_z}\\ a_z+\frac{b_z}{o_z+td_z}\end{pmatrix}=\begin{pmatrix}o_x’+t’d_x’\\ o_y’+t’d_y’\\ o_z’+t’d_z’\end{pmatrix}.$</p><p>其中：<br>$\begin{aligned}&amp;a_{x} :=-\frac{n}{r}  \\&amp;a_y :=-\frac nt  \\&amp;a_z :=\frac{f+n}{f-n}  \\&amp;b_z :=\frac{2nf}{f-n}\end{aligned}$</p><p>$\mathbf{o}’=\begin{pmatrix}o’_x\\ o’_y\\ o’_z\end{pmatrix}=\begin{pmatrix}a_x\frac{o_x}{o_z}\\ a_y\frac{o_y}{o_z}\\ a_z+\frac{b_z}{o_z}\end{pmatrix}=\pi(\mathbf{o}).$</p><p>交换顺序：</p><script type="math/tex; mode=display">\begin{aligned}\begin{pmatrix}t'd'_x\\ t'd'_y\\ t'd'_z\end{pmatrix}& =\begin{pmatrix}a_x\frac{o_x+t d_x}{o_z+t d_z}-a_x\frac{o_x}{o_z}\\ \\ a_y\frac{o_y+t d_y}{o_z+t d_z}-a_y\frac{o_y}{o_z}\\ \\ a_z+\frac{b_z}{o_z+t d_z}-a_z-\frac{b_z}{o_z}\end{pmatrix}  \\&=\begin{pmatrix}a_x\frac{o_z(o_x+td_x)-o_x(o_z+td_z)}{(o_z+td_z)o_z}\\ a_y\frac{o_z(o_y+td_y)-o_y(o_z+td_z)}{(o_z+td_z)o_z}\\ b_z\frac{o_z-(o_z+td_z)}{(o_z+td_z)o_z}\end{pmatrix} \\&=\begin{pmatrix}a_x\frac{td_z}{o_z+td_z}\left(\frac{d_x}{d_z}-\frac{o_x}{o_z}\right)\\ a_y\frac{td_z}{o_z+td_z}\left(\frac{d_y}{d_z}-\frac{o_y}{o_z}\right)\\ -b_z\frac{td_z}{o_z+td_z}\frac{1}{o_z}\end{pmatrix}\end{aligned}</script><p>可得：</p><p>$\begin{aligned}&amp; t^{\prime}=\frac{t d_{z}}{o_{z}+t d_{z}}=1-\frac{o_{z}}{o_{z}+t d_{z}}  \\&amp; \mathbf{d}^{\prime}=\left(\begin{matrix}{a_{x}\left(\frac{d_{x}}{d_{z}}-\frac{o_{x}}{o_{z}}\right)}\\ {o_{y}\left(\frac{d_{y}}{d_{z}}-\frac{o_{y}}{o_{z}}\right)}\\ {-b_{z}\frac{1}{o_{z}}}\end{matrix}\right). \end{aligned}$</p><p>又由：</p><p>$\begin{aligned}a_{x}&amp; =-\frac{n}{r}=\frac{f_\mathrm{camera}}{\frac{W}{2}}  \\a_y&amp; =-\frac{n}{t}=\frac{f_{\mathrm{camera}}}{\frac{H}{2}}  \\\lim\limits_{f\to\infty}a_z&amp; =\lim\limits_{f\to\infty}\frac{f+n}{f-n}=1  \\\lim\limits_{f\to\infty}b_z&amp; =\lim\limits_{f\to\infty}-\frac{2nf}{n-f}=2n \end{aligned}$</p><h3 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h3><script type="math/tex; mode=display">\begin{aligned}& \mathbf{o}^{\prime}=\left(\begin{matrix}{-\frac{f_{c a m}}{W/2}\frac{o_{x}}{o_{z}}}\\ {-\frac{f_{c a m}}{H/2}\frac{o_{y}}{o_{z}}}\\ {1+\frac{2n}{o_{z}}}\end{matrix}\right)  \\& \mathbf{d}^{\prime}=\left(\begin{matrix}{-\frac{f_{c a m}}{W/2}\left(\frac{d_{x}}{d_{z}}-\frac{o_{x}}{o_{z}}\right)}\\ {-\frac{f_{c a m}}{H/2}\left(\frac{d_{y}}{d_{z}}-\frac{o_{y}}{o_{z}}\right)}\\ {-2n\frac{1}{o_{z}}}\end{matrix}\right). \end{aligned}</script><div class="note info">            <ul><li>in NeRF :<br>One final detail in our implementation: we shift o to the ray’s intersection with the near plane at z = −n (before this NDC conversion) by taking on = o + tndfor tn = −(n + oz )/dz . Once we convert to the NDC ray, this allows us to simply sample t′ linearly from 0 to 1 in order to get a linear sampling in disparity fromn to ∞ in the original space.</li><li>TL;DR: 在NDC变换之前，将o移动到近平面z=-n的地方，令near=0，因此可以简单的从0线性采样t’到1，以便在原始空间中从 n 到 ∞ 的视差中获得线性采样</li><li>对应代码中的ndc_rays函数</li></ul>          </div><h1 id="体渲染函数"><a href="#体渲染函数" class="headerlink" title="体渲染函数"></a>体渲染函数</h1><script type="math/tex; mode=display">\mathrm{C}(r)=\int_{\mathrm{t}_{\mathrm{n}}}^{\mathrm{t}_{\mathrm{f}}} \mathrm{T}(\mathrm{t}) \sigma(\mathrm{r}(\mathrm{t})) \mathrm{c}(\mathrm{r}(\mathrm{t}), \mathrm{d}) \mathrm{dt}, \text { where } \mathrm{T}(\mathrm{t})=\exp \left(-\int_{\mathrm{t}_{\mathrm{n}}}^{\mathrm{t}} \sigma(\mathrm{r}(\mathrm{s})) \mathrm{ds}\right)</script><p>T(t)：$t_{n}$到t的透明度的累计</p><ul><li>光线距离越远 $\int_{t_n}^t\sigma(\mathbf{r}(s))ds$ 越大，T(t)就越小</li><li>T(t)作用：离相机近的不透明的粒子，会遮挡住后面的粒子，在渲染时的权重较大</li><li>类似：无偏，给定一束光线，在其表面处的点，占得权重应该最大；此外如果两个点，后面被前面堵塞，那么模型应该可以感知到。具体来说，前面的权重应该大于后面的权重。</li></ul><p><strong>主要思想：分段连续</strong></p><p>离散化：</p><script type="math/tex; mode=display">\hat{C}(\mathbf{r})=\sum_{i=1}^{N} T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right) \mathbf{c}_{i} \text {, where } T_{i}=\exp \left(-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}\right)</script><script type="math/tex; mode=display">t_{i} \sim \mathcal{U}\left[t_{n}+\frac{i-1}{N}\left(t_{f}-t_{n}\right), t_{n}+\frac{i}{N}\left(t_{f}-t_{n}\right)\right]</script><p>某个点的不透明度  $\sigma_i$</p><p>该点与相邻点之间的距离 $\delta_{i} = t_{i+1} - t_{i}$</p><p>体渲染函数根据光线上采样点的RGB和不透明度，得到该光线所经过的图片像素的颜色值。</p><h1 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h1><p>神经网络在表示颜色和几何形状的高频变化方面表现不佳。他人的研究也表明深度神经网络倾向于学习低频信息，可以通过在输入之前将高频函数映射到高维空间的方法来增强高频信息的拟合能力。</p><script type="math/tex; mode=display">\gamma(p)=\left(\sin \left(2^{0} \pi p\right), \cos \left(2^{0} \pi p\right), \cdots, \sin \left(2^{L-1} \pi p\right), \cos \left(2^{L-1} \pi p\right)\right)</script><ul><li>空间坐标xyz，L=10，10组sin和cos，总共转化为<code>20*3</code> =60个向量</li><li>方向两项$\theta \phi$，L=4，4组sin和cos，总共转化为<code>8*3</code> =24个向量(为了方便计算，使用相机7的xyz参数替代)</li></ul><h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p>网络：MLP<br>输入：$(x,y,z,\theta,\phi)$编码后的信息</p><ol><li>粒子的空间坐标 xyz</li><li>粒子的方向坐标 $(\theta,\phi)$</li></ol><p>输出：$R G B$   ${\sigma}$</p><ol><li>粒子的RGB颜色，由空间和方向信息共同得出</li><li>粒子的不透明度${\sigma}$，仅由空间信息得出，粒子的不透明度与观察方向无关</li></ol><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/Pasted%20image%2020221206180113.png" alt="Pasted image 20221206180113.png|600"></p><h1 id="分层采样"><a href="#分层采样" class="headerlink" title="分层采样"></a>分层采样</h1><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/20230615134639.png" alt="image.png"></p><p>目的：目标区域采样点多，其他区域采样点少</p><script type="math/tex; mode=display">t_{i} \sim \mathcal{U}\left[t_{n}+\frac{i-1}{N}\left(t_{f}-t_{n}\right), t_{n}+\frac{i}{N}\left(t_{f}-t_{n}\right)\right]</script><ol><li><p>粗网络采样：$N_{c}$</p><script type="math/tex; mode=display">\hat{C}(\mathbf{r})=\sum_{i=1}^{N} T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right) \mathbf{c}_{i} \text {, where } T_{i}=\exp \left(-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}\right)</script></li><li><p>采样$N_{c}$个点：</p><script type="math/tex; mode=display">\hat{C}_{c}(\mathbf{r})=\sum_{i=1}^{N_{c}} w_{i} c_{i}, \quad w_{i}=T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right)</script></li><li><p>对权重作标准化：</p><script type="math/tex; mode=display">\hat{w}_{i}=w_{i} / \sum_{j=1}^{N_{c}} w_{j}</script><p>产生沿射线的分段常数概率分布函数(piecewise-constant PDF)，从此分布中，利用逆变换采样采样出$N_{f}$个位置</p></li><li><p>精网络采样：<script type="math/tex">N_{c} + N_{f}</script></p></li><li><p>使用所有的$N_{c}$和$N_{f}$来计算最终的渲染颜色</p><script type="math/tex; mode=display">\hat{C}(\mathbf{r})=\sum_{i=1}^{N} T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right) \mathbf{c}_{i} \text {, where } T_{i}=\exp \left(-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}\right)</script></li></ol><hr><blockquote><p>参考资料</p><ul><li>Video:<br><a href="https://www.bilibili.com/video/BV1d841187tn/">NeRF源码解析_哔哩哔哩_bilibili</a><br><a href="https://zhuanlan.zhihu.com/p/628118376">论文解读：《NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis》 - 知乎 (zhihu.com)</a><br><a href="https://www.bilibili.com/video/BV1fL4y1T7Ag/">【原创】NeRF 三维重建 神经辐射场 建模 算法详解 NeRF相关项目汇总介绍。_哔哩哔哩_bilibili</a><br><a href="https://www.bilibili.com/video/BV1Nc411T7Jh/">光线采集+归一化采样点_哔哩哔哩_bilibili</a></li><li>Blog:<br><a href="https://dl.acm.org/doi/pdf/10.1145/3503250">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a><br><a href="https://zhuanlan.zhihu.com/p/80726483">逆变换采样 - 知乎 (zhihu.com)</a><br><a href="https://zhuanlan.zhihu.com/p/588902982">NeRF 的实现过程 - 知乎 (zhihu.com)</a><br><a href="https://zhuanlan.zhihu.com/p/486686928">新视角合成 (Novel View Synthesis) - (4) NeRF 实现细节 - 知乎 (zhihu.com)</a><br><a href="https://www.zhihu.com/column/c_1490274731060760576">计算机图形学 - 知乎 (zhihu.com)</a></li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 3DReconstruction/Basic Knowledge/NeRF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python调用LOL的API</title>
      <link href="/Learn/Learn-Python-LCU/"/>
      <url>/Learn/Learn-Python-LCU/</url>
      
        <content type="html"><![CDATA[<p>LOL提供了丰富的接口，可以利用这些接口来完成一些操作。使用Python来调用LOL提供的API，可以使用一些大佬造的轮子。</p><span id="more"></span><p>拳头提供的API接口说明可以在<a href="https://developer.riotgames.com/">开发者网站</a>中查看，可以使用拳头账号获得一个api key来调用api，且key需要每天进行更换。</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/riot_developer.png" alt="riot_developer.png"><br><img src="" alt="Uploading file...i2f0f"></p><p><a href="https://developer.riotgames.com/apis">网站API</a></p><ul><li><a href="http://www.mingweisamuel.com/lcu-schema/tool/#/">抓包得到的API</a></li></ul><p>拳头的开发者网站文档：<br>在<a href="https://developer.riotgames.com/docs/lol">开发者网站doc</a>可以得到一些json数据，如当前版本全部英雄的信息，全部装备的信息等等</p><h1 id="1-RiotWatcher"><a href="#1-RiotWatcher" class="headerlink" title="1.RiotWatcher"></a>1.RiotWatcher</h1><p><a href="https://riot-watcher.readthedocs.io/en/latest/">RiotWatcher</a>参考知乎大佬<a href="https://www.zhihu.com/people/luo-po-gong-zi">京暮研Shinra</a>的<a href="https://www.zhihu.com/column/c_1376912368698499072">教程</a>来进行初步学习</p><h2 id="1-1-使用方式"><a href="#1-1-使用方式" class="headerlink" title="1.1 使用方式"></a>1.1 使用方式</h2><ul><li>安装<code>pip install riotwatcher</code></li><li>使用</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> riotwatcher <span class="keyword">import</span> LolWatcher</span><br><span class="line"><span class="comment"># api-key需要自己去官网上获得</span></span><br><span class="line">lol_watcher = LolWatcher(<span class="string">&#x27;&lt;your-api-key&gt;&#x27;</span>)</span><br></pre></td></tr></table></figure><p>定义完成后，就可以使用lol_watcher来调用其他的类库</p><ul><li><a href="https://riot-watcher.readthedocs.io/en/latest/riotwatcher/LeagueOfLegends/index.html">League of Legends Watcher</a></li><li>Legends Of Runeterra Watcher</li><li>Riot Watcher</li><li>Team Fight Tactics Watcher</li><li>Valorant Watcher</li><li>Handlers</li><li>Testing</li></ul><p>每个类下拥有很多个函数，调用不同的API</p><h2 id="1-2-League-of-Legends-Watcher"><a href="#1-2-League-of-Legends-Watcher" class="headerlink" title="1.2 League of Legends Watcher"></a>1.2 League of Legends Watcher</h2><p>champion：英雄<br>summoner：召唤师</p><h3 id="1-2-1-获得某个区服的免费英雄"><a href="#1-2-1-获得某个区服的免费英雄" class="headerlink" title="1.2.1 获得某个区服的免费英雄"></a>1.2.1 获得某个区服的免费英雄</h3><p><a href="https://riot-watcher.readthedocs.io/en/latest/riotwatcher/LeagueOfLegends/ChampionApiV3.html#riotwatcher._apis.league_of_legends.ChampionApiV3">champion</a></p><p>使用<code>ChampionInfo = lol_watcher.champion.rotations(region: str)</code><br>需要输入一个服务器地区名称region，如日服jp1，并且返回一个ChampionInfo值</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">region = [<span class="string">&#x27;kr&#x27;</span>,<span class="string">&#x27;jp1&#x27;</span>,<span class="string">&#x27;br1&#x27;</span>,<span class="string">&#x27;eun1&#x27;</span>,<span class="string">&#x27;euw1&#x27;</span>,<span class="string">&#x27;la1&#x27;</span>,<span class="string">&#x27;la2&#x27;</span>,<span class="string">&#x27;na1&#x27;</span>,<span class="string">&#x27;oc1&#x27;</span>,<span class="string">&#x27;tr1&#x27;</span>,<span class="string">&#x27;ru&#x27;</span>]</span><br><span class="line">lol_region = region[<span class="number">1</span>]</span><br><span class="line">champion_kr = lol_watcher.champion.rotations(lol_region)</span><br><span class="line"><span class="built_in">print</span>(champion_kr)</span><br><span class="line"></span><br><span class="line"><span class="comment">#return:</span></span><br><span class="line">&#123;<span class="string">&#x27;freeChampionIds&#x27;</span>: [<span class="number">21</span>, <span class="number">33</span>, <span class="number">50</span>, <span class="number">57</span>, <span class="number">80</span>, <span class="number">81</span>, <span class="number">107</span>, <span class="number">111</span>, <span class="number">113</span>, <span class="number">202</span>, <span class="number">240</span>, <span class="number">246</span>, <span class="number">350</span>, <span class="number">497</span>, <span class="number">518</span>, <span class="number">875</span>], <span class="string">&#x27;freeChampionIdsForNewPlayers&#x27;</span>: [<span class="number">222</span>, <span class="number">254</span>, <span class="number">427</span>, <span class="number">82</span>, <span class="number">131</span>, <span class="number">147</span>, <span class="number">54</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">37</span>], <span class="string">&#x27;maxNewPlayerLevel&#x27;</span>: <span class="number">10</span>&#125;</span><br></pre></td></tr></table></figure><p>返回的ChampionInfo中</p><ul><li>freeChampionIds 免费英雄的ID</li><li>freeChampionIdsForNewPlayers 给新手玩家的免费英雄</li></ul><p>可以通过ID来查看英雄的名字，使用<a href="http://ddragon.leagueoflegends.com/cdn/12.9.1/data/zh_CN/champion.json">拳头官网的json文件</a><br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/2022-06-09-10-02-03.png" alt="2022-06-09-10-02-03.png"></p><p>实现思路：</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220609214627.png" alt="20220609214627" title="实现思路"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在官网获取英雄的json数据，zh_CN：中文数据</span></span><br><span class="line">champion_json_url = <span class="string">&quot;http://ddragon.leagueoflegends.com/cdn/12.9.1/data/zh_CN/champion.json&quot;</span></span><br><span class="line"><span class="comment"># 使用request的get获得</span></span><br><span class="line">r1 = requests.get(champion_json_url)</span><br><span class="line"><span class="comment"># 只需要data中的数据</span></span><br><span class="line">champ_data = r1.json()[<span class="string">&quot;data&quot;</span>]</span><br><span class="line"><span class="comment"># 处理json数据，使用pandas的json_normalize</span></span><br><span class="line">champ_df = pd.json_normalize(champ_data.values(),sep=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取出champion返回值中的freeChampionIds </span></span><br><span class="line">champions_kr_free = champion_kr[<span class="string">&#x27;freeChampionIds&#x27;</span>]</span><br><span class="line"><span class="comment"># 将freeChampionIds列表中的int数据转换成字符串并保存在champions_kr_free_str中</span></span><br><span class="line">champions_kr_free_str = []</span><br><span class="line"><span class="keyword">for</span> champion_kr_free <span class="keyword">in</span> champions_kr_free:</span><br><span class="line">    champion_kr_free_str = <span class="built_in">str</span>(champion_kr_free)</span><br><span class="line">    champions_kr_free_str.append(champion_kr_free_str)</span><br><span class="line"><span class="comment"># 在json文件的key值中搜索champions_kr_free_str，搜索到后将其提取出来，并打印name和title两列</span></span><br><span class="line"><span class="built_in">print</span>(champ_df[champ_df[<span class="string">&#x27;key&#x27;</span>].isin(champions_kr_free_str)][[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;title&#x27;</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">        name    title</span><br><span class="line"><span class="number">28</span>      探险家  伊泽瑞尔</span><br><span class="line"><span class="number">48</span>      戏命师     烬</span><br><span class="line"><span class="number">61</span>     暴怒骑士    克烈</span><br><span class="line"><span class="number">73</span>     扭曲树精    茂凯</span><br><span class="line"><span class="number">75</span>     赏金猎人  厄运小姐</span><br><span class="line"><span class="number">81</span>     深海泰坦  诺提勒斯</span><br><span class="line"><span class="number">82</span>     万花通灵    妮蔻</span><br><span class="line"><span class="number">89</span>     不屈之枪    潘森</span><br><span class="line"><span class="number">92</span>     元素女皇   奇亚娜</span><br><span class="line"><span class="number">94</span>       幻翎     洛</span><br><span class="line"><span class="number">95</span>     披甲龙龟   拉莫斯</span><br><span class="line"><span class="number">100</span>   傲之追猎者  雷恩加尔</span><br><span class="line"><span class="number">105</span>    北地之怒   瑟庄妮</span><br><span class="line"><span class="number">108</span>      腕豪    瑟提</span><br><span class="line"><span class="number">118</span>  诺克萨斯统领   斯维因</span><br><span class="line"><span class="number">151</span>    魔法猫咪    悠米</span><br></pre></td></tr></table></figure><p><strong>问题</strong></p><p>pandas在pycharm中使用时，在打印列表式，单元格内容会显示不全，采用如下方法来处理<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="literal">None</span>)   <span class="comment"># 显示完整的列</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_rows&#x27;</span>, <span class="literal">None</span>)  <span class="comment"># 显示完整的行</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.expand_frame_repr&#x27;</span>, <span class="literal">False</span>)  <span class="comment"># 设置不折叠数据</span></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_colwidth&#x27;</span>, <span class="number">200</span>)</span><br></pre></td></tr></table></figure></p><h3 id="1-2-2-获得某个召唤师的英雄熟练度等信息"><a href="#1-2-2-获得某个召唤师的英雄熟练度等信息" class="headerlink" title="1.2.2 获得某个召唤师的英雄熟练度等信息"></a>1.2.2 获得某个召唤师的英雄熟练度等信息</h3><p><a href="https://riot-watcher.readthedocs.io/en/latest/riotwatcher/LeagueOfLegends/ChampionMasteryApiV4.html#riotwatcher._apis.league_of_legends.ChampionMasteryApiV4">champion_mastery</a></p><p><strong>有三种获取方法可以选择</strong></p><h4 id="Ⅰ-by-summoner"><a href="#Ⅰ-by-summoner" class="headerlink" title="Ⅰ.by_summoner"></a>Ⅰ.by_summoner</h4><p><em>获得所有英雄的熟练度信息</em></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/2022-06-09-11-21-43.png" alt="2022-06-09-11-21-43.png"></p><p>使用：<code>lol_watcher.champion_mastery.by_summoner(region,summoner_id)</code><br>return: List[ChampionMasteryDTO]: This object contains a list of Champion Mastery information for player and champion combination.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">faker_champion_master = lol_watcher.champion_mastery.by_summoner(region,faker_id)</span><br><span class="line"><span class="built_in">print</span>(faker_champion_master)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;championId&quot;</span>: <span class="number">7</span>,</span><br><span class="line">        <span class="string">&quot;championLevel&quot;</span>: <span class="number">7</span>,</span><br><span class="line">        <span class="string">&quot;championPoints&quot;</span>: <span class="number">493741</span>,</span><br><span class="line">        <span class="string">&quot;lastPlayTime&quot;</span>: <span class="number">1653499775000</span>,</span><br><span class="line">        <span class="string">&quot;championPointsSinceLastLevel&quot;</span>: <span class="number">472141</span>,</span><br><span class="line">        <span class="string">&quot;championPointsUntilNextLevel&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;chestGranted&quot;</span>: true,</span><br><span class="line">        <span class="string">&quot;tokensEarned&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;summonerId&quot;</span>: <span class="string">&quot;ht4SB4hyU2CyiERUxtIE6ZD4bQbG4Djo0Hbrl2hU0ilBxg&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;&#125;,...&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>返回数据包括</p><ul><li>英雄id</li><li>英雄熟练度等级</li><li>英雄分数</li><li>最后使用时间</li><li>是否有战利品奖励（使用该英雄的对局中有人获得S-以上评分）<br>等等</li></ul><h4 id="Ⅱ-by-summoner-by-champion"><a href="#Ⅱ-by-summoner-by-champion" class="headerlink" title="Ⅱ.by_summoner_by_champion"></a>Ⅱ.by_summoner_by_champion</h4><p><em>获得特定英雄的熟练度信息</em></p><p>使用：<code>lol_watcher.champion_mastery.by_summoner_by_champion(region,summoner_id,champion_id)</code><br>return: ChampionMasteryDTO: This object contains single Champion Mastery information for player and champion combination.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">faker_champion_master_7 = lol_watcher.champion_mastery.by_summoner_by_champion(region,faker_id,<span class="number">7</span>)</span><br><span class="line"><span class="built_in">print</span>(faker_champion_master_7)</span><br><span class="line"></span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">&#123;<span class="string">&#x27;championId&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;championLevel&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;championPoints&#x27;</span>: <span class="number">493741</span>, <span class="string">&#x27;lastPlayTime&#x27;</span>: <span class="number">1653499775000</span>, <span class="string">&#x27;championPointsSinceLastLevel&#x27;</span>: <span class="number">472141</span>, <span class="string">&#x27;championPointsUntilNextLevel&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;chestGranted&#x27;</span>: <span class="literal">True</span>, <span class="string">&#x27;tokensEarned&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;summonerId&#x27;</span>: <span class="string">&#x27;ht4SB4hyU2CyiERUxtIE6ZD4bQbG4Djo0Hbrl2hU0ilBxg&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="Ⅲ-scores-by-summoner"><a href="#Ⅲ-scores-by-summoner" class="headerlink" title="Ⅲ.scores_by_summoner"></a>Ⅲ.scores_by_summoner</h4><p><em>获得玩家的总冠军精通分数，即每个冠军精通等级的总和</em></p><p>使用：<code>lol_watcher.champion_mastery.scores_by_summoner(region,summoner_id)</code></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">faker_champion_master_scores = lol_watcher.champion_mastery.scores_by_summoner(region,faker_id)</span><br><span class="line"><span class="built_in">print</span>(faker_champion_master_scores)</span><br><span class="line"></span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="number">675</span></span><br></pre></td></tr></table></figure><h3 id="1-2-4-clash"><a href="#1-2-4-clash" class="headerlink" title="1.2.4 clash"></a>1.2.4 clash</h3><h3 id="1-2-5-data-dragon"><a href="#1-2-5-data-dragon" class="headerlink" title="1.2.5 data_dragon"></a>1.2.5 data_dragon</h3><h3 id="1-2-6-league"><a href="#1-2-6-league" class="headerlink" title="1.2.6 league"></a>1.2.6 league</h3><h3 id="1-2-7-lol-status"><a href="#1-2-7-lol-status" class="headerlink" title="1.2.7 lol_status"></a>1.2.7 lol_status</h3><h3 id="1-2-8-lol-status-v3"><a href="#1-2-8-lol-status-v3" class="headerlink" title="1.2.8 lol_status_v3"></a>1.2.8 lol_status_v3</h3><h3 id="1-2-9-lol-status-v4"><a href="#1-2-9-lol-status-v4" class="headerlink" title="1.2.9 lol_status_v4"></a>1.2.9 lol_status_v4</h3><h3 id="1-2-10-match"><a href="#1-2-10-match" class="headerlink" title="1.2.10 match"></a>1.2.10 match</h3><h3 id="1-2-11-match-v4"><a href="#1-2-11-match-v4" class="headerlink" title="1.2.11 match_v4"></a>1.2.11 match_v4</h3><h3 id="1-2-12-match-v5"><a href="#1-2-12-match-v5" class="headerlink" title="1.2.12 match_v5"></a>1.2.12 match_v5</h3><h3 id="1-2-13-spectator"><a href="#1-2-13-spectator" class="headerlink" title="1.2.13 spectator"></a>1.2.13 spectator</h3><h3 id="1-2-14-查询一名召唤师的信息"><a href="#1-2-14-查询一名召唤师的信息" class="headerlink" title="1.2.14 查询一名召唤师的信息"></a>1.2.14 查询一名召唤师的信息</h3><p><a href="https://riot-watcher.readthedocs.io/en/latest/riotwatcher/LeagueOfLegends/SummonerApiV4.html#riotwatcher._apis.league_of_legends.SummonerApiV4">summoner</a></p><p><strong>可以通过四种方式查询</strong></p><p>使用：<code>summonerDTO = lol_watcher.summoner.by_xxxx(region,xxxx)</code><br>return:    SummonerDTO: represents a summoner</p><h4 id="Ⅰ-by-account"><a href="#Ⅰ-by-account" class="headerlink" title="Ⅰ.by_account"></a>Ⅰ.by_account</h4><h4 id="Ⅱ-by-id"><a href="#Ⅱ-by-id" class="headerlink" title="Ⅱ.by_id"></a>Ⅱ.by_id</h4><h4 id="Ⅲ-by-name"><a href="#Ⅲ-by-name" class="headerlink" title="Ⅲ.by_name"></a>Ⅲ.by_name</h4><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/2022-06-09-11-06-41.png" alt="2022-06-09-11-06-41.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">region = <span class="string">&quot;kr&quot;</span></span><br><span class="line">summoner_name = <span class="string">&#x27;Hide on bush&#x27;</span></span><br><span class="line"></span><br><span class="line">summoner_faker = lol_watcher.summoner.by_name(region, summoner_name)</span><br><span class="line"><span class="built_in">print</span>(summoner_faker)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">&#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;ht4SB4hyU2CyiERUxtIE6ZD4bQbG4Djo0Hbrl2hU0ilBxg&#x27;</span>, <span class="string">&#x27;accountId&#x27;</span>: <span class="string">&#x27;U9Wyp4etrbQuDFz3dqh8pA9HvqhLm1t7tKcwJbK13q1S&#x27;</span>, <span class="string">&#x27;puuid&#x27;</span>: <span class="string">&#x27;f51LYPjnONGFBPsvPdcXZVLr8JvlvVTb4SvGH0xZqGVe3sVsrUKoKZvlKciTh9xkLVl4npQ83zFLWQ&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Hide on bush&#x27;</span>, <span class="string">&#x27;profileIconId&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;revisionDate&#x27;</span>: <span class="number">1654345192719</span>, <span class="string">&#x27;summonerLevel&#x27;</span>: <span class="number">571</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="Ⅳ-by-puuid"><a href="#Ⅳ-by-puuid" class="headerlink" title="Ⅳ.by_puuid"></a>Ⅳ.by_puuid</h4><h3 id="1-2-15-third-party-code"><a href="#1-2-15-third-party-code" class="headerlink" title="1.2.15 third_party_code"></a>1.2.15 third_party_code</h3><h1 id="2-lcu-driver"><a href="#2-lcu-driver" class="headerlink" title="2.lcu-driver"></a>2.lcu-driver</h1><p><a href="https://lcu-driver.readthedocs.io/en/latest/index.html">lcu-driver</a></p>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> LOL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo 简单流程</title>
      <link href="/Learn/Learn-Hexo2/"/>
      <url>/Learn/Learn-Hexo2/</url>
      
        <content type="html"><![CDATA[<p>Hexo博客搭建教程，参考<a href="https://space.bilibili.com/384068749">CodeSheep</a>大佬的<a href="https://www.bilibili.com/video/BV1Yb411a7ty">视频教程</a>。主要是记录Hexo博客搭建和部署的步骤流程，方便更加快速的配置部署。<br><span id="more"></span></p><blockquote><p><a href="https://www.bilibili.com/video/BV1Yb411a7ty">手把手教你从0开始搭建自己的个人博客 |无坑版视频教程| hexo_哔哩哔哩_bilibili</a></p></blockquote><h1 id="1-下载安装node-js"><a href="#1-下载安装node-js" class="headerlink" title="1.下载安装node.js"></a>1.下载安装node.js</h1><blockquote><p><a href="http://nodejs.cn/">Node.js 中文网 (nodejs.cn)</a></p></blockquote><p><em>安装完成后，查看版本</em><br><code>npm -v</code><br><code>node -v</code></p><h1 id="2-换阿里的源"><a href="#2-换阿里的源" class="headerlink" title="2.换阿里的源"></a>2.换阿里的源</h1><p><code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code></p><p><em>完成cnpm的安装，查看版本</em><br><code>cnpm -v</code></p><h1 id="3-安装hexo博客框架"><a href="#3-安装hexo博客框架" class="headerlink" title="3.安装hexo博客框架"></a>3.安装hexo博客框架</h1><p><code>cnpm install -g hexo-cli</code> 最好不要用，替换为<code>npm install</code>来根据package-lock.json中的配置来安装环境</p><div class="note default">            <p>-g 全局安装<br>-v 查看版本</p>          </div><h1 id="4-新建一个hexo博客"><a href="#4-新建一个hexo博客" class="headerlink" title="4.新建一个hexo博客"></a>4.新建一个hexo博客</h1><ul><li>新建文件夹<em>Blog</em>(任意一个名字)</li><li>在该目录下打开终端输入<code>hexo init</code>完成博客初始化生成</li></ul><h1 id="5-在本地启动博客"><a href="#5-在本地启动博客" class="headerlink" title="5.在本地启动博客"></a>5.在本地启动博客</h1><p><code>hexo server</code> or <code>hexo s</code></p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p><code>hexo s</code>后无报错，但是打开本地端口后网页出现错误，显示此站点的连接不安全</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/2022-06-09-09-07-42.png" alt="2022-06-09-09-07-42.png" title="由于http自动变成https"></p><div class="note warning">            <p>注意：是在<a href="http://localhost:4000/">http://localhost:4000/</a> 上进行，而不是https </p>          </div><p>我的默认浏览器时Edge，但是Edge会自动将http转换成https，导致无法在本地查看Hexo博客，换成Chrome后可以解决。</p><h1 id="6-新建博客文章"><a href="#6-新建博客文章" class="headerlink" title="6.新建博客文章"></a>6.新建博客文章</h1><p><code>hexo n &quot;文章题目&quot;</code></p><p>写完一篇文章后</p><ul><li>清理一下<code>hexo clean</code></li><li>重新生成<code>hexo g</code></li></ul><h1 id="7-部署到github上"><a href="#7-部署到github上" class="headerlink" title="7.部署到github上"></a>7.部署到github上</h1><h2 id="7-1-在github上新建一个仓库"><a href="#7-1-在github上新建一个仓库" class="headerlink" title="7.1 在github上新建一个仓库"></a>7.1 在github上新建一个仓库</h2><p>仓库名字：<code>用户名.github.io</code></p><h2 id="7-2-在Blog文件夹下安装deployer"><a href="#7-2-在Blog文件夹下安装deployer" class="headerlink" title="7.2 在Blog文件夹下安装deployer"></a>7.2 在Blog文件夹下安装deployer</h2><p><code>cnpm install --save hexo-deployer-git</code></p><h2 id="7-3-在Blog文件夹下打开-config-yml文件"><a href="#7-3-在Blog文件夹下打开-config-yml文件" class="headerlink" title="7.3 在Blog文件夹下打开_config.yml文件"></a>7.3 在Blog文件夹下打开_config.yml文件</h2><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/one-command-deployment</span></span><br><span class="line"></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line"></span><br><span class="line">  <span class="string">type:</span> <span class="string">&#x27;git&#x27;</span></span><br><span class="line"></span><br><span class="line">  <span class="string">repo:</span> <span class="string">&#x27;https://github.com/yq010105/yq010105.github.io.git&#x27;</span></span><br><span class="line"></span><br><span class="line">  <span class="string">branch:</span> <span class="string">&#x27;master&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="7-4-然后将Blog部署到github中"><a href="#7-4-然后将Blog部署到github中" class="headerlink" title="7.4 然后将Blog部署到github中"></a>7.4 然后将Blog部署到github中</h2><p><code>hexo d</code></p><h1 id="Last"><a href="#Last" class="headerlink" title="Last"></a>Last</h1><p>使用Hexo博客方法<br>在写完文章后：</p><ul><li><code>hexo clean</code>清除</li><li><code>hexo g</code>生成</li><li><code>hexo s</code>在本地预览，在<a href="http://localhost:4000/">http://localhost:4000/</a> 中查看</li><li><code>hexo d</code>推到github上，在<a href="https://yq010105.github.io/">Hexo (yq010105.github.io)</a>中查看</li></ul><h1 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h1><h2 id="Next主题内置标签-Callout"><a href="#Next主题内置标签-Callout" class="headerlink" title="Next主题内置标签 Callout"></a>Next主题内置标签 Callout</h2><p><code>&#123;% note class_name %&#125; Content (md partial supported) &#123;% endnote %&#125;</code><br>其中，class_name 可以是以下列表中的一个值：</p><ul><li>default</li><li>primary</li><li>success</li><li>info</li><li>warning</li><li>danger</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220609172943.png" alt="20220609172943.png" title="内置标签预览"></p><div class="note default">            <p>default</p>          </div><div class="note primary">            <p>primary</p>          </div><div class="note success">            <p>success</p>          </div><div class="note info">            <p>info</p>          </div><div class="note warning">            <p>warning</p>          </div><div class="note danger">            <p>danger</p>          </div>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Arduino学习</title>
      <link href="/Learn/Learn-Arduino/"/>
      <url>/Learn/Learn-Arduino/</url>
      
        <content type="html"><![CDATA[<p>Arduino学习</p><span id="more"></span><h1 id="1-入门"><a href="#1-入门" class="headerlink" title="1. 入门"></a>1. 入门</h1><ul><li>数字IO</li><li>模拟IO</li><li>串口IO</li></ul><p><strong>输入设备 Input</strong></p><ul><li><p>开关功能:某种状态下接通电路<br>类型:数字输入(开或关)</p></li><li><p>电位计功能:调节旋转程度<br>类型:模拟输入(旋转不同值不同)</p></li><li><p>蓝牙从功能:无线接收数据<br>类型:串口输入(向Arduino发送信息)</p></li></ul><p><strong>输出设备 Output</strong></p><ul><li><p>LED功能:指示亮或灭<br>类型:数字输出</p></li><li><p>马达功能:转动<br>类型:模拟输出(调整转速)</p></li><li><p>蓝牙主功能:无线发送数据<br>类型:串口输出<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/arduino.png" alt="arduino.png"></p></li></ul><h2 id="1-1-注释"><a href="#1-1-注释" class="headerlink" title="1.1 注释"></a>1.1 注释</h2><p><code>/* */</code> 注释<br><code>int a = 0;</code> 要有分号结尾</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">setup</span><span class="params">()</span> &#123;</span><br><span class="line">启动时/按下复位后运行一次</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">loop</span><span class="params">()</span> &#123;</span><br><span class="line">永远重复运行</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-2-变量"><a href="#1-2-变量" class="headerlink" title="1.2 变量"></a>1.2 变量</h2><p><strong>变量类型</strong></p><ol><li><code>int 变量名</code> 整数 ，默认值为0</li></ol><p><code>int current_temperature</code> cdu为整数变量<br><code>int current_temperature = 18</code> 赋值</p><p>整数的范围为-32,768到32,767，超过上限，返回到最下限</p><blockquote><p><a href="http://www.taichi-maker.com/homepage/reference-index/arduino-code-reference/">变量参考资料-太极创客</a></p></blockquote><h3 id="1-2-1-变量的作用域"><a href="#1-2-1-变量的作用域" class="headerlink" title="1.2.1 变量的作用域"></a>1.2.1 变量的作用域</h3><p>全局变量 setup和loop外<br>局部变量 setup中或loop中，两个不同函数中，不能互相使用</p><h2 id="1-3-函数"><a href="#1-3-函数" class="headerlink" title="1.3 函数"></a>1.3 函数</h2><p><code>pinMode(LED_BUILTIN,OUTPUT);</code> 初始化led数字引脚为输出模式<br><code>pinMode(13,OUTPUT)</code>如果UNO中LED接的引脚为13，也可以这样写<br><code>digitalWrite(LED_BUILTIN,HIGH);</code> 将数字引脚写为高电平 /亮<br><code>digitalWrite(LED_BUILTIN,LOW);</code>  将数字引脚写为低电平 /灭<br><code>delay(1000);</code> 延时等待1秒</p><blockquote><p><a href="http://www.taichi-maker.com/homepage/reference-index/arduino-code-reference/">函数参考资料-太极创客</a></p></blockquote><h2 id="1-4-数字输出"><a href="#1-4-数字输出" class="headerlink" title="1.4 数字输出"></a>1.4 数字输出</h2><p>LED发光二极管<br>OUTPUT</p><h2 id="1-5-数字输入"><a href="#1-5-数字输入" class="headerlink" title="1.5 数字输入"></a>1.5 数字输入</h2><p>开关，输入信号<br>INPUT，识别HIGH和LOW两种状态<br>引脚悬空floating</p><p><code>int pushButton = 2;</code> 给引脚2取一个名字<br><code>pinMode(pushButton,INPUT);</code> 初始化2数字引脚为输入模式</p><p><code>Serial.begin(9600);</code> 启动串口通讯 ，每秒9600位<br><code>Serial.println(button);</code>显示按键状态</p><p><code>int buttonState = digitalRead(pushButton);</code></p><p><code>digitalRead(被读引脚号码)</code>读取数字引脚的电平状态，返回HIGH/LOW值，1/0</p><h2 id="1-6-输入上拉模式"><a href="#1-6-输入上拉模式" class="headerlink" title="1.6 输入上拉模式"></a>1.6 输入上拉模式</h2><p>INPUT_PULLUP</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">pinMode</span>(<span class="number">2</span>, INPUT_PULLUP); - <span class="number">2</span>引脚为开关输入上拉</span><br><span class="line"><span class="built_in">pinMode</span>(<span class="number">13</span>, OUTPUT); - <span class="number">13</span>引脚为输出LED</span><br></pre></td></tr></table></figure><p><code>int sensorVal = digitalRead(2)</code> 将开关状态数值读取到变量中</p><p>上拉模式:开关断开-高电平，开关闭合-低电平</p><p>sensorVal 高电平 - 开关断开 - 13输出LOW</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (sensorVal == HIGH) &#123;</span><br><span class="line">  <span class="built_in">digitalWrite</span>(<span class="number">13</span>, LOW);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">digitalWrite</span>(<span class="number">13</span>, HIGH);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关系运算符 ： <code>==</code>等于 <code>!=</code>不等于</p><h2 id="1-7-boolean运算符"><a href="#1-7-boolean运算符" class="headerlink" title="1.7 boolean运算符"></a>1.7 boolean运算符</h2><p><code>&amp;&amp;</code> 与 、 <code>||</code> 或 、 <code>!</code> 非</p><p><strong>非</strong><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">boolean pushButton;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setup</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="built_in">pinMode</span>(<span class="number">2</span>,INPUT_PULLUP);</span><br><span class="line">  <span class="built_in">pinMode</span>(<span class="number">13</span>,OUTPUT);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">loop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  pushButton = <span class="built_in">digitalRead</span>(<span class="number">2</span>);</span><br><span class="line"><span class="comment">/*逻辑非运算*/</span></span><br><span class="line">  <span class="keyword">if</span> (!pushButton) &#123;</span><br><span class="line">    <span class="built_in">digitalWrite</span>(<span class="number">13</span>, HIGH); <span class="comment">/*pushButton 为false ，则点亮LED*/</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">digitalWrite</span>(<span class="number">13</span>, LOW);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*if括号中如果为1.运行if后程序，如果为0，则运行else*/</span></span><br></pre></td></tr></table></figure></p><p><strong>与</strong> ： 都真，才真<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">boolean pushButton1;</span><br><span class="line">boolean pushButton2;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setup</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="built_in">pinMode</span>(<span class="number">2</span>,INPUT_PULLUP);</span><br><span class="line">  <span class="built_in">pinMode</span>(<span class="number">8</span>,INPUT_PULLUP);</span><br><span class="line">  <span class="built_in">pinMode</span>(<span class="number">13</span>,OUTPUT);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">loop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  pushButton1 = <span class="built_in">digitalRead</span>(<span class="number">2</span>);</span><br><span class="line">  pushButton2 = <span class="built_in">digitalRead</span>(<span class="number">8</span>);</span><br><span class="line"><span class="comment">/*逻辑与运算*/</span></span><br><span class="line">  <span class="keyword">if</span> (!pushButton1 &amp;&amp; !push Button) &#123;</span><br><span class="line">    <span class="built_in">digitalWrite</span>(<span class="number">13</span>, HIGH); <span class="comment">/*1和2都是1，才会点亮*/</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">digitalWrite</span>(<span class="number">13</span>, LOW);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>或</strong> ：只要有一个为真，就是真</p><h2 id="1-8-LED数码管"><a href="#1-8-LED数码管" class="headerlink" title="1.8 LED数码管"></a>1.8 LED数码管</h2><p>一位8段共阴极数码管</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/led.png" alt="led.png"></p><p>不同引脚给出高电平，就能显示不同的数字</p><h1 id="2-基础知识学习"><a href="#2-基础知识学习" class="headerlink" title="2. 基础知识学习"></a>2. 基础知识学习</h1><h2 id="2-1-循环"><a href="#2-1-循环" class="headerlink" title="2.1 循环"></a>2.1 循环</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> pinNumber = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">while</span>(pinNumber &lt;=<span class="number">9</span>&gt;)&#123;</span><br><span class="line">  <span class="built_in">pinMode</span>(pinNumber, OUTPUT);</span><br><span class="line">  pinNumber = pinNumber +<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*do while*/</span></span><br><span class="line"><span class="keyword">do</span>&#123;</span><br><span class="line">  语句</span><br><span class="line">&#125;<span class="keyword">while</span>(表达式);</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> brightness = <span class="number">0</span> ; brightness &lt;=<span class="number">255</span> ; brightness++) &#123;</span><br><span class="line">  语句</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-2-if"><a href="#2-2-if" class="headerlink" title="2.2 if"></a>2.2 if</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!pushButton) &#123;</span><br><span class="line">  <span class="built_in">digitalWrite</span>(<span class="number">13</span>, HIGH); <span class="comment">/*pushButton 为false ，则点亮LED*/</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">digitalWrite</span>(<span class="number">13</span>, LOW);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!pushButton) &#123;</span><br><span class="line">  <span class="built_in">digitalWrite</span>(<span class="number">13</span>, HIGH); <span class="comment">/*pushButton 为false ，则点亮LED*/</span></span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> &#123;</span><br><span class="line">  <span class="built_in">digitalWrite</span>(<span class="number">13</span>, LOW);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span>&#123;</span><br><span class="line">  语句</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-3-switch…case"><a href="#2-3-switch…case" class="headerlink" title="2.3 switch…case"></a>2.3 switch…case</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span> (var) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">        <span class="comment">//当var等于1时执行这里的程序</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">        <span class="comment">//当var等于2时执行这里的程序</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="comment">// 如果var的值与以上case中的值都不匹配</span></span><br><span class="line">        <span class="comment">// 则执行这里的程序</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-4-random"><a href="#2-4-random" class="headerlink" title="2.4 random()"></a>2.4 random()</h2><p><code>random(0,4)</code> 0/1/2/3随机数</p><h2 id="2-5-自定义函数"><a href="#2-5-自定义函数" class="headerlink" title="2.5 自定义函数"></a>2.5 自定义函数</h2><p>不带参数<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">displayClear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="built_in">digitalWrite</span>(<span class="number">3</span>,LOW);</span><br><span class="line">  <span class="built_in">digitalWrite</span>(<span class="number">4</span>,LOW);</span><br><span class="line">  <span class="built_in">digitalWrite</span>(<span class="number">5</span>,LOW);</span><br><span class="line">  <span class="built_in">digitalWrite</span>(<span class="number">7</span>,LOW);</span><br><span class="line">  <span class="built_in">digitalWrite</span>(<span class="number">8</span>,LOW);</span><br><span class="line">  <span class="built_in">digitalWrite</span>(<span class="number">9</span>,LOW);</span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="built_in">displayClear</span>()  <span class="comment">/*调用函数*/</span></span><br></pre></td></tr></table></figure></p><p>带参数<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">displayNumber</span><span class="params">(<span class="type">int</span> ledNumber)</span> </span>&#123;</span><br><span class="line">  语句</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">displayNumber</span><span class="params">(<span class="type">int</span> ledNumber , <span class="type">long</span> ledNumber2)</span> </span>&#123;</span><br><span class="line">  语句</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*调用*/</span></span><br><span class="line"><span class="built_in">displayNumber</span>(myNumber);</span><br></pre></td></tr></table></figure></p><p>带参数，带返回值<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">getRandomNumber</span><span class="params">(<span class="type">int</span> minNumber, <span class="type">int</span> maxNumber)</span></span>&#123;</span><br><span class="line">  <span class="type">int</span> randomNumber;</span><br><span class="line">  randomNumber = <span class="built_in">random</span>(minNumber, maxNumber);</span><br><span class="line">  <span class="keyword">return</span> randomNumber</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*返回randomNumber*/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> ;</span><br><span class="line"><span class="comment">/*返回0*/</span></span><br></pre></td></tr></table></figure></p><p><code>randomSeed(analogRead(A0))</code>来自A0引脚模拟输入的数值作为随机种子</p><h2 id="2-6-串口监视器"><a href="#2-6-串口监视器" class="headerlink" title="2.6 串口监视器"></a>2.6 串口监视器</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Serial.<span class="built_in">begin</span>(<span class="number">9600</span>);</span><br><span class="line">Serial.<span class="built_in">print</span>(<span class="string">&quot;变量的值是&quot;</span>); <span class="comment">/*字符串*/</span></span><br><span class="line">Serial.<span class="built_in">println</span>(bianliang); <span class="comment">/*变量*/</span></span><br><span class="line">serial.<span class="built_in">println</span>(<span class="string">&quot;&quot;</span>); <span class="comment">/*空白行*/</span></span><br></pre></td></tr></table></figure><h2 id="2-7-模拟输出"><a href="#2-7-模拟输出" class="headerlink" title="2.7 模拟输出"></a>2.7 模拟输出</h2><p>可以不用<code>pinMode()</code></p><p><code>analogWrite(ledPin, brightness)</code><br><code>analogWrite(引脚编号, 参数)</code></p><h2 id="2-8-PWM"><a href="#2-8-PWM" class="headerlink" title="2.8 PWM"></a>2.8 PWM</h2><p><code>analogWrite(ledPin, 127)</code> 50%亮度</p><p>2毫秒内，50%高电平，50%低电平</p><p>PWM脉冲宽度调制</p><h2 id="2-9-电位器模拟输入"><a href="#2-9-电位器模拟输入" class="headerlink" title="2.9 电位器模拟输入"></a>2.9 电位器模拟输入</h2><p>电位器，旋钮改变R1，R2比值<br>R1 + R2 = 固定值</p><p>3个引脚，1接地，2接输入引脚，3接5V引脚</p><p>旋钮旋转来改变第2引脚的电位，</p><p><code>analogRead(引脚号码)</code></p><p><code>analogRead(A0)</code><strong>读取</strong>模拟输入引脚数值，将<strong>0-5V</strong>的电压输入信号映射到数值<strong>0-1023</strong></p><p>将5V分为1024份， 2.5V 对应数值 512<br>引脚输入范围可以使用analogReference()进行调整</p><blockquote><blockquote><p><a href="http://www.taichi-maker.com/homepage/reference-index/arduino-code-reference/analogread/">太极创客解释</a></p></blockquote></blockquote><h2 id="2-10-等比映射"><a href="#2-10-等比映射" class="headerlink" title="2.10 等比映射"></a>2.10 等比映射</h2><p><code>map(analogInputVal, 0 ,1023 , 0 , 255);</code></p><p>将0-1023映射到0-255</p><h1 id="3-模块学习"><a href="#3-模块学习" class="headerlink" title="3. 模块学习"></a>3. 模块学习</h1><h2 id="3-ESP8266入门"><a href="#3-ESP8266入门" class="headerlink" title="3. ESP8266入门"></a>3. ESP8266入门</h2><p>使用arduino研究esp8266</p><h1 id="4-开始学库"><a href="#4-开始学库" class="headerlink" title="4. 开始学库"></a>4. 开始学库</h1><h2 id="4-1-servo"><a href="#4-1-servo" class="headerlink" title="4.1 servo"></a>4.1 servo</h2>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C#-Note</title>
      <link href="/Learn/Learn-Csharp/"/>
      <url>/Learn/Learn-Csharp/</url>
      
        <content type="html"><![CDATA[<p>学习 C#的笔记</p><span id="more"></span><blockquote><p>参考<a href="https://www.bilibili.com/video/BV1wx411K7rb">b 站的搬运视频学习</a><br>原视频是 YouTube 上的<a href="https://www.youtube.com/watch?v=EgIbwCnQ680&amp;list=PLZX6sKChTg8GQxnABqxYGX2zLs4Hfa4Ca">刘铁猛老师录制的教程</a><br>Timothy Liu 老师新一期 C#教程的<a href="https://www.youtube.com/watch?v=HF3JGXV07Uo&amp;list=PLZX6sKChTg8HP3MF9d8CEc3nrtEpiQ4Jf">C#语言入门详解》第二季</a></p></blockquote><p>还发现刘老师还做了 Python 的教程，有机会去看看<a href="https://www.youtube.com/watch?v=i3TYCCY2WSE&amp;list=PLZX6sKChTg8HyHfrmk97iblmsQLrlRHxn">pandas 玩转 excel</a></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220609173231.png" alt="20220609173231.png"></p><h1 id="0-C-学习前的-bb"><a href="#0-C-学习前的-bb" class="headerlink" title="0. C#学习前的 bb"></a>0. C#学习前的 bb</h1><p>C# 窗体应用，我的初步感受就是 VB+Java（狗头）<br>VB 现在正在学，但是 Java 是没学过 hhh</p><p>C# 是面向对象的语言，然而 C# 进一步提供了对面向组件 (component-oriented) 编程的支持。现代软件设计日益依赖于自包含和自描述功能包形式的软件组件。这种组件的关键在于，它们通过属性、方法和事件来提供编程模型；它们具有提供了关于组件的声明性信息的特性；同时，它们还编入了自己的文档。C# 提供的语言构造直接支持这些概念，这使得 C# 语言自然而然成为创建和使用软件组件之选。（copy from _csharp language specification 5.0 中文_）</p><h1 id="1-语言标准的——hello-world"><a href="#1-语言标准的——hello-world" class="headerlink" title="1. 语言标准的——hello world!!!"></a>1. 语言标准的——hello world!!!</h1><h2 id="1-1-Console"><a href="#1-1-Console" class="headerlink" title="1.1 Console"></a>1.1 Console</h2><p>控制台.NET Framework</p><p><code>Console.WriteLine(&quot;Hello World!!!&quot;);</code></p><h2 id="1-2-WPF"><a href="#1-2-WPF" class="headerlink" title="1.2 WPF"></a>1.2 WPF</h2><p>新的 windows forms（大概）,感觉就是更高级的 VB，更自由更美观的界面开发<br>跟 windows forms 一样</p><p><code>textBoxShowHello.Text = &quot;Hello World!&quot;;</code></p><h3 id="WPF-App-NET"><a href="#WPF-App-NET" class="headerlink" title="WPF App(.NET)"></a>WPF App(.NET)</h3><p>跟 VB 很像，拖拽控件，编写控件程序，设计界面，所见即所得。</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/2022-06-09-14-56-30.png" alt="2022-06-09-14-56-30.png" title="Visual Studio的WPF App项目界面"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/2022-06-09-15-00-46.png" alt="2022-06-09-15-00-46.png" title="HelloWorld-program"></p><h2 id="1-3-Windows-Forms-App-old"><a href="#1-3-Windows-Forms-App-old" class="headerlink" title="1.3 Windows Forms App(old)"></a>1.3 Windows Forms App(old)</h2><p>窗体程序，学过一点 VB，无所畏惧</p><p>button<br><code>textBoxShowHello.Text = &quot;Hello World!&quot;;</code></p><h2 id="1-4-ASP-NET-Web-Forms-old"><a href="#1-4-ASP-NET-Web-Forms-old" class="headerlink" title="1.4 ASP.NET Web Forms(old)"></a>1.4 ASP.NET Web Forms(old)</h2><p>网络应用程序，网页<br>Controller 中<br><code>&lt;h1&gt;Hello World!&lt;h1&gt;</code></p><h2 id="1-5-ASP-NET-MVC"><a href="#1-5-ASP-NET-MVC" class="headerlink" title="1.5 ASP.NET MVC"></a>1.5 ASP.NET MVC</h2><p>程序开发架构，可以将不同语言的代码放在不同的目录中<br>Controller 中<br><code>&lt;h1&gt;Hello World!&lt;h1&gt;</code></p><h2 id="1-6-WCF"><a href="#1-6-WCF" class="headerlink" title="1.6 WCF"></a>1.6 WCF</h2><p>纯网络服务，读取数据库、向数据库输入数据</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="built_in">string</span> <span class="title">SayHello</span>()</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="string">&quot;hello world!!!&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-7-Windows-Store-Application"><a href="#1-7-Windows-Store-Application" class="headerlink" title="1.7 Windows Store Application"></a>1.7 Windows Store Application</h2><p>平板电脑,也是窗体设计<br><code>textBoxShowHello.Text = &quot;Hello World!!!&quot;;</code></p><h2 id="1-8-Windows-Phone-Application-已经凉透了？？？"><a href="#1-8-Windows-Phone-Application-已经凉透了？？？" class="headerlink" title="1.8 Windows Phone Application(已经凉透了？？？)"></a>1.8 Windows Phone Application(已经凉透了？？？)</h2><p><code>textBoxShowHello.Text = &quot;Hello World!!!&quot;;</code></p><h2 id="1-9-Cloud"><a href="#1-9-Cloud" class="headerlink" title="1.9 Cloud"></a>1.9 Cloud</h2><p>云计算 Azure<br><code>&lt;h1&gt;Hello World!&lt;h1&gt;</code></p><h2 id="1-10-WF"><a href="#1-10-WF" class="headerlink" title="1.10 WF"></a>1.10 WF</h2><p>窗体设计<br>直接在 writeline 控件里写<br><code>&quot;hello world!!!&quot;</code></p><hr><h1 id="2-类与名称空间"><a href="#2-类与名称空间" class="headerlink" title="2. 类与名称空间"></a>2. 类与名称空间</h1><p><strong>class &amp; namespace</strong></p><h2 id="2-1-剖析-Hello-World-程序"><a href="#2-1-剖析-Hello-World-程序" class="headerlink" title="2.1 剖析 Hello World 程序"></a>2.1 剖析 Hello World 程序</h2><ul><li>类是构成程序的主体</li><li>名称空间是以树型结构组织类（和其他类型），如 Button 类和 Path 类</li></ul><p>Console App : <code>Console.WriteLine(&quot;Hello World~&quot;);</code></p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> System;</span><br><span class="line"><span class="keyword">using</span> System.Collections.Generic;</span><br><span class="line"><span class="keyword">using</span> System.Linq;</span><br><span class="line"><span class="keyword">using</span> System.Text;</span><br><span class="line"><span class="keyword">using</span> System.Threading.Tasks;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> <span class="title">ConsoleApphello</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">class</span> <span class="title">Program</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">Main</span>(<span class="params"><span class="built_in">string</span>[] args</span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            Console.WriteLine(<span class="string">&quot;Hello World&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>类 class <code>Program</code>(自己写的)和<code>Console</code>(调用 C#的类)<br>使用 Console 类中的<code>WriteLine</code>方法来进行 print</p><p>名称空间 namespace <code>HelloWorld</code>，默认跟创建 project 时名称一样</p><p><strong>核心理解：</strong></p><p><strong>又如<code>System</code>名称空间中的<code>Console</code>类，类中的<code>WriteLine</code>方法</strong></p><p><strong><code>using System</code></strong> 跟 python 中的<code>import</code>差不多，将类引用到自己的程序中</p><p>就是：<br>有<code>using System;</code> 直接用<code>Console.WriteLine</code><br>没有<code>using System;</code>则必须用<code>System.Console.WriteLine</code></p><h2 id="2-2-类库"><a href="#2-2-类库" class="headerlink" title="2.2 类库"></a>2.2 类库</h2><p>类库的引用可以分为两种</p><ul><li>黑盒 DLL 引用（无源代码）</li><li>白盒 项目直接引用（有源代码）</li></ul><h3 id="黑盒："><a href="#黑盒：" class="headerlink" title="黑盒："></a>黑盒：</h3><h4 id="a-本地引用"><a href="#a-本地引用" class="headerlink" title="a.本地引用"></a>a.本地引用</h4><p>从其他人手中获得 dll 类库和 doc 文档，来进行引用</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220609192636.png" alt="20220609192636.png"></p><p>浏览打开 dll 文件，即可引用类库</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220609192848.png" alt="20220609192848.png"></p><h4 id="b-网络引用"><a href="#b-网络引用" class="headerlink" title="b.网络引用"></a>b.网络引用</h4><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220609194354.png" alt="20220609194354.png"></p><p>黑盒引用无法修改 dll 中的错误</p><h4 id="c-NuGet-添加类库"><a href="#c-NuGet-添加类库" class="headerlink" title="c.NuGet 添加类库"></a>c.NuGet 添加类库</h4><p>手动添加网络类库时，可能引用的某个类库还依赖着其他的底层类库，需要将该类库的所有依赖类库也一起引用进来，很麻烦。</p><p>使用 NuGet 可以将需要引用的类库与其依赖的底层类库一起打包引用进来，_有点像 Pycharm 中的安装轮子，pip？_<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220609200539.png" alt="20220609200539.png"></p><h3 id="白盒："><a href="#白盒：" class="headerlink" title="白盒："></a>白盒：</h3><p><strong>一个项目可以属于多个 solution，一个 solution 中可以有多个项目</strong></p><p>添加已存在项目到 solution<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220609201227.png" alt="20220609201227.png"></p><p>然后就可以直接引用项目中的类库了<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220609201316.png" alt="20220609201316.png"></p><p>可以通过打断点，然后 start debugging，逐语句执行找错误，<strong>找到真正的错误:Root cause!</strong><br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220609202044.png" alt="20220609202044.png"></p><h3 id="建立自己的类库项目"><a href="#建立自己的类库项目" class="headerlink" title="建立自己的类库项目"></a>建立自己的类库项目</h3><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220609202321.png" alt="20220609202321.png"></p><p><strong>注意：使用.Net Framework 创建</strong></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220609203808.png" alt="20220609203808.png"></p><h2 id="2-3-依赖关系"><a href="#2-3-依赖关系" class="headerlink" title="2.3 依赖关系"></a>2.3 依赖关系</h2><p>在自己写的类中引用了别人的类库，即为我的类依赖于别人的类库，如果该类库出现错误，则我的类也会有问题。</p><p>极其重要，尽量选择较弱的依赖关系，<strong>高内聚，低耦合</strong></p><h3 id="UML-类图"><a href="#UML-类图" class="headerlink" title="UML 类图"></a>UML 类图</h3><p>通用建模语言，用图表达程序。</p><h2 id="2-4-类"><a href="#2-4-类" class="headerlink" title="2.4 类"></a>2.4 类</h2><p>类(class)是对现实世界事务进行抽象所得到的的结果</p><h3 id="2-4-1-类与对象"><a href="#2-4-1-类与对象" class="headerlink" title="2.4.1 类与对象"></a>2.4.1 类与对象</h3><p>对象（现实世界）=实例（程序世界），是类经过实例化后得到的内存中的实体</p><p>可以将类看做概念，对象则是概念所指的实体</p><p>唯物主义</p><ul><li>类：脑子中的飞机概念</li><li>对象：现实世界的飞机</li></ul><h3 id="2-4-2-创建类的实例"><a href="#2-4-2-创建类的实例" class="headerlink" title="2.4.2 创建类的实例"></a>2.4.2 创建类的实例</h3><p>使用 new 操作符创建类的实例</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220610094719.png" alt="20220610094719"></p><h3 id="2-4-3-引用变量和实例的关系"><a href="#2-4-3-引用变量和实例的关系" class="headerlink" title="2.4.3 引用变量和实例的关系"></a>2.4.3 引用变量和实例的关系</h3><p><strong>就是 python 中的引用类<code>lol_watcher = LolWatcher(&#39;api-key&#39;)</code>，将一个类 new 出实例后复制个一个变量</strong></p><p><code>Form myForm;</code>引用变量定义<br><code>myForm = new Form();</code> new 出的实例赋值给引用变量</p><p>通过引用变量引用实例后，可以多次访问该实例</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220610095236.png" alt="20220610095236"></p><p>同一个实例可以被多个变量引用</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Form myForm1;</span><br><span class="line">Form myForm2;</span><br><span class="line">myForm1 = <span class="keyword">new</span> Form();</span><br><span class="line">myForm2 = myForm2;</span><br></pre></td></tr></table></figure><h3 id="2-4-4-类的三大成员"><a href="#2-4-4-类的三大成员" class="headerlink" title="2.4.4 类的三大成员"></a>2.4.4 类的三大成员</h3><ul><li>属性<ul><li>存储数据，组合起来表示类或对象当前的状态</li></ul></li><li>方法<ul><li>C 语言中的函数</li></ul></li><li>事件<ul><li>类或对象通知其它类或对象的机制，C#独有，要善用事件机制</li></ul></li></ul><h4 id="不同的类或对象侧重点不同"><a href="#不同的类或对象侧重点不同" class="headerlink" title="不同的类或对象侧重点不同"></a>不同的类或对象侧重点不同</h4><ul><li>侧重于属性，Entity Framework</li><li>侧重于方法，Math，Console</li><li>侧重于事件，Timer</li></ul><p><strong>Timer 的使用，新建一个 WPF APP 项目</strong></p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> System;</span><br><span class="line"><span class="keyword">using</span> System.Collections.Generic;</span><br><span class="line"><span class="keyword">using</span> System.Linq;</span><br><span class="line"><span class="keyword">using</span> System.Text;</span><br><span class="line"><span class="keyword">using</span> System.Threading.Tasks;</span><br><span class="line"><span class="keyword">using</span> System.Windows;</span><br><span class="line"><span class="keyword">using</span> System.Windows.Controls;</span><br><span class="line"><span class="keyword">using</span> System.Windows.Data;</span><br><span class="line"><span class="keyword">using</span> System.Windows.Documents;</span><br><span class="line"><span class="keyword">using</span> System.Windows.Input;</span><br><span class="line"><span class="keyword">using</span> System.Windows.Media;</span><br><span class="line"><span class="keyword">using</span> System.Windows.Media.Imaging;</span><br><span class="line"><span class="keyword">using</span> System.Windows.Navigation;</span><br><span class="line"><span class="keyword">using</span> System.Windows.Shapes;</span><br><span class="line"><span class="keyword">using</span> System.Windows.Threading;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> <span class="title">EventSample</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment"><span class="doctag">///</span> <span class="doctag">&lt;summary&gt;</span></span></span><br><span class="line">    <span class="comment"><span class="doctag">///</span> Interaction logic for MainWindow.xaml</span></span><br><span class="line">    <span class="comment"><span class="doctag">///</span> <span class="doctag">&lt;/summary&gt;</span></span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">partial</span> <span class="keyword">class</span> <span class="title">MainWindow</span> : <span class="title">Window</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">MainWindow</span>()</span></span><br><span class="line">        &#123;</span><br><span class="line">            InitializeComponent();</span><br><span class="line">            DispatcherTimer timer = <span class="keyword">new</span> DispatcherTimer();</span><br><span class="line">            timer.Interval = TimeSpan.FromSeconds(<span class="number">1</span>);</span><br><span class="line">            timer.Tick += Timer_Tick;</span><br><span class="line">            timer.Start();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">Timer_Tick</span>(<span class="params"><span class="built_in">object</span> sender, EventArgs e</span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">this</span>.timeTextBox.Text = DateTime.Now.ToString();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>效果：</strong><br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220610105152.png" alt="20220610105152"></p><h3 id="2-4-5-静态成员与实例成员"><a href="#2-4-5-静态成员与实例成员" class="headerlink" title="2.4.5 静态成员与实例成员"></a>2.4.5 静态成员与实例成员</h3><ul><li>静态成员（Static）：类的成员，eg：人类的总数，人类的平均身高<ul><li>使用时，不需要 new 一个实例出来，全部的实例都可以使用类中的成员</li></ul></li><li>实例成员（非静态）：对象的成员，eg：人的身高，体重<ul><li>使用时，需要先 new 一个实例出来，成员只能用于某一特定实例中</li></ul></li></ul><h4 id="绑定-Binding"><a href="#绑定-Binding" class="headerlink" title="绑定 Binding"></a>绑定 Binding</h4><ul><li>早绑定语言（微软 C#），编译器编译时就将一个成员与类或对象关联起来</li><li>晚绑定语言（动态语言 javascript），在程序运行时，由程序决定某个成员属于某个类还是某个对象</li></ul><h4 id="操作符"><a href="#操作符" class="headerlink" title="操作符."></a>操作符.</h4><p>成员访问操作符 <strong>.</strong></p><hr><h1 id="3-构成-C-语言的基本元素"><a href="#3-构成-C-语言的基本元素" class="headerlink" title="3. 构成 C#语言的基本元素"></a>3. 构成 C#语言的基本元素</h1><h2 id="3-1-标记-Token"><a href="#3-1-标记-Token" class="headerlink" title="3.1 标记 Token"></a>3.1 标记 Token</h2><p><strong>编译器能够识别的信息</strong></p><h3 id="3-1-1-关键字-Keyword"><a href="#3-1-1-关键字-Keyword" class="headerlink" title="3.1.1 关键字 Keyword"></a>3.1.1 关键字 Keyword</h3><p><a href="https://docs.microsoft.com/zh-cn/dotnet/csharp/language-reference/keywords/">C#关键字</a></p><h3 id="3-1-2-操作符-Operator"><a href="#3-1-2-操作符-Operator" class="headerlink" title="3.1.2 操作符 Operator"></a>3.1.2 操作符 Operator</h3><p><a href="https://docs.microsoft.com/zh-cn/dotnet/csharp/language-reference/operators/">C#操作符</a></p><h3 id="3-1-3-标识符-Identifier"><a href="#3-1-3-标识符-Identifier" class="headerlink" title="3.1.3 标识符 Identifier"></a>3.1.3 标识符 Identifier</h3><p>自己起的名字，必须有一套规范<br><a href="https://docs.microsoft.com/zh-cn/dotnet/csharp/fundamentals/coding-style/identifier-names">C#标识符</a></p><p><strong>命名规范：</strong></p><ul><li>类名：名词</li><li>类的成员：<ul><li>属性：名词</li><li>方法：动词、动词短语</li></ul></li></ul><p><strong>大小写规范：</strong></p><ul><li>驼峰命名法 myVariable（变量名）</li><li>MyVariable（方法名、类名…）</li></ul><h3 id="3-1-4-标点符号"><a href="#3-1-4-标点符号" class="headerlink" title="3.1.4 标点符号"></a>3.1.4 标点符号</h3><p>不表示运算思想，如：</p><ul><li>分号;表示语句结束</li><li>花括号{}</li></ul><h3 id="3-1-5-文本（字面值）"><a href="#3-1-5-文本（字面值）" class="headerlink" title="3.1.5 文本（字面值）"></a>3.1.5 文本（字面值）</h3><p>eg:<code>int x = 2;</code></p><ul><li>整数，_默认 int_<ul><li>int，32bit <code>int x = 2;</code></li><li>long 长整型，64bit <code>long y = 3l;``long y = 3L;</code></li></ul></li><li>实数，_默认 double_<ul><li>float，32bit <code>float x = 3.0F;</code></li><li>double 双精度浮点数，64bit <code>double y = 4.0D;</code></li></ul></li><li>字符<ul><li>char <code>char c = &#39;a&#39;;</code> _单引号只能引一个字符_</li></ul></li><li>字符串<ul><li>string <code>string str = &quot;abcd&quot;;</code> _双引号可以引字符串_</li></ul></li><li>布尔值<ul><li>bool <code>bool b = true;</code></li></ul></li><li>空 null<ul><li><code>string str = null;</code></li></ul></li></ul><h2 id="3-2-注释与空白"><a href="#3-2-注释与空白" class="headerlink" title="3.2 注释与空白"></a>3.2 注释与空白</h2><ul><li>行注释</li><li>块注释</li></ul><p>注释快捷键 ctrl+k , ctrl+c<br>取消注释快捷键 ctrl+k, ctrl+u</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 行注释</span></span><br><span class="line"><span class="comment">/*块注释*/</span></span><br></pre></td></tr></table></figure><ul><li>空白<ul><li>空格，有的地方必须加空格</li><li>回车</li></ul></li></ul><p>_也可以使用快捷键自动填补多余空白_ ctrl+k , ctrl+d<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220610120551.png" alt="20220610120551"></p><h2 id="3-3-数据类型"><a href="#3-3-数据类型" class="headerlink" title="3.3 数据类型"></a>3.3 数据类型</h2><p>var 声明变量，C#自动推断出变量的类型<br><code>var x = 1;</code><br>查看变量类型<br><code>Console.WriteLine(x.GetType().Name);</code><br>得到：<code>int32</code></p><h2 id="3-4-变量"><a href="#3-4-变量" class="headerlink" title="3.4 变量"></a>3.4 变量</h2><p>变量声明<code>int x;</code><br>变量赋值<code>x = 100;</code></p><h2 id="3-5-方法"><a href="#3-5-方法" class="headerlink" title="3.5 方法"></a>3.5 方法</h2><p>方法就是函数，加工数据</p><p>使用<code>public 方法</code>类外部也能访问该方法<br>默认是<code>private</code></p><p>方法的返回值类型写在方法名前</p><ul><li>int 表示方法返回一个 int 型变量</li><li>void 表示方法不返回任何值</li></ul><p>eg:</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> System;</span><br><span class="line"><span class="keyword">using</span> System.Collections.Generic;</span><br><span class="line"><span class="keyword">using</span> System.Linq;</span><br><span class="line"><span class="keyword">using</span> System.Text;</span><br><span class="line"><span class="keyword">using</span> System.Threading.Tasks;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> <span class="title">MyExample</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">class</span> <span class="title">Program</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">Main</span>(<span class="params"><span class="built_in">string</span>[] args</span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            Calculator c = <span class="keyword">new</span> Calculator();</span><br><span class="line">            <span class="built_in">int</span> x = c.Add(<span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">            Console.WriteLine(x);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">class</span> <span class="title">Calculator</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="built_in">int</span> <span class="title">Add</span>(<span class="params"><span class="built_in">int</span> a, <span class="built_in">int</span> b</span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">int</span> result = a + b;</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="3-6-程序-汉诺塔递归例子"><a href="#3-6-程序-汉诺塔递归例子" class="headerlink" title="3.6 程序(汉诺塔递归例子)"></a>3.6 程序(汉诺塔递归例子)</h2><p>程序 = 数据+算法</p><h3 id="汉诺塔递归法"><a href="#汉诺塔递归法" class="headerlink" title="汉诺塔递归法"></a>汉诺塔递归法</h3><p>参考<a href="https://www.bilibili.com/video/BV1SP4y137E9">木子喵 neko 大佬的视频</a></p><p>汉诺塔递归流程图</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/hannuota.png" alt="hannuota"></p><p><a href="https://zhangxiaoleiwk.gitee.io/h.html">自己找个在线游戏操作一下较好</a><br>要从 A 将 3 个盘子移到 C：</p><ul><li>先将 1、2 号盘子从 A 移动到 B<ul><li>先将 1 号盘子从 A 移动到 C <code>A--&gt;C</code></li><li>再将 2 号盘子从 A 移动到 B <code>A--&gt;B</code></li><li>最后将 1 号盘子从 C 移动到 B <code>C--&gt;B</code></li></ul></li><li>然后将最大的 3 号盘子从 A 移动到 C <code>A--&gt;C</code></li><li>最后将 1、2 号盘子从 B 移动到 C<ul><li>先将 1 号盘子从 B 移动到 A <code>B--&gt;A</code></li><li>再将 2 号盘子从 B 移动到 C <code>B--&gt;C</code></li><li>最后将 1 号盘子从 A 移动到 C <code>A--&gt;C</code></li></ul></li></ul><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> System;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> <span class="title">Digui</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">class</span> <span class="title">Program</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">Main</span>(<span class="params"><span class="built_in">string</span>[] args</span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">char</span> a = <span class="string">&#x27;A&#x27;</span>;</span><br><span class="line">            <span class="built_in">char</span> b = <span class="string">&#x27;B&#x27;</span>;</span><br><span class="line">            <span class="built_in">char</span> c = <span class="string">&#x27;C&#x27;</span>;</span><br><span class="line">            <span class="built_in">int</span> n = <span class="number">3</span>;</span><br><span class="line">            HanNuoTao hnt = <span class="keyword">new</span> HanNuoTao();</span><br><span class="line">            hnt.Move(n, a, b, c);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//汉诺塔</span></span><br><span class="line">    <span class="keyword">class</span> <span class="title">HanNuoTao</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Move</span>(<span class="params"><span class="built_in">int</span> n, <span class="built_in">char</span> a, <span class="built_in">char</span> b, <span class="built_in">char</span> c</span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (n == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                Move(n - <span class="number">1</span>, a, c, b);</span><br><span class="line">                Console.WriteLine(<span class="string">$&quot;<span class="subst">&#123;a&#125;</span>--&gt;<span class="subst">&#123;c&#125;</span>&quot;</span>);</span><br><span class="line">                Move(n - <span class="number">1</span>, b, a, c);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//output:</span></span><br><span class="line">A--&gt;C</span><br><span class="line">A--&gt;B</span><br><span class="line">C--&gt;B</span><br><span class="line">A--&gt;C</span><br><span class="line">B--&gt;A</span><br><span class="line">B--&gt;C</span><br><span class="line">A--&gt;C</span><br></pre></td></tr></table></figure><hr><h1 id="4-类型、变量和对象"><a href="#4-类型、变量和对象" class="headerlink" title="4. 类型、变量和对象"></a>4. 类型、变量和对象</h1><h2 id="4-1-数据类型作用"><a href="#4-1-数据类型作用" class="headerlink" title="4.1 数据类型作用"></a>4.1 数据类型作用</h2><ul><li>Data Type：性质相同值的集合，配备一系列针对这种类型值的操作<ul><li>小内存容纳大尺寸数据会丢失精确度，发生错误</li><li>大内存容纳小尺寸数据会导致浪费</li></ul></li></ul><div class="note primary">            <ul><li>强类型编程语言 C#<ul><li>数据受类型的约束很强</li></ul></li><li>弱类型编程语言 C++/JavaScript</li><li>数据受类型的 约束很弱 - c++中可能出现 if(x=200)可以编译成功的错误，一般在 c++写成 <code>if(200==x)</code>的形式，防止错误</li></ul>          </div><p>数据类型包含的信息</p><ul><li>存储此类型变量所需的内存空间大小</li><li>此类型的值可表示的最大、最小值返回</li><li>此类型所包含的成员（如方法、属性、时间等等）</li><li>此类型由何种基类派生而来</li><li><p>程序运行时，此类型的变量分配在内存什么位置</p><ul><li>程序分配到内存后分为堆栈两个区域：<ul><li>堆：存的东西多，存储对象（实例放在堆里） - Heap</li><li>栈：存的东西少，函数调用 - Stack</li></ul></li><li>Stack overflow ：栈溢出，函数调用太多/程序有错误，栈上分配太多内存</li><li>内存泄漏：分配对象忘了回收，造成内存浪费，_c#垃圾收集器中可以自动释放垃圾内存，防止内存泄漏_，_C++不会自动回收_</li></ul></li><li><p>此类型所允许的运算操作</p><ul><li><code>double result = 3.0/4.0</code>结果为 0.75 _double 类型做浮点除法_</li><li><code>double result = 3/4</code>结果为 0 _int 类型做整数除法_</li></ul></li></ul><div class="note primary">            <p>静态程序：硬盘<br>动态程序：内存</p><p>编译:</p><ul><li>build：编译自己的代码生成 Assembly，并跟别人的装配件 Assembly 组合在一起</li><li>compile：编译自己的代码，生成 Assembly 装配件</li></ul>          </div><h2 id="4-2-C-五大数据类型"><a href="#4-2-C-五大数据类型" class="headerlink" title="4.2 C#五大数据类型"></a>4.2 C#五大数据类型</h2><p>C#类型派生谱系<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220611111310.png" alt="20220611111310"></p><ul><li>横线上<br>object 和 string 都是类 class 的数据类型</li><li>横线下<br>class、delegate、interface 都是关键字，而不是具体的数据类型，用这三个关键字去定义其他数据的类型</li></ul><p>bool 的两个值：true、false<br>void：函数不返回值时，用 void 定义函数<br>null：一个变量的值是空的时候，给变量赋值 null<br>var、dynamic：声明变量，dynamic 仿照弱类型语言</p><p>五大数据类型：</p><ul><li>类 Class</li><li>结构体 Structures</li><li>枚举 Enumerations</li><li>接口 Interfaces</li><li>委托 Delegates</li></ul><h3 id="4-2-1-类类型-class"><a href="#4-2-1-类类型-class" class="headerlink" title="4.2.1 类类型 class"></a>4.2.1 类类型 class</h3><p>Form 是类类型</p><ul><li><code>Type myType = typeof(Form)</code> <code>cw(myType.FullName)</code> <code>cw(myType.IsClass)</code></li><li>定义的地方有<code>class Form</code></li></ul><h3 id="4-2-2-结构体类型-struct"><a href="#4-2-2-结构体类型-struct" class="headerlink" title="4.2.2 结构体类型 struct"></a>4.2.2 结构体类型 struct</h3><p>int,double 等等<br><code>struct Int32</code><br>将 int32 吸收为关键字 int，int64 为 long</p><h3 id="4-2-3-枚举-enum"><a href="#4-2-3-枚举-enum" class="headerlink" title="4.2.3 枚举 enum"></a>4.2.3 枚举 enum</h3><p>限定用户从一个集合中选取有效值,用户只能在给定的几个值里选择</p><p><code>enum FormWindowState</code></p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Form f = <span class="keyword">new</span> Form();</span><br><span class="line"><span class="comment">// 枚举类型窗口大小:只给定了Maximized,Normal,Minimized,选择其中一个</span></span><br><span class="line">f.WindowState = FormWindowState.Maximized;</span><br><span class="line">f.ShowDialog();</span><br><span class="line"><span class="comment">//窗口大小默认normal</span></span><br></pre></td></tr></table></figure><h2 id="4-3-变量、对象与内存"><a href="#4-3-变量、对象与内存" class="headerlink" title="4.3 变量、对象与内存"></a>4.3 变量、对象与内存</h2><h3 id="4-3-1-变量"><a href="#4-3-1-变量" class="headerlink" title="4.3.1 变量"></a>4.3.1 变量</h3><ul><li>表面上：变量是存储数据</li><li>实际上：变量表示了存储位置，并且每个变量有一个类型，来决定什么样的值能存入变量</li></ul><p><strong>变量就是以变量名所对应的内存地址为起点，以其数据类型所要求的的存储空间为长度的一块内存区域</strong></p><ul><li>变量一共有 7 种<ul><li>静态变量（静态成员变量）<ul><li><code>public static int Amount;</code></li></ul></li><li>实例变量（非静态成员变量，字段：属性雏形，属性就是让字段只赋规定的值）<ul><li><code>public int Age;</code></li></ul></li><li>数组元素<ul><li><code>int[] array = new int[100];</code>长度为 100 的整型数组</li><li>访问数组第一个元素：<code>array[0]</code>‘</li></ul></li><li>值参数 <code>double c</code></li><li>引用参数 <code>ref double a</code></li><li>输出形参 <code>out double b</code></li></ul></li></ul><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title">Student</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="built_in">double</span> <span class="title">Add</span>(<span class="params"><span class="keyword">ref</span> <span class="built_in">double</span> a ,<span class="keyword">out</span> <span class="built_in">double</span> b, <span class="built_in">double</span> c</span>)</span></span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">return</span> a+b+c;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>局部变量（狭义的变量，就是函数里声明的变量）</p></li><li><p>变量的声明<br>&lt;有效修饰符组合&gt; 类型 变量名 &lt;初始化器&gt;<br><code>int a;</code><br><code>public static int Amount;</code>有效修饰符组合：<code>public static</code><br><code>int a = 1;</code>初始化器 <code>= 1</code></p></li></ul><h3 id="4-3-2-值类型变量"><a href="#4-3-2-值类型变量" class="headerlink" title="4.3.2 值类型变量"></a>4.3.2 值类型变量</h3><p>结构体类型属于值类型<br>byte/sbyte/short/ushort</p><ul><li>值类型，按这种类型的实际大小来分配内存</li><li>值类型的变量没有实例，它的实例与变量是合二为一的，就是在声明变量并赋值时，就分配好了一块内存给他<br>变量地址的值就是给变量的赋值</li></ul><h3 id="4-3-3-引用类型变量"><a href="#4-3-3-引用类型变量" class="headerlink" title="4.3.3 引用类型变量"></a>4.3.3 引用类型变量</h3><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title">Student</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="built_in">uint</span> ID;</span><br><span class="line">  <span class="built_in">ushort</span> Score;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>引用类型<ul><li><code>Student stu;</code><ul><li>先给引用类型分 4Byte 地址，里面 32 位全设为 0</li></ul></li><li><code>stu = new Student();</code><ul><li>先去堆内存里创建 Student 实例，并将堆内存地址保存在 stu 的 4Byte 中</li><li>在堆内存里创建实例：<ul><li><code>uint ID</code>给 4Byte</li><li><code>ushort Score</code>给 2Byte</li></ul></li></ul></li><li><code>Student stu2;</code><ul><li>在内存中分配 4Byte 地址给 stu2</li></ul></li><li><code>stu2 = stu;</code><ul><li>将 stu 中的数据赋值给 stu2</li></ul></li></ul></li></ul><p><strong>stu 中装的是 Student 一个实例所在堆内存中的内存地址，也就是 30000001</strong></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220611152529.png" alt="20220611152529"></p><h3 id="4-3-4-其他变量知识"><a href="#4-3-4-其他变量知识" class="headerlink" title="4.3.4 其他变量知识"></a>4.3.4 其他变量知识</h3><ul><li>局部变量在 stack 栈上分配内存</li><li>变量的默认值：没有给变量赋值，变量有个默认值：0</li><li>常量：值不可以改变的变量：<code>const int changliangpi = 3.14;</code>,常量的初始化器不能省略，常量不能被再次赋值</li><li>装箱与拆箱<code>int x = 100;</code><ul><li>装箱<code>object obj = x;</code>obj 所引用的值是栈上值类型的值时，先把值类型的值 copy 到堆上，然后将堆内存地址放到 obj 变量对应的内存空间中<ul><li>将栈上的值类型值 封装成 obj 的实例，并刻在堆上</li></ul></li><li>拆箱<code>int y = (int)obj;</code>，现给 y 变量分配地址，将 obj 引用的在堆内存中的值移动到 y 变量的地址中<ul><li>将堆上 obj 的实例里的值，存到栈上</li></ul></li><li>装箱拆箱损失程序性能</li></ul></li></ul><div class="note primary">            <ul><li>内存<ul><li>栈:小数据 2M，值类型变量，引用类型变量<ul><li>值类型变量和实例合二为一</li><li>引用类型变量中存放的是实例所在堆内存的地址</li></ul></li><li>堆:大数据，引用类型变量的实例 - 引用类型变量的实例（or 对象 or 真正的数据值）存放在堆内存中</li></ul></li></ul>          </div><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/heap_stack.png" alt="heap_stack"></p><hr><h1 id="5-方法"><a href="#5-方法" class="headerlink" title="5. 方法"></a>5. 方法</h1><h2 id="5-1-方法的由来"><a href="#5-1-方法的由来" class="headerlink" title="5.1 方法的由来"></a>5.1 方法的由来</h2><p>函数(C/C++) —&gt; 方法(C#/Java 等面向对象的语言)</p><p><strong>C 中的函数写法：</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="comment">// 函数：</span></span><br><span class="line"><span class="type">double</span> <span class="title function_">Add</span><span class="params">(<span class="type">double</span> a, <span class="type">double</span> b)</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">double</span> x = <span class="number">3.0</span>;</span><br><span class="line"><span class="type">double</span> y = <span class="number">5.0</span>;</span><br><span class="line"><span class="type">double</span> result = Add(x, y);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%f+%f=%f&quot;</span>, x, y, result);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>C++中的函数写法</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">Add</span><span class="params">(<span class="type">double</span> a, <span class="type">double</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="type">double</span> x = <span class="number">3.0</span>;</span><br><span class="line"><span class="type">double</span> y = <span class="number">5.0</span>;</span><br><span class="line"><span class="type">double</span> result = <span class="built_in">Add</span>(x, y);</span><br><span class="line">std::cout &lt;&lt; x &lt;&lt; <span class="string">&quot;+&quot;</span> &lt;&lt; y &lt;&lt; <span class="string">&quot;=&quot;</span> &lt;&lt; result;</span><br><span class="line"><span class="comment">//std名称空间 ::为.</span></span><br><span class="line"><span class="comment">//std::cout &lt;&lt; &quot;Hello World~&quot;;</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当函数以类的成员出现的时候，就变成了方法</p><p><strong>C++中方法的写法：</strong></p><p>C++添加类的方法：<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220612125102.png" alt="20220612125102"><br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220612125033.png" alt="20220612125033"></p><p>生成.h 和.cpp 两个文件</p><ul><li>在.h 中声明方法：</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> once</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Student</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">SayHello</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">Add</span><span class="params">(<span class="type">double</span> a, <span class="type">double</span> b)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ul><li>在.cpp 中对方法定义：</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;Student.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Student::SayHello</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;Hello I&#x27;m a student~&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">Student::Add</span><span class="params">(<span class="type">double</span> a, <span class="type">double</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>然后在其他 cpp 文件中调用：</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;Student.h&quot;</span></span></span><br><span class="line"><span class="comment">// 标准的类用&lt;&gt;，自己的类用&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">//std名称空间 ::为.</span></span><br><span class="line"><span class="comment">//std::cout &lt;&lt; &quot;Hello World~&quot;;</span></span><br><span class="line">Student *pStu = <span class="keyword">new</span> <span class="built_in">Student</span>();</span><br><span class="line">pStu-&gt;<span class="built_in">SayHello</span>();</span><br><span class="line"><span class="type">double</span> x = <span class="number">3.0</span>;</span><br><span class="line"><span class="type">double</span> y = <span class="number">5.0</span>;</span><br><span class="line"><span class="type">double</span> result = pStu-&gt;<span class="built_in">Add</span>(x, y);</span><br><span class="line">std::cout &lt;&lt; x &lt;&lt; <span class="string">&quot;+&quot;</span> &lt;&lt; y &lt;&lt; <span class="string">&quot;=&quot;</span> &lt;&lt; result;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>C#中的方法(函数)写法</strong></p><div class="note primary">            <ul><li>函数</li><li>方法：函数以类的成员出现的时候叫方法</li></ul>          </div><p><strong>方法的作用</strong></p><ul><li>隐藏复杂的逻辑</li><li>将大算法分解为小算法</li><li>复用，示例：</li></ul><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> System;</span><br><span class="line"><span class="keyword">using</span> System.Collections.Generic;</span><br><span class="line"><span class="keyword">using</span> System.Linq;</span><br><span class="line"><span class="keyword">using</span> System.Text;</span><br><span class="line"><span class="keyword">using</span> System.Threading.Tasks;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> <span class="title">CSharpMethodExample</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">class</span> <span class="title">Program</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">Main</span>(<span class="params"><span class="built_in">string</span>[] args</span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            Calculator c = <span class="keyword">new</span> Calculator();</span><br><span class="line">            Console.WriteLine(c.GetCircleArea(<span class="number">10</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">class</span> <span class="title">Calculator</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 圆面积</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="built_in">double</span> <span class="title">GetCircleArea</span>(<span class="params"><span class="built_in">double</span> r</span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> Math.PI * r * r;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 圆柱体积</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="built_in">double</span> <span class="title">GetCylinderVolume</span>(<span class="params"><span class="built_in">double</span> r, <span class="built_in">double</span> h</span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 方法的复用</span></span><br><span class="line">            <span class="keyword">return</span> GetCircleArea(r) * h;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 圆锥体积</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="built_in">double</span> <span class="title">GetCooneVolume</span>(<span class="params"><span class="built_in">double</span> r, <span class="built_in">double</span> h</span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> GetCylinderVolume(r,h) / <span class="number">3</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-2-方法的定义与调用"><a href="#5-2-方法的定义与调用" class="headerlink" title="5.2 方法的定义与调用"></a>5.2 方法的定义与调用</h2><h3 id="5-2-1-方法的声明定义不分家"><a href="#5-2-1-方法的声明定义不分家" class="headerlink" title="5.2.1 方法的声明定义不分家"></a>5.2.1 方法的声明定义不分家</h3><ul><li>方法的声明：<br><code>&lt;attributes&gt; &lt;修饰符的有效组合&gt; &lt;partial&gt;</code>返回类型 方法名(形式参数parameter) 方法体<ul><li>修饰符：new public private async static 等等</li><li>返回类型：type void</li><li>方法名：标识符（动词或动词短语）</li><li>方法体：语句块：{语句} 或 分号：;</li></ul></li></ul><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="built_in">double</span> <span class="title">GetCircleArea</span>(<span class="params"><span class="built_in">double</span> r</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">return</span> Mati.PI * r * r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>静态方法 static 与类绑定</li><li>实例方法 与实例绑定</li></ul><p>例如一个类Calculator，new一个实例<code>Calculator c = new Calculator();</code>，如果是静态方法，用<code>Calculator.方法</code>来调用，如果是实例方法，则需要使用<code>c.方法</code>来调用</p><p>调用方法需要传入必要的实参：argument<br>要写成<code>c.GetCircleArea(x,y)</code>，而不能写成<code>c.GetCircleArea(double x,double y)</code></p><div class="note primary">            <p>形参parameter是变量，实参argument是值</p>          </div><h2 id="5-3-构造器-特殊的方法"><a href="#5-3-构造器-特殊的方法" class="headerlink" title="5.3 构造器-特殊的方法"></a>5.3 构造器-特殊的方法</h2><p>constructor构造器，是类型的成员之一</p><p>instance constructor实例构造器，构造实例在内存中的内部结构</p><p><strong>自定义构造器</strong><br><code>ctor + tab*2</code></p><ul><li>没有参数构造器</li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220629193542.png" alt="20220629193542"></p><ul><li>有参数构造器<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220629194137.png" alt="20220629194137"></li></ul><h2 id="5-4-方法的重载-overload"><a href="#5-4-方法的重载-overload" class="headerlink" title="5.4 方法的重载 overload"></a>5.4 方法的重载 overload</h2><p>同一个类下，有两个相同的方法名，但签名不同</p><p>声明带有重载的方法：</p><ul><li>方法签名method signature ，方法签名由方法的名称、类型形参<code>&lt;T&gt;</code>的个数和它的每一个形参的类型和<em>形参传递模式|种类</em>（值、引用ref或输出out）组成（按从左到右的顺序）。<strong>不包含返回类型</strong></li><li>实例构造函数签名由它的每一个形参的类型和形参传递模式（值、引用或输出）组成（按从左到右的顺序）。</li><li>重载决策：根据传入的参数类型选择一个最佳的函数成员来实施调用</li></ul><h2 id="5-5-如何对方法进行debug"><a href="#5-5-如何对方法进行debug" class="headerlink" title="5.5 如何对方法进行debug"></a>5.5 如何对方法进行debug</h2><ul><li>设置断点</li><li>观察差方法调用时的call stack调用堆栈：该程序语句的父级（谁调用的当前语句）</li><li>Step-into逐语句，Step-over逐过程（不用进入另一个方法），Step-out跳出（返回上层）</li><li>观察局部变量的值和变化</li></ul><h2 id="5-6-方法的调用与栈"><a href="#5-6-方法的调用与栈" class="headerlink" title="5.6 方法的调用与栈*"></a>5.6 方法的调用与栈*</h2><p>对satck frame（方法被调用时在内存中的布局）的分析，方法调用如何使用栈内存</p><p>main（主调者caller）中调用other（被调者callee）函数，需要传入实参时，实参归main管。</p><h1 id="6-操作符"><a href="#6-操作符" class="headerlink" title="6. 操作符"></a>6. 操作符</h1><h2 id="6-1-操作符概览"><a href="#6-1-操作符概览" class="headerlink" title="6.1 操作符概览"></a>6.1 操作符概览</h2><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/blogs/pictures/20220703193132.png" alt="20220703193132"></p><h2 id="6-1-操作符的本质"><a href="#6-1-操作符的本质" class="headerlink" title="6.1 操作符的本质"></a>6.1 操作符的本质</h2><h2 id="6-1-操作符的优先级"><a href="#6-1-操作符的优先级" class="headerlink" title="6.1 操作符的优先级"></a>6.1 操作符的优先级</h2><h2 id="6-1-同级操作符的运算顺序"><a href="#6-1-同级操作符的运算顺序" class="headerlink" title="6.1 同级操作符的运算顺序"></a>6.1 同级操作符的运算顺序</h2><h2 id="6-1-各类操作符示例"><a href="#6-1-各类操作符示例" class="headerlink" title="6.1 各类操作符示例"></a>6.1 各类操作符示例</h2><h1 id="7-表达式、语句"><a href="#7-表达式、语句" class="headerlink" title="7. 表达式、语句"></a>7. 表达式、语句</h1>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
            <tag> C# </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python多线程学习</title>
      <link href="/Learn/Learn-Python_Threading/"/>
      <url>/Learn/Learn-Python_Threading/</url>
      
        <content type="html"><![CDATA[<p>Python中的多线程，如何使用<br><span id="more"></span></p><h1 id="1-初步认识多线程"><a href="#1-初步认识多线程" class="headerlink" title="1. 初步认识多线程"></a>1. 初步认识多线程</h1><blockquote><p>参考<a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017627212385376">廖雪峰老师的教程</a></p></blockquote><p><strong>多进程和多线程</strong><br>我的理解就是<strong>单个CPU</strong>可以执行一个或多个进程，每个任务执行很短时间，从而骗过人的感觉，让我们感觉好像是多个任务一起进行，而<strong>多核CPU</strong>可以并行执行多任务，如果任务数量超过CPU的数量，则会让一个CPU轮流执行多个任务<br>每个进程，也就是每个任务中又分为很多个子任务，也就是线程，一个进程中可以启动多个线程，各个线程中的任务可以同时进行</p><p>Python中的多任务执行有三种方式：</p><ul><li>多进程模式，多个python程序同时进行，每个程序一个线程</li><li>多线程模式，一个python程序同时进行多个线程</li><li>多进程+多线程，这种模型非常复杂，我的脑子不够用，就不用了</li></ul><p>我也想要执行一个任务，直来直去，但是总有种情况，我迫不得已要多任务同时进行，我太难了<br>比如我做的GUI，要求可以执行多个任务，但是也要能够暂停其中的一个任务，其他任务不受干扰，好了，废话不多说了，开始学习</p><h1 id="2-线程基础"><a href="#2-线程基础" class="headerlink" title="2. 线程基础"></a>2. 线程基础</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入线程函数threading</span></span><br><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fund</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;执行的程序&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加一个线程</span></span><br><span class="line">th = Thread(target=fund)</span><br><span class="line"><span class="comment"># 开始进程</span></span><br><span class="line">th.start()</span><br><span class="line"><span class="comment"># 等到一个进程结束时退出</span></span><br><span class="line">th.join()</span><br></pre></td></tr></table></figure><blockquote><p>参考知乎的大佬写的教程<br>作者：Dwzb<br>链接：<a href="https://zhuanlan.zhihu.com/p/34004179">https://zhuanlan.zhihu.com/p/34004179</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><h2 id="2-1-线程的开始"><a href="#2-1-线程的开始" class="headerlink" title="2.1 线程的开始"></a>2.1 线程的开始</h2><p><code>th.start()</code> 可以放到循环中，同时进行多个线程</p><h2 id="2-2-线程结束时停止"><a href="#2-2-线程结束时停止" class="headerlink" title="2.2 线程结束时停止"></a>2.2 线程结束时停止</h2><p><code>th.join()</code> 加上这句话，则多线程只能一个结束后执行下一个<br>如果没有这句语句，就会直接用多线程开始执行，不会等一个结束再执行下一个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread</span><br><span class="line">t = time.time()</span><br><span class="line">ths = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    th = Thread(target = myfun)</span><br><span class="line">    th.start()</span><br><span class="line">    ths.append(th)</span><br><span class="line"><span class="keyword">for</span> th <span class="keyword">in</span> ths:</span><br><span class="line">    th.join()</span><br><span class="line"><span class="built_in">print</span>(time.time() - t)</span><br><span class="line"><span class="comment"># 结果为 1.0038363933563232</span></span><br></pre></td></tr></table></figure><h2 id="2-3-线程的名称"><a href="#2-3-线程的名称" class="headerlink" title="2.3 线程的名称"></a>2.3 线程的名称</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="built_in">print</span>(threading.current_thread().getName())</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">myfun</span>():</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(threading.current_thread().name)</span><br><span class="line">    a = <span class="number">1</span> + <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    th = threading.Thread(target = myfun, name = <span class="string">&#x27;thread &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    th.start()</span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">MainThread</span><br><span class="line">thread <span class="number">0</span></span><br><span class="line">thread <span class="number">1</span></span><br><span class="line">thread <span class="number">4</span></span><br><span class="line">thread <span class="number">3</span></span><br><span class="line">thread <span class="number">2</span></span><br></pre></td></tr></table></figure><ul><li><code>threading.current_thread()</code>表示当前线程，可以调用<code>name</code>或<code>getName()</code>获取线程名称</li><li>任何进程的都会有一个主线程，这个进程与新加的线程是相互独立的</li><li><code>Thread</code>表示启动一个新的线程<code>name</code>参数表示线程的名字</li><li><code>threading.current_thread().getName()</code>是主进程名字<code>MainThread</code>,<code>th.name</code>则是子thread名字</li></ul><h2 id="2-4-Thread函数"><a href="#2-4-Thread函数" class="headerlink" title="2.4 Thread函数"></a>2.4 Thread函数</h2><p>参数介绍：</p><ul><li><code>target</code> 线程执行的函数</li><li><code>name</code> 线程的名称</li><li><code>args</code> target对应得函数得参数，用元组传入，比如<code>func(age,name)</code> <code>Thread(target = func , args = (13, &#39;name&#39;))</code></li><li><code>daemon</code> 主线程默认是false，，如果没有指定则继承父线程的值。True则如果主线程运行结束，该线程也停止运行；False则该线程会继续运行直到运行结束，无视主线程如何</li><li><code>group</code> 是预留的一个参数，用于以后扩展ThreadGroup类，现在没用</li></ul><h2 id="2-5-Thread对象"><a href="#2-5-Thread对象" class="headerlink" title="2.5 Thread对象"></a>2.5 Thread对象</h2><p>属性和方法：</p><ul><li>name 线程名称</li><li>ident 线程标识符号</li><li>daemon 是否为守护线程</li></ul><p><strong>_init</strong>(self, group=None, target=None, name=None, args=(), kwargs=None, *, daemon=None)<br>参数：</p><ul><li>group 无用，保留参数</li><li>target 可调用的目标</li><li>name 线程的名称</li><li>args,kwargs 调用目标的参数</li><li>daemon 是否为守护线程</li><li>start() 开始执行</li><li>join(timeout=None) 阻塞timeout秒，否则直到启动的线程终止前一直挂起</li><li>is_alive () 线程是否存活</li><li>isDaemon() 是否为守护线程</li><li><p>setDaemon(daemonic) 设置为守护线程</p></li><li><p>getName()  .name  获取线程名</p></li><li>setName() 设置线程名</li><li>start()  join()</li><li>join()有一个timeout参数，表示等待这个线程结束时，如果等待时间超过这个时间，就不再等，继续进行下面的代码，但是这个线程不会被中断</li><li>run() 也是运行这个线程，但是必须等到这个线程运行结束才会继续执行之后的代码（如果将上面的start全换成run则相当于没有开多线程）</li><li>is_alive()如果该线程还没运行完，就是True否则False</li><li>daemon 返回该线程的daemon</li><li>setDaemon(True)设置线程的daemon</li></ul><h2 id="2-6-threading"><a href="#2-6-threading" class="headerlink" title="2.6 threading"></a>2.6 threading</h2><ul><li>threading.currentThread(): 返回当前的线程变量</li><li>threading.enumerate(): 返回一个包含正在运行的线程的list</li><li>threading.activeCount(): 返回正在运行的线程数量，与len(threading.enumerate())有相同的结果</li></ul><p>threading模块的类对象</p><ul><li>Thread 执行线程</li><li>Timer 在运行前等待一段时间的执行线程</li><li>Lock 原语锁（互斥锁，简单锁）</li><li>RLock 重入锁，使单一线程可以（再次）获得已持有的锁</li><li>Condition 条件变量，线程需要等待另一个线程满足特定条件</li><li>Event 事件变量，N个线程等待某个事件发生后激活所有线程</li><li>Semaphore 线程间共享资源的寄存器</li><li>BoundedSemaphore 与Semaphore 相似，它不允许超过初始值</li><li>Barrie 执行线程达到一定数量后才可以继续</li></ul><p>threading模块的函数</p><ul><li>activeCount() 获取当前活动中的Thread对象个数</li><li>currentThread() 获取当前的Thread对象</li><li>enumerate() 获取当前活动的Thread对象列表</li><li>settrace(func) 为所有线程设置一个跟踪（trace）函数</li><li>setprofile(func) 为所有线程设置配置文件（profile）函</li><li>stack_size(size=None) 获取新创建线程的栈大小，也可设置线程栈的大小为size。</li></ul><h1 id="3-线程进阶"><a href="#3-线程进阶" class="headerlink" title="3. 线程进阶"></a>3. 线程进阶</h1><h2 id="3-1-派生Thread-的子类，并创建子类的实例"><a href="#3-1-派生Thread-的子类，并创建子类的实例" class="headerlink" title="3.1 派生Thread 的子类，并创建子类的实例"></a>3.1 派生Thread 的子类，并创建子类的实例</h2><p>我们可以通过继承Thread类，派生出一个子类，使用子类来创建多线程</p><p><strong>记住要在子类中初始化父类的方法Thread.<strong>init</strong>(self) 。需要重构 run() 方法来执行多线程的程序</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep, ctime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Thread 的子类 </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyThread</span>(<span class="title class_ inherited__">Thread</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, func, args</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param func: 可调用的对象</span></span><br><span class="line"><span class="string">        :param args: 可调用对象的参数</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        Thread.__init__(self)   <span class="comment"># 不要忘记调用Thread的初始化方法</span></span><br><span class="line">        self.func = func</span><br><span class="line">        self.args = args</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        self.func(*self.args)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">name, sec</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;---开始---&#x27;</span>, name, <span class="string">&#x27;时间&#x27;</span>, ctime())</span><br><span class="line">    sleep(sec)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;***结束***&#x27;</span>, name, <span class="string">&#x27;时间&#x27;</span>, ctime())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment"># 创建 Thread 实例</span></span><br><span class="line">    t1 = MyThread(func, (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    t2 = MyThread(func, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 启动线程运行</span></span><br><span class="line">    t1.start()</span><br><span class="line">    t2.start()</span><br><span class="line">    <span class="comment"># 等待所有线程执行完毕</span></span><br><span class="line">    t1.join()</span><br><span class="line">    t2.join()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h2 id="3-2-获取可调用对象的返回值"><a href="#3-2-获取可调用对象的返回值" class="headerlink" title="3.2 获取可调用对象的返回值"></a>3.2 获取可调用对象的返回值</h2><p>在多线程中运行的程序时与主线程分开，我们没法直接获取线程中程序的返回值。这时就可以使用派生Thread 的子类，将给过保存的实例属性中，通过一个新方法获取运行结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep, ctime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Thread 的子类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyThread</span>(<span class="title class_ inherited__">Thread</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, func, args</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param func: 可调用的对象</span></span><br><span class="line"><span class="string">        :param args: 可调用对象的参数</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        Thread.__init__(self)</span><br><span class="line">        self.func = func</span><br><span class="line">        self.args = args</span><br><span class="line">        self.result = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        self.result = self.func(*self.args)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getResult</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">name, sec</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;---开始---&#x27;</span>, name, <span class="string">&#x27;时间&#x27;</span>, ctime())</span><br><span class="line">    sleep(sec)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;***结束***&#x27;</span>, name, <span class="string">&#x27;时间&#x27;</span>, ctime())</span><br><span class="line">    <span class="keyword">return</span> sec</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment"># 创建 Thread 实例</span></span><br><span class="line">    t1 = MyThread(func, (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    t2 = MyThread(func, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 启动线程运行</span></span><br><span class="line">    t1.start()</span><br><span class="line">    t2.start()</span><br><span class="line">    <span class="comment"># 等待所有线程执行完毕</span></span><br><span class="line">    t1.join()</span><br><span class="line">    t2.join()</span><br><span class="line">    <span class="comment"># 或线程中程序的运行结果</span></span><br><span class="line">    <span class="built_in">print</span>(t1.getResult())</span><br><span class="line">    <span class="built_in">print</span>(t2.getResult())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h2 id="3-3-多线程的同步问题"><a href="#3-3-多线程的同步问题" class="headerlink" title="3.3 多线程的同步问题"></a>3.3 多线程的同步问题</h2><p>一般在多线程代码中，总会有一些特定的函数或代码块不想被多个线程同时执行，如：修改数据库、更新文件或其他会产生程序冲突的类似情况</p><p>当任意数量的线程可以访问临界区的代码，当在同一时刻只能有一个线程可以通过时，就需要使用同步。我们可以选择合适的同步原语，也可以让线程控制机制来执行同步。</p><p>最常用的同理原语有：锁/互斥，以及信号量。锁是最简单最低级的机制。信号量用于多线程竞争有限资源的情况。</p><blockquote><p>强烈推荐<a href="https://zhuanlan.zhihu.com/p/94344847">大佬的教程</a></p></blockquote><h2 id="3-4-Lock-同步锁（原语锁）"><a href="#3-4-Lock-同步锁（原语锁）" class="headerlink" title="3.4 Lock 同步锁（原语锁）"></a>3.4 Lock 同步锁（原语锁）</h2><h3 id="3-4-1-同步锁的使用"><a href="#3-4-1-同步锁的使用" class="headerlink" title="3.4.1 同步锁的使用"></a>3.4.1 同步锁的使用</h3><p><strong>加锁 与 解锁</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个锁对象</span></span><br><span class="line">lock = threading.Lock()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得锁，加锁</span></span><br><span class="line">lock.acquire()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 释放锁，解锁</span></span><br><span class="line">lock.release()</span><br></pre></td></tr></table></figure><p>当我们通过 lock.acquire() 获得锁后线程程将一直执行不会中断，直到该线程 lock.release( )释放锁后线程才有可能被释放(注意：锁被释放后线程不一定会释放)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个锁对象</span></span><br><span class="line">lock = threading.Lock()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>():</span><br><span class="line">    <span class="keyword">global</span> num  <span class="comment"># 全局变量</span></span><br><span class="line">    <span class="comment"># lock.acquire()  # 获得锁，加锁</span></span><br><span class="line">    num1 = num</span><br><span class="line">    time.sleep(<span class="number">0.1</span>) </span><br><span class="line"><span class="comment"># sleep()操作，当在没有锁的情况下线程将在这里被释放出来，让给下一线程运行，而我们的num值还没有被修改，所以后面线程的num1的取值都是100</span></span><br><span class="line">    num = num1 - <span class="number">1</span></span><br><span class="line">    <span class="comment"># lock.release()  # 释放锁，解锁</span></span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">num = <span class="number">100</span></span><br><span class="line">l = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 开启100个线程</span></span><br><span class="line">    t = threading.Thread(target=func, args=())</span><br><span class="line">    t.start()</span><br><span class="line">    l.append(t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待线程运行结束. 等到线程结束后再print num</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> l:</span><br><span class="line">    i.join()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(num)</span><br></pre></td></tr></table></figure><p>注意：上面代码先将lock.acquire()和lock.release()行注释掉表示不使用锁，取消lock.acquire()和lock.release()行的注释表示使用锁</p><p>不使用锁程序运行输出为 99；使用锁程序运行结果为0</p><p><strong>Lock 与GIL(全局解释器锁）存在区别</strong></p><ul><li>Lock 锁的目的，它是为了保护共享的数据，同时刻只能有一个线程来修改共享的数据，而保护不同的数据需要使用不同的锁</li><li>GIL用于限制一个进程中同一时刻只有一个线程被CPU调度，GIL的级别比Lock高，GIL是解释器级别</li></ul><p><strong>GIL与Lock同时存在，程序执行如下：</strong></p><blockquote><ol><li>同时存在两个线程：线程A，线程B</li><li>线程A 抢占到GIL，进入CPU执行，并加了Lock，但为执行完毕，线程被释放</li><li>线程B 抢占到GIL，进入CPU执行，执行时发现数据被线程A Lock，于是线程B被阻塞</li><li>线程B的GIL被夺走，有可能线程A拿到GIL，执行完操作、解锁，并释放GIL</li><li>线程B再次拿到GIL，才可以正常执行</li></ol></blockquote><p>通过上述应该能看到，Lock 通过牺牲执行的效率换数据安全</p><h3 id="3-4-2-死锁"><a href="#3-4-2-死锁" class="headerlink" title="3.4.2 死锁"></a>3.4.2 死锁</h3><p>多线程最怕的是遇到死锁，两个或两个以上的线程在执行时，因争夺资源被相互锁住而相互等待</p><p><em>互锁造成死锁</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个锁对象</span></span><br><span class="line">lock1 = threading.Lock()</span><br><span class="line">lock2 = threading.Lock()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyThread</span>(threading.Thread):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.fun_A()</span><br><span class="line">        self.fun_B()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fun_A</span>(<span class="params">self</span>):</span><br><span class="line">        lock1.acquire()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;A_1 加锁&#x27;</span>, end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        lock2.acquire()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;A-2 加锁&#x27;</span>, end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        time.sleep(<span class="number">0.1</span>)</span><br><span class="line">        lock2.release()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;A-2 释放&#x27;</span>, end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        lock1.release()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;A-1 释放&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fun_B</span>(<span class="params">self</span>):</span><br><span class="line">        lock2.acquire()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;B-1 加锁&#x27;</span>, end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        lock1.acquire()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;B-2 加锁&#x27;</span>, end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        time.sleep(<span class="number">0.1</span>)</span><br><span class="line">        lock1.release()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;B-1 释放&#x27;</span>, end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        lock2.release()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;B-2 释放&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"><span class="comment"># 需要四个以上线程，才会出现死锁现象</span></span><br><span class="line">    t1 = MyThread()</span><br><span class="line">    t2 = MyThread()</span><br><span class="line">    t1.start()</span><br><span class="line">    t2.start()</span><br></pre></td></tr></table></figure><p>如果两个锁同时被多个线程运行，就有可能出现死锁，如果没出现死锁，就多运行几遍就会出现死锁现象</p><h3 id="3-4-3-重入锁-递归锁"><a href="#3-4-3-重入锁-递归锁" class="headerlink" title="3.4.3 重入锁/递归锁"></a>3.4.3 重入锁/递归锁</h3><p><code>threading.RLock()</code><br>为了支持同一个线程中多次请求同一资源，Python 提供了可重入锁(RLock)。这个RLock内部维护着一个锁(Lock)和一个计数器(counter)变量，counter 记录了acquire 的次数，从而使得资源可以被多次acquire。直到一个线程所有 acquire都被release(计数器counter变为0)，其他的线程才能获得资源。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个递归对象</span></span><br><span class="line">Rlock = threading.RLock()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyThread</span>(threading.Thread):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.fun_A()</span><br><span class="line">        self.fun_B()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fun_A</span>(<span class="params">self</span>):</span><br><span class="line">        Rlock.acquire()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;A加锁1&#x27;</span>, end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        Rlock.acquire()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;A加锁2&#x27;</span>, end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        time.sleep(<span class="number">0.2</span>)</span><br><span class="line">        Rlock.release()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;A释放1&#x27;</span>, end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        Rlock.release()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;A释放2&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fun_B</span>(<span class="params">self</span>):</span><br><span class="line">        Rlock.acquire()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;B加锁1&#x27;</span>, end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        Rlock.acquire()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;B加锁2&#x27;</span>, end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        time.sleep(<span class="number">3</span>)</span><br><span class="line">        Rlock.release()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;B释放1&#x27;</span>, end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        Rlock.release()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;B释放2&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    t1 = MyThread()</span><br><span class="line">    t2 = MyThread()</span><br><span class="line">    t1.start()</span><br><span class="line">    t2.start()</span><br></pre></td></tr></table></figure><p>当运行到程序B时，即使B休眠了3秒也不会切换线程。</p><p>使用重入锁时，counter 没有变为0(所有的acquire没有被释放掉)，即使遇到长时间的io操作也不会切换线程。</p><h1 id="4-线程实战"><a href="#4-线程实战" class="headerlink" title="4. 线程实战"></a>4. 线程实战</h1><h2 id="4-1-初步的练习和详细解释"><a href="#4-1-初步的练习和详细解释" class="headerlink" title="4.1 初步的练习和详细解释"></a>4.1 初步的练习和详细解释</h2>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>前端开发的基础知识</title>
      <link href="/Learn/Learn-HCJ%E5%89%8D%E7%AB%AF/"/>
      <url>/Learn/Learn-HCJ%E5%89%8D%E7%AB%AF/</url>
      
        <content type="html"><![CDATA[<p>学习前端前的基础知识，对，没错，就是html、css和js<br><span id="more"></span></p><p><strong>参考资料</strong>（大佬的笔记）</p><blockquote><p><a href="https://blog.csdn.net/wuyxinu/article/details/103515157">HTML</a><br><a href="https://blog.csdn.net/wuyxinu/article/details/103583618">CSS</a><br><a href="https://blog.csdn.net/wuyxinu/article/details/103642800">JS</a><br><a href="https://blog.csdn.net/wuyxinu/article/details/103646041">JS-下</a><br>还有 <a href="https://blog.csdn.net/wuyxinu/article/details/103669718">jQuery</a><br><a href="https://blog.csdn.net/wuyxinu/article/details/103774211">Node.js+Gulp</a></p></blockquote><p><strong>学习思路</strong></p><ul><li>[x] HTML 入门 </li><li>[ ] CSS 入门 </li><li>[ ] HTML 进阶 </li><li>[ ] CSS 进阶 </li><li>[ ] JavaScript 入门</li><li>[ ] jQuery 入门 </li><li>[ ] ASP.NET 入门（或 PHP 入门）</li><li>[ ] Ajax</li><li>[ ] ASP.NET 进阶（或 PHP 进阶）</li></ul><h1 id="1-html-html5-的基础"><a href="#1-html-html5-的基础" class="headerlink" title="1. html/html5 的基础"></a>1. html/html5 的基础</h1><ul><li><strong>1、什么是 HTML?</strong><br>HTML 是用来描述网页的一种语言<br>HTML 指超文本标记语言( Hyper Text Markup Language)<br>HTML 不是编程语言,是一种标记语言</li><li>2、<strong>HTML5 的新特性</strong><br>用于绘画的 canvas 标签<br>用于媒介回放的 vdeo 和 audo 元素<br>对本地离线储存的更好支持<br>新的特殊内容元素,如: article、 footer、 header、nav、 section<br>新的表单控件如: calendar、date、time、emai、url、 search</li></ul><h2 id="1-1-html-基础结构"><a href="#1-1-html-基础结构" class="headerlink" title="1.1 html 基础结构"></a>1.1 html 基础结构</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span>  #html file</span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span>&gt;</span><span class="tag">&lt;/<span class="name">meta</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span> #html main</span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span> <span class="attr">align</span> = <span class="string">&quot;center&quot;</span>&gt;</span>    <span class="comment">&lt;!--head first    居中center   h1~6--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>#paragraph</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span> = <span class="string">&quot;http://wfaief.com&quot;</span>&gt;</span>this is a link<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="1-2-页头-head"><a href="#1-2-页头-head" class="headerlink" title="1.2 页头 head"></a>1.2 页头 head</h2><ul><li>title 定义网页的标题</li><li>meta 定义网页的基本信息</li><li>style 定义 css 样式</li><li>link 定义链接外部 css 文件或脚本文件</li></ul><ul><li>script 定义脚本语言</li><li>base 定义页面所有链接的基本定位</li></ul><h2 id="1-3-页身-body"><a href="#1-3-页身-body" class="headerlink" title="1.3 页身 body"></a>1.3 页身 body</h2><h3 id="1-3-1-段落与文字"><a href="#1-3-1-段落与文字" class="headerlink" title="1.3.1 段落与文字"></a>1.3.1 段落与文字</h3><ul><li>段落<ul><li>h1~h6 多级标题</li><li>p 段落</li><li>br 换行<code>&lt;br/&gt;</code></li><li>hr 水平线<code>&lt;hr/&gt;</code></li><li>div 分割-块元素</li><li>span 区域-行内元素</li></ul></li><li>文字<ul><li>strong 加强-粗体</li><li>em 强调-斜体</li><li>cite 引用</li><li>sup 上标</li><li>sub 下标</li></ul></li><li>网页特殊符号<ul><li><code>&amp;nbsp;</code>空格</li></ul></li></ul><p><em>div和span没有任何语义,正是因为没有语义，这两个标签一般都是配合CSS来定义元素 样式的</em></p><ul><li><p>div是块元素，可以包含任何块元素和行内元素，不会与其他元素位于同一行；span 是行内元素，可以与其他行内元素位于同一行。</p></li><li><p>div常用于页面中较大块的结构划分，然后配合CSS来操作；span 一般用来包含文字等, 它没有结构的意义，纯粹是应用样式。当其他行内元素都不适合的时候，可以用span来配合CSS 操作</p></li></ul><h3 id="1-3-2-列表"><a href="#1-3-2-列表" class="headerlink" title="1.3.2 列表"></a>1.3.2 列表</h3><ul><li>ol 有序列表</li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">ol</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">li</span>&gt;</span>有序列表项<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">li</span>&gt;</span>有序列表项<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">li</span>&gt;</span>有序列表项<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ol</span>&gt;</span></span><br><span class="line"></span><br><span class="line">type 属性： 数字 123--- 小写字母 abc--- 大写字母 ABC--- 小写罗马数字</span><br><span class="line">i、ii、iii…… 大写罗马数字 I、II、III……</span><br></pre></td></tr></table></figure><ul><li>ul 无序列表（重点）</li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">ul</span> <span class="attr">type</span>=<span class="string">&quot;列表项符号&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">li</span>&gt;</span>无序列表项<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">li</span>&gt;</span>无序列表项<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">li</span>&gt;</span>无序列表项<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"></span><br><span class="line">type 属性值 disc 默认值，实心圆“●” circle 空心圆“○” square 实心正方形“■”</span><br></pre></td></tr></table></figure><ul><li>dl 定义列表</li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dl</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dt</span>&gt;</span>定义名词<span class="tag">&lt;/<span class="name">dt</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dd</span>&gt;</span>定义描述<span class="tag">&lt;/<span class="name">dd</span>&gt;</span></span><br><span class="line">  ……</span><br><span class="line"><span class="tag">&lt;/<span class="name">dl</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="1-3-3-表格"><a href="#1-3-3-表格" class="headerlink" title="1.3.3 表格"></a>1.3.3 表格</h3><ul><li><p>基本结构</p><ul><li>table 表格</li><li>tr 表格行</li><li>td 单元格</li></ul></li><li><p>完整结构</p><ul><li>caption 标题</li><li>thead 表头</li><li>th 表头单元格</li><li>tbody 表身</li><li>tfoot 表脚</li><li>td rowspan 合并行<br><code>&lt;td rowspan=&quot;跨度的行数&quot;&gt;</code></li><li>td colspan 合并列<br><code>&lt;td colspan=&quot;跨度的列数&quot;&gt;</code></li></ul></li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">table</span></span></span><br><span class="line"><span class="tag">    &lt;<span class="attr">cation</span>&gt;</span>表格标题<span class="tag">&lt;/<span class="name">cation</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--头--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">thead</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">th</span>&gt;</span>表头单元格1<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">th</span>&gt;</span>表头单元格2<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">thead</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--表身--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tbody</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>标准单元格1<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>标准单元格2<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>标准单元格1<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>标准单元格2<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tbody</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--表脚--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tfoot</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>标准单元格1<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>标准单元格2<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tfoot</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="1-3-4-图像"><a href="#1-3-4-图像" class="headerlink" title="1.3.4 图像"></a>1.3.4 图像</h3><p><code>&lt;img src=&quot;图片地址&quot; alt=&quot;图片描述（给搜索引擎看）&quot; title=&quot;图片描述&quot;&gt;</code></p><ul><li>src 图像的文件地址</li><li>alt 图片显示不出来时提示的文字</li><li>title 鼠标移动到图片上的提示文字</li></ul><h3 id="1-3-5-链接"><a href="#1-3-5-链接" class="headerlink" title="1.3.5 链接"></a>1.3.5 链接</h3><ul><li>href 链接</li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;链接地址&quot;</span> <span class="attr">target</span>=<span class="string">&quot;目标窗口的打开方式&quot;</span>&gt;</span></span><br><span class="line">  target 属性值 \_self 默认方式，即在当前窗口打开链接 \_blank</span><br><span class="line">  在一个全新的空白窗口中打开链接 \_top 在顶层框架中打开链接 \_parent</span><br><span class="line">  在当前框架的上一层里打开链接&lt;/a</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure><ul><li><p>内部链接</p></li><li><p>锚点链接</p></li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#music&quot;</span>&gt;</span>推荐音乐<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;<span class="name">br</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;music&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p>内部页面链接</p></li><li><p>外部链接</p></li></ul><h3 id="1-3-6-表单"><a href="#1-3-6-表单" class="headerlink" title="1.3.6 表单"></a>1.3.6 表单</h3><ul><li>input 的 type 中的属性<br><code>&lt;input type=&quot;表单类型&quot;/&gt;</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">- text单行文本框</span><br><span class="line">- password密码文本框</span><br><span class="line">- button按钮</span><br><span class="line">- reset重置按钮</span><br><span class="line">- image图像形式的提交按钮</span><br><span class="line">- radio单选按钮</span><br><span class="line">- checkbox复选框</span><br><span class="line">- hidden隐藏字段</span><br><span class="line">- file文件上传</span><br></pre></td></tr></table></figure><ul><li><p>textarea<br><code>&lt;textarea rows=&quot;行数&quot; cols=&quot;列数&quot;&gt;多行文本框内容&lt;/textarea&gt;</code> 多行文本框</p></li><li><p>select</p></li><li>option</li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">multiple</span>=<span class="string">&quot;mutiple&quot;</span> <span class="attr">size</span>=<span class="string">&quot;可见列表项的数目&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">option</span> <span class="attr">value</span>=<span class="string">&quot;选项值&quot;</span> <span class="attr">selected</span>=<span class="string">&quot;selected&quot;</span>&gt;</span>选项显示的内容<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">  ……</span><br><span class="line">  <span class="tag">&lt;<span class="name">option</span> <span class="attr">value</span>=<span class="string">&quot;选项值&quot;</span>&gt;</span>选项显示的内容<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="1-3-7-多媒体"><a href="#1-3-7-多媒体" class="headerlink" title="1.3.7 多媒体"></a>1.3.7 多媒体</h3><ul><li><p>embed 音频视频<br><code>&lt;embed src=&quot;多媒体文件地址&quot; width=&quot;播放界面的宽度&quot; height=&quot;播放界面的高度&quot;&gt;&lt;/embed&gt;</code><br>src 可以是绝对地址也可以是相对地址<br>width 和 height 使用 px 作为单位</p></li><li><p>bgsound 背景音乐-只适用 ie 浏览器<br><code>&lt;bgsound src=&quot;背景音乐的地址&quot;/&gt;</code></p></li></ul><p>loop=”2”表示重复 2 次，loop=”infinite”表示无限次循环播放，也可以使用 loop=”-1”表示无限次循环播放。</p><h3 id="1-3-8-浮动框架"><a href="#1-3-8-浮动框架" class="headerlink" title="1.3.8 浮动框架"></a>1.3.8 浮动框架</h3><p>浮动框架是一种较为特殊的框架，它是在浏览器窗口中嵌套的子窗口，整个页面并不一定是框架页面，但要包含一个框架窗口</p><ul><li>iframe</li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">iframe</span></span></span><br><span class="line"><span class="tag">  <span class="attr">src</span>=<span class="string">&quot;浮动框架的源文件&quot;</span></span></span><br><span class="line"><span class="tag">  <span class="attr">width</span>=<span class="string">&quot;浮动框架的宽&quot;</span></span></span><br><span class="line"><span class="tag">  <span class="attr">height</span>=<span class="string">&quot;浮动框架的高&quot;</span></span></span><br><span class="line"><span class="tag">&gt;</span><span class="tag">&lt;/<span class="name">iframe</span>&gt;</span></span><br><span class="line">width、height宽高 scrolling滚动条</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">iframe</span></span></span><br><span class="line"><span class="tag">  <span class="attr">src</span>=<span class="string">&quot;浮动框架的源文件&quot;</span></span></span><br><span class="line"><span class="tag">  <span class="attr">width</span>=<span class="string">&quot;浮动框架的宽&quot;</span></span></span><br><span class="line"><span class="tag">  <span class="attr">height</span>=<span class="string">&quot;浮动框架的高&quot;</span></span></span><br><span class="line"><span class="tag">  <span class="attr">scrolling</span>=<span class="string">&quot;取值&quot;</span></span></span><br><span class="line"><span class="tag">&gt;</span><span class="tag">&lt;/<span class="name">iframe</span>&gt;</span></span><br></pre></td></tr></table></figure><p>取值</p><ul><li>auto 默认整个表格左对齐</li><li>yes 总是显示滚动条</li><li>no 任何情况不显示滚动条</li></ul><h2 id="1-4-HTML-XHTML-HTML5-简介"><a href="#1-4-HTML-XHTML-HTML5-简介" class="headerlink" title="1.4 HTML XHTML HTML5 简介"></a>1.4 HTML XHTML HTML5 简介</h2><h3 id="1-4-1-XHTML"><a href="#1-4-1-XHTML" class="headerlink" title="1.4.1 XHTML"></a>1.4.1 XHTML</h3><p>xhtml 比 html 更加严格</p><ul><li>1、XHTML 标签必须闭合</li><li>2、XHTML 标签以及属性必须小写</li><li>3、XHTML 标签属性必须用引号</li><li>4、XHTML 标签用 jd 属性代替 name 属</li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://www.w3.org/1999/xhtml&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line">      “<span class="tag">&lt;<span class="name">span</span> <span class="attr">style</span>=<span class="string">&quot;font-weight:bold;color:Red;&quot;</span>&gt;</span>视觉化思考<span class="tag">&lt;/<span class="name">span</span>&gt;</span>”能以独特而有效的方式，让你的心有更大的空间来解决问题。</span><br><span class="line">    <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="1-4-2-HTML5"><a href="#1-4-2-HTML5" class="headerlink" title="1.4.2 HTML5"></a>1.4.2 HTML5</h3><p>HTML 5 除了新增部分标签之外，还增加了一组技术，包括 canvas、SVG、WebSocket.本地存储等。这些新增的技术都是使用 JavaScript 来操作。也就是说，HTML 5 使得 HTML 从一门“标记语言” 转变为一门 <strong>“编程语言”</strong></p><ul><li>声明更简洁 <code>&lt;!DOCTYPE html&gt;</code></li><li>不再区分大小写 <code>&lt;div&gt;绿叶学习网&lt;/DIV&gt;</code></li><li>允许属性值不加引号 <code>&lt;div id=wrapper style=co1or: red&gt; 绿叶学习网 &lt;/div&gt;</code></li><li>允许部分属性的属性值省略<ul><li><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">&quot;ntext&quot;</span> <span class="attr">readonly</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">&quot;checkbox&quot;</span> <span class="attr">checked</span>/&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><div class="table-container"><table><thead><tr><th>省略形式</th><th>等价于</th></tr></thead><tbody><tr><td>checked</td><td>checked=”nchecked”</td></tr><tr><td>readonly</td><td>readonly=”readonly”</td></tr><tr><td>defer</td><td>defer=”defer”</td></tr><tr><td>ismap</td><td>ismap=”ismap”</td></tr><tr><td>nohref</td><td>nohref=”nohref”</td></tr><tr><td>noshade</td><td>noshade=”noshade”</td></tr><tr><td>nowrap</td><td>nowrap=”nowrap”</td></tr><tr><td>selected</td><td>selected=”selected”</td></tr><tr><td>disabled</td><td>ciisabled=”disableci”</td></tr><tr><td>multiple</td><td>multiple=”multiple”</td></tr><tr><td>noresize</td><td>noresize=”disabled”</td></tr></tbody></table></div><h2 id="1-5-知识点"><a href="#1-5-知识点" class="headerlink" title="1.5 知识点"></a>1.5 知识点</h2><h3 id="1-5-1-class与id"><a href="#1-5-1-class与id" class="headerlink" title="1.5.1 class与id"></a>1.5.1 class与id</h3><p>class,顾名思义，就是“类”。它釆用的思想跟C、Java等编程语言中的“类”相似。 我们可以为同一个页面的相同元素或者不同元素设置相同的class,然后使得相同class的元 素具有相同的CSS样式</p><p><strong>可以用多类名</strong>，<strong>用空格隔开</strong><br>eg: <code>&lt;h1 class=&quot;title tac&quot;&gt;文章总标题&lt;/h1&gt;</code></p><p>id属性具有唯一性，也就是说在一个页面中相同的id只允许出现一次<br>W3C建议，对 于页面关键的结构或者大结构，我们才使用id。所谓的关键结构，指的是诸如LOGO、导航、 主体内容、底部信息栏等结构。对于一些小地方，还是建议使用class属性</p><h3 id="1-5-2-浏览器标题栏小图标"><a href="#1-5-2-浏览器标题栏小图标" class="headerlink" title="1.5.2 浏览器标题栏小图标"></a>1.5.2 浏览器标题栏小图标</h3><p>放在head里面</p><p><code>&lt;link rel=&quot;shortcut icon&quot; type=&quot;image/x-icon&quot; href=&quot;favicon.icon&quot;/&gt;</code></p><h3 id="1-5-3-语义化"><a href="#1-5-3-语义化" class="headerlink" title="1.5.3 语义化"></a>1.5.3 语义化</h3><p>即尽可能地不使用div来编写，用自带的语句来编写</p><ul><li><p>标题语义化</p><ul><li>（1）一个页面只能有一个h1标签</li><li>（2）hl ~ h6之间不要断层</li><li>（3）不要用h1 ~ h6来定义样式</li><li>（4）不要用div来代替h1 ~ h6.</li></ul></li><li><p>图片语义化</p><ul><li>（1） alt属性和title属性<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;&quot;</span> <span class="attr">alt</span>=<span class="string">&quot;图片描述(给搜索引擎看)&quot;</span> <span class="attr">title</span>=<span class="string">&quot;图片描述(给用户看)&quot;</span>&gt;</span></span><br><span class="line">````</span><br><span class="line">  - （2） figure 元素和 figcaption 元素</span><br><span class="line">```html</span><br><span class="line"><span class="tag">&lt;<span class="name">figure</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;&quot;</span> <span class="attr">alt</span>=<span class="string">&quot;&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">figcaption</span>&gt;</span>图注文字<span class="tag">&lt;/<span class="name">figcaption</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">figure</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>表格语义化（在实际开发中，我们不建议使用表格布局，应该使用浮动布局或者定位布局）</p><ul><li>th表示“表头单元格”</li><li>caption表示“表格标题”</li><li>thead、tbody和tfoot这3个标签把表格从语义上分为三部分: 表头、表身和表脚<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">html</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://www.w3.org/1999/xhtml&quot;</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">&quot;text/css&quot;</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css">          <span class="selector-class">.body</span> &#123;</span></span><br><span class="line"><span class="language-css">              <span class="attribute">font-family</span>: <span class="string">&quot;微软雅黑&quot;</span>;</span></span><br><span class="line"><span class="language-css">              <span class="attribute">font-size</span>: <span class="number">14px</span>;</span></span><br><span class="line"><span class="language-css">          &#125;</span></span><br><span class="line"><span class="language-css">      </span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&#x27;content&#x27;</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">table</span>&gt;</span></span><br><span class="line">              </span><br><span class="line">              <span class="tag">&lt;<span class="name">caption</span>&gt;</span> 表格标题 <span class="tag">&lt;/<span class="name">caption</span>&gt;</span> </span><br><span class="line">              <span class="comment">&lt;!--表头--&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">thead</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">th</span>&gt;</span>表头单元格l<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">th</span>&gt;</span>表头单元格2<span class="tag">&lt;/<span class="name">th</span>&gt;</span> <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;/<span class="name">thead</span>&gt;</span></span><br><span class="line">              <span class="comment">&lt;!--表身--&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">tbody</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">td</span>&gt;</span>标准单元格l<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">td</span>&gt;</span>标准单元格2<span class="tag">&lt;/<span class="name">td</span>&gt;</span> <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">td</span>&gt;</span>标准单元格l<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">td</span>&gt;</span>标准单元格2<span class="tag">&lt;/<span class="name">td</span>&gt;</span> <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;/<span class="name">tbody</span>&gt;</span></span><br><span class="line">              <span class="comment">&lt;!--表脚--&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">tfoot</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">td</span>&gt;</span>标准单元格l<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">td</span>&gt;</span>标准单元格2<span class="tag">&lt;/<span class="name">td</span>&gt;</span> <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;/<span class="name">tfoot</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>表单语义化</p><ul><li>（1）label 标签</li><li>（2）fieldset 标签和 legend 标签<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">html</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://www.w3.org/1999/xhtml&quot;</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">&quot;index.aspx&quot;</span> <span class="attr">method</span>=<span class="string">&quot;post&quot;</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">fieldset</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">legend</span>&gt;</span>登录绿叶学习网<span class="tag">&lt;/<span class="name">legend</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line">                  <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">&quot;name&quot;</span>&gt;</span> 账号：<span class="tag">&lt;/<span class="name">label</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">&quot;text&quot;</span> <span class="attr">id</span>=<span class="string">&quot;name&quot;</span> <span class="attr">name</span>=<span class="string">&quot;name&quot;</span> /&gt;</span></span><br><span class="line">              <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">&quot;pwd&quot;</span>&gt;</span> 密码：<span class="tag">&lt;/<span class="name">label</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">&quot;password&quot;</span> <span class="attr">id</span>=<span class="string">&quot;pwd&quot;</span> <span class="attr">name</span>=<span class="string">&quot;pwd&quot;</span> /&gt;</span></span><br><span class="line">              <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">&quot;checkbox&quot;</span> <span class="attr">id</span>=<span class="string">&quot;remember-me&quot;</span> <span class="attr">name</span>=<span class="string">&quot;remember-me&quot;</span> /&gt;</span> <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">&quot;remember-me&quot;</span>&gt;</span> 记住我 <span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">&quot;submit&quot;</span> <span class="attr">value</span>=<span class="string">&quot;登录&quot;</span> /&gt;</span></span><br><span class="line">  </span><br><span class="line">          <span class="tag">&lt;/<span class="name">fieldset</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>其他语义化</p><ul><li>换行符 <code>&lt;br/&gt;</code>只适合用于p标签内 部的换行，不能用于其他标签</li><li>在实际开发中，大多数情况下都是使用无序列表ul，极少情况下会使用有序列表</li><li>strong标签和em标签 <strong>加粗</strong>和<em>斜体</em></li><li>del标签和ins标签   <del>删除线</del> 和 <ins>下划线</ins></li><li>img标签</li></ul></li><li><p>语义化验证：去掉css，要有好的可读性</p></li><li><p>html5删除地标签</p></li></ul><div class="table-container"><table><thead><tr><th style="text-align:left">标签</th><th>说明</th></tr></thead><tbody><tr><td style="text-align:left">basefont</td><td>定义页面文本默认字体，颜色，尺寸</td></tr><tr><td style="text-align:left">big</td><td>定义大字号文本</td></tr><tr><td style="text-align:left">center</td><td>定义文本居中</td></tr><tr><td style="text-align:left">font</td><td>定义文本的字体样式</td></tr><tr><td style="text-align:left">strike</td><td>定义删除线文本</td></tr><tr><td style="text-align:left">s</td><td>定义删除线文本</td></tr><tr><td style="text-align:left">u</td><td>定义下划线文本</td></tr><tr><td style="text-align:left">dir</td><td>定义目录列表，应该用ul代替</td></tr><tr><td style="text-align:left">acronym</td><td>定义首字母缩写，应该用abbr代替</td></tr><tr><td style="text-align:left">isindex</td><td>定义与文档相关的可搜索索引</td></tr><tr><td style="text-align:left">applet</td><td>定义嵌入的applet,应该用object代替</td></tr><tr><td style="text-align:left">frame</td><td>定义frameset中的一个特定的框架</td></tr><tr><td style="text-align:left">frameset</td><td>定义一个框架集</td></tr><tr><td style="text-align:left">noframes</td><td>为那些不支持框架的浏览器显示文本</td></tr></tbody></table></div><h2 id="1-6-HTML5"><a href="#1-6-HTML5" class="headerlink" title="1.6 HTML5"></a>1.6 HTML5</h2><h3 id="1-6-1-新的语义化标签"><a href="#1-6-1-新的语义化标签" class="headerlink" title="1.6.1 新的语义化标签"></a>1.6.1 新的语义化标签</h3><ul><li>header头部标签</li><li>nav导航标签</li><li>article内容标签</li><li>section块级标签</li><li>aside侧边栏标签</li><li>footer尾部标签</li></ul><h3 id="1-6-2-多媒体音频标签"><a href="#1-6-2-多媒体音频标签" class="headerlink" title="1.6.2 多媒体音频标签"></a>1.6.2 多媒体音频标签</h3><ul><li>audio音频<br>可以在不使用标签的情况下，也能够原生的支持音频格式文件的播放，但是：播放格式是有限的<ul><li>autoplay自动播放</li><li>controls出现该属性，则向用户显示控件</li><li>loop每当音频结束时自动重新播放</li><li>src音频地url</li></ul></li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">audio</span> <span class="attr">controls</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">source</span> <span class="attr">src</span>=<span class="string">&quot;./media/snow.mp3&quot;</span> <span class="attr">type</span>=<span class="string">&quot;audio/mpeg&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">audio</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>video视频<ul><li>autoplay自动播放</li><li>controls出现该属性，则向用户显示控件</li><li>width px 宽</li><li>height px 高</li><li>loop每当音频结束时自动重新播放</li><li>src音频地url</li><li>preload 预先加载</li><li>poster 加载等待地画面图片imgurl</li><li>muted 静音播放</li></ul></li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- &lt;video src=&quot;./media/video.mp4&quot; controls=&quot;controls&quot;&gt;&lt;/video&gt; --&gt;</span></span><br><span class="line">​</span><br><span class="line">  <span class="comment">&lt;!-- 谷歌浏览器禁用了自动播放功能，如果想自动播放，需要添加 muted 属性 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">video</span> <span class="attr">controls</span>=<span class="string">&quot;controls&quot;</span> <span class="attr">autoplay</span> <span class="attr">muted</span> <span class="attr">loop</span> <span class="attr">poster</span>=<span class="string">&quot;./media/pig.jpg&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">source</span> <span class="attr">src</span>=<span class="string">&quot;./media/video.mp4&quot;</span> <span class="attr">type</span>=<span class="string">&quot;video/mp4&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">source</span> <span class="attr">src</span>=<span class="string">&quot;./media/video.ogg&quot;</span> <span class="attr">type</span>=<span class="string">&quot;video/ogg&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">video</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="1-6-3-新增-input-标签"><a href="#1-6-3-新增-input-标签" class="headerlink" title="1.6.3 新增 input 标签"></a>1.6.3 新增 input 标签</h3><p><img src="https://img-blog.csdnimg.cn/20191229133014181.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1eXhpbnU=,size_16,color_FFFFFF,t_70" alt="input标签"></p><h3 id="1-6-4-新增表单属性"><a href="#1-6-4-新增表单属性" class="headerlink" title="1.6.4 新增表单属性"></a>1.6.4 新增表单属性</h3><p><img src="https://img-blog.csdnimg.cn/20191229133048182.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1eXhpbnU=,size_16,color_FFFFFF,t_70" alt="表单属性"></p><h1 id="2-css-的基础"><a href="#2-css-的基础" class="headerlink" title="2. css 的基础"></a>2. css 的基础</h1><p>CSS是什么？CSS，即“Cascading Style Sheet（层叠样式表）”，是用来控制网页的外观的一门技术</p><h2 id="2-1-css的引入"><a href="#2-1-css的引入" class="headerlink" title="2.1 css的引入"></a>2.1 css的引入</h2><p>外部样式表、内部样式表、内联样式表</p><h3 id="2-1-1-外部样式表"><a href="#2-1-1-外部样式表" class="headerlink" title="2.1.1 外部样式表"></a>2.1.1 外部样式表</h3><p>把CSS代码和HTML代码都单独放在不同文件中，然后在HTML文档中使用link标签来引用CSS样式表</p><p>外部样式表都是在head标签内使用link标签来引用的</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">&quot;css文件路径&quot;</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text/css&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-1-2-内部样式表"><a href="#2-1-2-内部样式表" class="headerlink" title="2.1.2 内部样式表"></a>2.1.2 内部样式表</h3><p>指的就是把CSS代码和HTML代码放在同一个文件中，其中CSS代码是放在<strong>标签对内的</strong></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://www.w3.org/1999/xhtml&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">title</span>&gt;</span>    </span><br><span class="line">    <span class="comment">&lt;!--这是内部样式表，CSS样式在style标签中定义--&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">&quot;text/css&quot;</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css">          <span class="selector-tag">p</span>&#123;<span class="attribute">color</span>:Red;&#125;      </span></span><br><span class="line"><span class="language-css">    </span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span>绿叶学习网<span class="tag">&lt;/<span class="name">p</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span>绿叶学习网<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-1-3-内联样式表-行内式"><a href="#2-1-3-内联样式表-行内式" class="headerlink" title="2.1.3 内联样式表(行内式)"></a>2.1.3 内联样式表(行内式)</h3><p>也是把CSS代码和HTML代码放在同一个文件中，但是跟内部样式表不同，CSS样式不是在<style></style>标签对中定义，而是<strong>在标签的style属性中定义</strong></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://www.w3.org/1999/xhtml&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">style</span>=<span class="string">&quot;color:Red; &quot;</span>&gt;</span>绿叶学习网<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">style</span>=<span class="string">&quot;color:Red; &quot;</span>&gt;</span>绿叶学习网<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">style</span>=<span class="string">&quot;color:Red; &quot;</span>&gt;</span>绿叶学习网<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2-2-元素的id和class"><a href="#2-2-元素的id和class" class="headerlink" title="2.2 元素的id和class"></a>2.2 元素的id和class</h2><p>id属性是唯一的，class可以有多个<br><code>&lt;div id=&quot;first&quot;&gt;绿叶学习网&lt;/div&gt;</code><br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;first&quot;</span>&gt;</span>绿叶学习网<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;first&quot;</span>&gt;</span>绿叶学习网<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="2-3-CSS选择器"><a href="#2-3-CSS选择器" class="headerlink" title="2.3 CSS选择器"></a>2.3 CSS选择器</h2><p>将想要的标签选中，然后再操作标签</p><p>选择器<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">选择器&#123;</span><br><span class="line">    样式属性<span class="number">1</span>:取值<span class="number">1</span>;</span><br><span class="line">    样式属性<span class="number">2</span>:取值<span class="number">2</span>;</span><br><span class="line">    ……</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*注释*/</span></span><br></pre></td></tr></table></figure></p><h3 id="2-3-1-基本选择器"><a href="#2-3-1-基本选择器" class="headerlink" title="2.3.1 基本选择器"></a>2.3.1 基本选择器</h3><h4 id="2-3-1-1-元素选择器"><a href="#2-3-1-1-元素选择器" class="headerlink" title="2.3.1.1 元素选择器"></a>2.3.1.1 元素选择器</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">div</span> &#123;<span class="attribute">width</span>:<span class="number">100px</span>; <span class="attribute">height</span>:<span class="number">100px</span>;&#125;</span><br><span class="line">元素符号 &#123;属性:属性值&#125;</span><br></pre></td></tr></table></figure><h4 id="2-3-1-2-id选择器"><a href="#2-3-1-2-id选择器" class="headerlink" title="2.3.1.2 id选择器"></a>2.3.1.2 id选择器</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#box</span> &#123;<span class="attribute">width</span>:<span class="number">100px</span>&#125;</span><br><span class="line"><span class="selector-id">#id</span>属性值 &#123;属性:属性值&#125;</span><br></pre></td></tr></table></figure><h4 id="2-3-1-3-class选择器"><a href="#2-3-1-3-class选择器" class="headerlink" title="2.3.1.3 class选择器"></a>2.3.1.3 class选择器</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.red</span> &#123;<span class="attribute">width</span>:<span class="number">100px</span>&#125;</span><br><span class="line"><span class="selector-class">.class</span>属性值 &#123;属性:属性值&#125;</span><br></pre></td></tr></table></figure><h4 id="2-3-2-2-子元素选择器"><a href="#2-3-2-2-子元素选择器" class="headerlink" title="2.3.2.2 子元素选择器"></a>2.3.2.2 子元素选择器</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#father1</span> <span class="selector-id">#p1</span> &#123;<span class="attribute">color</span>:red;&#125;</span><br><span class="line">#父元素选择 #子元素选择 &#123;属性:属性值&#125;</span><br><span class="line"></span><br><span class="line">&lt;<span class="selector-tag">div</span> id=&quot;father1&quot;&gt;</span><br><span class="line">    &lt;<span class="selector-tag">div</span> id=&quot;p1&quot;&gt;绿叶学习网&lt;/<span class="selector-tag">div</span>&gt;</span><br><span class="line">    &lt;<span class="selector-tag">div</span>&gt;绿叶学习网&lt;/<span class="selector-tag">div</span>&gt;</span><br><span class="line">&lt;/<span class="selector-tag">div</span>&gt;</span><br></pre></td></tr></table></figure><h4 id="2-3-2-5-相邻选择器"><a href="#2-3-2-5-相邻选择器" class="headerlink" title="2.3.2.5 相邻选择器"></a>2.3.2.5 相邻选择器</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#lv</span> + <span class="selector-tag">div</span> &#123;<span class="attribute">color</span>:red;&#125;</span><br><span class="line">#元素<span class="selector-tag">a</span> + 元素<span class="selector-tag">a</span>相邻的兄弟元素 &#123;属性:属性值&#125;</span><br><span class="line"></span><br><span class="line">则是第三个<span class="selector-tag">div</span>文本为红色red</span><br><span class="line">&lt;<span class="selector-tag">div</span>&gt;绿叶学习网&lt;/<span class="selector-tag">div</span>&gt;</span><br><span class="line">&lt;<span class="selector-tag">div</span> id=&quot;lv&quot;&gt;</span><br><span class="line">    &lt;<span class="selector-tag">p</span>&gt;绿叶学习网&lt;/<span class="selector-tag">p</span>&gt;</span><br><span class="line">&lt;/<span class="selector-tag">div</span>&gt;</span><br><span class="line">&lt;<span class="selector-tag">div</span>&gt;绿叶学习网&lt;/<span class="selector-tag">div</span>&gt;</span><br><span class="line">&lt;<span class="selector-tag">div</span>&gt;绿叶学习网&lt;/<span class="selector-tag">div</span>&gt;</span><br></pre></td></tr></table></figure><h4 id="2-3-2-6-群组选择器"><a href="#2-3-2-6-群组选择器" class="headerlink" title="2.3.2.6 群组选择器"></a>2.3.2.6 群组选择器</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">h3</span>,<span class="selector-tag">div</span>,<span class="selector-tag">p</span>,<span class="selector-tag">span</span> &#123;<span class="attribute">color</span>:red;&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-2-复合选择器"><a href="#2-3-2-复合选择器" class="headerlink" title="2.3.2 复合选择器"></a>2.3.2 复合选择器</h3><h4 id="2-3-2-1-后代选择器"><a href="#2-3-2-1-后代选择器" class="headerlink" title="2.3.2.1 后代选择器"></a>2.3.2.1 后代选择器</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.class</span> <span class="selector-tag">h3</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: red;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-tag">h3</span>是<span class="selector-tag">div</span>的后代，<span class="selector-tag">div</span>的元素名为class</span><br><span class="line"></span><br><span class="line"><span class="selector-id">#father1</span> <span class="selector-id">#p1</span> &#123;<span class="attribute">color</span>:red;&#125;</span><br><span class="line">#父元素选择 #子元素选择 &#123;属性:属性值&#125;</span><br><span class="line"></span><br><span class="line">&lt;<span class="selector-tag">div</span> id=&quot;father1&quot;&gt;</span><br><span class="line">    &lt;<span class="selector-tag">div</span> id=&quot;p1&quot;&gt;绿叶学习网&lt;/<span class="selector-tag">div</span>&gt;</span><br><span class="line">    &lt;<span class="selector-tag">div</span>&gt;绿叶学习网&lt;/<span class="selector-tag">div</span>&gt;</span><br><span class="line">&lt;/<span class="selector-tag">div</span>&gt;</span><br></pre></td></tr></table></figure><h4 id="2-3-2-2-子元素选择器-1"><a href="#2-3-2-2-子元素选择器-1" class="headerlink" title="2.3.2.2 子元素选择器"></a>2.3.2.2 子元素选择器</h4><p><strong>只选择儿子</strong>，<strong>不会选择所有的后代</strong><br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 子代选择器 */</span></span><br><span class="line"><span class="selector-class">.class</span>&gt;<span class="selector-tag">h3</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="2-3-2-3-交集选择器"><a href="#2-3-2-3-交集选择器" class="headerlink" title="2.3.2.3 交集选择器"></a>2.3.2.3 交集选择器</h4><p>既是p标签 又是.red类 关系</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">p</span><span class="selector-class">.red</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>:red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2-3-2-4-并集选择器-群组选择器"><a href="#2-3-2-4-并集选择器-群组选择器" class="headerlink" title="2.3.2.4 并集选择器(群组选择器)"></a>2.3.2.4 并集选择器(群组选择器)</h4><p>样式相同时，用并集选择<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">p</span>,<span class="selector-tag">span</span>,<span class="selector-class">.red</span>,<span class="selector-id">#id</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>:red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="2-3-2-5-链接伪类选择器"><a href="#2-3-2-5-链接伪类选择器" class="headerlink" title="2.3.2.5 链接伪类选择器"></a>2.3.2.5 链接伪类选择器</h4><h5 id="2-3-2-5-1-伪选择器"><a href="#2-3-2-5-1-伪选择器" class="headerlink" title="2.3.2.5.1 伪选择器"></a>2.3.2.5.1 伪选择器</h5><p>类选择器 <code>.demo</code><br>伪类选择器 <code>:demo</code><br>为链接添加一些特殊效果</p><h5 id="2-3-2-5-2-链接伪类选择器"><a href="#2-3-2-5-2-链接伪类选择器" class="headerlink" title="2.3.2.5.2 链接伪类选择器"></a>2.3.2.5.2 链接伪类选择器</h5><ul><li>a:link 未访问链接</li><li>a:visited 已访问链接</li><li>a:hover 鼠标移动到链接上</li><li>a:active 选定链接</li></ul><p>顺序不能颠倒：lvha<br>也算是交集选择器的变种<br>实际开发，都是先用一个总的a风格，然后只是写a:hover</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">a</span><span class="selector-pseudo">:link</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#333</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-tag">a</span><span class="selector-pseudo">:visited</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#9ca7a5</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-tag">a</span><span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: red;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-tag">a</span><span class="selector-pseudo">:active</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: green;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 子代选择，只是nav中的链接有hover样式 */</span></span><br><span class="line"><span class="selector-class">.nav</span> <span class="selector-tag">a</span><span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-4-CSS的font文本样式"><a href="#2-4-CSS的font文本样式" class="headerlink" title="2.4 CSS的font文本样式"></a>2.4 CSS的font文本样式</h2><ul><li>font-family 字体名：微软雅黑等等<br><code>font-family:微软雅黑;</code> 还有 <code>宋体</code> <code>Times New Roman等等</code></li></ul><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">h1</span> &#123;</span><br><span class="line"><span class="attribute">color</span>: deeppink;</span><br><span class="line"><span class="attribute">font-size</span>: xx-large;</span><br><span class="line"><span class="attribute">font-family</span>: JetBrains Mono,<span class="string">&quot;幼圆&quot;</span>,<span class="string">&quot;microsoft yahei&quot;</span>,<span class="string">&quot;微软雅黑&quot;</span>;</span><br><span class="line"><span class="comment">/*多个字体用逗号隔开，从开始寻找字体：如果第一个没有，用第二个，如果都没有，用默认字体*/</span></span><br><span class="line"><span class="comment">/*中文字体需要引号，英文字体无特殊字符，不需要引号*/</span></span><br><span class="line">  <span class="attribute">font-family</span>: <span class="string">&quot;\5FAE\8F6F\96C5\9ED1&quot;</span>   <span class="comment">/*= 微软雅黑*/</span></span><br><span class="line">  <span class="comment">/*Unicode字体，可以查表*/</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>font-size 字体大小: px/百分比/em<br><code>font-size:15px;</code> 一般都用px，像素</p></li><li><p>font-weight 字体粗细<br><code>font-weight:bold;</code> <code>normal</code> <code>100~900   400为normal，700为bold</code><br>用数字更好</p></li><li><p>font-style 字体斜体<br><code>font-style:italic;</code>斜体 <code>oblique</code>特殊字体（无italic变量） <code>normal</code>默认</p></li><li><p>color 颜色<br><code>color:blue;</code> <code>color:#000000;</code></p></li></ul><p><strong>综合性font语法</strong><br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.imporp</span> &#123;</span><br><span class="line"><span class="attribute">font</span>: initial <span class="number">700</span> <span class="number">20px</span> <span class="string">&quot;幼圆&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">也就是:</span><br><span class="line">.xuanze &#123;</span><br><span class="line">  <span class="attribute">font</span>: font-style font-weight font-size font-family;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="2-5-CSS的外观属性"><a href="#2-5-CSS的外观属性" class="headerlink" title="2.5 CSS的外观属性"></a>2.5 CSS的外观属性</h2><h3 id="2-5-1-color文本颜色"><a href="#2-5-1-color文本颜色" class="headerlink" title="2.5.1 color文本颜色"></a>2.5.1 color文本颜色</h3><ul><li>预定义颜色：red，green，blue，pink等等</li><li>十六进制：#FF0000，#000000黑色，#ffffff白色，#ff0000红色<ul><li>简写：#ccc   由 <code>#cccccc</code>简写，两两相同</li></ul></li><li>RGB颜色：<code>rgb(255,0,0)</code> <code>rgb(100%,0%,0%)</code></li></ul><h3 id="2-5-2-文本水平对齐方式"><a href="#2-5-2-文本水平对齐方式" class="headerlink" title="2.5.2 文本水平对齐方式"></a>2.5.2 文本水平对齐方式</h3><p><code>text-align:</code><br>left 左对齐（默认）<br>right 右对齐<br>center 居中对齐</p><h3 id="2-5-3-行间距"><a href="#2-5-3-行间距" class="headerlink" title="2.5.3 行间距"></a>2.5.3 行间距</h3><p><code>line-height:</code><br>em 百分比 px<br>一般用像素px，作为单位，一般行距比字号大7、8像素</p><h3 id="2-5-4-首行缩进"><a href="#2-5-4-首行缩进" class="headerlink" title="2.5.4 首行缩进"></a>2.5.4 首行缩进</h3><p><code>text-indent:</code> ， em为常用单位<br>为字符单位的倍数，如1em为一个字的宽度</p><h3 id="2-5-5-文本的装饰"><a href="#2-5-5-文本的装饰" class="headerlink" title="2.5.5 文本的装饰"></a>2.5.5 文本的装饰</h3><p><code>text-decoration:</code> 去掉连接下划线<br>none 默认，无装饰<br>blink 闪烁<br>underline 下划线<br>line-through 删除线<br>overline 上划线</p><h2 id="2-6-标签的显示模式"><a href="#2-6-标签的显示模式" class="headerlink" title="2.6 标签的显示模式"></a>2.6 标签的显示模式</h2><p><strong>这节是重点</strong></p><h3 id="2-6-1-标签的显示模式"><a href="#2-6-1-标签的显示模式" class="headerlink" title="2.6.1 标签的显示模式"></a>2.6.1 标签的显示模式</h3><p>div只能自己占一行，可以把多个span放到一行，a链接也可以一行放多个</p><p>块标签和行标签</p><h3 id="2-6-2-块级元素"><a href="#2-6-2-块级元素" class="headerlink" title="2.6.2 块级元素"></a>2.6.2 块级元素</h3><p><code>block-level</code></p><p>h1~h6、p、div、ul、ol、li等，div是典型</p><ul><li>高度、宽度、外边距、内边距都可以控制（可以自己设置）</li><li>宽度默认为容器（父级宽度）的100%</li><li>里面可以放行内或者块级元素</li><li>p、h1~h6、dt不能放块级元素，p不能放div</li></ul><h3 id="2-6-3-行内元素"><a href="#2-6-3-行内元素" class="headerlink" title="2.6.3 行内元素"></a>2.6.3 行内元素</h3><p><code>inline-level</code></p><p>a、strong、b、em、i、del、s、ins、u、span等，span典型</p><ul><li>相邻行内元素再一行上、一行可以显示多个</li><li>高宽不能直接设置</li><li>默认宽度就是它本身的宽度,与文本内容多少有关</li><li>行内元素只能容纳文本或者其他行内元素</li><li>链接里面不能再含有链接</li><li>特殊情况：a内可以放块级元素，但给a转换一下块级模式更加安全</li></ul><h3 id="2-6-4-行内块元素"><a href="#2-6-4-行内块元素" class="headerlink" title="2.6.4 行内块元素"></a>2.6.4 行内块元素</h3><p><code>inline-block</code></p><p>img、input表单类、td单元格等，可以对他们设置宽高和对齐属性</p><ul><li>和相邻的行内元素在一行上、但会有空白缝隙，一行可以显示多个</li><li>默认宽度和高度是本身内容的宽度</li><li>高度、行高、外边距、内边距可以控制</li></ul><h3 id="2-6-5-显示模式转换display"><a href="#2-6-5-显示模式转换display" class="headerlink" title="2.6.5 显示模式转换display"></a>2.6.5 显示模式转换display</h3><ul><li>块转换为行：<code>display:inline;</code></li><li>行转换为块：<code>display:block;</code></li><li>块、行转换为行内块：<code>display:inline-block</code></li></ul><h2 id="2-7-行高line-height"><a href="#2-7-行高line-height" class="headerlink" title="2.7 行高line-height"></a>2.7 行高line-height</h2><p>可以间接让文字垂直居中</p><h3 id="2-7-1-行高测量"><a href="#2-7-1-行高测量" class="headerlink" title="2.7.1 行高测量"></a>2.7.1 行高测量</h3><p>行高测量：有英文的话基线与基线间的距离、中文可以直接量底线</p><p><code>![行高测量](/img/hcj/hgcl.png)</code></p><h3 id="2-7-2-单行文本垂直居中"><a href="#2-7-2-单行文本垂直居中" class="headerlink" title="2.7.2 单行文本垂直居中"></a>2.7.2 单行文本垂直居中</h3><p>上距离+文字高度+下距离 = 块高</p><p><strong>行高</strong><br><strong>等于高度</strong>，<strong>文字垂直居中</strong><br>大于高度，文字偏下<br>小于高度，文字偏上</p><h2 id="2-8-CSS背景background"><a href="#2-8-CSS背景background" class="headerlink" title="2.8 CSS背景background"></a>2.8 CSS背景background</h2><h3 id="2-8-1-背景颜色color"><a href="#2-8-1-背景颜色color" class="headerlink" title="2.8.1 背景颜色color"></a>2.8.1 背景颜色color</h3><p><code>-color:</code><br>transparent透明色—默认透明</p><h3 id="2-8-2-背景图片image"><a href="#2-8-2-背景图片image" class="headerlink" title="2.8.2 背景图片image"></a>2.8.2 背景图片image</h3><p><code>background-image: url(images/1.jpg)</code><br>默认url为none,url中的地址不用加引号</p><h3 id="2-8-3-背景平铺repeat"><a href="#2-8-3-背景平铺repeat" class="headerlink" title="2.8.3 背景平铺repeat"></a>2.8.3 背景平铺repeat</h3><p><code>background-repeat:</code><br>默认repeat 平铺<br>no-repeat 不平铺<br>repeat-x 横向平铺<br>repeat-y 纵向平铺</p><h3 id="2-8-4-背景位置position（重点）"><a href="#2-8-4-背景位置position（重点）" class="headerlink" title="2.8.4 背景位置position（重点）"></a>2.8.4 背景位置position（重点）</h3><p><code>background-position:</code><br>length 百分数，浮点数子和单位标识符组成的长度值<br>position后面是完整的位置坐标，可以加方位名词或者数字</p><ul><li>方位名词：<code>right top;</code> 右上角,,left,bottom,center<ul><li><code>left top</code> 与 <code>top left</code>效果相同，顺序无关</li><li>只写一个的话，另一个默认居中</li></ul></li></ul><p>常用的超大背景图片<code>center top</code></p><ul><li>数字坐标： <code>x坐标 y坐标;</code>,如<code>10px 50px;</code><ul><li>第一个为x坐标，第二个为y坐标，顺序有关</li><li>只给一个值，则默认为x坐标，另一个默认居中</li><li>如果精确和方位混合使用，则精确值第一个为x，第二个为y</li></ul></li></ul><h3 id="2-8-5-背景附着"><a href="#2-8-5-背景附着" class="headerlink" title="2.8.5 背景附着"></a>2.8.5 背景附着</h3><p><code>background-attachment:</code><br><code>scroll</code> 滚动<br><code>fixed</code> 固定</p><h3 id="2-8-6-背景简写"><a href="#2-8-6-背景简写" class="headerlink" title="2.8.6 背景简写"></a>2.8.6 背景简写</h3><p><code>background: transparent url(image.jpg) repeat-y scroll center top;</code><br><code>background:</code>背景颜色 图片地址 背景平铺 背景滚动 背景位置;</p><h3 id="2-8-7-背景透明（CSS3）"><a href="#2-8-7-背景透明（CSS3）" class="headerlink" title="2.8.7 背景透明（CSS3）"></a>2.8.7 背景透明（CSS3）</h3><p>黑色半透明，rgba(red,green,blue,alpha)<br>alpha：透明度0~1</p><p><code>background: rgba(0,0,0,0.3);</code></p><ul><li>简略显示<code>rgba(0,0,0,.3)</code></li><li>半透明只是盒子半透明，内容不受影响</li></ul><h2 id="2-9-CSS三大特性"><a href="#2-9-CSS三大特性" class="headerlink" title="2.9 CSS三大特性"></a>2.9 CSS三大特性</h2><h3 id="2-9-1-CSS层叠性"><a href="#2-9-1-CSS层叠性" class="headerlink" title="2.9.1 CSS层叠性"></a>2.9.1 CSS层叠性</h3><p>通俗：前面有一个div属性，后面再写一个一样的，则会使用后面的</p><ul><li>就近原则，那个样式离着结构近，就执行那个样式</li><li>样式不冲突，不会层叠</li></ul><h3 id="2-9-2-CSS继承性"><a href="#2-9-2-CSS继承性" class="headerlink" title="2.9.2 CSS继承性"></a>2.9.2 CSS继承性</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css"><span class="selector-tag">div</span> &#123;</span></span><br><span class="line"><span class="language-css">  <span class="attribute">color</span>: red;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>内容<span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p>p中的内容也是red红色</p><ul><li>恰当的使用继承可以让代码更加简洁</li><li>text-，font-，line-，只有这些元素可以继承</li></ul><h3 id="2-9-3-CSS优先级（重点）"><a href="#2-9-3-CSS优先级（重点）" class="headerlink" title="2.9.3 CSS优先级（重点）"></a>2.9.3 CSS优先级（重点）</h3><p>同一元素上有不同的规则</p><ul><li>选择器相同，执行就近原则（层叠性）</li><li>选择器不同，如一个<code>div</code>，一个<code>.class</code>，两个选择器，选择<code>.class</code></li></ul><h4 id="2-9-3-1-权重计算公式"><a href="#2-9-3-1-权重计算公式" class="headerlink" title="2.9.3.1 权重计算公式"></a>2.9.3.1 权重计算公式</h4><div class="table-container"><table><thead><tr><th>选择器</th><th>计算权重公式</th></tr></thead><tbody><tr><td>继承或者*</td><td>0,0,0,0</td></tr><tr><td>每个元素标签选择器</td><td>0,0,0,1</td></tr><tr><td>每个元素类选择器.</td><td>0,0,1,0</td></tr><tr><td>每个元素id选择器#</td><td>0,1,0,0</td></tr><tr><td>每个元素行内选择器style</td><td>1,0,0,0</td></tr><tr><td>直接用!important</td><td>权重最高</td></tr></tbody></table></div><p>最高权重的用法<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">div</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: red<span class="meta">!important</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>只要是继承权重一定为0</strong></p><ul><li>如果该标签被选中，则比较权重</li><li>如果标签没有被选中，则始终为0</li></ul><h4 id="2-9-3-2-权重计算叠加"><a href="#2-9-3-2-权重计算叠加" class="headerlink" title="2.9.3.2 权重计算叠加"></a>2.9.3.2 权重计算叠加</h4><p>交集选择器、后代选择器会出现权重叠加<br>如：</p><ul><li><code>div ul li</code>   0,0,0,3</li><li><code>.nav ul li</code>  0,0,1,2</li><li><code>a:hover</code>  0,0,1,1</li><li><code>.nav a</code>  0,0,1,1</li></ul><h2 id="2-10-盒子模型（CSS重点）"><a href="#2-10-盒子模型（CSS重点）" class="headerlink" title="2.10 盒子模型（CSS重点）"></a>2.10 盒子模型（CSS重点）</h2><h3 id="2-10-1-盒子模型介绍"><a href="#2-10-1-盒子模型介绍" class="headerlink" title="2.10.1 盒子模型介绍"></a>2.10.1 盒子模型介绍</h3><p>网页布局本质</p><ul><li>设置好各个盒子的大小，摆放好盒子的位置</li><li>然后放内容（图片，文字等等）</li></ul><p><strong>盒子模型Box Model</strong></p><ul><li>盒子组成四部分：Border厚度（边缘），content内容，Padding内边距，Margin盒子与盒子间的距离（外边距）</li></ul><p><code>![盒子模型](/img/hcj/boxmodel.png)</code></p><h3 id="2-10-2-盒子边框border"><a href="#2-10-2-盒子边框border" class="headerlink" title="2.10.2 盒子边框border"></a>2.10.2 盒子边框border</h3><p>综合性写法：<code>border: 1px solid pink;</code>没有顺序 </p><p><code>border-width:</code> 默认medium，边框宽度px<br><code>border-style:</code> 边框样式，none，hidden，dotted点线，dashed虚线，<strong>solid实线</strong><br><code>border-color:</code> 颜色</p><h4 id="2-10-2-1-边框分开写"><a href="#2-10-2-1-边框分开写" class="headerlink" title="2.10.2.1 边框分开写"></a>2.10.2.1 边框分开写</h4><p><code>border-top:</code> 上边框综合写法，具体写<code>border-top-width</code>等等<br><code>border-bottom:</code> 下边框<br><code>border-right:</code> 右边框<br><code>border-left:</code> 左边框</p><p>例子<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">input</span> &#123;</span><br><span class="line">  <span class="comment">/* border-top: none;</span></span><br><span class="line"><span class="comment">  border-left: none;</span></span><br><span class="line"><span class="comment">  border-right: none;</span></span><br><span class="line"><span class="comment">  border-bottom: 1px dashed pink;</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="attribute">border</span>: none;</span><br><span class="line">  <span class="attribute">border-bottom</span>: <span class="number">1px</span> dashed red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">用户名: <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">&quot;text&quot;</span>&gt;</span></span><br><span class="line">密码： <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">&quot;text&quot;</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="2-10-2-2-表格的细线边框"><a href="#2-10-2-2-表格的细线边框" class="headerlink" title="2.10.2.2 表格的细线边框"></a>2.10.2.2 表格的细线边框</h4><p>表格的两个单元格在重合时会发生重叠，从而使得边框变粗</p><p>合并单元格：<code>table &#123;border-collapse:collapse;&#125;</code><br>相邻边框合并在一起，而不会变粗</p><h3 id="2-10-3-盒子内边距padding"><a href="#2-10-3-盒子内边距padding" class="headerlink" title="2.10.3 盒子内边距padding"></a>2.10.3 盒子内边距padding</h3><p>单位px<br><code>padding-left:</code><br><code>padding-right:</code><br><code>padding-top:</code><br><code>padding-bottom:</code></p><ul><li>内容和边框有了距离</li><li>添加内边距，盒子会变大</li><li>简写：<br><code>padding: 20px;</code> 解释：上下左右都是20px的内边距<br><code>padding: 10px 20px;</code>  上下10px，左右20px<br><code>padding: 10px 20px 30px;</code> 上10px，左右20px，下30px<br><code>padding: 10px 20px 30px 40px;</code> 上10 ，右20，下30，左40，顺时针</li></ul><hr><p><code>![新浪导航](/img/hcj/xldh.png)</code></p><p>给盒子指定一个padding而不给宽度，实现不同内容，不同宽度</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;zh&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;viewport&quot;</span> <span class="attr">content</span>=<span class="string">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;X-UA-Compatible&quot;</span> <span class="attr">content</span>=<span class="string">&quot;ie=edge&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>xl<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css"><span class="selector-class">.nav</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">height</span>: <span class="number">41px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">background-color</span>: <span class="built_in">rgb</span>(<span class="number">252</span>, <span class="number">252</span>, <span class="number">252</span>);</span></span><br><span class="line"><span class="language-css"><span class="attribute">border-top</span>: <span class="number">4px</span> solid <span class="number">#dd5600</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">border-bottom</span>: <span class="number">1px</span> solid <span class="number">#edefec</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.nav</span> <span class="selector-tag">a</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">display</span>: inline-block;</span></span><br><span class="line"><span class="language-css"><span class="attribute">height</span>: <span class="number">41px</span>;</span></span><br><span class="line"><span class="language-css"><span class="comment">/* background-color: pink; */</span></span></span><br><span class="line"><span class="language-css"><span class="attribute">text-decoration</span>: none;</span></span><br><span class="line"><span class="language-css"><span class="attribute">padding</span>: <span class="number">0px</span> <span class="number">20px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">line-height</span>: <span class="number">41px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">color</span>: <span class="number">#4c4c4c</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">font-size</span>: <span class="number">12px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.nav</span> <span class="selector-tag">a</span><span class="selector-pseudo">:hover</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">background-color</span>: <span class="number">#eaedea</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;nav&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>设为首页<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>手机新浪网<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>移动客户端<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>博客<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>微博<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>关注我<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><hr><p><strong>盒子模型的计算</strong></p><p><code>![盒子模型](/img/hcj/boxmodel.png)</code></p><p><strong>盒子的实际大小 = 内容宽度 高度 + 内边距 + 边框</strong></p><ul><li><code>padding: 10px</code>会把盒子的宽高各增加20px</li><li>解决方法： 要让盒子保持原大小，则要修改width，height，减去内边距即可</li></ul><p>特殊情况：padding不会撑开的情况<br>如果这个盒子没有宽度，则padding不会撑开盒子，盒子内的盒子不会撑开盒子</p><h3 id="2-10-4-盒子的外边距margin"><a href="#2-10-4-盒子的外边距margin" class="headerlink" title="2.10.4 盒子的外边距margin"></a>2.10.4 盒子的外边距margin</h3><p><code>margin-left:</code><br><code>margin-right:</code><br><code>margin-top:</code><br><code>margin-bottom:</code><br>简写，与padding一样<br><code>margin: 10px</code> 上下左右</p><h4 id="2-10-4-1-块级盒子居中对齐"><a href="#2-10-4-1-块级盒子居中对齐" class="headerlink" title="2.10.4.1 块级盒子居中对齐"></a>2.10.4.1 块级盒子居中对齐</h4><p>居中对齐：有宽度、左右外边距设为<code>auto</code></p><ul><li><p>第一种写法<br><code>width: 600px;</code><br><code>margin-left: auto;</code><br><code>margin-right: auto;</code></p></li><li><p>第二种写法<br><code>margin: auto;</code></p></li><li><p>第三种写法<br><code>margin: 0 auto;</code><br>上下为<code>0</code> ，左右<code>auto</code></p></li></ul><h4 id="2-10-4-2-文字居中，盒子居中"><a href="#2-10-4-2-文字居中，盒子居中" class="headerlink" title="2.10.4.2 文字居中，盒子居中"></a>2.10.4.2 文字居中，盒子居中</h4><ul><li><p>文字水平居中：<code>text-aligh: center;</code><br>也可以让行内元素和行内块元素居中对齐</p></li><li><p>盒子水平居中：<code>margin: 10px auto;</code></p></li></ul><h4 id="2-10-4-3-插入图片和背景图片"><a href="#2-10-4-3-插入图片和背景图片" class="headerlink" title="2.10.4.3 插入图片和背景图片"></a>2.10.4.3 插入图片和背景图片</h4><p>通常用插入图片</p><ul><li>插入图片<code>&lt;img src=&quot;url&quot; alt=&quot;&quot;&gt;</code><br>移动位置只能通过盒模型<code>padding</code>或<code>margin</code>值来移动</li></ul><p>背景图片很少用，小图标或者超大背景</p><ul><li>背景图片<code>background-image: url(url);</code><br>移动位置只能用<code>background-position: x,y;</code></li></ul><h4 id="2-10-4-4-清除元素默认的内外边距（写css第一句代码，很重要）"><a href="#2-10-4-4-清除元素默认的内外边距（写css第一句代码，很重要）" class="headerlink" title="2.10.4.4 清除元素默认的内外边距（写css第一句代码，很重要）"></a>2.10.4.4 清除元素默认的内外边距（写css第一句代码，很重要）</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">* &#123;</span><br><span class="line">  <span class="attribute">padding</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">margin</span>: <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>一些元素有默认的内外边距，要美观就要删掉</li><li><strong>行内标签，尽量只设置左右内外边距，不设置上下内外边距（不管用）</strong></li></ul><h4 id="2-10-4-5-外边距合并"><a href="#2-10-4-5-外边距合并" class="headerlink" title="2.10.4.5 外边距合并"></a>2.10.4.5 外边距合并</h4><ol><li><ul><li>相邻块元素垂直外边距合并，取两个值中的较大值（外边距塌陷）</li><li>解决方法：尽量只给一个盒子添加margin值</li></ul></li><li><ul><li>嵌套关系的垂直外边距合并，（塌陷）父子级关系，给儿子一个margin，父亲也会有margin</li><li>解决方案<ul><li>为父元素指定外边框<code>border-top:</code></li><li>为父元素指定一个上<code>padding-top:</code></li><li>为父元素添加<code>overflow: hidden;</code></li></ul></li></ul></li></ol><h4 id="2-10-4-6-盒子模型的布局稳定性"><a href="#2-10-4-6-盒子模型的布局稳定性" class="headerlink" title="2.10.4.6 盒子模型的布局稳定性"></a>2.10.4.6 盒子模型的布局稳定性</h4><p>优先级使用，先宽度width，然后内边距padding，最后外边距margin</p><h3 id="2-10-5-盒子综合案例"><a href="#2-10-5-盒子综合案例" class="headerlink" title="2.10.5 盒子综合案例"></a>2.10.5 盒子综合案例</h3><p><code>![盒子综合案例](/img/hcj/boxlizi.png)</code></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;zh&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;viewport&quot;</span> <span class="attr">content</span>=<span class="string">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;X-UA-Compatible&quot;</span> <span class="attr">content</span>=<span class="string">&quot;ie=edge&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>盒子综合案例<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css">* &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">padding</span>: <span class="number">0</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">margin</span>: <span class="number">0</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.box</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">width</span>: <span class="number">298px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">height</span>: <span class="number">198px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">margin</span>: <span class="number">100px</span> auto; <span class="comment">/* 水平居中 */</span></span></span><br><span class="line"><span class="language-css"><span class="attribute">background</span>: <span class="built_in">url</span>(<span class="string">../images/1.ico</span>);</span></span><br><span class="line"><span class="language-css"><span class="attribute">padding</span>: <span class="number">15px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.box</span> <span class="selector-tag">ul</span> <span class="selector-tag">li</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="comment">/* 去掉列表样式 */</span></span></span><br><span class="line"><span class="language-css"><span class="attribute">list-style</span>: none;</span></span><br><span class="line"><span class="language-css"><span class="attribute">height</span>: <span class="number">30px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">border-bottom</span>: <span class="number">1px</span> dashed <span class="number">#ccc</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">line-height</span>: <span class="number">30px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">background</span>: <span class="built_in">url</span>(<span class="string">../images/1.ico</span>) no-repeat <span class="number">5px</span> center;</span></span><br><span class="line"><span class="language-css"><span class="comment">/* background-position: 5px center; */</span></span></span><br><span class="line"><span class="language-css"><span class="attribute">padding-left</span>: <span class="number">20px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.box</span> <span class="selector-tag">ul</span> <span class="selector-tag">li</span> <span class="selector-tag">a</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">text-decoration</span>: none;</span></span><br><span class="line"><span class="language-css"><span class="attribute">font-size</span>: <span class="number">12px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">color</span>: <span class="number">#2a2a29</span>;</span></span><br><span class="line"><span class="language-css"><span class="comment">/* margin-left: 20px; */</span></span></span><br><span class="line"><span class="language-css"><span class="comment">/* padding-left: 20px; */</span></span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.box</span> <span class="selector-tag">ul</span> <span class="selector-tag">li</span> <span class="selector-tag">a</span><span class="selector-pseudo">:hover</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">text-decoration</span>: underline;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.box</span> <span class="selector-tag">h2</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">font-size</span>: <span class="number">18px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">padding</span>: <span class="number">5px</span> <span class="number">0</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">border-bottom</span>: <span class="number">1px</span> solid <span class="number">#ccc</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">margin-bottom</span>: <span class="number">10px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;box&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h2</span>&gt;</span>最新文章/New Articles<span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>北京招聘<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>体验js<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>jquery<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>网页设计师<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>链式编程<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>去掉列表默认的样式（前面的小点）</strong><br>无序列表和有序列表列表样式在不同浏览器显示样式不一样所以去掉<br><code>li &#123; list-style: none; &#125;</code></p><h3 id="2-10-6-拓展CSS3"><a href="#2-10-6-拓展CSS3" class="headerlink" title="2.10.6 拓展CSS3"></a>2.10.6 拓展CSS3</h3><h4 id="2-10-6-1-边框圆角"><a href="#2-10-6-1-边框圆角" class="headerlink" title="2.10.6.1 边框圆角"></a>2.10.6.1 边框圆角</h4><p><code>border-radius: length</code><br>length可以为数值或百分比</p><p>横向矩形为高度一般设置为高度的一半</p><h4 id="2-10-6-2-盒子阴影"><a href="#2-10-6-2-盒子阴影" class="headerlink" title="2.10.6.2 盒子阴影"></a>2.10.6.2 盒子阴影</h4><p><code>box-shadow: 水平阴影 垂直阴影 模糊距离(虚实) 阴影尺寸(大小) 阴影颜色 内外阴影</code><br><code>h-shadow v-shadow</code>前两个必须写<br><code>blur spread color inset</code>可以省略<br>内外阴影默认为outset，且不用写</p><p><code>美观的</code> <code>box-shadow: 0 15px 30px rgba(0,0,0,.3);</code></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">width</span>: <span class="number">300px</span>;</span><br><span class="line"><span class="attribute">height</span>: <span class="number">300px</span>;</span><br><span class="line"><span class="attribute">background-color</span>: pink;</span><br><span class="line"><span class="attribute">margin-left</span>: auto;</span><br><span class="line"><span class="attribute">margin-right</span>: auto;</span><br><span class="line"><span class="attribute">text-align</span>: center;</span><br><span class="line"><span class="comment">/* border-radius: 150px; */</span></span><br><span class="line"><span class="attribute">border-radius</span>: <span class="number">20%</span>;</span><br><span class="line"><span class="attribute">box-shadow</span>: <span class="number">2px</span> <span class="number">2px</span> <span class="number">2px</span> <span class="number">2px</span> <span class="number">#000</span> ;</span><br></pre></td></tr></table></figure><p><code>![picture](/img/hcj/shadow.png)</code></p><h2 id="2-11-CSS书写规范"><a href="#2-11-CSS书写规范" class="headerlink" title="2.11 CSS书写规范"></a>2.11 CSS书写规范</h2><p><strong>空格规范</strong></p><ul><li><code>选择器</code>与<code>&#123;&#125;</code>之间有空格</li><li>属性的<code>冒号</code>跟<code>值</code>有空格</li></ul><p><strong>选择器规范</strong></p><ul><li>并集选择器竖着写</li><li>花括号对齐</li><li>选择器嵌套层数最好不超过3级</li></ul><p><strong>属性规范</strong></p><ul><li>属性必须另起一行</li><li>属性定义后必须有<code>;</code>结束</li></ul><h3 id="2-11-1-CSS属性书写顺序（重点）"><a href="#2-11-1-CSS属性书写顺序（重点）" class="headerlink" title="2.11.1 CSS属性书写顺序（重点）"></a>2.11.1 CSS属性书写顺序（重点）</h3><p>按照顺序书写</p><ol><li>布局定位属性：<code>display / position / float / clear / visibility / overflow</code> （display第一个写）</li><li>自身属性： <code>width / height / margin / padding / border / background /</code></li><li>文本属性： <code>color / font / text-decoration / text-align / vertical-align / white-space / break-word</code></li><li>其他属性（CSS3）： <code>content / cursor / border-radius / box-shadow / text-shadow / background:linear-gradient ...</code> </li></ol><h2 id="2-12-浮动float"><a href="#2-12-浮动float" class="headerlink" title="2.12 浮动float"></a>2.12 浮动float</h2><p>布局页面</p><h3 id="2-12-1-CSS布局的三种机制"><a href="#2-12-1-CSS布局的三种机制" class="headerlink" title="2.12.1 CSS布局的三种机制"></a>2.12.1 CSS布局的三种机制</h3><p>网页布局的核心就是用CSS来摆放盒子</p><p>三种方法：</p><ul><li>普通流</li></ul><p>块级元素会独占一行，从上向下顺序排序<br><code>div、hr、p、h1~h6、ul、ol、dl、form、table</code></p><p>行内元素会多个在一行内显示，从左到右，碰到父元素边缘自动换行<br><code>span、a、i、em</code></p><ul><li>浮动流</li></ul><p>让盒子从普通流中浮动起来，主要作用让多个块级盒子一行显示</p><ul><li>定位流</li></ul><p>将盒子定在浏览器中的某一个位置，CSS离不开定位，特别是后面的js特效</p><h3 id="2-12-2-浮动"><a href="#2-12-2-浮动" class="headerlink" title="2.12.2 浮动"></a>2.12.2 浮动</h3><h4 id="1-为什么用浮动？？？"><a href="#1-为什么用浮动？？？" class="headerlink" title="1. 为什么用浮动？？？"></a><strong>1. 为什么用浮动？？？</strong></h4><ul><li>让多个盒子水平排列成一行</li></ul><p>&emsp;行内块也可以让多个块水平排列在一行<code>display: inline-block</code>，<strong>但中间会有一个空隙，而且很难去掉，也不能简单地调整空隙大小,但浮动可以实现</strong></p><ul><li>实现盒子的左右对齐</li></ul><h4 id="2-什么是浮动？？？"><a href="#2-什么是浮动？？？" class="headerlink" title="2. 什么是浮动？？？"></a><strong>2. 什么是浮动？？？</strong></h4><p>设置了浮动属性地元素：</p><ul><li>脱离标准普通流地控制</li><li>移动到指定地位置</li></ul><h4 id="3-浮动的作用："><a href="#3-浮动的作用：" class="headerlink" title="3. 浮动的作用："></a><strong>3. 浮动的作用</strong>：</h4><ul><li>让多个盒子div水平排列成一行</li><li>可以实现盒子地左右对齐</li><li>最早是用来控制图片，实现<strong>文字环绕图片</strong>的效果</li></ul><h4 id="4-浮动的语法"><a href="#4-浮动的语法" class="headerlink" title="4. 浮动的语法"></a><strong>4. 浮动的语法</strong></h4><p>float属性定义浮动<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">选择器 &#123;</span><br><span class="line">  <span class="attribute">float</span>: none</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* none 、 left 、 right  */</span></span><br><span class="line"><span class="comment">/* 不浮动 、 左浮 、 右浮 */</span></span><br></pre></td></tr></table></figure></p><h4 id="5-浮动的口诀"><a href="#5-浮动的口诀" class="headerlink" title="5. 浮动的口诀"></a><strong>5. 浮动的口诀</strong></h4><p><strong>浮</strong><br>漂浮在普通流的上面，脱离标准流，<em>脱标</em><br>float让盒子漂浮在标准流的上面，所以第二个标准流的盒子跑到浮动盒子的地下</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/float.png" alt="float.png"></p><p><strong>漏</strong><br>浮动的盒子原来的位置留给下面标准流的盒子，不占有原来的位置</p><p><strong>特</strong><br>float特性，float会改变元素的display属性<br>任何元素都可以浮动，浮动的元素会生成一个块级框，而不论他本身是什么元素，生成的块级框跟行内块相似</p><p>两个块元素都给上float属性，会都转化为<strong>行内块元素，而且中间没有空隙</strong></p><p><strong>浮动的盒子会相互靠在一起，如果父级宽度装不下这些浮动的盒子，多出的盒子会另起一行对齐</strong></p><h4 id="6-浮动的应用-重点"><a href="#6-浮动的应用-重点" class="headerlink" title="6. 浮动的应用(重点)"></a><strong>6. 浮动的应用(重点)</strong></h4><p>浮动流和标准流的父盒子的搭配<br>先给定一个标准流的父级盒子，然后浮动流在标准流中浮动<br><strong>完整的网页 = 标准流 + 浮动 + 定位</strong></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/floatex1.png" alt="floatex1.png"></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;zh&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;viewport&quot;</span> <span class="attr">content</span>=<span class="string">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;X-UA-Compatible&quot;</span> <span class="attr">content</span>=<span class="string">&quot;ie=edge&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>Float<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css">* &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">padding</span>: <span class="number">0</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">margin</span>: <span class="number">0</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.box</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">width</span>: <span class="number">1226px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">height</span>: <span class="number">615px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">background-color</span>: <span class="number">#CCCCCC</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">margin</span>: auto;</span></span><br><span class="line"><span class="language-css"><span class="attribute">margin-top</span>: <span class="number">30px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.left</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">width</span>: <span class="number">234px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">height</span>: <span class="number">615px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">background-color</span>: <span class="number">#f7ebcd</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">float</span>: left;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.left</span> <span class="selector-tag">img</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">width</span>: <span class="number">234px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.right</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">width</span>: <span class="number">992px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">height</span>: <span class="number">615px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">background-color</span>: skyblue;</span></span><br><span class="line"><span class="language-css"><span class="attribute">float</span>: right;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.right</span> <span class="selector-tag">li</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="comment">/* 清除列表样式 */</span></span></span><br><span class="line"><span class="language-css"><span class="attribute">list-style</span>: none;</span></span><br><span class="line"><span class="language-css"><span class="attribute">width</span>: <span class="number">234px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">height</span>: <span class="number">300px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">background-color</span>: pink;</span></span><br><span class="line"><span class="language-css"><span class="attribute">float</span>: left;</span></span><br><span class="line"><span class="language-css"><span class="attribute">margin-left</span>: <span class="number">14px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">margin-bottom</span>: <span class="number">14px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;box&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;left&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;../images/1.ico&quot;</span> <span class="attr">alt</span>=<span class="string">&quot;&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ul</span> <span class="attr">class</span>=<span class="string">&quot;right&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span>1<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span>2<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span>3<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span>4<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span>5<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span>6<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span>7<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span>8<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/floatex2.png" alt="floatex2.png"></p><p><strong>导航栏用li+a搭配更好</strong></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;zh&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;viewport&quot;</span> <span class="attr">content</span>=<span class="string">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;X-UA-Compatible&quot;</span> <span class="attr">content</span>=<span class="string">&quot;ie=edge&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>导航栏<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css">* &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">padding</span>: <span class="number">0</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">margin</span>: <span class="number">0</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-tag">li</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">list-style</span>: none;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.banner</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">width</span>: <span class="number">760px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">height</span>: <span class="number">150px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">background-color</span>: <span class="number">#CCCCCC</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">margin</span>: auto;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.banner</span> <span class="selector-tag">img</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">width</span>: <span class="number">760px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.nav</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">width</span>: <span class="number">760px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">height</span>: <span class="number">32px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">background-color</span>: pink;</span></span><br><span class="line"><span class="language-css"><span class="attribute">margin</span>: <span class="number">0</span> auto;</span></span><br><span class="line"><span class="language-css"><span class="attribute">background-image</span>: <span class="built_in">url</span>(<span class="string">../images/dhl.png</span>) ;</span></span><br><span class="line"><span class="language-css"><span class="attribute">background-repeat</span>: repeat-x;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.nav</span> <span class="selector-tag">ul</span> <span class="selector-tag">li</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">float</span>: left;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="comment">/* 可以给a和li大小,但是a要求hover,所以要直接给a大小 */</span></span></span><br><span class="line"><span class="language-css"><span class="selector-class">.nav</span> <span class="selector-tag">ul</span> <span class="selector-tag">li</span> <span class="selector-tag">a</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">text-decoration</span>: none;</span></span><br><span class="line"><span class="language-css"><span class="attribute">display</span>: block;</span></span><br><span class="line"><span class="language-css"><span class="attribute">width</span>: <span class="number">80px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">height</span>: <span class="number">32px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">background</span>: <span class="built_in">url</span>(<span class="string">../images/banner0.png</span>) no-repeat; </span></span><br><span class="line"><span class="language-css"></span></span><br><span class="line"><span class="language-css"><span class="attribute">line-height</span>: <span class="number">32px</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">text-align</span>: center;</span></span><br><span class="line"><span class="language-css"></span></span><br><span class="line"><span class="language-css"><span class="attribute">color</span>: <span class="number">#3b4300</span>;</span></span><br><span class="line"><span class="language-css"><span class="attribute">font-size</span>: <span class="number">12px</span>;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"><span class="selector-class">.nav</span> <span class="selector-tag">ul</span> <span class="selector-tag">li</span> <span class="selector-tag">a</span><span class="selector-pseudo">:hover</span> &#123;</span></span><br><span class="line"><span class="language-css"><span class="attribute">background</span>: <span class="built_in">url</span>(<span class="string">../images/banner1.png</span>) no-repeat;</span></span><br><span class="line"><span class="language-css">&#125;</span></span><br><span class="line"><span class="language-css"></span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- banner广告条 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;banner&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;../images/banner.png&quot;</span> &gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;nav&quot;</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 重要的导航栏用li+a的写法 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>网站首页<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>网站首页<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>网站首页<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>网站首页<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>网站首页<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#&quot;</span>&gt;</span>网站首页<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="7-浮动的扩展"><a href="#7-浮动的扩展" class="headerlink" title="7. 浮动的扩展"></a><strong>7. 浮动的扩展</strong></h4><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/floatkz.png" alt="floatkz.png"></p><ul><li>浮动元素与父盒子的关系<ul><li>子盒子的浮动参考父盒子对齐</li><li>不会与父元素的边框重叠，也不会超过父盒子的内边距</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/floatkz1.png" alt="floatkz1.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/floatkz2.png" alt="floatkz2.png"></p><p>两个都不浮动，则就两行显示，跟1不浮动、2浮动一样</p><ul><li>浮动元素与兄弟盒子的关系，在一个父级盒子中，如果前一个兄弟盒子是<ul><li>浮动的，那么当前盒子会与钱一个盒子顶部对齐（1、2都浮动）</li><li>普通流的，那么当前盒子会显示正在前一个兄弟盒子的下方（1不浮动、2浮动）</li></ul></li></ul><p><strong>一个父盒子中有很多盒子，如果其中一个盒子浮动，其他也应该浮动，防止出现问题</strong></p><h3 id="2-12-3-清除浮动"><a href="#2-12-3-清除浮动" class="headerlink" title="2.12.3 清除浮动"></a>2.12.3 清除浮动</h3><h4 id="1-为什么要清除浮动？？？"><a href="#1-为什么要清除浮动？？？" class="headerlink" title="1. 为什么要清除浮动？？？"></a>1. 为什么要清除浮动？？？</h4><p><strong>父级盒子很多情况下，不方便给出高度</strong>，但是子盒子如果浮动就不会占用位置，最后父级盒子的高度为0，就会影响下面的标准流盒子</p><p>正常的标准流盒子会把没有固定高度的父盒子撑开，而浮动的盒子不会撑开父亲<br>不方便给高度：固定的样式，内容不同的情况下</p><h4 id="2-清除浮动"><a href="#2-清除浮动" class="headerlink" title="2. 清除浮动"></a>2. 清除浮动</h4><p>主要是解决浮动元素撑不开<strong>未给高度的父盒子</strong>的问题，清除浮动后，父级会根据浮动的子盒子自动检测高度</p><h4 id="3-清除浮动的方法"><a href="#3-清除浮动的方法" class="headerlink" title="3. 清除浮动的方法"></a>3. 清除浮动的方法</h4><p><code>clear: 属性值;</code><br><code>left清除左侧浮动影响 right both</code><br>一般直接用<code>clear: both</code></p><h5 id="①-额外标签法（隔墙法）"><a href="#①-额外标签法（隔墙法）" class="headerlink" title="① 额外标签法（隔墙法）"></a>① 额外标签法（隔墙法）</h5><p>在浮动元素末尾添加一个空的标签</p><p><code>&lt;div style=&quot;clear: both&quot;&gt;&lt;/div&gt;</code>或者其他如<code>&lt;br&gt;</code>标签等</p><ul><li>缺点： 添加了许多无意义标签，结构性差</li></ul><h5 id="②-父级添加overflow属性方法"><a href="#②-父级添加overflow属性方法" class="headerlink" title="② 父级添加overflow属性方法"></a>② 父级添加overflow属性方法</h5><p>给父级添加<code>overflow</code>为<code>hidden</code> <code>auto</code> <code>scroll</code> 都可以<br><code>overflow: hidden;</code><br>auto生成滚动条，scroll生成两个滚动条</p><p>代码简洁，但是内容增多时荣一造成不会自动换行而导致内容被隐藏掉，无法显示需要溢出的元素</p><h5 id="③-使用after伪元素清除浮动"><a href="#③-使用after伪元素清除浮动" class="headerlink" title="③ 使用after伪元素清除浮动"></a>③ 使用after伪元素清除浮动</h5><p>after方式为空元素添加额外标签的升级版，不用单独添加标签</p><p>相当于给这个标签后面添加了一个新的标签，但是结构里面不可见，虚拟，更好</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 声明清除浮动的样式 */</span></span><br><span class="line"><span class="selector-class">.clearfix</span><span class="selector-pseudo">:after</span> &#123;</span><br><span class="line">  <span class="attribute">content</span>: <span class="string">&quot;&quot;</span>;</span><br><span class="line">  <span class="attribute">display</span>: block;</span><br><span class="line">  <span class="attribute">height</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">visibility</span>: hidden;</span><br><span class="line">  <span class="attribute">clear</span>: both;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/* 防止低版本浏览器不支持after */</span></span><br><span class="line"><span class="comment">/* ie6、7清除浮动的样式 */</span></span><br><span class="line"><span class="selector-class">.clearfix</span> &#123;</span><br><span class="line">  *zoom: <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 然后给需要的标签添加一个类名：clearfix */</span></span><br></pre></td></tr></table></figure><h5 id="④-使用双伪元素清除浮动"><a href="#④-使用双伪元素清除浮动" class="headerlink" title="④ 使用双伪元素清除浮动"></a>④ 使用双伪元素清除浮动</h5><p>代码更加简洁</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.clearfix</span><span class="selector-pseudo">:before</span>,</span><br><span class="line"><span class="selector-class">.clearfix</span><span class="selector-pseudo">:after</span> &#123;</span><br><span class="line">  <span class="attribute">content</span>: <span class="string">&quot;&quot;</span>;</span><br><span class="line">  <span class="attribute">display</span>: table;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.clearfix</span><span class="selector-pseudo">:after</span> &#123;</span><br><span class="line">  <span class="attribute">clear</span>: both;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.clearfix</span> &#123;</span><br><span class="line">  *zoom: <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-清除浮动总结"><a href="#4-清除浮动总结" class="headerlink" title="4. 清除浮动总结"></a>4. 清除浮动总结</h4><p>当</p><ul><li>父级没高度</li><li>子盒子浮动</li><li>影响下面的布局<br>就需要清除浮动，四种方法清除浮动</li></ul><h4 id="5-PS切片"><a href="#5-PS切片" class="headerlink" title="5. PS切片"></a>5. PS切片</h4><p><strong>PS切片常用切片工具</strong></p><p>大致步骤：切片后然后，导出—&gt;存储为web常用—&gt;然后用切片保存选中的切片<br>切片方法：</p><ol><li>切片工具直接切片</li><li>图层—&gt;新建基于图层的切片</li><li>利用辅助线<code>ctrl+ R</code>调出辅助线</li></ol><p>清除切片：视图—&gt;清除切片</p><p>或者使用ps切片的插件（免费但是需要注册自己的帐号）<br>Cutterman切图神器</p><hr><h1 id="✳快捷键操作（常用）"><a href="#✳快捷键操作（常用）" class="headerlink" title="✳快捷键操作（常用）"></a>✳快捷键操作（常用）</h1><p><strong>html编辑的快捷方法</strong></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/kjj.png" alt="kjj.png"></p><ul><li><code>w200</code> 生成 <code>width: 200px;</code></li><li><code>h200</code> 生成 <code>height: 200px;</code></li></ul><p>我的HB不行hhh草</p><ul><li><code>bd+</code> 生成 <code>border: 1px solid #000;</code></li><li><code>bd-</code> 生成 <code>border: none;</code></li></ul><hr><h1 id="3-js-的基础"><a href="#3-js-的基础" class="headerlink" title="3. js 的基础"></a>3. js 的基础</h1>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HCJ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python中Plt库的学习</title>
      <link href="/Learn/Learn-Python_plt/"/>
      <url>/Learn/Learn-Python_plt/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://blog.csdn.net/qiurisiyu2016/article/details/80187177">参考教程</a></p></blockquote><span id="more"></span><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="布局"><a href="#布局" class="headerlink" title="布局"></a>布局</h2><p>plt.tight_layout()<br>通常在绘制多个子图时使用，用于自动调整图形中的子图布局，以避免子图之间的重叠或太过拥挤</p><h1 id="0-plt-打开-opencv-图像"><a href="#0-plt-打开-opencv-图像" class="headerlink" title="0.plt 打开 opencv 图像"></a>0.plt 打开 opencv 图像</h1><p><strong>BGR 用 RGB 打开</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img=cv2.imread(<span class="string">&#x27;lena.jpg&#x27;</span>,cv2.IMREAD_COLOR)</span><br><span class="line"></span><br><span class="line"><span class="comment">#method1</span></span><br><span class="line">b,g,r=cv2.split(img)</span><br><span class="line">img2=cv2.merge([r,g,b])</span><br><span class="line">plt.imshow(img2)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#method2</span></span><br><span class="line">img3=img[:,:,::-<span class="number">1</span>]</span><br><span class="line">plt.imshow(img3)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#method3</span></span><br><span class="line">img4=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">plt.imshow(img4)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="1-plt-plot-x-y"><a href="#1-plt-plot-x-y" class="headerlink" title="1.plt.plot(x,y)"></a>1.plt.plot(x,y)</h1><p><code>format_string的内容</code></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/plot1.png" alt="plot1.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/plot2.png" alt="plot2.png"></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/plot3.png" alt="plot3.png"></p><p><code>**kwargs</code></p><p>**kwards：<br>color 颜色<br>linestyle 线条样式<br>marker 标记风格<br>markerfacecolor 标记颜色<br>markersize 标记大小 等等</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plt.plot(x,y,format_string,**kwargs)</span></span><br><span class="line"><span class="comment"># x轴数据，y轴数据，控制曲线格式的字符串format_string颜色字符，风格字符，和标记字符</span></span><br><span class="line">plt.plot([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">6</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">1</span>],<span class="string">&#x27;r-s&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>展示</strong></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/plot4.png" alt="plot4.png"></p><h1 id="2-plt-figure-用来画图，自定义画布大小"><a href="#2-plt-figure-用来画图，自定义画布大小" class="headerlink" title="2.plt.figure()用来画图，自定义画布大小"></a>2.plt.figure()用来画图，自定义画布大小</h1><p>定义画布大小，然后用 plot 画图</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plt.figure() # 用来画图,自定义画布大小</span></span><br><span class="line">fig1 = plt.figure(num=<span class="string">&#x27;fig111111&#x27;</span>, figsize=(<span class="number">10</span>, <span class="number">3</span>), dpi=<span class="number">75</span>, facecolor=<span class="string">&#x27;#FFFFFF&#x27;</span>, edgecolor=<span class="string">&#x27;#0000FF&#x27;</span>)</span><br><span class="line"><span class="comment"># 名字,宽*高，dpi图像每英寸长度内的像素点数 一般75，</span></span><br><span class="line">plt.plot([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/figure1.png" alt="figure1.png"></p><h1 id="3-plt-subplot-222"><a href="#3-plt-subplot-222" class="headerlink" title="3.plt.subplot(222)"></a>3.plt.subplot(222)</h1><p>将 figure 设置的画布大小分成几个部分，参数‘221’表示 2(row)x2(colu),即将画布分成 2x2，两行两列的 4 块区域，1 表示选择图形输出的区域在第一块，图形输出区域参数必须在“行 x 列”范围，此处必须在 1 和 2 之间选择——如果参数设置为 subplot(111)，则表示画布整个输出，不分割成小块区域，图形直接输出在整块画布上</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">plt.subplot(<span class="number">222</span>) </span><br><span class="line">plt.plot(y,xx)    <span class="comment">#在2x2画布中第二块区域输出图形</span></span><br><span class="line">plt.show()</span><br><span class="line">plt.subplot(<span class="number">223</span>)  <span class="comment">#在2x2画布中第三块区域输出图形</span></span><br><span class="line">plt.plot(y,xx)</span><br><span class="line">plt.subplot(<span class="number">224</span>)  <span class="comment"># 在在2x2画布中第四块区域输出图形</span></span><br><span class="line">plt.plot(y,xx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 子图</span></span><br><span class="line">plt.add_subplot(<span class="number">221</span>)</span><br><span class="line">plt.add_subplot(<span class="number">222</span>)</span><br></pre></td></tr></table></figure><h1 id="4-plt-xlim-设置-x-轴或者-y-轴刻度范围"><a href="#4-plt-xlim-设置-x-轴或者-y-轴刻度范围" class="headerlink" title="4.plt.xlim 设置 x 轴或者 y 轴刻度范围"></a>4.plt.xlim 设置 x 轴或者 y 轴刻度范围</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.xlim(<span class="number">0</span>,<span class="number">1000</span>)  <span class="comment">#  设置x轴刻度范围，从0~1000         #lim为极限，范围</span></span><br><span class="line">plt.ylim(<span class="number">0</span>,<span class="number">20</span>)   <span class="comment"># 设置y轴刻度的范围，从0~20</span></span><br></pre></td></tr></table></figure><h1 id="5-plt-xticks-：设置-x-轴刻度的表现方式"><a href="#5-plt-xticks-：设置-x-轴刻度的表现方式" class="headerlink" title="5.plt.xticks()：设置 x 轴刻度的表现方式"></a>5.plt.xticks()：设置 x 轴刻度的表现方式</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig2 = plt.figure(num=<span class="string">&#x27;fig222222&#x27;</span>, figsize=(<span class="number">6</span>, <span class="number">3</span>), dpi=<span class="number">75</span>, facecolor=<span class="string">&#x27;#FFFFFF&#x27;</span>, edgecolor=<span class="string">&#x27;#FF0000&#x27;</span>)</span><br><span class="line">plt.plot()</span><br><span class="line"><span class="comment"># np.linspace 创建等差数列</span></span><br><span class="line">plt.xticks(np.linspace(<span class="number">0</span>,<span class="number">1000</span>,<span class="number">15</span>,endpoint=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># 设置x轴刻度</span></span><br><span class="line">plt.yticks(np.linspace(<span class="number">0</span>,<span class="number">20</span>,<span class="number">10</span>,endpoint=<span class="literal">True</span>))</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br></pre></td></tr></table></figure><p><strong>展示</strong></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/xticks.png" alt="xticks.png"></p><h1 id="6-plt-scatter-—散点图"><a href="#6-plt-scatter-—散点图" class="headerlink" title="6.plt.scatter()—散点图"></a>6.plt.scatter()—散点图</h1><p><code>matplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None,alpha=None, linewidths=None, verts=None, edgecolors=None, *, data=None, **kwargs)</code></p><ul><li>x，y：表示的是大小为(n,)的数组，也就是我们即将绘制散点图的数据点</li><li>s:是一个实数或者是一个数组大小为(n,)，这个是一个可选的参数</li><li>c:表示的是颜色，也是一个可选项。默认是蓝色’b’,表示的是标记的颜色，或者可以是一个表示颜色的字符，或者是一个长度为 n 的表示颜色的序列等等</li><li>marker:表示的是标记的样式，默认的是’o’</li><li>cmap:Colormap 实体或者是一个 colormap 的名字，cmap 仅仅当 c 是一个浮点数数组的时候才使用。如果没有申明就是 image.cmap</li><li>norm:Normalize 实体来将数据亮度转化到 0-1 之间，也是只有 c 是一个浮点数的数组的时候才使用。如果没有申明，就是默认为 colors.Normalize</li><li>vmin,vmax:实数，当 norm 存在的时候忽略。用来进行亮度数据的归一化</li><li>alpha：实数，0-1 之间</li><li>linewidths:也就是标记点的长度</li></ul><blockquote><p><a href="https://blog.csdn.net/m0_37393514/article/details/81298503">参考教程</a></p></blockquote><h1 id="7-ax2-set-title-‘xxx’-设置标题，画图"><a href="#7-ax2-set-title-‘xxx’-设置标题，画图" class="headerlink" title="7.ax2.set_title(‘xxx’)设置标题，画图"></a>7.ax2.set_title(‘xxx’)设置标题，画图</h1><p><code>plt.xlabel()</code> <code>plt.ylabel()</code>xy 轴标签</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#产生[1,2,3,...,9]的序列</span></span><br><span class="line">x = np.arange(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">y = x</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">221</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置标题</span></span><br><span class="line">ax1.set_title(<span class="string">&#x27;Scatter Plot1&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;M&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;N&#x27;</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">222</span>)</span><br><span class="line">ax2.set_title(<span class="string">&#x27;Scatter Plot2clf&#x27;</span>)</span><br><span class="line"><span class="comment">#设置X轴标签</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;X&#x27;</span>)           <span class="comment">#设置X/Y轴标签是在对应的figure后进行操作才对应到该figure</span></span><br><span class="line"><span class="comment">#设置Y轴标签</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line"><span class="comment">#画散点图</span></span><br><span class="line">ax1.scatter(x,y,c = <span class="string">&#x27;r&#x27;</span>,marker = <span class="string">&#x27;o&#x27;</span>)          <span class="comment">#可以看出画散点图是在对figure进行操作</span></span><br><span class="line">ax2.scatter(x,y,c = <span class="string">&#x27;b&#x27;</span>,marker = <span class="string">&#x27;x&#x27;</span>)</span><br><span class="line"><span class="comment">#设置图标</span></span><br><span class="line">plt.legend(<span class="string">&#x27;show picture x1 &#x27;</span>)</span><br><span class="line"><span class="comment">#显示所画的图</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>展示</strong></p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/title1.png" alt="title1.png"></p><h1 id="8-plt-hist（）绘制直方图"><a href="#8-plt-hist（）绘制直方图" class="headerlink" title="8.plt.hist（）绘制直方图"></a>8.plt.hist（）绘制直方图</h1><p>_可以将高斯函数这些画出来_</p><p><code>n, bins, patches = plt.hist(arr, bins=10, normed=0, facecolor=&#39;black&#39;, edgecolor=&#39;black&#39;,alpha=1，histtype=&#39;bar&#39;)</code><br>hist 的参数非常多，但常用的就这六个，只有第一个是必须的，后面四个可选</p><ul><li>arr: 需要计算直方图的一维数组</li><li>bins: 直方图的柱数，可选项，默认为 10</li><li>normed: 是否将得到的直方图向量归一化。默认为 0</li><li>facecolor: 直方图颜色</li><li>edgecolor: 直方图边框颜色</li><li>alpha: 透明度</li><li>histtype: 直方图类型，‘bar’, ‘barstacked’, ‘step’, ‘stepfilled’<br>返回值 ：<br>n: 直方图向量，是否归一化由参数 normed 设定<br>bins: 返回各个 bin 的区间范围<br>patches: 返回每个 bin 里面包含的数据，是一个 list</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mu, sigma = 0, .1</span><br><span class="line">s = np.random.normal(loc=mu, scale=sigma, size=1000)</span><br><span class="line">a,b,c = plt.hist(s, bins=3)</span><br><span class="line">print(&quot;a: &quot;,a)</span><br><span class="line">print(&quot;b: &quot;,b)</span><br><span class="line">print(&quot;c: &quot;,c)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line"></span><br><span class="line">a:  [ 85. 720. 195.]         #每个柱子的值</span><br><span class="line">b:  [-0.36109509 -0.1357318   0.08963149  0.31499478]   #每个柱的区间范围</span><br><span class="line">c:  &lt;a list of 3 Patch objects&gt;       #总共多少柱子</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>萌芽杯项目的python代码</title>
      <link href="/Python%20Practise/Make-mengya/"/>
      <url>/Python%20Practise/Make-mengya/</url>
      
        <content type="html"><![CDATA[<p>项目-识别表的指针，python代码的构造过程，并没有写完（萌芽杯比赛项目）</p><span id="more"></span><h1 id="1-测试"><a href="#1-测试" class="headerlink" title="1. 测试"></a>1. 测试</h1><h2 id="1-1-视频经过梯度处理"><a href="#1-1-视频经过梯度处理" class="headerlink" title="1.1 视频经过梯度处理"></a>1.1 视频经过梯度处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">vc = cv2.VideoCapture(<span class="string">&#x27;./video/1.mp4&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> vc.isOpened():</span><br><span class="line">    <span class="built_in">open</span>, frame = vc.read()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">open</span> = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">open</span>:</span><br><span class="line">    ret, frame = vc.read()</span><br><span class="line">    <span class="keyword">if</span> frame <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> ret == <span class="literal">True</span>:</span><br><span class="line">        gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)</span><br><span class="line">        <span class="comment"># sobelx = cv2.Sobel(gray,cv2.CV_64F,1,0)</span></span><br><span class="line">        <span class="comment"># sobely = cv2.Sobel(gray,cv2.CV_64F,0,1)</span></span><br><span class="line">        <span class="comment"># sobelx = cv2.convertScaleAbs(sobelx)</span></span><br><span class="line">        <span class="comment"># sobely = cv2.convertScaleAbs(sobely)</span></span><br><span class="line">        <span class="comment"># sobelxy = cv2.addWeighted(sobelx,0.5,sobely,0.5,0)</span></span><br><span class="line">        v1 = cv2.Canny(gray,<span class="number">10</span>,<span class="number">50</span>)</span><br><span class="line">        cv2.namedWindow(<span class="string">&#x27;result&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># cv2.imshow(&#x27;result&#x27;,sobelxy)</span></span><br><span class="line">        cv2.imshow(<span class="string">&#x27;result&#x27;</span>,v1)</span><br><span class="line">        <span class="keyword">if</span> cv2.waitKey(<span class="number">10</span>) &amp; <span class="number">0xFF</span> == <span class="number">27</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">vc.release()</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><h2 id="1-2-检测指针，绘制直线"><a href="#1-2-检测指针，绘制直线" class="headerlink" title="1.2 检测指针，绘制直线"></a>1.2 检测指针，绘制直线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;./img/clock2.jpg&#x27;</span>)    <span class="comment"># + input(&#x27;请输入文件路径：&#x27;)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cv_show</span>(<span class="params">name,img</span>):</span><br><span class="line">    cv2.namedWindow(name,<span class="number">0</span>)</span><br><span class="line">    cv2.imshow(name,img)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linedraw</span>(<span class="params">image</span>):</span><br><span class="line">    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br><span class="line">        <span class="comment"># edges = cv.Canny(gray, 50, 150, apertureSize=3)</span></span><br><span class="line">        <span class="comment"># 自动检测可能的直线，返回的是一条条线段</span></span><br><span class="line">    ret, thresohold = cv2.threshold(gray, <span class="number">55</span>, <span class="number">255</span>, cv2.THRESH_BINARY)</span><br><span class="line">    <span class="comment"># aussian = cv2.GaussianBlur(thresohold, (5, 5), 1) 高斯</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># kernel = np.ones((5, 5), np.uint8)  # （5，5）腐蚀大小</span></span><br><span class="line">    <span class="comment"># erosion = cv2.erode(thresohold, kernel, iterations=1) 去噪音点</span></span><br><span class="line"></span><br><span class="line">    edges = cv2.Canny(thresohold, <span class="number">10</span>, <span class="number">150</span>, apertureSize=<span class="number">3</span>)</span><br><span class="line">    lines = cv2.HoughLinesP(edges, <span class="number">1</span>, np.pi / <span class="number">180</span>, <span class="number">80</span>, minLineLength=<span class="number">110</span>, maxLineGap=<span class="number">10</span>)</span><br><span class="line">        <span class="comment"># print(type(lines))</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        x1, y1, x2, y2 = line[<span class="number">0</span>]</span><br><span class="line">        cv2.line(image, (x1, y1), (x2, y2), (<span class="number">0</span>, <span class="number">255</span>,<span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">    cv_show(<span class="string">&#x27;result&#x27;</span>,image)</span><br><span class="line"></span><br><span class="line">linedraw(img)</span><br><span class="line">os.system(<span class="string">&quot;pause&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="1-3-小综合；根据表盘识别指针并求出斜率"><a href="#1-3-小综合；根据表盘识别指针并求出斜率" class="headerlink" title="1.3 小综合；根据表盘识别指针并求出斜率"></a>1.3 小综合；根据表盘识别指针并求出斜率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;./img/clock1.jpg&#x27;</span>, cv2.IMREAD_UNCHANGED)</span><br><span class="line"><span class="comment"># 裁剪图片  具体指针要改</span></span><br><span class="line">imgbufen = img[<span class="number">70</span>:<span class="number">566</span>,<span class="number">0</span>:<span class="number">1000</span>]</span><br><span class="line"><span class="comment"># 将图像的阈值化 小于阈值的为max 大于阈值的为0，</span></span><br><span class="line"><span class="comment"># 主要是因为这个指针是黑的,圆形遮罩也是黑的，所以先把指针反转为白的</span></span><br><span class="line"><span class="comment"># 具体指针要改</span></span><br><span class="line">imgfan = cv2.threshold(imgbufen,<span class="number">127</span>,<span class="number">255</span>,cv2.THRESH_BINARY_INV)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cv_show</span>(<span class="params">name,img</span>):</span><br><span class="line">    cv2.namedWindow(name,<span class="number">0</span>)</span><br><span class="line">    cv2.imshow(name,img)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到一个圆形遮罩，返回中间圆形</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_zhezhao</span>(<span class="params">imgfan</span>):</span><br><span class="line">    <span class="comment">#获取图片尺寸</span></span><br><span class="line">    height, width = imgfan.shape[:<span class="number">2</span>]</span><br><span class="line">    height = <span class="built_in">int</span>(height)</span><br><span class="line">    width = <span class="built_in">int</span>(width)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#生成内显示模板</span></span><br><span class="line">    circleIn = np.zeros((height, width, <span class="number">1</span>), np.uint8)</span><br><span class="line">    circleIn = cv2.circle(circleIn, (width // <span class="number">2</span>, height // <span class="number">2</span>), <span class="built_in">min</span>(height, width) // <span class="number">2</span>, (<span class="number">1</span>), -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#原图与内显示模板融合</span></span><br><span class="line">    <span class="comment">#生成空白图片</span></span><br><span class="line">    imgIn = np.zeros((height, width, <span class="number">4</span>), np.uint8)</span><br><span class="line">    <span class="comment">#复制前3个通道</span></span><br><span class="line">    imgIn[:, :, <span class="number">0</span>] = np.multiply(imgfan[:, :, <span class="number">0</span>], circleIn[:, :, <span class="number">0</span>])</span><br><span class="line">    imgIn[:, :, <span class="number">1</span>] = np.multiply(imgfan[:, :, <span class="number">1</span>], circleIn[:, :, <span class="number">0</span>])</span><br><span class="line">    imgIn[:, :, <span class="number">2</span>] = np.multiply(imgfan[:, :, <span class="number">2</span>], circleIn[:, :, <span class="number">0</span>])</span><br><span class="line">    <span class="comment">#设置α通道的不透明部分</span></span><br><span class="line">    circleIn[circleIn == <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">    imgIn[:, :, <span class="number">3</span>] = circleIn[:, :, <span class="number">0</span>]</span><br><span class="line">    <span class="comment"># cv2.imwrite(&#x27;./img/result1.jpg&#x27;, imgIn)</span></span><br><span class="line">    <span class="comment"># cv_show(&#x27;imgin&#x27;,imgIn)</span></span><br><span class="line">    <span class="keyword">return</span> imgIn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到线，霍夫直线</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getlines</span>(<span class="params">imgIn</span>):</span><br><span class="line">    gray = cv2.cvtColor(imgIn,cv2.COLOR_BGR2GRAY)</span><br><span class="line">    <span class="comment"># cv_show(&quot;gray&quot;,gray)</span></span><br><span class="line">    edges = cv2.Canny(gray, <span class="number">10</span>, <span class="number">150</span>, apertureSize=<span class="number">3</span>)</span><br><span class="line">    lines = cv2.HoughLinesP(edges, <span class="number">1</span>, np.pi / <span class="number">180</span>, <span class="number">80</span>, minLineLength=<span class="number">110</span>, maxLineGap=<span class="number">10</span>)</span><br><span class="line">            <span class="comment"># print(type(lines))</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        x1, y1, x2, y2 = line[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># print(x1,y1,x2,y2)</span></span><br><span class="line">        cv2.line(imgbufen, (x1, y1), (x2, y2), (<span class="number">0</span>, <span class="number">255</span>,<span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># cv_show(&#x27;img&#x27;,imgbufen)</span></span><br><span class="line">        floatx1.append(<span class="built_in">float</span>(x1))</span><br><span class="line">        floaty1.append(<span class="built_in">float</span>(y1))</span><br><span class="line">        floatx2.append(<span class="built_in">float</span>(x2))</span><br><span class="line">        floaty2.append(<span class="built_in">float</span>(y2))</span><br><span class="line">    <span class="comment"># cv_show(&#x27;imgline&#x27;,imgbufen)</span></span><br><span class="line">    <span class="keyword">return</span> floatx1,floaty1,floatx2,floaty2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_thr_k</span>():</span><br><span class="line">    thrs = []</span><br><span class="line">    ks = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(floatx1)):</span><br><span class="line">        x1 = floatx1[i]</span><br><span class="line">        y1 = floaty1[i]</span><br><span class="line">        x2 = floatx2[i]</span><br><span class="line">        y2 = floaty2[i]</span><br><span class="line">        k = (y1-y2)/(x2-x1)</span><br><span class="line">        thr = math.atan(k)</span><br><span class="line">        thr = math.degrees(thr)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        ks.append(k)</span><br><span class="line">        thrs.append(thr)</span><br><span class="line">    <span class="keyword">return</span> ks,thrs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">imgIn = get_zhezhao(imgfan)</span><br><span class="line">floatx1,floaty1,floatx2,floaty2 = [],[],[],[]</span><br><span class="line">getlines(imgIn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(floatx1,floaty1,floatx2,floaty2)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多条直线的角度列表，单条直线直接thrs[0]</span></span><br><span class="line">thrs = get_thr_k()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;直线的斜率为：<span class="subst">&#123;thrs[<span class="number">0</span>][<span class="number">0</span>]&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;直线与横线间角度值：<span class="subst">&#123;thrs[<span class="number">1</span>][<span class="number">0</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">cv_show(<span class="string">&#x27;imgline&#x27;</span>,imgbufen)</span><br></pre></td></tr></table></figure><h2 id="1-4-更高级的检测指针，然后返回指针斜率"><a href="#1-4-更高级的检测指针，然后返回指针斜率" class="headerlink" title="1.4 更高级的检测指针，然后返回指针斜率"></a>1.4 更高级的检测指针，然后返回指针斜率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> linekthr <span class="keyword">as</span> lt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_pointer_rad</span>(<span class="params">img</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;获取角度&#x27;&#x27;&#x27;</span></span><br><span class="line">    shape = img.shape</span><br><span class="line">    c_y, c_x, depth = <span class="built_in">int</span>(shape[<span class="number">0</span>] / <span class="number">2</span>), <span class="built_in">int</span>(shape[<span class="number">1</span>] / <span class="number">2</span>), shape[<span class="number">2</span>]    <span class="comment"># h,w,cute</span></span><br><span class="line">    x1=c_x+c_x*<span class="number">1.5</span>  <span class="comment"># 指针长度--宽 2.5倍</span></span><br><span class="line">    src = img.copy()</span><br><span class="line">    freq_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">361</span>):        <span class="comment"># 算法</span></span><br><span class="line">        x = (x1 - c_x) * math.cos(i * math.pi / <span class="number">180</span>) + c_x</span><br><span class="line">        y = (x1 - c_x) * math.sin(i * math.pi / <span class="number">180</span>) + c_y</span><br><span class="line">        temp = src.copy()   <span class="comment"># 备份</span></span><br><span class="line">        cv2.line(temp, (c_x, c_y), (<span class="built_in">int</span>(x), <span class="built_in">int</span>(y)), (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), thickness=<span class="number">1</span>)  <span class="comment"># 在temp上画线</span></span><br><span class="line">        t1 = img.copy()</span><br><span class="line">        t1[temp[:, :, <span class="number">1</span>] == <span class="number">255</span>] = <span class="number">255</span></span><br><span class="line">        c = img[temp[:, :, <span class="number">1</span>] == <span class="number">255</span>]</span><br><span class="line">        points = c[c == <span class="number">0</span>]</span><br><span class="line">        freq_list.append((<span class="built_in">len</span>(points), i))</span><br><span class="line">        cv2.imshow(<span class="string">&#x27;d&#x27;</span>, temp)</span><br><span class="line">        <span class="comment"># cv2.imshow(&#x27;d1&#x27;, t1)</span></span><br><span class="line">        cv2.waitKey(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># key = lambda x: float(x[0])</span></span><br><span class="line">    <span class="comment"># keytup = max(freq_list, key=key)</span></span><br><span class="line">    <span class="comment"># thrth = keytup[1]</span></span><br><span class="line">    <span class="comment"># print(f&#x27;当前角度:&#123;thrth&#125;度&#x27;)</span></span><br><span class="line">    <span class="comment"># print(&#x27;当前角度：&#x27;,max(freq_list, key=key),&#x27;度&#x27;)</span></span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(freq_list, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;./img/clock_re.png&#x27;</span>)</span><br><span class="line">imgc = img[<span class="number">0</span>:<span class="number">165</span>,<span class="number">6</span>:<span class="number">171</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getthr</span>():</span><br><span class="line">    thres = np.random.randint(<span class="number">40</span>,<span class="number">100</span>)   <span class="comment"># 随机数范围</span></span><br><span class="line">    <span class="comment"># print(thres)</span></span><br><span class="line">    imgfan = cv2.threshold(imgc, thres, <span class="number">255</span>, cv2.THRESH_BINARY)[<span class="number">1</span>]</span><br><span class="line">    <span class="built_in">max</span> = get_pointer_rad(imgfan)</span><br><span class="line">    <span class="comment"># lt.cv_show(&#x27;imgfan&#x27;,imgfan)</span></span><br><span class="line">    <span class="comment"># print(max)</span></span><br><span class="line">    <span class="comment"># exit()</span></span><br><span class="line">    thrs = []</span><br><span class="line">    thr = <span class="built_in">max</span>[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> thr</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_averg</span>():</span><br><span class="line">    tol = <span class="number">0</span></span><br><span class="line">    h = <span class="number">20</span>          <span class="comment"># 统计次数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(h):</span><br><span class="line">        thr = getthr()</span><br><span class="line">        tol = tol + thr</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;第<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>次的角度:<span class="subst">&#123;thr&#125;</span>&#x27;</span>)</span><br><span class="line">    averg = tol / h</span><br><span class="line">    <span class="keyword">return</span> averg</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;角度的平均值：<span class="subst">&#123;get_averg()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="2-初步完成核心"><a href="#2-初步完成核心" class="headerlink" title="2. 初步完成核心"></a>2. 初步完成<del>核心</del></h1><p><code>line_get.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取指针角度值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_pointer_rad</span>(<span class="params">img</span>):</span><br><span class="line">    shape = img.shape</span><br><span class="line">    c_y, c_x, depth = <span class="built_in">int</span>(shape[<span class="number">0</span>] / <span class="number">2</span>), <span class="built_in">int</span>(shape[<span class="number">1</span>] / <span class="number">2</span>), shape[<span class="number">2</span>]    <span class="comment"># h,w,cute</span></span><br><span class="line">    x1=c_x+c_x*<span class="number">1.5</span>  <span class="comment"># 指针长度--宽 2.5倍</span></span><br><span class="line">    src = img.copy()</span><br><span class="line">    freq_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">361</span>):        <span class="comment"># 算法</span></span><br><span class="line">        x = (x1 - c_x) * math.cos(i * math.pi / <span class="number">180</span>) + c_x</span><br><span class="line">        y = (x1 - c_x) * math.sin(i * math.pi / <span class="number">180</span>) + c_y</span><br><span class="line">        temp = src.copy()   <span class="comment"># 备份</span></span><br><span class="line">        cv2.line(temp, (c_x, c_y), (<span class="built_in">int</span>(x), <span class="built_in">int</span>(y)), (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), thickness=<span class="number">1</span>)  <span class="comment"># 在temp上画线</span></span><br><span class="line">        t1 = img.copy()</span><br><span class="line">        t1[temp[:, :, <span class="number">1</span>] == <span class="number">255</span>] = <span class="number">255</span></span><br><span class="line">        c = img[temp[:, :, <span class="number">1</span>] == <span class="number">255</span>]</span><br><span class="line">        points = c[c == <span class="number">0</span>]</span><br><span class="line">        freq_list.append((<span class="built_in">len</span>(points), i))</span><br><span class="line">        <span class="comment"># 可以展示匹配过程</span></span><br><span class="line">        <span class="comment"># cv2.imshow(&#x27;d&#x27;, temp)</span></span><br><span class="line">        <span class="comment"># cv2.imshow(&#x27;d1&#x27;, t1)</span></span><br><span class="line">        <span class="comment"># 如果要求固定检测时间不要太快，可以在这里调慢</span></span><br><span class="line">        cv2.waitKey(<span class="number">1</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(freq_list, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getthr</span>(<span class="params">imgc</span>):</span><br><span class="line">    thres = np.random.randint(<span class="number">40</span>,<span class="number">100</span>)   <span class="comment"># 随机数范围</span></span><br><span class="line">    imgfan = cv2.threshold(imgc, thres, <span class="number">255</span>, cv2.THRESH_BINARY)[<span class="number">1</span>]</span><br><span class="line">    <span class="built_in">max</span> = get_pointer_rad(imgfan)</span><br><span class="line">    thr = <span class="built_in">max</span>[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> thr</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_averg</span>(<span class="params">imgc,h</span>):</span><br><span class="line">    tol = <span class="number">0</span></span><br><span class="line">    h = <span class="built_in">int</span>(h)          <span class="comment"># 统计次数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(h):</span><br><span class="line">        thr = getthr(imgc)</span><br><span class="line">        tol = tol + thr</span><br><span class="line">        <span class="comment"># debug 看看角度是否正确统计</span></span><br><span class="line">        <span class="comment"># print(f&#x27;第&#123;i+1&#125;次的角度:&#123;thr&#125;&#x27;)</span></span><br><span class="line">        <span class="built_in">print</span>(i+<span class="number">1</span>,end=<span class="string">&#x27;、&#x27;</span>)</span><br><span class="line">    averg = tol / h</span><br><span class="line">    <span class="keyword">return</span> averg</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    img = cv2.imread(<span class="string">&#x27;./img/clock_re.png&#x27;</span>)</span><br><span class="line">    imgc = img[<span class="number">0</span>:<span class="number">165</span>, <span class="number">6</span>:<span class="number">171</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;角度的平均值：<span class="subst">&#123;get_averg(imgc,<span class="number">5</span>)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><code>xlsx_get.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> openpyxl <span class="keyword">import</span> Workbook</span><br><span class="line"><span class="keyword">import</span> openpyxl <span class="keyword">as</span> xl</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> line_get <span class="keyword">as</span> lg</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_xlsx</span>():</span><br><span class="line">    <span class="comment"># 新建xlsx或打开已有xlsx</span></span><br><span class="line">    filex = <span class="string">&#x27;data.xlsx&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(filex):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;--在result.xlsx中写入数据--&#x27;</span>)</span><br><span class="line">        wb = xl.load_workbook(<span class="string">&#x27;data.xlsx&#x27;</span>)</span><br><span class="line">        <span class="comment"># 文件表单定位</span></span><br><span class="line">        sheet = wb[<span class="string">&#x27;Sheet&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        wb = Workbook()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;--新建一个data.xlsx--&#x27;</span>)</span><br><span class="line">        sheet = wb[<span class="string">&#x27;Sheet&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 储存数据</span></span><br><span class="line">    ws = wb.active</span><br><span class="line">    ws[<span class="string">&#x27;A1&#x27;</span>] = <span class="string">&#x27;指针角度&#x27;</span></span><br><span class="line">    ws[<span class="string">&#x27;B1&#x27;</span>] = <span class="string">&#x27;转换角度&#x27;</span></span><br><span class="line">    ws[<span class="string">&#x27;C1&#x27;</span>] = <span class="string">&#x27;测试时间&#x27;</span></span><br><span class="line">    img = cv2.imread(<span class="string">&#x27;./img/clock_re.png&#x27;</span>)</span><br><span class="line">    imgh = img[<span class="number">0</span>:<span class="number">165</span>,<span class="number">6</span>:<span class="number">171</span>]</span><br><span class="line"></span><br><span class="line">    cishu = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入需要多少组数据--一直测则输入0--：&#x27;</span>))</span><br><span class="line">    pingjun = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入多少组算一次平均值--推荐10--:&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    row = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> cishu == <span class="number">0</span> <span class="keyword">or</span> cishu &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">while</span> row &gt;= cishu :</span><br><span class="line">            maxrow = sheet.max_row</span><br><span class="line">            <span class="comment"># print(f&#x27;excel中第&#123;maxrow + 1&#125;行输入数据-----&#x27;)</span></span><br><span class="line">            <span class="comment"># 测几次来算平均值 imgh,10--10次</span></span><br><span class="line">            thr = lg.get_averg(imgh,pingjun)</span><br><span class="line">            ws.cell(row=maxrow+<span class="number">1</span>,column=<span class="number">1</span>,value=thr)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;第<span class="subst">&#123;row + <span class="number">1</span>&#125;</span>组-----角度平均值数据：<span class="subst">&#123;thr&#125;</span>，输入到第<span class="subst">&#123;maxrow + <span class="number">1</span>&#125;</span>行中&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> thr&gt;<span class="number">0</span> <span class="keyword">and</span> thr&lt;=<span class="number">45</span>:</span><br><span class="line">                cdu = thr/<span class="number">2.25</span> +<span class="number">100</span></span><br><span class="line">            <span class="keyword">elif</span> thr&gt;=<span class="number">135</span> <span class="keyword">and</span> thr &lt;= <span class="number">360</span>:</span><br><span class="line">                cdu = (thr - <span class="number">135</span>)/<span class="number">2.25</span></span><br><span class="line">            <span class="keyword">else</span> :</span><br><span class="line">                cdu = <span class="string">&#x27;故障&#x27;</span></span><br><span class="line">            ws.cell(row=maxrow+<span class="number">1</span>,column=<span class="number">2</span>,value=cdu)</span><br><span class="line">            timed = time.strftime(<span class="string">&quot;%H:%M:%S&quot;</span>, time.localtime())  <span class="comment"># %Y-%m-%d</span></span><br><span class="line">            ws.cell(row=maxrow+<span class="number">1</span>,column=<span class="number">3</span>,value=timed)</span><br><span class="line">            row += <span class="number">1</span></span><br><span class="line">            wb.save(<span class="string">&#x27;result.xlsx&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">while</span> row &lt; cishu :</span><br><span class="line">            maxrow = sheet.max_row</span><br><span class="line">            <span class="comment"># print(f&#x27;excel中第&#123;maxrow + 1&#125;行输入数据-----&#x27;)</span></span><br><span class="line">            <span class="comment"># 测几次来算平均值 imgh,10--10次</span></span><br><span class="line">            thr = lg.get_averg(imgh,pingjun)</span><br><span class="line">            ws.cell(row=maxrow+<span class="number">1</span>,column=<span class="number">1</span>,value=thr)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;第<span class="subst">&#123;row + <span class="number">1</span>&#125;</span>组-----角度平均值数据：<span class="subst">&#123;thr&#125;</span>，输入到第<span class="subst">&#123;maxrow + <span class="number">1</span>&#125;</span>行中&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> thr&gt;<span class="number">0</span> <span class="keyword">and</span> thr&lt;=<span class="number">45</span>:</span><br><span class="line">                cdu = thr/<span class="number">2.25</span> +<span class="number">100</span></span><br><span class="line">            <span class="keyword">elif</span> thr&gt;=<span class="number">135</span> <span class="keyword">and</span> thr &lt;= <span class="number">360</span>:</span><br><span class="line">                cdu = (thr - <span class="number">135</span>)/<span class="number">2.25</span></span><br><span class="line">            <span class="keyword">else</span> :</span><br><span class="line">                cdu = <span class="string">&#x27;故障&#x27;</span></span><br><span class="line">            ws.cell(row=maxrow+<span class="number">1</span>,column=<span class="number">2</span>,value=cdu)</span><br><span class="line">            timed = time.strftime(<span class="string">&quot;%H:%M:%S&quot;</span>, time.localtime())  <span class="comment"># %Y-%m-%d</span></span><br><span class="line">            ws.cell(row=maxrow+<span class="number">1</span>,column=<span class="number">3</span>,value=timed)</span><br><span class="line">            row += <span class="number">1</span></span><br><span class="line">            wb.save(<span class="string">&#x27;data.xlsx&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    get_xlsx()</span><br></pre></td></tr></table></figure><p><code>keshihua_data.py</code><br><strong>主要运行文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> plotly.offline <span class="keyword">as</span> ptly</span><br><span class="line"><span class="keyword">import</span> plotly.graph_objs <span class="keyword">as</span> go</span><br><span class="line"><span class="keyword">import</span> openpyxl <span class="keyword">as</span> xl</span><br><span class="line"><span class="keyword">import</span> xlsx_get <span class="keyword">as</span> xg</span><br><span class="line"></span><br><span class="line">xg.get_xlsx()</span><br><span class="line">data=[]</span><br><span class="line">wb = xl.load_workbook(<span class="string">&#x27;data.xlsx&#x27;</span>)</span><br><span class="line">sheet = wb[<span class="string">&#x27;Sheet&#x27;</span>]</span><br><span class="line">ws = wb.active</span><br><span class="line"></span><br><span class="line">cdu = []</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> ws.iter_rows(min_row=<span class="number">2</span>, min_col=<span class="number">2</span>,max_col=<span class="number">2</span>,max_row=sheet.max_row ):</span><br><span class="line">    <span class="keyword">for</span> cell <span class="keyword">in</span> row:</span><br><span class="line">        cdu.append(cell.value)</span><br><span class="line">timed = []</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> ws.iter_rows(min_row=<span class="number">2</span>, min_col=<span class="number">3</span>,max_col=<span class="number">3</span>,max_row=sheet.max_row ):</span><br><span class="line">    <span class="keyword">for</span> cell <span class="keyword">in</span> row:</span><br><span class="line">        timed.append(cell.value)</span><br><span class="line"></span><br><span class="line">trace1 = go.Scatter(x = timed,</span><br><span class="line">                    y = cdu,</span><br><span class="line">                    mode = <span class="string">&#x27;lines+markers&#x27;</span>,   <span class="comment">#mode可选&#x27;markers&#x27;,&#x27;lines&#x27;,&#x27;lines+markers&#x27;</span></span><br><span class="line">                    name = <span class="string">&#x27;data&#x27;</span>,</span><br><span class="line">                    marker = <span class="built_in">dict</span>(size = <span class="number">10</span>,        <span class="comment">#若设为变量则可用散点大小表示变量大小</span></span><br><span class="line">                                  color = <span class="string">&#x27;rgba(152, 0, 0, .8)&#x27;</span>,</span><br><span class="line">                                  line = <span class="built_in">dict</span>(width = <span class="number">2</span>,</span><br><span class="line">                                              color = <span class="string">&#x27;rgb(0, 0, 0)&#x27;</span></span><br><span class="line">                                              ),</span><br><span class="line">                                  opacity=[]</span><br><span class="line">                                )</span><br><span class="line">            )</span><br><span class="line">data.append(trace1)</span><br><span class="line">layout = go.Layout(font=<span class="built_in">dict</span>(family=<span class="string">&#x27;Courier New, monospace&#x27;</span>, size=<span class="number">18</span>, color=<span class="string">&#x27;#3D3D3D&#x27;</span>),</span><br><span class="line">                   title=<span class="string">&#x27;温度值&#x27;</span></span><br><span class="line">    )</span><br><span class="line">fig = go.Figure(data=data, layout=layout)</span><br><span class="line">ptly.plot(fig, filename = <span class="string">&#x27;data.html&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="3-进一步优化，直接再存到-excel-同时绘制图线，达到粗略得实时"><a href="#3-进一步优化，直接再存到-excel-同时绘制图线，达到粗略得实时" class="headerlink" title="3. 进一步优化，直接再存到 excel 同时绘制图线，达到粗略得实时"></a>3. 进一步优化，直接再存到 excel 同时绘制图线，达到粗略得实时</h1><p><code>keshihua_data_shishi.py</code></p><ul><li>1.实时显示，但不太美观，而且不丝滑</li><li>2.界面美观，但不会实时显示</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import plotly.offline as ptly</span></span><br><span class="line"><span class="comment"># import plotly.graph_objs as go</span></span><br><span class="line"><span class="keyword">import</span> openpyxl <span class="keyword">as</span> xl</span><br><span class="line"><span class="keyword">from</span> openpyxl <span class="keyword">import</span> Workbook</span><br><span class="line"><span class="keyword">import</span> line_get <span class="keyword">as</span> lg</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_xlsx</span>(<span class="params">thr,cdu,timed</span>):</span><br><span class="line">    filex = <span class="string">&#x27;ssdata.xlsx&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(filex):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;--在ssdata.xlsx中写入数据--&#x27;</span>)</span><br><span class="line">        wb = xl.load_workbook(<span class="string">&#x27;ssdata.xlsx&#x27;</span>)</span><br><span class="line">        <span class="comment"># 文件表单定位</span></span><br><span class="line">        sheet = wb[<span class="string">&#x27;Sheet&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        wb = Workbook()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;--新建一个ssdata.xlsx--&#x27;</span>)</span><br><span class="line">        sheet = wb[<span class="string">&#x27;Sheet&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    ws = wb.active</span><br><span class="line">    ws[<span class="string">&#x27;A1&#x27;</span>] = <span class="string">&#x27;指针角度&#x27;</span></span><br><span class="line">    ws[<span class="string">&#x27;B1&#x27;</span>] = <span class="string">&#x27;转换角度&#x27;</span></span><br><span class="line">    ws[<span class="string">&#x27;C1&#x27;</span>] = <span class="string">&#x27;测试时间&#x27;</span></span><br><span class="line"></span><br><span class="line">    maxrow = sheet.max_row + <span class="number">1</span></span><br><span class="line">    ws.cell(row=maxrow,column=<span class="number">1</span>,value=thr)</span><br><span class="line">    ws.cell(row=maxrow,column=<span class="number">2</span>,value=cdu)</span><br><span class="line">    ws.cell(row=maxrow,column=<span class="number">3</span>,value=timed)</span><br><span class="line"></span><br><span class="line">    wb.save(<span class="string">&#x27;ssdata.xlsx&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_cts</span>():</span><br><span class="line">    filex = <span class="string">&#x27;ssdata.xlsx&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(filex):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;--在ssdata.xlsx中写入数据--&#x27;</span>)</span><br><span class="line">        wb = xl.load_workbook(<span class="string">&#x27;ssdata.xlsx&#x27;</span>)</span><br><span class="line">        <span class="comment"># 文件表单定位</span></span><br><span class="line">        sheet = wb[<span class="string">&#x27;Sheet&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        wb = Workbook()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;--新建一个ssdata.xlsx--&#x27;</span>)</span><br><span class="line">        sheet = wb[<span class="string">&#x27;Sheet&#x27;</span>]</span><br><span class="line">    ws = wb.active</span><br><span class="line">    cdu = []</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> ws.iter_rows(min_row=<span class="number">2</span>, min_col=<span class="number">2</span>,max_col=<span class="number">2</span>,max_row=sheet.max_row ):</span><br><span class="line">        <span class="keyword">for</span> cell <span class="keyword">in</span> row:</span><br><span class="line">            cdu.append(<span class="built_in">str</span>(cell.value))</span><br><span class="line">    timed = []</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> ws.iter_rows(min_row=<span class="number">2</span>, min_col=<span class="number">3</span>,max_col=<span class="number">3</span>,max_row=sheet.max_row ):</span><br><span class="line">        <span class="keyword">for</span> cell <span class="keyword">in</span> row:</span><br><span class="line">            timed.append(cell.value)</span><br><span class="line">    <span class="keyword">return</span> cdu,timed</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_shishi</span>(<span class="params">cishu,pingjun</span>):</span><br><span class="line">    img = cv2.imread(<span class="string">&#x27;./img/clock_re.png&#x27;</span>)</span><br><span class="line">    imgh = img[<span class="number">0</span>:<span class="number">165</span>,<span class="number">6</span>:<span class="number">171</span>]</span><br><span class="line"></span><br><span class="line">    row = <span class="number">0</span></span><br><span class="line">    cts = get_cts()</span><br><span class="line">    cdus = cts[<span class="number">0</span>]</span><br><span class="line">    timeds = cts[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> row &gt;= cishu :</span><br><span class="line">        thr = lg.get_averg(imgh,pingjun)</span><br><span class="line">        <span class="keyword">if</span> thr &gt; <span class="number">0</span> <span class="keyword">and</span> thr &lt;= <span class="number">45</span>:</span><br><span class="line">            cdu = thr / <span class="number">2.25</span> + <span class="number">100</span></span><br><span class="line">        <span class="keyword">elif</span> thr &gt;= <span class="number">135</span> <span class="keyword">and</span> thr &lt;= <span class="number">360</span>:</span><br><span class="line">            cdu = (thr - <span class="number">135</span>) / <span class="number">2.25</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cdu = <span class="string">&#x27;测试故障&#x27;</span></span><br><span class="line">        cdus.append(<span class="built_in">str</span>(cdu))</span><br><span class="line">        timed = time.strftime(<span class="string">&quot;%H:%M:%S&quot;</span>, time.localtime())</span><br><span class="line">        timeds.append(timed)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存数据</span></span><br><span class="line">        save_xlsx(thr,cdu,timed)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># plt.figure(1)</span></span><br><span class="line">        plt.clf()  <span class="comment"># 清空画布上的所有内容</span></span><br><span class="line">        fig1 = plt.figure(num=<span class="string">&#x27;温度-时间&#x27;</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>), dpi=<span class="number">75</span>, facecolor=<span class="string">&#x27;#FFFFFF&#x27;</span>, edgecolor=<span class="string">&#x27;#0000FF&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;Time&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Temp&#x27;</span>)</span><br><span class="line">        plt.plot(timeds, cdus, <span class="string">&#x27;r-s&#x27;</span>)</span><br><span class="line">        <span class="comment"># plt.draw()  # 注意此函数需要调用</span></span><br><span class="line">        <span class="comment"># time.sleep(0.01)</span></span><br><span class="line">        plt.pause(<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(timeds) &gt;= <span class="number">22</span>:</span><br><span class="line">            timeds = []</span><br><span class="line">            cdus = []</span><br><span class="line">            plt.clf()</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># # plotly 数据可视化 美观，可是不会动态</span></span><br><span class="line">        <span class="comment"># data = []</span></span><br><span class="line">        <span class="comment"># namee = str(round(cdu,1)) + &#x27;°C&#x27;</span></span><br><span class="line">        <span class="comment"># trace1 = go.Scatter(x=timeds,</span></span><br><span class="line">        <span class="comment">#                     y=cdus,</span></span><br><span class="line">        <span class="comment">#                     mode=&#x27;lines+markers&#x27;,  # mode可选&#x27;markers&#x27;,&#x27;lines&#x27;,&#x27;lines+markers&#x27;</span></span><br><span class="line">        <span class="comment">#                     name= namee,</span></span><br><span class="line">        <span class="comment">#                     marker=dict(size=10,  # 若设为变量则可用散点大小表示变量大小</span></span><br><span class="line">        <span class="comment">#                                 color=&#x27;rgba(152, 0, 0, .8)&#x27;,</span></span><br><span class="line">        <span class="comment">#                                 line=dict(width=2,</span></span><br><span class="line">        <span class="comment">#                                           color=&#x27;rgb(0, 0, 0)&#x27;</span></span><br><span class="line">        <span class="comment">#                                           ),</span></span><br><span class="line">        <span class="comment">#                                 opacity=[]</span></span><br><span class="line">        <span class="comment">#                                 )</span></span><br><span class="line">        <span class="comment">#                     )</span></span><br><span class="line">        <span class="comment"># data.append(trace1)</span></span><br><span class="line">        <span class="comment"># axis_template = dict(</span></span><br><span class="line">        <span class="comment">#     showgrid=True,  # 网格</span></span><br><span class="line">        <span class="comment">#     zeroline=True,  # 是否显示基线,即沿着(0,0)画出x轴和y轴</span></span><br><span class="line">        <span class="comment">#     nticks=20,</span></span><br><span class="line">        <span class="comment">#     showline=True,</span></span><br><span class="line">        <span class="comment">#     title=&#x27;Time&#x27;,</span></span><br><span class="line">        <span class="comment">#     mirror=&#x27;all&#x27;,</span></span><br><span class="line">        <span class="comment">#     zerolinecolor=&quot;#FF0000&quot;</span></span><br><span class="line">        <span class="comment"># )</span></span><br><span class="line">        <span class="comment"># ayis_template = dict(</span></span><br><span class="line">        <span class="comment">#     showgrid=True,  # 网格</span></span><br><span class="line">        <span class="comment">#     zeroline=True,  # 是否显示基线,即沿着(0,0)画出x轴和y轴</span></span><br><span class="line">        <span class="comment">#     nticks=20,</span></span><br><span class="line">        <span class="comment">#     showline=True,</span></span><br><span class="line">        <span class="comment">#     title=&#x27;Temp&#x27;,</span></span><br><span class="line">        <span class="comment">#     mirror=&#x27;all&#x27;,</span></span><br><span class="line">        <span class="comment">#     zerolinecolor=&quot;#FF0000&quot;</span></span><br><span class="line">        <span class="comment"># )</span></span><br><span class="line">        <span class="comment"># layout = go.Layout(font=dict(family=&#x27;Courier New, monospace&#x27;, size=18, color=&#x27;#3D3D3D&#x27;),</span></span><br><span class="line">        <span class="comment">#                    title=&#x27;温度值&#x27; ,xaxis=axis_template,yaxis=ayis_template</span></span><br><span class="line">        <span class="comment">#                    )</span></span><br><span class="line">        <span class="comment"># fig = go.Figure(data=data, layout=layout)</span></span><br><span class="line">        <span class="comment"># ptly.plot(fig, filename=&#x27;ssdata.html&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    get_shishi(<span class="number">0</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><code>keshihua_shishi.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keshihua_data_shishi <span class="keyword">as</span> kds</span><br><span class="line"></span><br><span class="line">cishu = <span class="number">0</span>   <span class="comment"># 0 - 一直测</span></span><br><span class="line">pingjun = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入多少组算一次平均数--建议10组--:&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    kds.get_shishi(cishu,pingjun)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="4-粗糙的整合—待优化"><a href="#4-粗糙的整合—待优化" class="headerlink" title="4. 粗糙的整合—待优化"></a>4. 粗糙的整合—待优化</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> openpyxl <span class="keyword">as</span> xl</span><br><span class="line"><span class="keyword">from</span> openpyxl <span class="keyword">import</span> Workbook</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> plotly.offline <span class="keyword">as</span> ptly</span><br><span class="line"><span class="keyword">import</span> plotly.graph_objs <span class="keyword">as</span> go</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取指针角度值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_pointer_rad</span>(<span class="params">img</span>):</span><br><span class="line">    shape = img.shape</span><br><span class="line">    c_y, c_x, depth = <span class="built_in">int</span>(shape[<span class="number">0</span>] / <span class="number">2</span>), <span class="built_in">int</span>(shape[<span class="number">1</span>] / <span class="number">2</span>), shape[<span class="number">2</span>]    <span class="comment"># h,w,cute</span></span><br><span class="line">    x1=c_x+c_x*<span class="number">1.5</span>  <span class="comment"># 指针长度--宽 2.5倍</span></span><br><span class="line">    src = img.copy()</span><br><span class="line">    freq_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">361</span>):        <span class="comment"># 算法</span></span><br><span class="line">        x = (x1 - c_x) * math.cos(i * math.pi / <span class="number">180</span>) + c_x</span><br><span class="line">        y = (x1 - c_x) * math.sin(i * math.pi / <span class="number">180</span>) + c_y</span><br><span class="line">        temp = src.copy()   <span class="comment"># 备份</span></span><br><span class="line">        cv2.line(temp, (c_x, c_y), (<span class="built_in">int</span>(x), <span class="built_in">int</span>(y)), (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), thickness=<span class="number">1</span>)  <span class="comment"># 在temp上画线</span></span><br><span class="line">        t1 = img.copy()</span><br><span class="line">        t1[temp[:, :, <span class="number">1</span>] == <span class="number">255</span>] = <span class="number">255</span></span><br><span class="line">        c = img[temp[:, :, <span class="number">1</span>] == <span class="number">255</span>]</span><br><span class="line">        points = c[c == <span class="number">0</span>]</span><br><span class="line">        freq_list.append((<span class="built_in">len</span>(points), i))</span><br><span class="line">        <span class="comment"># 可以展示匹配过程</span></span><br><span class="line">        <span class="comment"># cv2.imshow(&#x27;d&#x27;, temp)</span></span><br><span class="line">        <span class="comment"># cv2.imshow(&#x27;d1&#x27;, t1)</span></span><br><span class="line">        <span class="comment"># 如果要求固定检测时间不要太快，可以在这里调慢</span></span><br><span class="line">        cv2.waitKey(<span class="number">1</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(freq_list, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getthr</span>(<span class="params">imgc</span>):</span><br><span class="line">    thres = np.random.randint(<span class="number">40</span>,<span class="number">100</span>)   <span class="comment"># 随机数范围</span></span><br><span class="line">    imgfan = cv2.threshold(imgc, thres, <span class="number">255</span>, cv2.THRESH_BINARY)[<span class="number">1</span>]</span><br><span class="line">    <span class="built_in">max</span> = get_pointer_rad(imgfan)</span><br><span class="line">    thr = <span class="built_in">max</span>[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> thr</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_averg</span>(<span class="params">imgc,h</span>):</span><br><span class="line">    tol = <span class="number">0</span></span><br><span class="line">    h = <span class="built_in">int</span>(h)          <span class="comment"># 统计次数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(h):</span><br><span class="line">        thr = getthr(imgc)</span><br><span class="line">        tol = tol + thr</span><br><span class="line">        <span class="comment"># debug 看看角度是否正确统计</span></span><br><span class="line">        <span class="comment"># print(f&#x27;第&#123;i+1&#125;次的角度:&#123;thr&#125;&#x27;)</span></span><br><span class="line">        <span class="built_in">print</span>(i+<span class="number">1</span>,end=<span class="string">&#x27;、&#x27;</span>)</span><br><span class="line">    averg = tol / h</span><br><span class="line">    <span class="keyword">return</span> averg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_xlsx</span>(<span class="params">thr,cdu,timed</span>):</span><br><span class="line">    filex = <span class="string">&#x27;ssdata.xlsx&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(filex):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;--在ssdata.xlsx中写入数据--&#x27;</span>)</span><br><span class="line">        wb = xl.load_workbook(<span class="string">&#x27;ssdata.xlsx&#x27;</span>)</span><br><span class="line">        <span class="comment"># 文件表单定位</span></span><br><span class="line">        sheet = wb[<span class="string">&#x27;Sheet&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        wb = Workbook()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;--新建一个ssdata.xlsx--&#x27;</span>)</span><br><span class="line">        sheet = wb[<span class="string">&#x27;Sheet&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    ws = wb.active</span><br><span class="line">    ws[<span class="string">&#x27;A1&#x27;</span>] = <span class="string">&#x27;指针角度&#x27;</span></span><br><span class="line">    ws[<span class="string">&#x27;B1&#x27;</span>] = <span class="string">&#x27;转换角度&#x27;</span></span><br><span class="line">    ws[<span class="string">&#x27;C1&#x27;</span>] = <span class="string">&#x27;测试时间&#x27;</span></span><br><span class="line"></span><br><span class="line">    maxrow = sheet.max_row + <span class="number">1</span></span><br><span class="line">    ws.cell(row=maxrow,column=<span class="number">1</span>,value=thr)</span><br><span class="line">    ws.cell(row=maxrow,column=<span class="number">2</span>,value=cdu)</span><br><span class="line">    ws.cell(row=maxrow,column=<span class="number">3</span>,value=timed)</span><br><span class="line"></span><br><span class="line">    wb.save(<span class="string">&#x27;ssdata.xlsx&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_cts</span>():</span><br><span class="line">    filex = <span class="string">&#x27;ssdata.xlsx&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(filex):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;--在ssdata.xlsx中写入数据--&#x27;</span>)</span><br><span class="line">        wb = xl.load_workbook(<span class="string">&#x27;ssdata.xlsx&#x27;</span>)</span><br><span class="line">        <span class="comment"># 文件表单定位</span></span><br><span class="line">        sheet = wb[<span class="string">&#x27;Sheet&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        wb = Workbook()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;--新建一个ssdata.xlsx--&#x27;</span>)</span><br><span class="line">        sheet = wb[<span class="string">&#x27;Sheet&#x27;</span>]</span><br><span class="line">    ws = wb.active</span><br><span class="line">    cdu = []</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> ws.iter_rows(min_row=<span class="number">2</span>, min_col=<span class="number">2</span>,max_col=<span class="number">2</span>,max_row=sheet.max_row ):</span><br><span class="line">        <span class="keyword">for</span> cell <span class="keyword">in</span> row:</span><br><span class="line">            cdu.append(<span class="built_in">str</span>(cell.value))</span><br><span class="line">    timed = []</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> ws.iter_rows(min_row=<span class="number">2</span>, min_col=<span class="number">3</span>,max_col=<span class="number">3</span>,max_row=sheet.max_row ):</span><br><span class="line">        <span class="keyword">for</span> cell <span class="keyword">in</span> row:</span><br><span class="line">            timed.append(cell.value)</span><br><span class="line">    <span class="keyword">return</span> cdu,timed</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_shishi</span>(<span class="params">cishu,pingjun</span>):</span><br><span class="line">    <span class="comment"># img = cv2.imread(&#x27;./img/clock_re.png&#x27;)</span></span><br><span class="line">    imgh = cv2.imread(<span class="string">&#x27;frame.jpg&#x27;</span>)</span><br><span class="line">    <span class="comment"># imgh = img[0:165,6:171]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取excle中数据</span></span><br><span class="line">    cts = get_cts()</span><br><span class="line">    cdus = cts[<span class="number">0</span>]</span><br><span class="line">    timeds = cts[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    row = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">while</span> row:</span><br><span class="line">        thr = get_averg(imgh,pingjun)</span><br><span class="line">        <span class="keyword">if</span> thr &gt; <span class="number">0</span> <span class="keyword">and</span> thr &lt;= <span class="number">45</span>:</span><br><span class="line">            cdu = thr / <span class="number">2.25</span> + <span class="number">100</span></span><br><span class="line">        <span class="keyword">elif</span> thr &gt;= <span class="number">135</span> <span class="keyword">and</span> thr &lt;= <span class="number">360</span>:</span><br><span class="line">            cdu = (thr - <span class="number">135</span>) / <span class="number">2.25</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cdu = <span class="string">&#x27;测试故障&#x27;</span></span><br><span class="line">        cdus.append(<span class="built_in">str</span>(cdu))</span><br><span class="line">        timed = time.strftime(<span class="string">&quot;%H:%M:%S&quot;</span>, time.localtime())</span><br><span class="line">        timeds.append(timed)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存数据</span></span><br><span class="line">        save_xlsx(thr,cdu,timed)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># # plotly 数据可视化 美观，可是不会动态</span></span><br><span class="line">        <span class="comment"># data = []</span></span><br><span class="line">        <span class="comment"># cdu1 = round(cdu ,1)</span></span><br><span class="line">        <span class="comment"># namee = str(cdu1) + &#x27;°C&#x27;</span></span><br><span class="line">        <span class="comment"># trace1 = go.Scatter(x=timeds,</span></span><br><span class="line">        <span class="comment">#                     y=cdus,</span></span><br><span class="line">        <span class="comment">#                     mode=&#x27;lines+markers&#x27;,  # mode可选&#x27;markers&#x27;,&#x27;lines&#x27;,&#x27;lines+markers&#x27;</span></span><br><span class="line">        <span class="comment">#                     name= namee,</span></span><br><span class="line">        <span class="comment">#                     marker=dict(size=10,  # 若设为变量则可用散点大小表示变量大小</span></span><br><span class="line">        <span class="comment">#                                 color=&#x27;rgba(152, 0, 0, .8)&#x27;,</span></span><br><span class="line">        <span class="comment">#                                 line=dict(width=2,</span></span><br><span class="line">        <span class="comment">#                                           color=&#x27;rgb(0, 0, 0)&#x27;</span></span><br><span class="line">        <span class="comment">#                                           ),</span></span><br><span class="line">        <span class="comment">#                                 opacity=[]</span></span><br><span class="line">        <span class="comment">#                                 )</span></span><br><span class="line">        <span class="comment">#                     )</span></span><br><span class="line">        <span class="comment"># data.append(trace1)</span></span><br><span class="line">        <span class="comment"># axis_template = dict(</span></span><br><span class="line">        <span class="comment">#     showgrid=True,  # 网格</span></span><br><span class="line">        <span class="comment">#     zeroline=True,  # 是否显示基线,即沿着(0,0)画出x轴和y轴</span></span><br><span class="line">        <span class="comment">#     nticks=20,</span></span><br><span class="line">        <span class="comment">#     showline=True,</span></span><br><span class="line">        <span class="comment">#     title=&#x27;Time&#x27;,</span></span><br><span class="line">        <span class="comment">#     mirror=&#x27;all&#x27;,</span></span><br><span class="line">        <span class="comment">#     zerolinecolor=&quot;#FF0000&quot;</span></span><br><span class="line">        <span class="comment"># )</span></span><br><span class="line">        <span class="comment"># ayis_template = dict(</span></span><br><span class="line">        <span class="comment">#     showgrid=True,  # 网格</span></span><br><span class="line">        <span class="comment">#     zeroline=True,  # 是否显示基线,即沿着(0,0)画出x轴和y轴</span></span><br><span class="line">        <span class="comment">#     nticks=20,</span></span><br><span class="line">        <span class="comment">#     showline=True,</span></span><br><span class="line">        <span class="comment">#     title=&#x27;Temp&#x27;,</span></span><br><span class="line">        <span class="comment">#     mirror=&#x27;all&#x27;,</span></span><br><span class="line">        <span class="comment">#     zerolinecolor=&quot;#FF0000&quot;</span></span><br><span class="line">        <span class="comment"># )</span></span><br><span class="line">        <span class="comment"># layout = go.Layout(font=dict(family=&#x27;Courier New, monospace&#x27;, size=18, color=&#x27;#3D3D3D&#x27;),</span></span><br><span class="line">        <span class="comment">#                    title=&#x27;温度值&#x27; ,xaxis=axis_template,yaxis=ayis_template</span></span><br><span class="line">        <span class="comment">#                    )</span></span><br><span class="line">        <span class="comment"># fig = go.Figure(data=data, layout=layout)</span></span><br><span class="line">        <span class="comment"># ptly.plot(fig, filename=&#x27;ssdata.html&#x27;)</span></span><br><span class="line">        <span class="comment"># break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实时数据，停不下来</span></span><br><span class="line">        <span class="comment"># plt.figure(1)</span></span><br><span class="line">        plt.clf()  <span class="comment"># 清空画布上的所有内容</span></span><br><span class="line">        fig1 = plt.figure(num=<span class="string">&#x27;温度-时间&#x27;</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>), dpi=<span class="number">75</span>, facecolor=<span class="string">&#x27;#FFFFFF&#x27;</span>, edgecolor=<span class="string">&#x27;#0000FF&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;Time&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Temp&#x27;</span>)</span><br><span class="line">        plt.plot(timeds, cdus, <span class="string">&#x27;r-s&#x27;</span>)</span><br><span class="line">        <span class="comment"># plt.draw()  # 注意此函数需要调用</span></span><br><span class="line">        <span class="comment"># time.sleep(0.01)</span></span><br><span class="line">        plt.pause(<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(timeds) &gt;= <span class="number">22</span>:</span><br><span class="line">            timeds = []</span><br><span class="line">            cdus = []</span><br><span class="line">            plt.clf()</span><br><span class="line">            row = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">video_show</span>(<span class="params">video,ci</span>):</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        ret1,frame = video.read()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> ret1:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;视频获取失败！&quot;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        framegray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line">        template = cv2.imread(<span class="string">&#x27;./img/moban_c.jpg&#x27;</span>)</span><br><span class="line">        <span class="comment"># print(template.shape)</span></span><br><span class="line">        <span class="comment"># exit()</span></span><br><span class="line">        theight , twidth = template.shape[<span class="number">0</span>] , template.shape[<span class="number">1</span>]</span><br><span class="line">        result = cv2.matchTemplate(frame,template,cv2.TM_SQDIFF_NORMED)</span><br><span class="line">        cv2.normalize(result,result,<span class="number">0</span>,<span class="number">1</span>,cv2.NORM_MINMAX,-<span class="number">1</span>)</span><br><span class="line">        min_val,max_val,min_loc,max_loc = cv2.minMaxLoc(result)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># # min_loc：矩形定点</span></span><br><span class="line">        <span class="comment"># # (min_loc[0]+twidth,min_loc[1]+theight)：矩形的宽高</span></span><br><span class="line">        <span class="comment"># # (0,0,225)：矩形的边框颜色；2：矩形边框宽度</span></span><br><span class="line">        cv2.rectangle(frame ,min_loc ,(min_loc[<span class="number">0</span>] + twidth ,min_loc[<span class="number">1</span>] + theight) ,(<span class="number">255</span> ,<span class="number">0</span> ,<span class="number">0</span>) ,<span class="number">2</span>)</span><br><span class="line">        cv2.imshow(<span class="string">&quot;Video_show&quot;</span>,frame)</span><br><span class="line"></span><br><span class="line">        choose_data = framegray[min_loc[<span class="number">0</span>]:(min_loc[<span class="number">0</span>] + twidth ),min_loc[<span class="number">1</span>]: (min_loc[<span class="number">1</span>] + theight)]</span><br><span class="line">        <span class="comment"># choose_datafan = cv2.threshold(choose_data ,110,255 ,cv2.THRESH_BINARY_INV)[1]</span></span><br><span class="line">        cv2.imshow(<span class="string">&quot;choose_video&quot;</span>,choose_data)</span><br><span class="line">        <span class="keyword">if</span> i%ci == <span class="number">0</span> :</span><br><span class="line">            cv2.imwrite(<span class="string">f&#x27;frame.jpg&#x27;</span>,choose_data)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;照片已保存--frame.jpg--：&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> cv2.waitKey(<span class="number">1</span>) &amp; <span class="number">0xff</span> == <span class="built_in">ord</span>(<span class="string">&quot;e&quot;</span>):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    video.release()</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span> :</span><br><span class="line">    cishu = <span class="number">0</span>  <span class="comment"># 0 - 一直测</span></span><br><span class="line">    <span class="comment"># pingjun = int(input(&#x27;请输入多少组算一次平均数--建议2组--:&#x27;))</span></span><br><span class="line">    pingjun = <span class="number">5</span></span><br><span class="line">    ci = <span class="number">100</span></span><br><span class="line">    <span class="comment"># ci = int(input(&#x27;请输入多少时间换一组照片--100大概3.638s左右--：&#x27;))</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        video = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line">        video_show(video ,ci)</span><br><span class="line">        get_shishi(cishu ,pingjun)</span><br></pre></td></tr></table></figure><h1 id="5-进一步优化"><a href="#5-进一步优化" class="headerlink" title="5. 进一步优化"></a>5. 进一步优化</h1><h2 id="5-1-分析误差的准备"><a href="#5-1-分析误差的准备" class="headerlink" title="5.1 分析误差的准备"></a>5.1 分析误差的准备</h2><h3 id="5-1-1-构造一个桌面指针"><a href="#5-1-1-构造一个桌面指针" class="headerlink" title="5.1.1 构造一个桌面指针"></a>5.1.1 构造一个桌面指针</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> turtle <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime <span class="keyword">as</span> dt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">drawuse</span>():</span><br><span class="line">    <span class="comment"># background</span></span><br><span class="line">    game = t.Screen()</span><br><span class="line">    game.bgcolor(<span class="string">&#x27;#A4D3EE&#x27;</span>)</span><br><span class="line">    game.setup(<span class="number">500</span>,<span class="number">500</span>,<span class="number">0</span>,<span class="number">10</span>)</span><br><span class="line">    game.tracer(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    pen = t.Turtle()</span><br><span class="line">    pen.ht()</span><br><span class="line">    pen.speed(<span class="number">0</span>)</span><br><span class="line">    pen.up()</span><br><span class="line">    pen.pensize(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">draw</span>():</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            pen.clear()</span><br><span class="line">            <span class="comment"># 圆</span></span><br><span class="line">            pen.pensize(<span class="number">3</span>)</span><br><span class="line">            pen.up()</span><br><span class="line">            pen.color(<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line">            pen.goto(<span class="number">0</span>,-<span class="number">200</span>)</span><br><span class="line">            pen.down()</span><br><span class="line">            pen.seth(<span class="number">0</span>)</span><br><span class="line">            pen.circle(<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 刻度</span></span><br><span class="line">            pen.up()</span><br><span class="line">            pen.goto(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">            pen.seth(<span class="number">225</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">13</span>):</span><br><span class="line">                pen.fd(<span class="number">180</span>)</span><br><span class="line">                pen.down()</span><br><span class="line">                pen.fd(<span class="number">20</span>)</span><br><span class="line">                pen.up()</span><br><span class="line">                pen.goto(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">                pen.right(<span class="number">22.5</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 指针</span></span><br><span class="line">            pen.up()</span><br><span class="line">            pen.goto(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">            pen.down()</span><br><span class="line">            pen.seth(<span class="number">225</span> - i)</span><br><span class="line">            pen.color(<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">            pen.pensize(<span class="number">6</span>)</span><br><span class="line">            pen.fd(<span class="number">120</span>)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> i &gt;= <span class="number">271</span>:</span><br><span class="line">                i = <span class="number">0</span></span><br><span class="line">            game.update()</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">drawgd</span>(<span class="params">thr</span>):</span><br><span class="line">        pen.clear()</span><br><span class="line">        <span class="comment"># 圆</span></span><br><span class="line">        pen.up()</span><br><span class="line">        pen.color(<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line">        pen.goto(<span class="number">0</span>,-<span class="number">200</span>)</span><br><span class="line">        pen.down()</span><br><span class="line">        pen.seth(<span class="number">0</span>)</span><br><span class="line">        pen.circle(<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 刻度</span></span><br><span class="line">        pen.up()</span><br><span class="line">        pen.goto(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">        pen.seth(<span class="number">225</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">13</span>):</span><br><span class="line">            pen.fd(<span class="number">180</span>)</span><br><span class="line">            pen.down()</span><br><span class="line">            pen.fd(<span class="number">20</span>)</span><br><span class="line">            pen.up()</span><br><span class="line">            pen.goto(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">            pen.right(<span class="number">22.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 指针</span></span><br><span class="line">        pen.up()</span><br><span class="line">        pen.goto(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">        pen.down()</span><br><span class="line">        pen.seth(<span class="number">225</span> - thr)</span><br><span class="line">        pen.color(<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">        pen.pensize(<span class="number">6</span>)</span><br><span class="line">        pen.fd(<span class="number">120</span>)</span><br><span class="line">        game.update()</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模式一--动态</span></span><br><span class="line">    draw()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># # 模式二--固定</span></span><br><span class="line">    <span class="comment"># thr = int(input(&quot;输入指针度数:--角度--&quot;))</span></span><br><span class="line">    <span class="comment"># drawgd(thr)</span></span><br><span class="line"></span><br><span class="line">    game.mainloop()</span><br></pre></td></tr></table></figure><h3 id="5-1-2-手机上下载一个模拟时钟"><a href="#5-1-2-手机上下载一个模拟时钟" class="headerlink" title="5.1.2 手机上下载一个模拟时钟"></a>5.1.2 手机上下载一个模拟时钟</h3><h3 id="5-1-3-获取两者数据对比来分析误差"><a href="#5-1-3-获取两者数据对比来分析误差" class="headerlink" title="5.1.3 获取两者数据对比来分析误差"></a>5.1.3 获取两者数据对比来分析误差</h3>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Practise </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python中Opencv库的学习</title>
      <link href="/Learn/Learn-Python_OPEN_CV/"/>
      <url>/Learn/Learn-Python_OPEN_CV/</url>
      
        <content type="html"><![CDATA[<p>Python中opencv的学习和使用，可以操作图片，跟ps中知识挺像的hhh<br><span id="more"></span></p><h1 id="0-基本知识的学习"><a href="#0-基本知识的学习" class="headerlink" title="0.基本知识的学习"></a>0.基本知识的学习</h1><h2 id="0-1-基本操作"><a href="#0-1-基本操作" class="headerlink" title="0.1 基本操作"></a>0.1 基本操作</h2><p>引用库<br><code>import cv2</code></p><p>cv2.IMREAD_COLOR:彩色图像 RGB 三通道<br>cv2.IMREAD_GRAYSCALE：灰度图像 灰度一个通道</p><ul><li><code>img = cv2.imread(&#39;1.jpg&#39;)</code>打开图像——type(img) = numpy.ndarry</li><li><code>img = cv2.imread(&#39;1.jpg&#39;,cv2.IMREAD_GRAYSCALE)</code>打开为灰度图像</li><li><code>cv2.imshow(&#39;image&#39;,img)</code>展示图像，窗口 image</li><li><code>cv2.waitKey(0)</code>窗口停留时间毫秒级，0 表示按任意键退出</li><li><code>cv2.destroyALLWindows()</code>销毁窗口</li><li><code>cv2.imwrite(&#39;result.jpg&#39;,img)</code>保存图像，（文件名，图片）</li></ul><p>_可以直接定义一个函数_</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cv_show</span>(<span class="params">name,img</span>):</span><br><span class="line">    cv2.imshow(name,img)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyALLWindows()</span><br></pre></td></tr></table></figure><h3 id="0-1-1-画直线"><a href="#0-1-1-画直线" class="headerlink" title="0.1.1 画直线"></a>0.1.1 画直线</h3><p>像素点坐标，左为零，上为零<br>左上角为坐标原点，而坐标系是从左到右 x 符合，从上到下，y 要取负</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">img = np.zeros((<span class="number">320</span>, <span class="number">320</span>, <span class="number">3</span>), np.uint8) <span class="comment">#生成一个空灰度图像</span></span><br><span class="line"><span class="built_in">print</span>(img.shape) <span class="comment"># 输出：(320, 320, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cv_show</span>(<span class="params">name,img</span>):</span><br><span class="line">    cv2.namedWindow(name,<span class="number">0</span>)</span><br><span class="line">    cv2.imshow(name,img)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图片，初始坐标，结束坐标，图线颜色，图线粗细</span></span><br><span class="line">cv2.line(img,(<span class="number">0</span>,<span class="number">100</span>),(<span class="number">100</span>,<span class="number">0</span>),(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">2</span>)</span><br><span class="line">cv2.line(img,(<span class="number">0</span>,<span class="number">200</span>),(<span class="number">100</span>,<span class="number">0</span>),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">cv_show(<span class="string">&#x27;line&#x27;</span>,img)</span><br></pre></td></tr></table></figure><h3 id="0-1-2-调用笔记本摄像头"><a href="#0-1-2-调用笔记本摄像头" class="headerlink" title="0.1.2 调用笔记本摄像头"></a>0.1.2 调用笔记本摄像头</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">video = cv2.VideoCapture(<span class="number">0</span>) <span class="comment"># 参数是0，表示打开笔记本的内置摄像头，参数是视频文件路径则打开视频，如cap = cv2.VideoCapture(&quot;../test.avi&quot;)</span></span><br><span class="line">ret,frame = video.read() <span class="comment"># ret,frame是获cap.read()方法的两个返回值。其中ret是布尔值，如果读取帧是正确的则返回True，如果文件读取到结尾，它的返回值就为False</span></span><br><span class="line"></span><br><span class="line">cv2.waitKey(<span class="number">0</span>) <span class="comment"># 如cv2.waitKey(0)只显示当前帧图像，相当于视频暂停</span></span><br><span class="line">cv2.waitKey(<span class="number">1</span>) <span class="comment"># 表示延时1ms切换到下一帧图像，对于视频而言</span></span><br><span class="line">cv2.waitKey(<span class="number">1000</span>) <span class="comment"># 会因为延时过久而卡顿感觉到卡顿</span></span><br><span class="line"></span><br><span class="line">cv2.release()</span><br><span class="line">调用release()释放摄像头，调用destroyAllWindows()关闭所有图像窗口</span><br></pre></td></tr></table></figure><h2 id="0-2-基本属性-函数"><a href="#0-2-基本属性-函数" class="headerlink" title="0.2 基本属性/函数"></a>0.2 基本属性/函数</h2><p>img.shape # (414.500.3) (h,w,rgb=3)<br>img.size # h*w*rgb<br>img.dtype # uint8</p><h2 id="0-3-读取视频"><a href="#0-3-读取视频" class="headerlink" title="0.3 读取视频"></a>0.3 读取视频</h2><p>cv2.VideoCapture 捕获摄像头</p><ul><li><code>vc = cv2.VideoCapture(&#39;test.mp4&#39;)</code> 打开视频</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查是否正确打开</span></span><br><span class="line"><span class="keyword">if</span> vc.isOpened():</span><br><span class="line">    <span class="built_in">open</span>. frame = vc.read()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">open</span> = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">open</span>:</span><br><span class="line"></span><br><span class="line"><span class="comment"># 播放视频 循环每一帧</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">open</span>:</span><br><span class="line">    ret, frame = vc.read()</span><br><span class="line">    <span class="keyword">if</span> frame <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> ret == <span class="literal">True</span>:</span><br><span class="line">        gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)   <span class="comment">#转化为灰度</span></span><br><span class="line">        cv2.namedWindow(<span class="string">&#x27;result&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">        cv2.imshow(<span class="string">&#x27;result&#x27;</span>,gray)</span><br><span class="line">        <span class="keyword">if</span> cv2.waitKey(<span class="number">10</span>) &amp; <span class="number">0xFF</span> == <span class="number">27</span>: <span class="comment"># 10刚刚好速度，27按esc退出</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">vc.release()</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><h2 id="0-4-感兴趣区域"><a href="#0-4-感兴趣区域" class="headerlink" title="0.4 感兴趣区域"></a>0.4 感兴趣区域</h2><p>图像截取</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img = cv.imread(<span class="string">&#x27;1.jpg&#x27;</span>)</span><br><span class="line">cat = img[<span class="number">0</span>:<span class="number">200</span>,<span class="number">0</span>:<span class="number">200</span>]</span><br><span class="line">cv.show(<span class="string">&#x27;cat&#x27;</span>,cat)</span><br></pre></td></tr></table></figure><p>图片截取，（识别技术将匹配到的数据展示）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">image_clip = image_rgb[<span class="built_in">int</span>(top):(<span class="built_in">int</span>(top) + <span class="built_in">int</span>(height)), <span class="built_in">int</span>(left):(<span class="built_in">int</span>(left) + <span class="built_in">int</span>(width))]</span><br><span class="line">顺序为[y0:y1, x0:x1]</span><br></pre></td></tr></table></figure></p><h2 id="0-5-特殊选取-切分通道"><a href="#0-5-特殊选取-切分通道" class="headerlink" title="0.5 特殊选取,切分通道"></a>0.5 特殊选取,切分通道</h2><p>b：::0<br>g：::1<br>r：::2</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b,g,r = cv2.split(img)  <span class="comment"># 切分</span></span><br><span class="line">b.shape == g.shape == r.shape</span><br><span class="line"></span><br><span class="line">img = cv2.merge((b,g,r))    <span class="comment"># 合并</span></span><br><span class="line">img.shape</span><br></pre></td></tr></table></figure><h2 id="0-6-边界填充"><a href="#0-6-边界填充" class="headerlink" title="0.6 边界填充"></a>0.6 边界填充</h2><p>图像的边界</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上下左右填充大小</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cv_show</span>(<span class="params">name,img</span>):</span><br><span class="line">    cv2.imshow(name,img)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyALLWindows()</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;./img/1.jpg&#x27;</span>)</span><br><span class="line"><span class="comment"># 定义边界大小</span></span><br><span class="line">top_size, bottom_size, left_size,right_size = (<span class="number">50</span>,<span class="number">50</span>,<span class="number">50</span>,<span class="number">50</span>)</span><br><span class="line"><span class="comment"># 复制法，复制最边缘像素</span></span><br><span class="line">replicate = cv2.copyMakeBorder(img,top_size,bottom_size,left_size,right_size,borderType = cv2.BORDER_REPLICATE)</span><br><span class="line"><span class="comment"># 反射法 对感兴趣的图像中的像素在两边进行复制  ba|abc|cb</span></span><br><span class="line">reflect = cv2.copyMakeBorder(img,top_size,bottom_size,left_size,right_size,borderType = cv2.BORDER_REFLECT)</span><br><span class="line"><span class="comment"># 反射法   edcb|abcdefgh|gfedc</span></span><br><span class="line">reflect2 = cv2.copyMakeBorder(img,top_size,bottom_size,left_size,right_size,borderType = cv2.BORDER_REFLECT_101)</span><br><span class="line"><span class="comment"># 外包装法  cdefgh|abcdefgh|abcdef</span></span><br><span class="line">wrap = cv2.copyMakeBorder(img,top_size,bottom_size,left_size,right_size,borderType = cv2.BORDER_WRAP)</span><br><span class="line"><span class="comment"># 常量法   常数值补充</span></span><br><span class="line">constant = cv2.copyMakeBorder(img,top_size,bottom_size,left_size,right_size,borderType = cv2.BORDER_CONSTANT)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="0-7-数值计算-图像融合-大小放缩"><a href="#0-7-数值计算-图像融合-大小放缩" class="headerlink" title="0.7 数值计算\图像融合\大小放缩"></a>0.7 数值计算\图像融合\大小放缩</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">img1 = cv2.imread(<span class="string">&#x27;1.jpg&#x27;</span>)</span><br><span class="line">img2 = cv2.imread(<span class="string">&#x27;2.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">img11 = img1 + <span class="number">10</span>  <span class="comment"># [500,500,3] 中每一块都加10</span></span><br><span class="line">img1[:<span class="number">5</span>,:,<span class="number">0</span>].shape 只打印前五行</span><br><span class="line">img11[:<span class="number">5</span>,:,<span class="number">0</span>]</span><br><span class="line">(img1+img11)[:<span class="number">5</span>,:,<span class="number">0</span>]    如果超出，则结果取余</span><br><span class="line">cv2.add(img1,img11)[:<span class="number">5</span>,:,<span class="number">0</span>] 如果超出<span class="number">255</span>，则不取余直接用<span class="number">255</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像融合</span></span><br><span class="line"></span><br><span class="line">img1+img2 <span class="comment">#shape不同，则加不了</span></span><br><span class="line">img1.shape  <span class="comment"># (414,500,3)</span></span><br><span class="line">img2.shape  <span class="comment"># (419,499,3)</span></span><br><span class="line">img2 = cv2.resize(img2,(<span class="number">500</span>,<span class="number">414</span>))   <span class="comment"># 改变大小</span></span><br><span class="line">img2.shape  <span class="comment"># (414,500,3)</span></span><br><span class="line"></span><br><span class="line">res = cv2.addWeighted(img1,<span class="number">0.4</span>,img2,<span class="number">0.6</span>,<span class="number">0</span>)</span><br><span class="line"><span class="comment"># res = img1*0.4+img2*0.6+0     权重</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像大小放缩</span></span><br><span class="line">res = cv2.resize(img,(<span class="number">0</span>,<span class="number">0</span>),fx=<span class="number">0.5</span>,fy=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="0-8-图像阈值"><a href="#0-8-图像阈值" class="headerlink" title="0.8 图像阈值"></a>0.8 图像阈值</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">cv2.threshold():</span><br><span class="line">参数：</span><br><span class="line">    img:图像对象，必须是灰度图</span><br><span class="line">    thresh:阈值 <span class="number">0</span>~<span class="number">255</span>   eg:<span class="number">127</span></span><br><span class="line">    maxval：最大值  <span class="number">255</span></span><br><span class="line">    <span class="built_in">type</span>:</span><br><span class="line">        cv2.THRESH_BINARY:     小于阈值的像素置为<span class="number">0</span>，大于阈值的置为maxval</span><br><span class="line">            超过阈值部分取最大值maxval=<span class="number">255</span> white，否则取<span class="number">0</span> black</span><br><span class="line">            亮的地方白，暗的地方黑</span><br><span class="line">        cv2.THRESH_BINARY_INV： 小于阈值的像素置为maxval，大于阈值的置为<span class="number">0</span></span><br><span class="line">            亮的地方黑，暗的地方白</span><br><span class="line">        cv2.THRESH_TRUNC：      小于阈值的像素不变，大于阈值的置为thresh</span><br><span class="line">            指定一个截断值，大于阈值部分变成阈值，小于的不变</span><br><span class="line">        cv2.THRESH_TOZERO       小于阈值的像素置<span class="number">0</span>，大于阈值的不变</span><br><span class="line">            大于阈值部分不变，小于的全变为<span class="number">0</span></span><br><span class="line">        cv2.THRESH_TOZERO_INV   小于阈值的不变，大于阈值的像素置<span class="number">0</span></span><br><span class="line">            大于阈值变为<span class="number">0</span>   ，小于阈值的不变</span><br><span class="line">返回两个值</span><br><span class="line">    ret:阈值</span><br><span class="line">    img：阈值化处理后的图像</span><br><span class="line"></span><br><span class="line">cv2.adaptiveThreshold() 自适应阈值处理，图像不同部位采用不同的阈值进行处理</span><br><span class="line">参数：</span><br><span class="line">    img: 图像对象，<span class="number">8</span>-bit单通道图</span><br><span class="line">    maxValue:最大值</span><br><span class="line">    adaptiveMethod: 自适应方法</span><br><span class="line">        cv2.ADAPTIVE_THRESH_MEAN_C     ：阈值为周围像素的平均值</span><br><span class="line">        cv2.ADAPTIVE_THRESH_GAUSSIAN_C : 阈值为周围像素的高斯均值（按权重）</span><br><span class="line">    threshType:</span><br><span class="line">        cv2.THRESH_BINARY:     小于阈值的像素置为<span class="number">0</span>，大于阈值的置为maxValuel</span><br><span class="line">        cv2.THRESH_BINARY_INV:  小于阈值的像素置为maxValue，大于阈值的置为<span class="number">0</span></span><br><span class="line">    blocksize: 计算阈值时，自适应的窗口大小,必须为奇数 （如<span class="number">3</span>：表示附近<span class="number">3</span>个像素范围内的像素点，进行计算阈值）</span><br><span class="line">    C： 常数值，通过自适应方法计算的值，减去该常数值</span><br><span class="line">(mean value of the blocksize*blocksize neighborhood of (x, y) minus C)</span><br></pre></td></tr></table></figure><h2 id="0-9-图像平滑-去掉噪音点"><a href="#0-9-图像平滑-去掉噪音点" class="headerlink" title="0.9 图像平滑-去掉噪音点"></a>0.9 图像平滑-去掉噪音点</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;./img/1.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cv_show</span>(<span class="params">name,img</span>):</span><br><span class="line">    cv2.namedWindow(name,<span class="number">0</span>)</span><br><span class="line">    cv2.imshow(name,img)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyALLWindows()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 均值滤波</span></span><br><span class="line"><span class="comment"># 简单的平均卷积操作 # (3,3)卷积盒</span></span><br><span class="line">blur = cv2.blur(img,(<span class="number">3</span>,<span class="number">3</span>))  <span class="comment"># （奇数，奇数） 中心的值根据周围数改变</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方框滤波</span></span><br><span class="line"><span class="comment"># 与均值滤波相似   normalize 是否做归一化，true与均值一样</span></span><br><span class="line"><span class="comment">#                                       false 会越界&gt;255 ，所有越界值全为255</span></span><br><span class="line">box = cv2.boxFilter(img,-<span class="number">1</span>,(<span class="number">3</span>,<span class="number">3</span>),normalize =<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 高斯滤波</span></span><br><span class="line"><span class="comment"># 高斯模糊---正态分布，离中心值越远，值越小</span></span><br><span class="line">aussian = cv2.GaussianBlur(img,(<span class="number">5</span>,<span class="number">5</span>),<span class="number">1</span>)     <span class="comment"># (5,5)的盒</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 中值滤波</span></span><br><span class="line"><span class="comment"># 中间的值</span></span><br><span class="line">median = cv2.medianBlur(img,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示所有的</span></span><br><span class="line"><span class="comment"># 三张图片拼接在一起</span></span><br><span class="line">res = np.hstack((blur,aussian,median))</span><br><span class="line">resv = np.vstack((blur,aussian,median))</span><br><span class="line"></span><br><span class="line">cv_show(<span class="string">&#x27;res&#x27;</span>,res)</span><br></pre></td></tr></table></figure><h2 id="0-10-形态学-腐蚀操作-去掉毛刺"><a href="#0-10-形态学-腐蚀操作-去掉毛刺" class="headerlink" title="0.10 形态学-腐蚀操作-去掉毛刺"></a>0.10 形态学-腐蚀操作-去掉毛刺</h2><p>边界里的盒子如果有 0 有 255，则全变为 0</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 去毛刺，r通道</span></span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.uint8)    <span class="comment"># （5，5）腐蚀大小</span></span><br><span class="line">erosion = cv2.erode(img,kernel,iterations = <span class="number">2</span>)  <span class="comment"># iterations 做几次腐蚀</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="0-11-形态学-膨胀操作"><a href="#0-11-形态学-膨胀操作" class="headerlink" title="0.11 形态学-膨胀操作"></a>0.11 形态学-膨胀操作</h2><p>_腐蚀后图像太细，使用膨胀_<br>边界里的盒子如果有 0 有 255，则全变为 255 白</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kernel = np.ones((<span class="number">3</span>,<span class="number">3</span>),np.uint8)</span><br><span class="line">dilate = cv2.dilate(erosion,kernel,iterations=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="0-12-开运算与闭运算"><a href="#0-12-开运算与闭运算" class="headerlink" title="0.12 开运算与闭运算"></a>0.12 开运算与闭运算</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开运算--先腐蚀后膨胀</span></span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.uint8)</span><br><span class="line">opening = cv2.morphologyEx(img,cv2.MORPH_OPEN,KERNEL)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 闭运算--先膨胀后腐蚀</span></span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.uint8)</span><br><span class="line">closing = cv2.morphologyEx(img,cv2.MORPH_CLOSE,kernel)</span><br></pre></td></tr></table></figure><h2 id="0-13-梯度运算"><a href="#0-13-梯度运算" class="headerlink" title="0.13 梯度运算"></a>0.13 梯度运算</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度运算 = 膨胀-腐蚀 = 轮廓</span></span><br><span class="line">kernel = np.ones((<span class="number">7</span>,<span class="number">7</span>),np.uint8)</span><br><span class="line">gradient = cv2.morphologyEx(img,cv2.MORPH_GRADIENT,kernel)  <span class="comment"># 轮廓</span></span><br></pre></td></tr></table></figure><h2 id="0-14-礼帽-黑帽"><a href="#0-14-礼帽-黑帽" class="headerlink" title="0.14 礼帽 黑帽"></a>0.14 礼帽 黑帽</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 礼帽 = 原始输入 - 开运算 = 毛刺</span></span><br><span class="line">tophat = cv2.morphologyEx(img,cv2.MORPH_TOPHAT,kernel)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 黑帽 = 闭运算 - 原始输入 = 毛刺+更胖整体 - 毛刺</span></span><br><span class="line">blackhat = cv2.morphologyEx(img,cv2.MORPH_BLACKHAT,kernel)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="0-15-图像梯度-Sobel-算子"><a href="#0-15-图像梯度-Sobel-算子" class="headerlink" title="0.15 图像梯度-Sobel 算子"></a>0.15 图像梯度-Sobel 算子</h2><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/1.jpg" alt="1.jpg"></p><p>梯度：边缘位置的像素数值不同，数值差越大，梯度越大<br>边缘检测，物体分辨<br>右减左，下减上 &emsp; 从右到左，从下至上</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dst = cv2.Sobel(img,ddepth,dx,dy,ksize)</span></span><br><span class="line"><span class="comment"># ddepth:图像深度 -1</span></span><br><span class="line"><span class="comment"># dx dy：水平，竖直 1 0</span></span><br><span class="line"><span class="comment"># ksize：盒的大小</span></span><br><span class="line"></span><br><span class="line">sobelx = cv2.Sobel(img,cv2.CV_64F,<span class="number">1</span>,<span class="number">0</span>,ksize = <span class="number">3</span>)</span><br><span class="line"><span class="comment"># cv2.CV_64F 负数形式</span></span><br><span class="line"><span class="comment"># 白-黑是正数，黑-白是负数，所有负数会被截断为0，所以要取绝对值</span></span><br><span class="line"></span><br><span class="line">sobelx = cv2.convertScaleAbs(sobelx)</span><br><span class="line"><span class="comment"># 取绝对值</span></span><br><span class="line"></span><br><span class="line">sobely = cv2.Sobel(img,cv2.CV_64F,<span class="number">0</span>,<span class="number">1</span>,ksize = <span class="number">3</span>)</span><br><span class="line"><span class="comment"># y方向</span></span><br><span class="line">sobely = cv2.convertScaleAbs(sobely)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x,y分别求出，再求和</span></span><br><span class="line">sobelxy = cv2.addWeighted(sobelx,<span class="number">0.5</span>,sobely,<span class="number">0.5</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不建议直接计算，及dx，dy都为 1</span></span><br><span class="line"><span class="comment"># 轮廓会更加的虚</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="0-16-图像梯度-Scharr-amp-amp-Laplacian-算子"><a href="#0-16-图像梯度-Scharr-amp-amp-Laplacian-算子" class="headerlink" title="0.16 图像梯度 Scharr&amp;&amp;Laplacian 算子"></a>0.16 图像梯度 Scharr&amp;&amp;Laplacian 算子</h2><p>scharr — 更敏感 — 描绘轮廓更细致<br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/2.jpg" alt="2.jpg"></p><p>laplacian — 二阶导 — 更更敏感，对噪音点敏感，很少单独使用</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/3.png" alt="3.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scharrx = cv2.Scharr(img,cv2.CV_64F,1,0)</span><br><span class="line">scharry = cv2.Scharr(img,cv2.CV_64F,0,1)</span><br><span class="line">scharrx = cv2.convertScaleAbs(scharrx)</span><br><span class="line">scharry = cv2.convertScaleAbs(scharry)</span><br><span class="line">scharrxy = cv2.addWeighted(scharrx,0.5,scharry,0.5,0)</span><br><span class="line"></span><br><span class="line">laplacian = cv2.Laplacian(img,cv2.CV_64F)</span><br><span class="line">laplacian = cv2.convertScaleAbs(laplacian)</span><br><span class="line"></span><br><span class="line">res = np.hstack((scharrxy,laplacian))</span><br></pre></td></tr></table></figure><p>三种算子区别</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/123%E5%8C%BA%E5%88%AB.png" alt="123区别.png"></p><h2 id="0-17-Canny-边缘检测—综合"><a href="#0-17-Canny-边缘检测—综合" class="headerlink" title="0.17 Canny 边缘检测—综合"></a>0.17 Canny 边缘检测—综合</h2><ul><li>高斯滤波器，平滑处理，滤除噪声</li><li>计算图像中每个像素点的梯度强度和方向</li><li>应用非极大值抑制，消除小的不明显的地方<br>a 检测出目标可能性 90%，b 是 80%，则会抑制掉 b，把 b 丢掉</li><li>应用双阈值，检测来确定真实的和潜在的边缘</li><li>通过抑制孤立的弱边缘最终完成边缘检测</li></ul><p>① 高斯滤波器<br>归一化平滑处理<br>② 梯度和方向，Sobel 算子<br>③ 非极大值抑制<br>a. 线性插值法<br>b. 简便算法;八个方向分别比较<br>④ 双阈值检测<br>maxVal|minVal<br>梯度值&gt;maxVal 边界<br>minVal &lt; 梯度 &lt; maxVal 连有边界，保留，否则舍弃<br>梯度 &lt; minVal 舍弃<br>⑤ 最终结果<br>canny 函数</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">&#x27;1.jpg&#x27;</span>,cv2.IMREAD_GRAYSCALE)</span><br><span class="line"></span><br><span class="line">v1 = cv2.Canny(img,<span class="number">80</span>,<span class="number">150</span>)  <span class="comment"># minVal,maxVal</span></span><br><span class="line">v2 = cv2.Canny(img,<span class="number">50</span>,<span class="number">150</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="0-18-图像金字塔"><a href="#0-18-图像金字塔" class="headerlink" title="0.18 图像金字塔"></a>0.18 图像金字塔</h2><p>越往上走图像越小<br>图像 800*800 变为 400*400<br>各层分别提取</p><ul><li>高斯金字塔（高斯滤波）</li></ul><ol><li>向下采样，缩小，往金字塔顶走<br>将偶数行和列去掉，1234 去掉 24，行列变为原来的一半<br>4*4 —- 2*2</li><li>向上采样，放大<br>将数据高斯分布给周边新加列行<br>2*2 —- 4*4</li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">&#x27;1.jpg&#x27;</span>)</span><br><span class="line"><span class="comment"># 向上</span></span><br><span class="line">up = cv2.pyrUp(img)</span><br><span class="line"><span class="comment"># 向下</span></span><br><span class="line">down = cv2.pyrDown(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先上 再下，，会变模糊</span></span><br><span class="line">down_up = cv2.pyrDown(up)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF.png" alt="拉普拉斯.png"></p><ul><li>拉普拉斯金字塔<br>原始 - 先 down 再 up = result<br>result - down up = reslt1</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">down = cv2.pyrDown(img)</span><br><span class="line">down_up = cv2.pyrUp(down)</span><br><span class="line">result = img - down_up</span><br></pre></td></tr></table></figure><h2 id="0-19-图像轮廓"><a href="#0-19-图像轮廓" class="headerlink" title="0.19 图像轮廓"></a>0.19 图像轮廓</h2><p>图像边缘—零散<br>图像轮廓—完整<br>cv2.findContours(img,mode,method)</p><ul><li>mode:轮廓检测模式<br>RETR_EXTERNAL 只检测外轮廓<br>RETR_LIST 检索所有的轮廓，将其保存到一条链表中<br>RETR_CCOMP 检索所有轮廓，并将他们组织为两层，顶层是各部分的外部边界，第二层空洞边界<br>RETR_TREE（常用）检测所有轮廓，并重构嵌套轮廓的整个层次</li><li>method:轮廓逼近方法<br>CHAIN_APPROX_NONE：以 freeman 链码方式输出轮廓，其他方法输出多边形<br>CHAIN_APPROX_SIMPLE：压缩水平，垂直，斜的部分</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">&#x27;1.jpg&#x27;</span>)</span><br><span class="line">gray = cv2.cvColor(img,cv2.COLOR_BGR2GRAY)</span><br><span class="line"><span class="comment"># 使用二值图像---更好的边缘检测</span></span><br><span class="line">ret, thresh = cv2.threshold(gray,<span class="number">122</span>,<span class="number">255</span>,cv2.THRESH_BINARY)</span><br><span class="line"><span class="comment"># cv2.imshow(&#x27;result&#x27;,thresh)</span></span><br><span class="line"></span><br><span class="line">binary,contours,hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)</span><br><span class="line"><span class="comment"># binary 二值图像</span></span><br><span class="line"><span class="comment"># contours 轮廓信息</span></span><br><span class="line"><span class="comment"># hierarchy 层级</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制轮廓,必须先备份</span></span><br><span class="line">draw_img = img.copy()</span><br><span class="line">res = cv2.drawContours(draw_img,contours,-<span class="number">1</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">2</span>)</span><br><span class="line"><span class="comment"># draw_img 原图像上绘制</span></span><br><span class="line"><span class="comment"># contours 轮廓信息</span></span><br><span class="line"><span class="comment"># -1 所有轮廓，（几个轮廓） 0 第一个轮廓外圈，1 轮廓里圈</span></span><br><span class="line"><span class="comment"># (0,0,255) BGR 红色的线</span></span><br><span class="line"><span class="comment"># 2 线条宽度--不能太大，能看出轮廓内外层差异</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 轮廓得到以后,具体轮廓拿出来，contours为list</span></span><br><span class="line">cnt = contours[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 算面积</span></span><br><span class="line">cv2.contourArea(cnt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 周长 true 闭合</span></span><br><span class="line">cu2.arcLength(cnt,<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>轮廓近似</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 轮廓近似，将轮廓变得规则起来</span></span><br><span class="line"><span class="comment"># 用直线近似曲线,曲线上找一点，到直线的距离最大，</span></span><br><span class="line"><span class="comment"># d&lt;T 可以近似     d&gt;T,不可以直接用一条直线近似，而是分割开来，继续判断</span></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;1.jpg&#x27;</span>)</span><br><span class="line">gray = cv2.cvColor(img,cv2.COLOR_BGR2GRAY)</span><br><span class="line">ret, thresh = cv2.threshold(gray,<span class="number">122</span>,<span class="number">255</span>,cv2.THRESH_BINARY)</span><br><span class="line">binary,contours,hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)</span><br><span class="line">cnt = contours[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">draw_img = img.copy()</span><br><span class="line">res = cv2.drawContours(draw_img,[cnt],-<span class="number">1</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">epsilon = <span class="number">0.1</span>*cv2.arcLength(cnt,<span class="literal">True</span>)   <span class="comment"># 周长百分比做阈值</span></span><br><span class="line"><span class="comment"># 0.1百分比，越大，轮廓变化越大</span></span><br><span class="line">approx = cv2.approxPolyDP(cnt,epsilon,<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">draw_img = img.copy()</span><br><span class="line">res = cv2.drawContours(draw_img,[approx],-<span class="number">1</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>边界矩形(外接矩形)</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cnt = contours[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 外接矩形</span></span><br><span class="line">x,y,w,h = cv2.boundingRect(cnt)</span><br><span class="line">img = cv2.rectangle(img,(x,y),(x+w,y+h),(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩形面积</span></span><br><span class="line">area = cv2.contourArea(cnt)</span><br><span class="line">x,y,w,h = cv2.boundingRect(cnt)</span><br><span class="line">rect_area = w*h</span><br><span class="line">extent = <span class="built_in">float</span>(area) / rect_area</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;轮廓面积与边界矩形比:<span class="subst">&#123;extent&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>外接圆</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(x,y).radius = cv2.minEnclosingCircle(cnt)</span><br><span class="line">center = (<span class="built_in">int</span>(x),<span class="built_in">int</span>(y))</span><br><span class="line">radius = <span class="built_in">int</span>(radius)</span><br><span class="line">img = cv2.circle(img,center,radius,(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="0-20-模板匹配——匹配对象在另一个图像哪里"><a href="#0-20-模板匹配——匹配对象在另一个图像哪里" class="headerlink" title="0.20 模板匹配——匹配对象在另一个图像哪里"></a>0.20 模板匹配——匹配对象在另一个图像哪里</h2><p>从左到右，从上到下，进行匹配<br>匹配计算方法</p><ul><li>TM_SQDIFF——平方项匹配，值越小，越相关</li><li>TM_CCORR——-计算相关性，值越大，越相关</li><li>TM_CCOEFF——计算相关系数，值越大，越相关</li><li>TM_SQDIFF_NORMED:计算归一化平方不同，越接近 0，越相关</li><li>TM_CCORR_NORMED:计算归一化相关性，越接近 1，越相关</li><li>TM_CCOEFF_NORMED:计算归一化的相关系数，越接近 1，越相关</li><li>最好用归一化的方法<br>返回结果<br>匹配的地方大小：<br>原图结果 A*B,模板大小 a*b,返回结果矩阵：(A-a+1)*(B-b+1)</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">&#x27;1.jpg&#x27;</span>,<span class="number">0</span>)</span><br><span class="line"><span class="comment"># (263,263)</span></span><br><span class="line">template = cv2.imread(<span class="string">&#x27;11.jpg&#x27;</span>,<span class="number">0</span>)</span><br><span class="line"><span class="comment"># (110,85)</span></span><br><span class="line">h,w = template.shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">res = cv2.matchTemplate(img,template,cv2.TM_SQDIFE)</span><br><span class="line"><span class="comment"># (154,179)</span></span><br><span class="line"></span><br><span class="line">min_val,max_val,min_loc,max_loc = cv2.minMaxLoc(res)</span><br><span class="line"><span class="comment"># 最小值，最大值，最小值位置，最大值位置</span></span><br><span class="line"><span class="comment"># 该方法关注最小值位置 框左上角的点，根据res.shape画出图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># top_left &amp; res.shape得到最好匹配结果</span></span><br><span class="line">top_left = min_loc</span><br><span class="line">bottom_right = (top_left[<span class="number">0</span>] + w,top_left[<span class="number">1</span>] + h)</span><br><span class="line">img2 = img.copy()</span><br><span class="line">cv2.rectangle(img2,top_left,bottom_right,<span class="number">255</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 展示出来</span></span><br><span class="line">cv_show(<span class="string">&#x27;res&#x27;</span>,img2)</span><br></pre></td></tr></table></figure><p>匹配多个对象</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 匹配多个对象</span></span><br><span class="line">img_rgb = cv2.imread(<span class="string">&#x27;1.jpg&#x27;</span>)</span><br><span class="line">img_gray = cv2.cvtColor(img_rgb,cv2.COLOR_BGR2GRAY)</span><br><span class="line">template = cv2.imread(<span class="string">&#x27;11.jpg&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">h,w = template.shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">res = cv2.matchTemplate(img_gray,template,cv2.TM_CCOEFF_NORMED)</span><br><span class="line">threshold = <span class="number">0.8</span></span><br><span class="line"><span class="comment"># 取匹配程度大于0.8的坐标</span></span><br><span class="line">loc = np.where(res&gt;= threshold)</span><br><span class="line"><span class="keyword">for</span> pt <span class="keyword">in</span> <span class="built_in">zip</span>(*loc[::<span class="number">1</span>]):   <span class="comment"># * 表示可选参数</span></span><br><span class="line">    bottom_right = (pt[<span class="number">0</span>]+w,pt[<span class="number">1</span>]+h)</span><br><span class="line">    cv2.rectangle(img_rgb,pt,bootom_right,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">2</span>)</span><br><span class="line">cv2.imshow(<span class="string">&#x27;img&#x27;</span>,img_rgb)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="0-21-直方图"><a href="#0-21-直方图" class="headerlink" title="0.21 直方图"></a>0.21 直方图</h2><p>图片像素的统计直方图<br>cv2.calcHist(img,channels,mask,histSize,ranges)<br>img — 图片<br>channels—通道 0 — 自动灰度图 ‘b’ ‘g’ ‘r’<br>mask—淹模图像，掩码，统计某一部分<br>创建掩码<br>mask = np.zeros(img,shape[:2],np.uint8)<br>选择掩码保存部分<br>mask[100:300,100:400] = 255 白色保存部分 # masked_img = cv2.bitwise_and(img,img,mask=mask)<br>hisSize — BIN 的数目，直方图范围<br>ranges — 像素值取值反围</p><p><code>hist = cv2.calcHist([img],[0],None,[256],[0,256])</code></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img =cv2.imread(<span class="string">&#x27;1.jpg&#x27;</span>)</span><br><span class="line">hist = cv2.calcHist([img],[<span class="number">0</span>],<span class="literal">None</span>,[<span class="number">256</span>],[<span class="number">0</span>,<span class="number">256</span>])</span><br><span class="line"><span class="comment"># 画出直方图</span></span><br><span class="line">plt.hist(img,ravel(),<span class="number">256</span>):</span><br><span class="line">plt.show</span><br></pre></td></tr></table></figure><p>直方图均衡化</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果统计出来的直方图 不太平均</span></span><br><span class="line"><span class="comment"># 平均化</span></span><br><span class="line">equ = cv2.equalizeHist(img)</span><br><span class="line">plt.hist(equ,ravel(),<span class="number">256</span>)</span><br><span class="line">plt.show</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果更加的明显</span></span><br><span class="line"><span class="comment"># 一个部分分给其他部分，进行均衡</span></span><br><span class="line"><span class="comment"># 分模块进行均衡化</span></span><br><span class="line"><span class="comment"># 但有的图会出现边界</span></span><br></pre></td></tr></table></figure><p>自适应直方图均衡化</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clahe = cv2.createCLAHE(clipLimit = <span class="number">2.0</span>,tileGridSize = (<span class="number">8</span>，<span class="number">8</span>))</span><br><span class="line">res_clahe = clahe.apply(img)</span><br><span class="line">cv2.imshow(<span class="string">&#x27;result&#x27;</span>,res_clahe)</span><br></pre></td></tr></table></figure><h2 id="0-22-傅里叶变换"><a href="#0-22-傅里叶变换" class="headerlink" title="0.22 傅里叶变换"></a>0.22 傅里叶变换</h2><p>现实中的事物都是运动的<br>而傅里叶的频域中一切都是静止的，现实中的东西在频域中分为高频，低频</p><ul><li>高频：变化剧烈的灰度分量，eg：边界</li><li><p>低频：变化缓慢的灰度变量，eg：一片大海</p><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/%E5%82%85%E9%87%8C%E5%8F%B6.png" alt="傅里叶.png"></p><p>滤波<br>低通滤波器：只保留低频，图像变得模糊<br>高通滤波器：只保留高频，图像细节增强</p></li></ul><p>在 频域中处理，更加方便</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cv2.dft()</span><br><span class="line"><span class="comment"># 逆变换</span></span><br><span class="line">cv2.idft()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;1.jpg&#x27;</span>,<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 输入图像必须先转换成float32格式</span></span><br><span class="line">img_float32 = np.float32(img)</span><br><span class="line"><span class="comment"># 得到的结果中 频率为0的部分在左上角，通常要转换到中心位置，用shift变换</span></span><br><span class="line">dft = cv2.dft(img_float32,flags = cv2.DFT_COMPLEX_OUTPUT)</span><br><span class="line">dft_shift = np.fft.fftshift(dft)</span><br><span class="line"><span class="comment"># cv2.dft()返回结果是双通道的，通常还要转换为图像格式</span></span><br><span class="line">magnitude_spectrum = <span class="number">20</span>*np.log(cv2.magnitude(dft_shift[:,:,<span class="number">0</span>],dft_shift[:,:,<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>低通：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cv_show</span>(<span class="params">name,img</span>):</span><br><span class="line">    cv2.namedWindow(name,<span class="number">0</span>)</span><br><span class="line">    cv2.imshow(name,img)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;./img/clock1.jpg&#x27;</span>,<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 输入图像必须先转换成float32格式</span></span><br><span class="line">img_float32 = np.float32(img)</span><br><span class="line"><span class="comment"># 得到的结果中 频率为0的部分在左上角，通常要转换到中心位置，用shift变换</span></span><br><span class="line">dft = cv2.dft(img_float32,flags = cv2.DFT_COMPLEX_OUTPUT)</span><br><span class="line">dft_shift = np.fft.fftshift(dft)</span><br><span class="line"><span class="comment"># cv2.dft()返回结果是双通道的，通常还要转换为图像格式</span></span><br><span class="line"><span class="comment"># magnitude_spectrum = 20*np.log(cv2.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))</span></span><br><span class="line"></span><br><span class="line">rows,cols = img.shape</span><br><span class="line">crow,ccol = <span class="built_in">int</span>(rows/<span class="number">2</span>) , <span class="built_in">int</span>(cols/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要magn里面</span></span><br><span class="line"><span class="comment"># 创建一个掩码  zeros 全为0 全不要</span></span><br><span class="line">mask = np.zeros((rows,cols,<span class="number">2</span>),np.uint8)</span><br><span class="line">mask[crow-<span class="number">30</span>:crow+<span class="number">30</span>,ccol-<span class="number">30</span>:ccol+<span class="number">30</span>] = <span class="number">1</span>   <span class="comment"># 中间为低频，低频要</span></span><br><span class="line">fshift = dft_shift*mask</span><br><span class="line"><span class="comment"># shift 回去</span></span><br><span class="line">f_ishift = np.fft.ifftshift(fshift)</span><br><span class="line"></span><br><span class="line">img_back = cv2.idft(f_ishift)</span><br><span class="line">img_back = cv2.magnitude(img_back[:,:,<span class="number">0</span>],img_back[:,:,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(img,cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;input image&#x27;</span>), plt.xticks([]),plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(img_back, cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;magnitude spectrum&#x27;</span>), plt.xticks([]),plt.yticks([])</span><br><span class="line">plt.show()</span><br><span class="line">图像模糊</span><br></pre></td></tr></table></figure><p>高通</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cv_show</span>(<span class="params">name,img</span>):</span><br><span class="line">    cv2.namedWindow(name,<span class="number">0</span>)</span><br><span class="line">    cv2.imshow(name,img)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;./img/clock1.jpg&#x27;</span>,<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 输入图像必须先转换成float32格式</span></span><br><span class="line">img_float32 = np.float32(img)</span><br><span class="line"><span class="comment"># 得到的结果中 频率为0的部分在左上角，通常要转换到中心位置，用shift变换</span></span><br><span class="line">dft = cv2.dft(img_float32,flags = cv2.DFT_COMPLEX_OUTPUT)</span><br><span class="line">dft_shift = np.fft.fftshift(dft)</span><br><span class="line"><span class="comment"># cv2.dft()返回结果是双通道的，通常还要转换为图像格式</span></span><br><span class="line"><span class="comment"># magnitude_spectrum = 20*np.log(cv2.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))</span></span><br><span class="line"></span><br><span class="line">rows,cols = img.shape</span><br><span class="line">crow,ccol = <span class="built_in">int</span>(rows/<span class="number">2</span>) , <span class="built_in">int</span>(cols/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全为1 全要</span></span><br><span class="line">mask = np.ones((rows,cols,<span class="number">2</span>),np.uint8)</span><br><span class="line">mask[crow-<span class="number">30</span>:crow+<span class="number">30</span>,ccol-<span class="number">30</span>:ccol+<span class="number">30</span>] = <span class="number">0</span>   <span class="comment"># 中间为0，中间不要，即低频不要</span></span><br><span class="line">fshift = dft_shift*mask</span><br><span class="line"><span class="comment"># shift 回去</span></span><br><span class="line">f_ishift = np.fft.ifftshift(fshift)</span><br><span class="line"></span><br><span class="line">img_back = cv2.idft(f_ishift)</span><br><span class="line">img_back = cv2.magnitude(img_back[:,:,<span class="number">0</span>],img_back[:,:,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示</span></span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(img,cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;input image&#x27;</span>), plt.xticks([]),plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(img_back, cmap = <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;magnitude spectrum&#x27;</span>), plt.xticks([]),plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="0-23-摄像头获取视频或图片获取感兴趣部分"><a href="#0-23-摄像头获取视频或图片获取感兴趣部分" class="headerlink" title="0.23 摄像头获取视频或图片获取感兴趣部分"></a>0.23 摄像头获取视频或图片获取感兴趣部分</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">video_show</span>():</span><br><span class="line">    choose_video = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        ret1,frame = video.read()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> ret1:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;视频获取失败！&quot;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        cv2.imshow(<span class="string">&quot;Video_show&quot;</span>,frame)</span><br><span class="line">        <span class="keyword">if</span> cv2.waitKey(<span class="number">1</span>) &amp; <span class="number">0xff</span> == <span class="built_in">ord</span>(<span class="string">&quot;q&quot;</span>):</span><br><span class="line">            <span class="comment"># rects = []</span></span><br><span class="line">            <span class="comment"># fromCenter = False</span></span><br><span class="line">            <span class="comment"># Select multiple rectangles</span></span><br><span class="line">            <span class="comment"># select_data = cv2.selectROI(&quot;Image&quot;, frame, rects, fromCenter)</span></span><br><span class="line">            select_data = cv2.selectROI(<span class="string">&quot;Video_show&quot;</span>,frame)</span><br><span class="line">            choose_video = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> choose_video :</span><br><span class="line">            <span class="comment">#获取选择框内的图像</span></span><br><span class="line">            choose_data = frame[select_data[<span class="number">1</span>]:select_data[<span class="number">1</span>]+select_data[<span class="number">3</span>],select_data[<span class="number">0</span>]:select_data[<span class="number">0</span>]+select_data[<span class="number">2</span>]]</span><br><span class="line">            cv2.imshow(<span class="string">&quot;choose_video&quot;</span>,choose_data)</span><br><span class="line">        <span class="keyword">if</span> cv2.waitKey(<span class="number">1</span>) &amp; <span class="number">0xff</span> == <span class="built_in">ord</span>(<span class="string">&quot;p&quot;</span>):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    video.release()</span><br><span class="line">    <span class="comment"># cv2.destroyAllWindows()</span></span><br><span class="line">    <span class="keyword">return</span> choose_data</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    video = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line">    video_show()</span><br></pre></td></tr></table></figure><h3 id="0-23-1-选取-roi-区域定义"><a href="#0-23-1-选取-roi-区域定义" class="headerlink" title="0.23.1 选取 roi 区域定义"></a>0.23.1 选取 roi 区域定义</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法1：使用表示矩形区域的Rect，参数有矩形左上角坐标、矩形的长和宽</span></span><br><span class="line"><span class="comment"># Mat imageROI;</span></span><br><span class="line">imageROI = image(Rect(<span class="number">500</span>,<span class="number">250</span>,logo.cols,logo.rows));</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：指定感兴趣的行或列的范围（Range），Range是指从起始索引到终止索引（不包括终止索引）的一段连续序列</span></span><br><span class="line"><span class="comment"># Mat imageROI;</span></span><br><span class="line">imageROI = image(Range(<span class="number">250</span>,<span class="number">250</span>+logoImage.rows),Range(<span class="number">200</span>,<span class="number">200</span>+logoImage.cols));</span><br></pre></td></tr></table></figure><h1 id="1-图像识别相关"><a href="#1-图像识别相关" class="headerlink" title="1. 图像识别相关"></a>1. 图像识别相关</h1><h2 id="1-1-两张图片对比"><a href="#1-1-两张图片对比" class="headerlink" title="1.1 两张图片对比"></a>1.1 两张图片对比</h2><p><strong>返回一张对比后的图片</strong></p><p>轮子安装<br><code>pip install pillow</code><br><code>pip install PIL</code></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> ImageChops</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compare_images</span>(<span class="params">path_one, path_two, diff_save_location</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    比较图片，如果有不同则生成展示不同的图片</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    @参数一: path_one: 第一张图片的路径</span></span><br><span class="line"><span class="string">    @参数二: path_two: 第二张图片的路径</span></span><br><span class="line"><span class="string">    @参数三: diff_save_location: 不同图的保存路径</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    image_one = Image.<span class="built_in">open</span>(path_one)</span><br><span class="line">    image_two = Image.<span class="built_in">open</span>(path_two)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        diff = ImageChops.difference(image_one, image_two)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> diff.getbbox() <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 图片间没有任何不同则直接退出</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;【+】We are the same!&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            diff.save(diff_save_location)</span><br><span class="line">    <span class="keyword">except</span> ValueError <span class="keyword">as</span> e:</span><br><span class="line">        text = (<span class="string">&quot;表示图片大小和box对应的宽度不一致，参考API说明：Pastes another image into this image.&quot;</span></span><br><span class="line">                <span class="string">&quot;The box argument is either a 2-tuple giving the upper left corner, a 4-tuple defining the left, upper, &quot;</span></span><br><span class="line">                <span class="string">&quot;right, and lower pixel coordinate, or None (same as (0, 0)). If a 4-tuple is given, the size of the pasted &quot;</span></span><br><span class="line">                <span class="string">&quot;image must match the size of the region.使用2纬的box避免上述问题&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;【&#123;0&#125;】&#123;1&#125;&quot;</span>.<span class="built_in">format</span>(e, text))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    name1 = <span class="string">&#x27;./对比图片/&#x27;</span> + <span class="built_in">input</span>(<span class="string">&#x27;输入要对比的图片名字---带后缀格式----：&#x27;</span>)</span><br><span class="line">    name2 = <span class="string">&#x27;./对比图片/&#x27;</span> + <span class="built_in">input</span>(<span class="string">&#x27;第二张图片的名字：&#x27;</span>)</span><br><span class="line">    name = <span class="string">&#x27;对比结果&#x27;</span> + <span class="built_in">input</span>(<span class="string">&#x27;你的对比结果后缀是什么：&#x27;</span>)</span><br><span class="line">    compare_images(name1, name2, name)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-------已完成-------&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p><a href="https://www.cnblogs.com/botoo/p/8416315.html">参考教程</a></p></blockquote><h1 id="2-根据数据生成表格，图线"><a href="#2-根据数据生成表格，图线" class="headerlink" title="2. 根据数据生成表格，图线"></a>2. 根据数据生成表格，图线</h1><p><strong><a href="https://yq010105.github.io/2020/02/22/python-openpyxl/">openpyxl</a> and <a href="https://yq010105.github.io/2020/02/22/python-plotly/">plotly</a> or <a href="https://yq010105.github.io/2020/02/21/python-plt/">plt</a></strong></p><h1 id="3-慢慢学-opencv"><a href="#3-慢慢学-opencv" class="headerlink" title="3. 慢慢学 opencv"></a>3. 慢慢学 opencv</h1><p><a href="https://www.cnblogs.com/silence-cho/p/10926248.html">先行教程</a></p><h2 id="3-1-在新窗口打开图片，保存图片，基操"><a href="#3-1-在新窗口打开图片，保存图片，基操" class="headerlink" title="3.1 在新窗口打开图片，保存图片，基操"></a>3.1 在新窗口打开图片，保存图片，基操</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&#x27;./img/&#x27;</span> + <span class="built_in">input</span>(<span class="string">&#x27;输入图像路径：--带后缀--&#x27;</span>)</span><br><span class="line"><span class="comment"># 读取图片</span></span><br><span class="line">h = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)</span><br><span class="line"><span class="comment"># 创建一个窗口</span></span><br><span class="line">cv2.namedWindow(<span class="string">&#x27;printwindow&#x27;</span>)</span><br><span class="line">cv2.namedWindow(<span class="string">&#x27;window&#x27;</span>,<span class="number">0</span>)    <span class="comment"># 0 自由改变窗口大小</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 改变图片颜色</span></span><br><span class="line">imgviewx = cv2.cvtColor(imgviewx,cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图片，（窗口名，读入的图像）</span></span><br><span class="line">cv2.imshow(<span class="string">&#x27;printwindow&#x27;</span>,h)</span><br><span class="line"><span class="comment"># 窗口等待任意键盘按键输入，0为一直等待</span></span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 保存图片</span></span><br><span class="line">cv2.imwrite(<span class="string">&#x27;./img/result.jpg&#x27;</span>,imgviewx)</span><br><span class="line"><span class="comment"># 销毁窗口</span></span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><h2 id="3-2-图像阈值化"><a href="#3-2-图像阈值化" class="headerlink" title="3.2 图像阈值化"></a>3.2 图像阈值化</h2><p><strong>参数说明：</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">cv2.threshold():</span><br><span class="line">参数：</span><br><span class="line">    img:图像对象，必须是灰度图</span><br><span class="line">    thresh:阈值</span><br><span class="line">    maxval：最大值</span><br><span class="line">    <span class="built_in">type</span>:</span><br><span class="line">        cv2.THRESH_BINARY:     小于阈值的像素置为<span class="number">0</span>，大于阈值的置为maxval</span><br><span class="line">        cv2.THRESH_BINARY_INV： 小于阈值的像素置为maxval，大于阈值的置为<span class="number">0</span></span><br><span class="line">        cv2.THRESH_TRUNC：      小于阈值的像素不变，大于阈值的置为thresh</span><br><span class="line">        cv2.THRESH_TOZERO       小于阈值的像素置<span class="number">0</span>，大于阈值的不变</span><br><span class="line">        cv2.THRESH_TOZERO_INV   小于阈值的不变，大于阈值的像素置<span class="number">0</span></span><br><span class="line">返回两个值</span><br><span class="line">    ret:阈值</span><br><span class="line">    img：阈值化处理后的图像</span><br><span class="line"></span><br><span class="line">cv2.adaptiveThreshold() 自适应阈值处理，图像不同部位采用不同的阈值进行处理</span><br><span class="line">参数：</span><br><span class="line">    img: 图像对象，<span class="number">8</span>-bit单通道图</span><br><span class="line">    maxValue:最大值</span><br><span class="line">    adaptiveMethod: 自适应方法</span><br><span class="line">        cv2.ADAPTIVE_THRESH_MEAN_C     ：阈值为周围像素的平均值</span><br><span class="line">        cv2.ADAPTIVE_THRESH_GAUSSIAN_C : 阈值为周围像素的高斯均值（按权重）</span><br><span class="line">    threshType:</span><br><span class="line">        cv2.THRESH_BINARY:     小于阈值的像素置为<span class="number">0</span>，大于阈值的置为maxValuel</span><br><span class="line">        cv2.THRESH_BINARY_INV:  小于阈值的像素置为maxValue，大于阈值的置为<span class="number">0</span></span><br><span class="line">    blocksize: 计算阈值时，自适应的窗口大小,必须为奇数 （如<span class="number">3</span>：表示附近<span class="number">3</span>个像素范围内的像素点，进行计算阈值）</span><br><span class="line">    C： 常数值，通过自适应方法计算的值，减去该常数值</span><br><span class="line">(mean value of the blocksize*blocksize neighborhood of (x, y) minus C)</span><br></pre></td></tr></table></figure><p><strong>例子</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">imgpath = <span class="string">&#x27;./img/&#x27;</span> + <span class="built_in">input</span>(<span class="string">&#x27;输入图像路径&#x27;</span>)</span><br><span class="line">imgviewx = cv2.imread(imgpath)</span><br><span class="line"><span class="comment"># 将图像转化为灰度</span></span><br><span class="line">imgviewx = cv2.cvtColor(imgviewx,cv2.COLOR_BGR2GRAY)</span><br><span class="line"><span class="comment"># 边界设置</span></span><br><span class="line">imgresult = cv2.copyMakeBorder(imgviewx,<span class="number">20</span>,<span class="number">20</span>,<span class="number">20</span>,<span class="number">20</span>,cv2.BORDER_DEFAULT)</span><br><span class="line"></span><br><span class="line">ret,threl = cv2.threshold(imgviewx,<span class="number">127</span>,<span class="number">255</span>,cv2.THRESH_BINARY)</span><br><span class="line"></span><br><span class="line">cv2.namedWindow(<span class="string">&#x27;window2&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">cv2.imshow(<span class="string">&#x27;window2&#x27;</span>,threl)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyALLWindows()</span><br></pre></td></tr></table></figure><h2 id="3-3-图像形状变化"><a href="#3-3-图像形状变化" class="headerlink" title="3.3 图像形状变化"></a>3.3 图像形状变化</h2><h3 id="3-3-1-cv2-resize-图像缩放"><a href="#3-3-1-cv2-resize-图像缩放" class="headerlink" title="3.3.1 cv2.resize() 图像缩放"></a>3.3.1 cv2.resize() 图像缩放</h3><p>参数</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cv2.resize() 放大和缩小图像</span><br><span class="line">    参数：</span><br><span class="line">        src: 输入图像对象</span><br><span class="line">        dsize：输出矩阵/图像的大小，为<span class="number">0</span>时计算方式如下：dsize = Size(<span class="built_in">round</span>(fx*src.cols),<span class="built_in">round</span>(fy*src.rows))</span><br><span class="line">        fx: 水平轴的缩放因子，为<span class="number">0</span>时计算方式：  (double)dsize.width/src.cols</span><br><span class="line">        fy: 垂直轴的缩放因子，为<span class="number">0</span>时计算方式：  (double)dsize.heigh/src.rows</span><br><span class="line">        interpolation：插值算法</span><br><span class="line">            cv2.INTER_NEAREST : 最近邻插值法</span><br><span class="line">            cv2.INTER_LINEAR   默认值，双线性插值法</span><br><span class="line">            cv2.INTER_AREA        基于局部像素的重采样（resampling using pixel area relation）。对于图像抽取（image decimation）来说，这可能是一个更好的方法。但如果是放大图像时，它和最近邻法的效果类似。</span><br><span class="line">            cv2.INTER_CUBIC        基于4x4像素邻域的<span class="number">3</span>次插值法</span><br><span class="line">            cv2.INTER_LANCZOS4     基于8x8像素邻域的Lanczos插值</span><br><span class="line"></span><br><span class="line">    cv2.INTER_AREA 适合于图像缩小， cv2.INTER_CUBIC (slow) &amp; cv2.INTER_LINEAR 适合于图像放大</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 缩小图像为原来的一半</span></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;messi5.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">res = cv2.resize(img,<span class="literal">None</span>,fx=<span class="number">2</span>, fy=<span class="number">2</span>, interpolation = cv2.INTER_CUBIC)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"></span><br><span class="line">height, width = img.shape[:<span class="number">2</span>]</span><br><span class="line">res = cv2.resize(img,(<span class="number">2</span>*width, <span class="number">2</span>*height), interpolation = cv2.INTER_CUBIC)</span><br></pre></td></tr></table></figure><h3 id="3-3-2-仿射变换"><a href="#3-3-2-仿射变换" class="headerlink" title="3.3.2 仿射变换"></a>3.3.2 仿射变换</h3><p>仿射变换（从二维坐标到二维坐标之间的线性变换，且保持二维图形的“平直性”和“平行性”。仿射变换可以通过一系列的原子变换的复合来实现，包括平移，缩放，翻转，旋转和剪切）<br><strong>参数</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cv2.warpAffine()   仿射变换（从二维坐标到二维坐标之间的线性变换，且保持二维图形的“平直性”和“平行性”。仿射变换可以通过一系列的原子变换的复合来实现，包括平移，缩放，翻转，旋转和剪切）</span><br><span class="line">    参数：</span><br><span class="line">        img: 图像对象</span><br><span class="line">        M：<span class="number">2</span>*<span class="number">3</span> transformation matrix (转变矩阵)</span><br><span class="line">        dsize：输出矩阵的大小,注意格式为（cols，rows）  即width对应cols，height对应rows</span><br><span class="line">        flags：可选，插值算法标识符，有默认值INTER_LINEAR，</span><br><span class="line">               如果插值算法为WARP_INVERSE_MAP, warpAffine函数使用如下矩阵进行图像转dst(x,y)=src(M11*x+M12*y+M13,M21*x+M22*y+M23)</span><br><span class="line">        borderMode：可选， 边界像素模式，有默认值BORDER_CONSTANT</span><br><span class="line">        borderValue:可选，边界取值，有默认值Scalar()即<span class="number">0</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用Python写的一些获取b站信息的小程序</title>
      <link href="/Python%20Practise/Make-Python_bilibili/"/>
      <url>/Python%20Practise/Make-Python_bilibili/</url>
      
        <content type="html"><![CDATA[<p><strong>bilibili 提供的 api 接口(一串 json 字符)</strong><br>_让基于 bilibili 的开发更简单_<br><strong>我基于 python 写的几个使用 api 获取信息的例子</strong></p><span id="more"></span><h1 id="1-bilibili-用户基本信息-name，level，关注，粉丝-获取"><a href="#1-bilibili-用户基本信息-name，level，关注，粉丝-获取" class="headerlink" title="1. bilibili 用户基本信息(name，level，关注，粉丝)获取"></a>1. bilibili 用户基本信息(name，level，关注，粉丝)获取</h1><p><code>https://api.bilibili.com/x/space/upstat?mid=UUID&amp;jsonp=jsonp</code>_up 信息，名字，等级，视频总播放量，文章总浏览数_<br><code>https://api.bilibili.com/x/relation/stat?vmid=UUID&amp;jsonp=jsonp</code>_up 信息，关注数，黑名单，粉丝数_</p><p><strong>简单的代码获取 up 信息</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">mid = <span class="built_in">input</span>(<span class="string">&#x27;输入要查询的up的uid：&#x27;</span>)</span><br><span class="line">url_space = <span class="string">&#x27;https://api.bilibili.com/x/space/acc/info?mid=&#x27;</span> + mid</span><br><span class="line">url_relation = <span class="string">&#x27;https://api.bilibili.com/x/relation/stat?vmid=&#x27;</span>+mid</span><br><span class="line">space = requests.get(url_space).content.decode()</span><br><span class="line">relation =requests.get(url_relation).content.decode()</span><br><span class="line"><span class="comment"># print(type(html))</span></span><br><span class="line">dict_space = json.loads(space)</span><br><span class="line">dict_rela = json.loads(relation)</span><br><span class="line"><span class="comment"># print(dict)</span></span><br><span class="line">up_name = dict_space[<span class="string">&quot;data&quot;</span>][<span class="string">&quot;name&quot;</span>]</span><br><span class="line">up_level = dict_space[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;level&#x27;</span>]</span><br><span class="line"></span><br><span class="line">up_following_num = dict_rela[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;following&#x27;</span>]</span><br><span class="line">up_follower_num = dict_rela[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;follower&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;up名字是:<span class="subst">&#123;up_name&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;up等级达到:<span class="subst">&#123;up_level&#125;</span>级&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">int</span>(up_level)&gt;=<span class="number">5</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;----哇是个大佬！！！----&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;up关注了<span class="subst">&#123;up_following_num&#125;</span>个人&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">int</span>(up_following_num)&gt;=<span class="number">700</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;----铁定是个dd！！！----&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;up有<span class="subst">&#123;up_follower_num&#125;</span>个粉丝&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>示例：</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">输入要查询的up的uid：<span class="number">2</span></span><br><span class="line">up名字是:碧诗</span><br><span class="line">up等级达到:<span class="number">6</span>级</span><br><span class="line">----哇是个大佬！！！----</span><br><span class="line">up关注了<span class="number">191</span>个人</span><br><span class="line">up有<span class="number">804598</span>个粉丝</span><br></pre></td></tr></table></figure><h1 id="2-bilibili-统计某视频评论区，并生成词云"><a href="#2-bilibili-统计某视频评论区，并生成词云" class="headerlink" title="2. bilibili 统计某视频评论区，并生成词云"></a>2. bilibili 统计某视频评论区，并生成词云</h1><ul><li><strong>获取某视频评论区评论</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> multiprocessing.dummy <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">av = <span class="built_in">input</span>(<span class="string">&#x27;请输入视频的av号:&#x27;</span>)</span><br><span class="line">p_total = <span class="built_in">input</span>(<span class="string">&#x27;请输入评论要几页:&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_urls</span>():</span><br><span class="line">    urls = []</span><br><span class="line">    p = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> p &lt;= <span class="built_in">int</span>(p_total):</span><br><span class="line">        url = <span class="string">&#x27;http://api.bilibili.com/x/v2/reply?jsonp=jsonp&amp;;pn=&#x27;</span> + <span class="built_in">str</span>(p) + <span class="string">&#x27;&amp;type=1&amp;oid=&#x27;</span> + av</span><br><span class="line">        urls.append(url)</span><br><span class="line">        p += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> urls</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_name_con</span>(<span class="params">url</span>):</span><br><span class="line">    html = requests.get(url).content.decode()</span><br><span class="line">    yh_names = re.findall(<span class="string">r&#x27;&quot;uname&quot;:&quot;(.*?)&quot;,&quot;sex&quot;:&#x27;</span>,html,re.S)</span><br><span class="line">    yh_contents = re.findall(<span class="string">r&#x27;&quot;message&quot;:&quot;(.*?)&quot;,&quot;plat&quot;&#x27;</span>,html,re.S)</span><br><span class="line">    <span class="keyword">del</span> yh_contents[<span class="number">0</span>]</span><br><span class="line">    yh_contents2 = []</span><br><span class="line">    <span class="keyword">for</span> yh_content <span class="keyword">in</span> yh_contents:</span><br><span class="line">        yh_contents2.append(yh_content.replace(<span class="string">&#x27;\\n&#x27;</span>,<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    <span class="comment"># print(yh_contents2)</span></span><br><span class="line">    <span class="comment"># exit()</span></span><br><span class="line">    <span class="keyword">return</span> yh_names,yh_contents2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_names_cons</span>():</span><br><span class="line">    pool = Pool(<span class="number">5</span>)</span><br><span class="line">    urls = get_urls()</span><br><span class="line">    namecons = pool.<span class="built_in">map</span>(get_name_con,urls)</span><br><span class="line">    names = []</span><br><span class="line">    cons = []</span><br><span class="line">    <span class="keyword">for</span> namecon <span class="keyword">in</span> namecons:</span><br><span class="line">        name = namecon[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> name :</span><br><span class="line">            names.append(n)</span><br><span class="line">        con = namecon[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> con:</span><br><span class="line">            cons.append(c)</span><br><span class="line">    <span class="keyword">return</span> names,cons</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save</span>():</span><br><span class="line">    tumple = get_names_cons()</span><br><span class="line">    namelst = tumple[<span class="number">0</span>]</span><br><span class="line">    conlst = tumple[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># print(len(conlst))</span></span><br><span class="line">    <span class="comment"># # print(type(namelst))</span></span><br><span class="line">    <span class="comment"># print(len(namelst))</span></span><br><span class="line">    <span class="comment"># exit()</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(namelst) != <span class="built_in">len</span>(conlst):</span><br><span class="line">        tot = <span class="built_in">len</span>(conlst)</span><br><span class="line">    g = <span class="number">0</span></span><br><span class="line">    main_path = <span class="string">&#x27;E:\\learn\\py\\git\\spider\\spider_learn\\bilibili\\bilibili_api\\txt&#x27;</span> <span class="comment">#修改路径-自定义</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(main_path):</span><br><span class="line">        os.makedirs(main_path)</span><br><span class="line"></span><br><span class="line">    dir1 = <span class="string">&#x27;E:\\learn\\py\\git\\spider\\spider_learn\\bilibili\\bilibili_api\\txt\\&#x27;</span> + <span class="string">&#x27;comment&#x27;</span>  + <span class="string">&#x27;.txt&#x27;</span>  <span class="comment"># 自定义文件名</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(dir1,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fb:</span><br><span class="line">        <span class="keyword">for</span> g <span class="keyword">in</span> <span class="built_in">range</span>(tot):</span><br><span class="line">            <span class="comment"># fb.write(namelst[g])</span></span><br><span class="line">            <span class="comment"># fb.write(&#x27;\t\t\t&#x27;)</span></span><br><span class="line">            fb.write(conlst[g])</span><br><span class="line">            <span class="comment"># fb.write(&#x27;\n&#x27;)</span></span><br><span class="line">            g += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    save()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;----已完成----&#x27;</span>,end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;此视频已获得 <span class="subst">&#123;p_total&#125;</span> 页的评论&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li><strong>将生成的评论 txt 文件统计为词云</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line"><span class="keyword">import</span> PIL .Image <span class="keyword">as</span> image</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">trans_cn</span>(<span class="params">text</span>):</span><br><span class="line">    word_list = jieba.cut(text)</span><br><span class="line">    result = <span class="string">&#x27; &#x27;</span>.join(word_list)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">wc</span>():</span><br><span class="line">    dir1 = <span class="string">&#x27;./txt/comment.txt&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(dir1,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        text = f.read()</span><br><span class="line">        text = trans_cn(text)</span><br><span class="line">        WordCloud2 = WordCloud(</span><br><span class="line">            font_path = <span class="string">&#x27;C:\\windows\\Fonts\\simfang.ttf&#x27;</span></span><br><span class="line">        ).generate(text)</span><br><span class="line">        image_produce = WordCloud2.to_image()</span><br><span class="line">        image_produce.show()</span><br><span class="line">        WordCloud2.to_file(<span class="string">&#x27;./txt/comment.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line">wc()</span><br></pre></td></tr></table></figure><h1 id="3-获取-bilibili-主页各个分区的视频封面和-av-号"><a href="#3-获取-bilibili-主页各个分区的视频封面和-av-号" class="headerlink" title="3. 获取 bilibili 主页各个分区的视频封面和 av 号"></a>3. 获取 bilibili 主页各个分区的视频封面和 av 号</h1><p><code>https://www.bilibili.com/index/ding.json</code>_首页 api，每刷新一次，信息就会改变一次_<br>_获取的视频信息也就不同，所以可以一直获取信息(理论上来说)_<br>_缺点是每次只能获取十张图片信息_<br>_用的是 wb 写入文件，所以即使文件有一样的也会被覆盖…_</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-douga-teleplay-kichiku-dance-bangumi-fashion-life-ad-guochuang-movie-music-technology-game-ent--&#x27;</span>)</span><br><span class="line">fenqu = <span class="built_in">input</span>(<span class="string">&#x27;请输入爬取分区:&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> fenqu == <span class="string">&#x27;&#x27;</span>:</span><br><span class="line">    fenqu1 = <span class="string">&#x27;shuma&#x27;</span></span><br><span class="line"><span class="keyword">else</span> :</span><br><span class="line">    fenqu1 = fenqu</span><br><span class="line"></span><br><span class="line">html = requests.get(</span><br><span class="line">    <span class="string">&#x27;https://www.bilibili.com/index/ding.json&#x27;</span>).content.decode()</span><br><span class="line"></span><br><span class="line">dict_html = json.loads(html)</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line">aids = []</span><br><span class="line">pics = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    aid = dict_html[fenqu][<span class="built_in">str</span>(i)][<span class="string">&#x27;aid&#x27;</span>]</span><br><span class="line">    pic = dict_html[fenqu][<span class="built_in">str</span>(i)][<span class="string">&#x27;pic&#x27;</span>]</span><br><span class="line">    aids.append(aid)</span><br><span class="line">    pics.append(pic)</span><br><span class="line"></span><br><span class="line">j = <span class="number">1</span></span><br><span class="line">h = j-<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    main_path = <span class="string">&#x27;E:\\learn\\py\\git\\spider\\spider_learn\\bilibili\\bilibili_api\\pic\\&#x27;</span>+fenqu1</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(main_path):</span><br><span class="line">        os.makedirs(main_path)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        piccc = requests.get(pics[h])</span><br><span class="line">    <span class="keyword">except</span> requests.exceptions.ConnectionError:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;图片无法下载&#x27;</span>)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">except</span> requests.exceptions.ReadTimeout:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;requests.exceptions.ReadTimeout&#x27;</span>)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="built_in">dir</span> = <span class="string">&#x27;E:\\learn\\py\\git\\spider\\spider_learn\\bilibili\\bilibili_api\\pic\\&#x27;</span> + \</span><br><span class="line">         fenqu1 + <span class="string">&#x27;\\&#x27;</span>  +<span class="string">&#x27;av&#x27;</span> + <span class="built_in">str</span>(aids[h]) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="built_in">dir</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;正在爬取第<span class="subst">&#123;j&#125;</span>张图&#x27;</span>)</span><br><span class="line">        f.write(piccc.content)</span><br><span class="line">    j += <span class="number">1</span></span><br><span class="line">    h += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;----完成图片爬取----&#x27;</span>)</span><br></pre></td></tr></table></figure><p>_略微修改后_<br>_可能就是因为有重复的，会覆盖前面已下载的_<br>_爬个 5 次本该有 50 张，但才有 20 几张(dance 区)_<br>_可能 dance 区首页视频比较少吧，游戏区很多_<br><strong>不管了反正这个爬虫也没什么用 hhh</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_pic</span>():</span><br><span class="line">    <span class="keyword">if</span> fenqu == <span class="string">&#x27;&#x27;</span>:</span><br><span class="line">        fenqu1 = <span class="string">&#x27;shuma&#x27;</span></span><br><span class="line">    <span class="keyword">else</span> :</span><br><span class="line">        fenqu1 = fenqu</span><br><span class="line"></span><br><span class="line">    html = requests.get(</span><br><span class="line">        <span class="string">&#x27;https://www.bilibili.com/index/ding.json&#x27;</span>).content.decode()</span><br><span class="line"></span><br><span class="line">    dict_html = json.loads(html)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    aids = []</span><br><span class="line">    pics = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        aid = dict_html[fenqu][<span class="built_in">str</span>(i)][<span class="string">&#x27;aid&#x27;</span>]</span><br><span class="line">        pic = dict_html[fenqu][<span class="built_in">str</span>(i)][<span class="string">&#x27;pic&#x27;</span>]</span><br><span class="line">        aids.append(aid)</span><br><span class="line">        pics.append(pic)</span><br><span class="line"></span><br><span class="line">    j = <span class="number">1</span></span><br><span class="line">    h = j-<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        main_path = <span class="string">&#x27;E:\\learn\\py\\git\\spider\\spider_learn\\bilibili\\bilibili_api\\pic\\&#x27;</span>+fenqu1</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(main_path):</span><br><span class="line">            os.makedirs(main_path)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            piccc = requests.get(pics[h])</span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.ConnectionError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;图片无法下载&#x27;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.ReadTimeout:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;requests.exceptions.ReadTimeout&#x27;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="built_in">dir</span> = <span class="string">&#x27;E:\\learn\\py\\git\\spider\\spider_learn\\bilibili\\bilibili_api\\pic\\&#x27;</span> + \</span><br><span class="line">            fenqu1 + <span class="string">&#x27;\\&#x27;</span>  +<span class="string">&#x27;av&#x27;</span> + <span class="built_in">str</span>(aids[h]) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="built_in">dir</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;正在爬取第<span class="subst">&#123;j&#125;</span>张图&#x27;</span>)</span><br><span class="line">            f.write(piccc.content)</span><br><span class="line">        j += <span class="number">1</span></span><br><span class="line">        h += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">to = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入你要爬多少次---一次最多十张：&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-douga-teleplay-kichiku-dance-bangumi-fashion-life-ad-guochuang-movie-music-technology-game-ent--&#x27;</span>)</span><br><span class="line">fenqu = <span class="built_in">input</span>(<span class="string">&#x27;请输入爬取分区:&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(to):</span><br><span class="line">    get_pic()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;----完成第<span class="subst">&#123;i&#125;</span>次图片爬取----&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p><a href="https://github.com/yq010105/spider_learn/tree/master/bilibili/bilibili_api">Github 源码链接</a></p></blockquote><h1 id="4-主站上的实时人数"><a href="#4-主站上的实时人数" class="headerlink" title="4. 主站上的实时人数"></a>4. 主站上的实时人数</h1><p>_所用 api 接口_<code>https://api.bilibili.com/x/web-interface/online?&amp;;jsonp=jsonp</code></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_num</span>():</span><br><span class="line">    index = requests.get(</span><br><span class="line">    <span class="string">&#x27;https://api.bilibili.com/x/web-interface/online?&amp;;jsonp=jsonp&#x27;</span>).content.decode()</span><br><span class="line">    dict_index = json.loads(index)</span><br><span class="line">    all_count = dict_index[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;all_count&#x27;</span>]</span><br><span class="line">    web_online = dict_index[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;web_online&#x27;</span>]</span><br><span class="line">    play_online = dict_index[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;play_online&#x27;</span>]</span><br><span class="line"><span class="comment"># 应该是人数和实时在线人数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;all_count:<span class="subst">&#123;all_count&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;web_online:<span class="subst">&#123;web_online&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;play_online:<span class="subst">&#123;play_online&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;第<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>次计数&#x27;</span>)</span><br><span class="line">    print_num()</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h1 id="5-用户的粉丝数"><a href="#5-用户的粉丝数" class="headerlink" title="5. 用户的粉丝数"></a>5. 用户的粉丝数</h1><p>_只能获取一页，b 站最多是五页，多了就会有限制_</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">uid = <span class="built_in">input</span>(<span class="string">&#x27;请输入查找的up主的uid:&#x27;</span>)</span><br><span class="line">url = <span class="string">&#x27;https://api.bilibili.com/x/relation/followers?vmid=&#x27;</span> + \</span><br><span class="line">    uid + <span class="string">&#x27;&amp;ps=0&amp;order=desc&amp;jsonp=jsonp&#x27;</span></span><br><span class="line"></span><br><span class="line">html = requests.get(url).content.decode()</span><br><span class="line">dic_html = json.loads(html)</span><br><span class="line"></span><br><span class="line">index_order = dic_html[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;list&#x27;</span>]</span><br><span class="line">mids, mtimes, unames, signs = [], [], [], []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> index_order:</span><br><span class="line">    mid = i[<span class="string">&#x27;mid&#x27;</span>]</span><br><span class="line">    mids.append(mid)</span><br><span class="line">    mtime = i[<span class="string">&#x27;mtime&#x27;</span>]</span><br><span class="line">    mmtime = time.asctime(time.localtime(mtime))</span><br><span class="line">    mtimes.append(mmtime)</span><br><span class="line">    uname = i[<span class="string">&#x27;uname&#x27;</span>]</span><br><span class="line">    unames.append(uname)</span><br><span class="line">    sign = i[<span class="string">&#x27;sign&#x27;</span>]</span><br><span class="line">    signs.append(sign)</span><br><span class="line"><span class="comment"># print(index_order)</span></span><br><span class="line"><span class="comment"># print(mids)</span></span><br><span class="line">headers = [<span class="string">&#x27;uid&#x27;</span>, <span class="string">&#x27;注册时间&#x27;</span>, <span class="string">&#x27;up姓名&#x27;</span>, <span class="string">&#x27;个性签名&#x27;</span>]</span><br><span class="line">rows = []</span><br><span class="line">j = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(mids)):</span><br><span class="line">    rows.append([mids[j], mtimes[j], unames[j], signs[j]])</span><br><span class="line"></span><br><span class="line">main_path = <span class="string">&#x27;E:\\learn\\py\\git\\spider\\spider_learn\\bilibili\\bilibili_api\\csv&#x27;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(main_path):</span><br><span class="line">    os.makedirs(main_path)</span><br><span class="line"></span><br><span class="line"><span class="built_in">dir</span> = <span class="string">&#x27;E:\\learn\\py\\git\\spider\\spider_learn\\bilibili\\bilibili_api\\csv\\&#x27;</span> + \</span><br><span class="line">    <span class="string">&#x27;follers&#x27;</span> + <span class="string">&#x27;.csv&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="built_in">dir</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    fb = csv.writer(f)</span><br><span class="line">    fb.writerow(headers)</span><br><span class="line">    fb.writerows(rows)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;----最多只显示一页的粉丝数，也就是50个----&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;共有<span class="subst">&#123;<span class="built_in">len</span>(mids)&#125;</span>个粉丝&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python Practise </tag>
            
            <tag> Bilibili </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自己写的几个爬虫</title>
      <link href="/Python%20Practise/Make-Python_%E7%88%AC%E8%99%AB/"/>
      <url>/Python%20Practise/Make-Python_%E7%88%AC%E8%99%AB/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>自己动手做的 python 爬虫</strong>&emsp;&emsp;&emsp;<a href="https://github.com/yq010105/spider_learn" title="github">GitHub 链接</a><br>WARNING :逻辑混乱，语法不顺！！！</p></blockquote><span id="more"></span><h1 id="1-爬取-bilibili-每日排行榜数据"><a href="#1-爬取-bilibili-每日排行榜数据" class="headerlink" title="1. 爬取 bilibili 每日排行榜数据"></a>1. 爬取 bilibili 每日排行榜数据</h1><ul><li><strong>使用 XPath 爬取,并将数据保存到 csv 文件中</strong></li><li><strong>文件名使用该排行榜所在时间段</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> lxml.html</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.bilibili.com/ranking/&#x27;</span></span><br><span class="line">html = requests.get(url).content.decode()</span><br><span class="line"><span class="comment"># print(html)</span></span><br><span class="line"></span><br><span class="line">selector = lxml.html.fromstring(html)</span><br><span class="line"></span><br><span class="line">title = selector.xpath(<span class="string">&#x27;//*[@id=&quot;app&quot;]/div[1]/div/div[1]/div[2]/div[3]/ul/li/div[2]/div[2]/a/text()&#x27;</span>)</span><br><span class="line"><span class="comment"># print(len(title))</span></span><br><span class="line"></span><br><span class="line">link = selector.xpath(<span class="string">&#x27;//*[@id=&quot;app&quot;]/div[1]/div/div[1]/div[2]/div[3]/ul/li/div[2]/div[1]/a/@href&#x27;</span>)</span><br><span class="line"><span class="comment"># print(link[0])</span></span><br><span class="line"><span class="comment"># cover = selector.xpath(&#x27;//*[@id=&quot;app&quot;]/div[1]/div/div[1]/div[2]/div[3]/ul/li/div[2]/div[1]/a/div/img/@src&#x27;)</span></span><br><span class="line"><span class="comment"># print(cover[0])</span></span><br><span class="line"></span><br><span class="line">up_name = selector.xpath(<span class="string">&#x27;//*[@id=&quot;app&quot;]/div[1]/div/div[1]/div[2]/div[3]/ul/li/div[2]/div[2]/div[1]/a/span/text()&#x27;</span>)</span><br><span class="line"><span class="comment"># print(up_name[5])</span></span><br><span class="line">up_videoplay = selector.xpath(<span class="string">&#x27;//*[@id=&quot;app&quot;]/div[1]/div/div[1]/div[2]/div[3]/ul/li/div[2]/div[2]/div[1]/span[1]/text()&#x27;</span>)</span><br><span class="line"></span><br><span class="line">time = selector.xpath(<span class="string">&#x27;//*[@id=&quot;app&quot;]/div[1]/div/div[1]/div[2]/div[2]/div/span/text()&#x27;</span>)</span><br><span class="line">time_num = time[<span class="number">0</span>]</span><br><span class="line">str1 = time_num.replace(<span class="string">&#x27; 的数据综合得分，每日更新一次&#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">str2 = str1.replace(<span class="string">&#x27;统计所有投稿在 &#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">time_num2 = str2</span><br><span class="line"></span><br><span class="line">headers = [<span class="string">&#x27;up_name&#x27;</span>,<span class="string">&#x27;title&#x27;</span>,<span class="string">&#x27;link&#x27;</span>]</span><br><span class="line">rows = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    rows.append([up_name[i],title[i],link[i]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&#x27;<span class="subst">&#123;time_num2&#125;</span>.csv&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.writer(f)</span><br><span class="line">    f_csv.writerow(headers)</span><br><span class="line">    f_csv.writerows(rows)</span><br></pre></td></tr></table></figure><ul><li><strong>csv 部分展示</strong><br><code>2020年02月07日 - 2020年02月10日</code></li></ul><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/master/img/bilibili_csv.png" alt="bilibili_csv"></p><h1 id="2-爬取-baidu-上搜到的图片-初级"><a href="#2-爬取-baidu-上搜到的图片-初级" class="headerlink" title="2. 爬取 baidu 上搜到的图片(初级)"></a>2. 爬取 baidu 上搜到的图片(初级)</h1><h2 id="2-1-thumbURL"><a href="#2-1-thumbURL" class="headerlink" title="2.1 thumbURL"></a>2.1 thumbURL</h2><ul><li>_分辨率极低_</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download</span>(<span class="params">html</span>):</span><br><span class="line">    <span class="comment">#通过正则匹配</span></span><br><span class="line">    pic_url = re.findall(<span class="string">&#x27;&quot;thumbURL&quot;:&quot;(.*?)&quot;,&#x27;</span>,html, re.S)</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> pic_url:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;开始下载图片：&quot;</span>+key +<span class="string">&quot;\r\n&quot;</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            pic = requests.get(key, timeout=<span class="number">10</span>)</span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.ConnectionError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;图片无法下载&#x27;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment">#保存图片路径</span></span><br><span class="line">        main_path=<span class="string">&quot;E:/baidu/&quot;</span> <span class="comment">#文件保存路径，如果不存在就会被重建</span></span><br><span class="line">        <span class="keyword">if</span>  <span class="keyword">not</span> os.path.exists(main_path):<span class="comment">#如果路径不存在</span></span><br><span class="line">            os.makedirs(main_path)</span><br><span class="line">        <span class="built_in">dir</span> = <span class="string">&quot;E:/baidu/&quot;</span> + <span class="built_in">str</span>(i) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">        fp = <span class="built_in">open</span>(<span class="built_in">dir</span>, <span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line">        fp.write(pic.content)</span><br><span class="line">        fp.close()</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">        url = <span class="string">&#x27;https://image.baidu.com/search/index?tn=baiduimage&amp;ipn=r&amp;ct=201326592&amp;cl=2&amp;lm=-1&amp;st=-1&amp;sf=1&amp;fmq=&amp;pv=&amp;ic=0&amp;nc=1&amp;z=&amp;se=1&amp;showtab=0&amp;fb=0&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;ie=utf-8&amp;fm=result&amp;pos=history&amp;word=siyueshinide&#x27;</span></span><br><span class="line">        result = requests.get(url)</span><br><span class="line">        download(result.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">        main()</span><br></pre></td></tr></table></figure><h2 id="2-2-objURL"><a href="#2-2-objURL" class="headerlink" title="2.2 objURL"></a>2.2 objURL</h2><p>_分辨率较高，但有的图爬不了_</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download</span>(<span class="params">html</span>):</span><br><span class="line">    <span class="comment">#通过正则匹配</span></span><br><span class="line">    pic_url = re.findall(<span class="string">&#x27;&quot;objURL&quot;:&quot;(.*?)&quot;,&#x27;</span>,html, re.S)</span><br><span class="line">    <span class="comment"># for pic_url_li in pic_url:</span></span><br><span class="line">        <span class="comment"># pic_url_js = &#x27;&#123;&#x27;+&#x27;&quot;link&quot;&#x27;+&#x27;:&#x27; +pic_url_li+&#x27;&#125;&#x27;</span></span><br><span class="line">        <span class="comment"># pic_url_py = json.loads(pic_url_li)</span></span><br><span class="line">    <span class="comment"># print(pic_url)</span></span><br><span class="line">    <span class="comment"># exit()</span></span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> pic_url:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;开始下载图片：&quot;</span>+key +<span class="string">&quot;\r\n&quot;</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            pic = requests.get(key, timeout=<span class="number">10</span>)</span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.ConnectionError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;图片无法下载&#x27;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.ReadTimeout:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;requests.exceptions.ReadTimeout&#x27;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment">#保存图片路径</span></span><br><span class="line">        main_path=<span class="string">&quot;E:/baidu/&quot;</span> <span class="comment">#文件保存路径，如果不存在就会被重建</span></span><br><span class="line">        <span class="keyword">if</span>  <span class="keyword">not</span> os.path.exists(main_path):<span class="comment">#如果路径不存在</span></span><br><span class="line">            os.makedirs(main_path)</span><br><span class="line">        <span class="built_in">dir</span> = <span class="string">&quot;E:/baidu/&quot;</span> + <span class="built_in">str</span>(i) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">        fp = <span class="built_in">open</span>(<span class="built_in">dir</span>, <span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line">        fp.write(pic.content)</span><br><span class="line">        fp.close()</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">        url = <span class="string">&#x27;https://image.baidu.com/search/index?tn=baiduimage&amp;ipn=r&amp;ct=201326592&amp;cl=2&amp;lm=-1&amp;st=-1&amp;sf=1&amp;fmq=&amp;pv=&amp;ic=0&amp;nc=1&amp;z=&amp;se=1&amp;showtab=0&amp;fb=0&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;ie=utf-8&amp;fm=result&amp;pos=history&amp;word=siyueshinide&#x27;</span></span><br><span class="line">        result = requests.get(url)</span><br><span class="line">        download(result.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">        main()</span><br></pre></td></tr></table></figure><h2 id="2-3-baidu-面向对象"><a href="#2-3-baidu-面向对象" class="headerlink" title="2.3 baidu 面向对象"></a>2.3 baidu 面向对象</h2><ul><li>输入想爬取的关键词，自动爬取(只能下 30 张)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_id</span>(<span class="params">search_id</span>):</span><br><span class="line">    url = <span class="string">&#x27;http://image.baidu.com/search/index?tn=baiduimage&amp;ps=1&amp;ct=201326592&amp;lm=-1&amp;cl=2&amp;nc=1&amp;ie=utf-8&amp;word=&#x27;</span> + search_id</span><br><span class="line">    <span class="keyword">return</span> url</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_obj</span>():</span><br><span class="line">    url = get_id(search_id)</span><br><span class="line">    html = requests.get(url).content.decode()</span><br><span class="line">    obj_URL = re.findall(<span class="string">&#x27;&quot;objURL&quot;:&quot;(.*?)&quot;,&#x27;</span>,html,re.S)</span><br><span class="line">    <span class="keyword">return</span> obj_URL</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_pic</span>():</span><br><span class="line">    obj_url = get_obj()</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> objurl <span class="keyword">in</span> obj_url:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;开始下载图片&#x27;</span>+<span class="string">&#x27;\t&#x27;</span>+<span class="string">&#x27;第&#x27;</span>+<span class="built_in">str</span>(i)+<span class="string">&#x27;张&#x27;</span>)</span><br><span class="line">        <span class="keyword">try</span> :</span><br><span class="line">            pic = requests.get(objurl,timeout = <span class="number">10</span>)</span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.ConnectionError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;图片无法下载&#x27;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.ReadTimeout:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;requests.exceptions.ReadTimeout&#x27;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">global</span> search_id</span><br><span class="line">        main_path = <span class="string">r&#x27;E:\learn\py\git\spider\spider_learn\baidu\pic\\&#x27;</span> + search_id +<span class="string">&#x27;\\&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(main_path):</span><br><span class="line">            os.makedirs(main_path)</span><br><span class="line">        <span class="built_in">dir</span> = <span class="string">&quot;E:\learn\py\git\spider\spider_learn\\baidu\pic\\&quot;</span> +search_id +<span class="string">&#x27;\\&#x27;</span>+ search_id+ <span class="built_in">str</span>(i) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="built_in">dir</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(pic.content)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    search_id = <span class="built_in">input</span>(<span class="string">&#x27;请输入要下载的内容:&#x27;</span>)</span><br><span class="line">    save_pic()</span><br></pre></td></tr></table></figure><h2 id="2-4-baidu-more"><a href="#2-4-baidu-more" class="headerlink" title="2.4 baidu_more"></a>2.4 baidu_more</h2><ul><li><strong>进一步升级，可以爬任意数量图片</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> multiprocessing.dummy <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_urls</span>(<span class="params">search_id</span>):</span><br><span class="line">    total = (<span class="built_in">input</span>(<span class="string">&#x27;请输入要几页----30张一页----：&#x27;</span>))</span><br><span class="line">    url = <span class="string">&#x27;http://image.baidu.com/search/index?tn=baiduimage&amp;ps=1&amp;ct=201326592&amp;lm=-1&amp;cl=2&amp;nc=1&amp;ie=utf-8&amp;word=&#x27;</span> + search_id+ <span class="string">&#x27;&amp;pn=&#x27;</span></span><br><span class="line">    t = <span class="number">0</span></span><br><span class="line">    URLS = []</span><br><span class="line">    <span class="keyword">while</span> t &lt; <span class="built_in">int</span>(total)*<span class="number">30</span>:</span><br><span class="line">        URL = url + <span class="built_in">str</span>(t)</span><br><span class="line">        t = t + <span class="number">30</span></span><br><span class="line">        URLS.append(URL)</span><br><span class="line">    <span class="keyword">return</span> URLS</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_obj</span>(<span class="params">url</span>):</span><br><span class="line">    html = requests.get(url).content.decode()</span><br><span class="line">    obj_URL = re.findall(<span class="string">&#x27;&quot;objURL&quot;:&quot;(.*?)&quot;,&#x27;</span>,html,re.S)</span><br><span class="line">    <span class="keyword">return</span> obj_URL</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_pic</span>():</span><br><span class="line">    pool=Pool(<span class="number">5</span>)</span><br><span class="line">    objurls = pool.<span class="built_in">map</span>(get_obj,URLS)</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> objurl <span class="keyword">in</span> objurls:</span><br><span class="line">        <span class="keyword">for</span> obj <span class="keyword">in</span> objurl:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;开始下载图片&#x27;</span>+<span class="string">&#x27;\t&#x27;</span>+<span class="string">&#x27;第&#x27;</span>+<span class="built_in">str</span>(i)+<span class="string">&#x27;张&#x27;</span>)</span><br><span class="line">            <span class="keyword">try</span> :</span><br><span class="line">                pic = requests.get(obj,timeout = <span class="number">10</span>)</span><br><span class="line">            <span class="keyword">except</span> requests.exceptions.ConnectionError:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;图片无法下载&#x27;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">except</span> requests.exceptions.ReadTimeout:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;requests.exceptions.ReadTimeout&#x27;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">global</span> search_id</span><br><span class="line">            main_path = patha +<span class="string">&#x27;\\&#x27;</span> + search_id +<span class="string">&#x27;\\&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(main_path):</span><br><span class="line">                os.makedirs(main_path)</span><br><span class="line">            <span class="built_in">dir</span> = main_path + search_id+ <span class="built_in">str</span>(i) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(<span class="built_in">dir</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(pic.content)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    search_id = <span class="built_in">input</span>(<span class="string">&#x27;请输入要下载的内容:&#x27;</span>)</span><br><span class="line">    URLS = get_urls(search_id)</span><br><span class="line">    patha = <span class="built_in">input</span>(<span class="string">&#x27;输入文件保存路径----示例:E:\\baidu----:&#x27;</span>)</span><br><span class="line">    save_pic()</span><br></pre></td></tr></table></figure><h1 id="3-爬取-ins-上的图片-初级版"><a href="#3-爬取-ins-上的图片-初级版" class="headerlink" title="3. 爬取 ins 上的图片(初级版)"></a>3. 爬取 ins 上的图片(初级版)</h1><ul><li>_分辨率低_</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> lxml.html</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取src</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_src</span>():</span><br><span class="line">    url = <span class="string">&#x27;https://www.instagram.com/baaaakuuuu&#x27;</span></span><br><span class="line">    html = requests.get(url).content.decode()</span><br><span class="line">    selector = lxml.html.fromstring(html)</span><br><span class="line">    script = selector.xpath(<span class="string">&#x27;/html/body/script[1]/text()&#x27;</span>)[<span class="number">0</span>].strip()</span><br><span class="line">    <span class="comment"># print(script)</span></span><br><span class="line">    <span class="comment"># print(type(script))       #str</span></span><br><span class="line">    <span class="comment"># exit()</span></span><br><span class="line">    <span class="comment"># for script_in in script :</span></span><br><span class="line">        <span class="comment"># try:</span></span><br><span class="line">        <span class="comment">#     script_dic = json.loads(script_in)</span></span><br><span class="line">        <span class="comment"># print(script_dic)</span></span><br><span class="line">    src = re.findall(<span class="string">r&#x27;&quot;thumbnail_resources&quot;:\[(.*?)\]&#x27;</span>,script,re.S)</span><br><span class="line">    <span class="comment"># print(src[0]) #str</span></span><br><span class="line">    <span class="comment"># print(type(src[0]))</span></span><br><span class="line">    <span class="comment"># exit()</span></span><br><span class="line">    <span class="keyword">return</span> src</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取图片链接</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_picurl</span>():</span><br><span class="line">    src = get_src()</span><br><span class="line">    <span class="comment"># print(src)</span></span><br><span class="line">    <span class="comment"># exit()</span></span><br><span class="line">    pic_url_lst = []</span><br><span class="line">    <span class="keyword">for</span> src_ls <span class="keyword">in</span> src :         <span class="comment">#&quot;config_height&quot;:480&#125;,&#123; ... ,&quot;config_width&quot;:640,&quot;config_height&quot;:640&#125;</span></span><br><span class="line">        thumb = re.findall(<span class="string">r&#x27;&quot;config_height&quot;:480&#125;,&#123;(.*?),&quot;config_width&quot;:640,&quot;config_height&quot;:640&#125;&#x27;</span>,src_ls)[<span class="number">0</span>]</span><br><span class="line">        thumb_json = <span class="string">&#x27;&#123;&#x27;</span> + thumb + <span class="string">&#x27;&#125;&#x27;</span></span><br><span class="line">        <span class="comment"># print(thumb_json)</span></span><br><span class="line">        <span class="comment"># exit()</span></span><br><span class="line">        thumb_py = json.loads(thumb_json)</span><br><span class="line">        pic_url = thumb_py[<span class="string">&#x27;src&#x27;</span>]</span><br><span class="line">        <span class="comment"># print(pic_url)</span></span><br><span class="line">        <span class="comment"># exit()</span></span><br><span class="line">        pic_url_lst.append(pic_url)</span><br><span class="line">    <span class="comment"># print(pic_url_lst)</span></span><br><span class="line">    <span class="comment"># exit()</span></span><br><span class="line">    <span class="keyword">return</span> pic_url_lst</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图片链接保存</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_pic</span>():</span><br><span class="line">    pic_url_lst = get_picurl()</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="comment"># print(pic_url_lst)</span></span><br><span class="line">    <span class="comment"># exit()</span></span><br><span class="line">    <span class="keyword">for</span> pic_con <span class="keyword">in</span> pic_url_lst:</span><br><span class="line">        <span class="comment"># print(pic_con)</span></span><br><span class="line">        <span class="comment"># exit()</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            pic = requests.get(pic_con, timeout=<span class="number">10</span>)</span><br><span class="line">            main_path = <span class="string">&#x27;E:/ins/&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(main_path):</span><br><span class="line">                os.makedirs(main_path)</span><br><span class="line">            path = <span class="string">&#x27;E:/ins/&#x27;</span> + <span class="string">&#x27;baku&#x27;</span> + <span class="built_in">str</span>(i) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(path,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(pic.content)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;第<span class="subst">&#123;i&#125;</span>张已下载&#x27;</span>)</span><br><span class="line">            i +=<span class="number">1</span></span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.ConnectionError:        <span class="comment">#requests.exceptions.ConnectionError</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;图片无法下载&#x27;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">save_pic()</span><br></pre></td></tr></table></figure><h1 id="4-爬取-Wallhaven-上的图片"><a href="#4-爬取-Wallhaven-上的图片" class="headerlink" title="4. 爬取 Wallhaven 上的图片"></a>4. 爬取 Wallhaven 上的图片</h1><h2 id="4-1-龟速爬取-只是用来爬了一下博客需要的图片-hhh"><a href="#4-1-龟速爬取-只是用来爬了一下博客需要的图片-hhh" class="headerlink" title="4.1 龟速爬取,只是用来爬了一下博客需要的图片 hhh"></a>4.1 龟速爬取,只是用来爬了一下博客需要的图片 hhh</h2><p>_爬取速度慢，要等半天才能开始保存文件，应该是我代码结构的问题，以后再做优化_</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lxml.html</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.116 Safari/537.36&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_url</span>():</span><br><span class="line">    pages = <span class="built_in">input</span>(<span class="string">&#x27;输入页数：&#x27;</span>)</span><br><span class="line">    <span class="comment"># pages = &#x27;1&#x27;</span></span><br><span class="line">    url_pics = []</span><br><span class="line">    page = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> page &lt;= <span class="built_in">int</span>(pages):</span><br><span class="line">        url = <span class="string">&#x27;https://wallhaven.cc/search?categories=010&amp;purity=100&amp;resolutions=1280x800&amp;sorting=relevance&amp;order=desc&amp;page=&#x27;</span> + <span class="built_in">str</span>(page)</span><br><span class="line">        html = requests.get(url,headers = headers).content.decode()</span><br><span class="line">    <span class="comment"># print(html)</span></span><br><span class="line">    <span class="comment"># exit()</span></span><br><span class="line">        selector = lxml.html.fromstring(html)</span><br><span class="line"></span><br><span class="line">        url_pic = selector.xpath(<span class="string">&#x27;//*[@id=&quot;thumbs&quot;]/section/ul/li/figure/a/@href&#x27;</span>)</span><br><span class="line">        url_pics.append(url_pic)</span><br><span class="line">        page += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;得到了内层url&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> url_pics</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_pic</span>():</span><br><span class="line">    url_pics = get_url()</span><br><span class="line">    img_urls = []</span><br><span class="line">    <span class="keyword">for</span> urlst <span class="keyword">in</span> url_pics:</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urlst:</span><br><span class="line">            htmlp = requests.get(url,headers = headers).content.decode()</span><br><span class="line">            <span class="comment"># print(htmlp)</span></span><br><span class="line">            <span class="comment"># exit()</span></span><br><span class="line">            img_url = re.findall(<span class="string">r&#x27;&quot;wallpaper&quot; src=&quot;(.*?)&quot;&#x27;</span>,htmlp,re.S)[<span class="number">0</span>]</span><br><span class="line">            img_urls.append(img_url)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;得到图片的url&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> img_urls</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_img</span>(<span class="params">imgurl_list</span>):</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> imgurl_list:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;开始下载图片&#x27;</span>+<span class="string">&#x27;\t&#x27;</span>+<span class="string">&#x27;第&#x27;</span>+<span class="built_in">str</span>(i)+<span class="string">&#x27;张&#x27;</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            pic = requests.get(url, timeout=<span class="number">10</span>)</span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.ConnectionError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;图片无法下载&#x27;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">except</span> requests.exceptions.ReadTimeout:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;requests.exceptions.ReadTimeout&#x27;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        main_path = <span class="string">r&#x27;E:\\wallhaven\\&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(main_path):</span><br><span class="line">            os.makedirs(main_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="built_in">dir</span> = <span class="string">&#x27;E:\\wallhaven\\&#x27;</span> + <span class="built_in">str</span>(i) +<span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="built_in">dir</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(pic.content)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    imgurl_list = get_pic()</span><br><span class="line">    get_img(imgurl_list)</span><br></pre></td></tr></table></figure><h2 id="4-2-多线程爬取"><a href="#4-2-多线程爬取" class="headerlink" title="4.2 多线程爬取"></a>4.2 多线程爬取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">遇到了一个bug，等到bug解决再写上来</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python Practise </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python-Note</title>
      <link href="/Learn/Learn-Python/"/>
      <url>/Learn/Learn-Python/</url>
      
        <content type="html"><![CDATA[<p><strong>有一定的 python 自学基础，基础不扎实</strong><br><strong>python，边学爬虫，边把不会的 python 知识不全</strong></p><span id="more"></span><h1 id="1-文件管理（txt）"><a href="#1-文件管理（txt）" class="headerlink" title="1. 文件管理（txt）"></a>1. 文件管理（txt）</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fd = <span class="built_in">open</span>(<span class="string">&#x27;file.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)      <span class="comment">#utf-8 or GBK</span></span><br><span class="line">fd.write(content)</span><br><span class="line">fd.close()</span><br></pre></td></tr></table></figure><p>其中 content 可以是字符串，变量，\t ……</p><hr/><div class="table-container"><table><thead><tr><th style="text-align:center">r</th><th style="text-align:center">w</th><th style="text-align:center">a</th></tr></thead><tbody><tr><td style="text-align:center">只读</td><td style="text-align:center">覆盖写</td><td style="text-align:center">添加写</td></tr></tbody></table></div><h2 id="1-1-文件的readlines和readline"><a href="#1-1-文件的readlines和readline" class="headerlink" title="1.1 文件的readlines和readline"></a>1.1 文件的readlines和readline</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">f.readlines()<span class="comment"># 全部读取最后返回一个列表存所有的类,每行后面都会带有“\n”</span></span><br><span class="line">f.readline()<span class="comment"># 读取一列数据</span></span><br></pre></td></tr></table></figure><h1 id="2-csv-文件"><a href="#2-csv-文件" class="headerlink" title="2. csv 文件"></a>2. csv 文件</h1><h2 id="2-1-列表序列数据"><a href="#2-1-列表序列数据" class="headerlink" title="2.1 列表序列数据"></a>2.1 列表序列数据</h2><ul><li>headers :表头</li><li>rows :内容</li><li><code>f*csv = csv.writer(f)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;\_f 为 open(&#39;file.txt&#39;,&#39;w&#39;,encoding=&#39;utf-8&#39;)*</code></li><li>f_csv.writerow(headers)</li><li>f_csv.writerows(rows)</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">headers = [<span class="string">&#x27;class&#x27;</span>,<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;sex&#x27;</span>,<span class="string">&#x27;height&#x27;</span>,<span class="string">&#x27;year&#x27;</span>]</span><br><span class="line"></span><br><span class="line">rows = [</span><br><span class="line">        [<span class="number">1</span>,<span class="string">&#x27;xiaoming&#x27;</span>,<span class="string">&#x27;male&#x27;</span>,<span class="number">168</span>,<span class="number">23</span>],</span><br><span class="line">        [<span class="number">1</span>,<span class="string">&#x27;xiaohong&#x27;</span>,<span class="string">&#x27;female&#x27;</span>,<span class="number">162</span>,<span class="number">22</span>],</span><br><span class="line">        [<span class="number">2</span>,<span class="string">&#x27;xiaozhang&#x27;</span>,<span class="string">&#x27;female&#x27;</span>,<span class="number">163</span>,<span class="number">21</span>],</span><br><span class="line">        [<span class="number">2</span>,<span class="string">&#x27;xiaoli&#x27;</span>,<span class="string">&#x27;male&#x27;</span>,<span class="number">158</span>,<span class="number">21</span>]</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;test.csv&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,newline=<span class="string">&#x27;&#x27;</span>)<span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.writer(f)</span><br><span class="line">    f_csv.writerow(headers)</span><br><span class="line">    f_csv.writerows(rows)</span><br></pre></td></tr></table></figure><p>_注意：如果打开 csv 文件出现空行的情况，那么需要添加一个参数 newline=”_<br><code>with open(&#39;test.csv&#39;,&#39;w&#39;,newline=&#39;&#39;)as f:</code></p><div class="table-container"><table><thead><tr><th style="text-align:center">class</th><th style="text-align:center">name</th><th style="text-align:center">sex</th><th style="text-align:center">height</th><th style="text-align:center">year</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">xiaoming</td><td style="text-align:center">male</td><td style="text-align:center">168</td><td style="text-align:center">23</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">xiaohong</td><td style="text-align:center">female</td><td style="text-align:center">162</td><td style="text-align:center">22</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">xiaozhang</td><td style="text-align:center">female</td><td style="text-align:center">163</td><td style="text-align:center">21</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">xiaoli</td><td style="text-align:center">male</td><td style="text-align:center">158</td><td style="text-align:center">21</td></tr></tbody></table></div><h2 id="2-2-字典序列数据"><a href="#2-2-字典序列数据" class="headerlink" title="2.2 字典序列数据"></a>2.2 字典序列数据</h2><ul><li>headers :表头</li><li>rows :内容</li><li>f_csv = DictWriter(f,headers)</li><li>f_csv.writeheader()</li><li>f_csv.writerows(rows)</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">headers = [<span class="string">&#x27;class&#x27;</span>,<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;sex&#x27;</span>,<span class="string">&#x27;height&#x27;</span>,<span class="string">&#x27;year&#x27;</span>]</span><br><span class="line"></span><br><span class="line">rows = [</span><br><span class="line">        &#123;<span class="string">&#x27;class&#x27;</span>:<span class="number">1</span>,<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;xiaoming&#x27;</span>,<span class="string">&#x27;sex&#x27;</span>:<span class="string">&#x27;male&#x27;</span>,<span class="string">&#x27;height&#x27;</span>:<span class="number">168</span>,<span class="string">&#x27;year&#x27;</span>:<span class="number">23</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;class&#x27;</span>:<span class="number">1</span>,<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;xiaohong&#x27;</span>,<span class="string">&#x27;sex&#x27;</span>:<span class="string">&#x27;female&#x27;</span>,<span class="string">&#x27;height&#x27;</span>:<span class="number">162</span>,<span class="string">&#x27;year&#x27;</span>:<span class="number">22</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;class&#x27;</span>:<span class="number">2</span>,<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;xiaozhang&#x27;</span>,<span class="string">&#x27;sex&#x27;</span>:<span class="string">&#x27;female&#x27;</span>,<span class="string">&#x27;height&#x27;</span>:<span class="number">163</span>,<span class="string">&#x27;year&#x27;</span>:<span class="number">21</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;class&#x27;</span>:<span class="number">2</span>,<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;xiaoli&#x27;</span>,<span class="string">&#x27;sex&#x27;</span>:<span class="string">&#x27;male&#x27;</span>,<span class="string">&#x27;height&#x27;</span>:<span class="number">158</span>,<span class="string">&#x27;year&#x27;</span>:<span class="number">21</span>&#125;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;test2.csv&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,newline=<span class="string">&#x27;&#x27;</span>)<span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.DictWriter(f,headers)</span><br><span class="line">    f_csv.writeheader()</span><br><span class="line">    f_csv.writerows(rows)</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th style="text-align:center">class</th><th style="text-align:center">name</th><th style="text-align:center">sex</th><th style="text-align:center">height</th><th style="text-align:center">year</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">xiaoming</td><td style="text-align:center">male</td><td style="text-align:center">168</td><td style="text-align:center">23</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">xiaohong</td><td style="text-align:center">female</td><td style="text-align:center">162</td><td style="text-align:center">22</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">xiaozhang</td><td style="text-align:center">female</td><td style="text-align:center">163</td><td style="text-align:center">21</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">xiaoli</td><td style="text-align:center">male</td><td style="text-align:center">158</td><td style="text-align:center">21</td></tr></tbody></table></div><h2 id="2-3-csv-文件的读"><a href="#2-3-csv-文件的读" class="headerlink" title="2.3 csv 文件的读"></a>2.3 csv 文件的读</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;test.csv&#x27;</span>)<span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.reader(f)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> f_csv:</span><br><span class="line">        <span class="built_in">print</span>(row)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;result</span></span><br><span class="line"><span class="string">[&#x27;class&#x27;, &#x27;name&#x27;, &#x27;sex&#x27;, &#x27;height&#x27;, &#x27;year&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;1&#x27;, &#x27;xiaoming&#x27;, &#x27;male&#x27;, &#x27;168&#x27;, &#x27;23&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;1&#x27;, &#x27;xiaohong&#x27;, &#x27;female&#x27;, &#x27;162&#x27;, &#x27;22&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;2&#x27;, &#x27;xiaozhang&#x27;, &#x27;female&#x27;, &#x27;163&#x27;, &#x27;21&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;2&#x27;, &#x27;xiaoli&#x27;, &#x27;male&#x27;, &#x27;158&#x27;, &#x27;21&#x27;]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;test.csv&#x27;</span>)<span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.reader(f)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> f_csv:</span><br><span class="line">        <span class="built_in">print</span>(row[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;result</span></span><br><span class="line"><span class="string">class</span></span><br><span class="line"><span class="string">1</span></span><br><span class="line"><span class="string">1</span></span><br><span class="line"><span class="string">2</span></span><br><span class="line"><span class="string">2</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>参考<a href="https://blog.csdn.net/katyusha1/article/details/81606175" title="CSDN">网站</a></p></blockquote><h1 id="3-with-open-as-读写文件"><a href="#3-with-open-as-读写文件" class="headerlink" title="3. with open () as 读写文件"></a>3. with open () as 读写文件</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;file.txt&#x27;</span>,<span class="string">&#x27;r&#x27;</span>,) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="built_in">print</span>(f.read())</span><br><span class="line"><span class="comment"># 不需调用f.close()</span></span><br><span class="line"><span class="comment"># 如果文件过大则用read(size)比较保险</span></span><br><span class="line"><span class="comment"># 如果文件是配置文件readlines()较为方便</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;file.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;Hello World !&#x27;</span>)</span><br><span class="line"><span class="comment"># 文本文件    encoding 字符编码：gbk，utf-8</span></span><br><span class="line"><span class="comment"># 二进制文件  rb模式读取:图片,视频</span></span><br></pre></td></tr></table></figure><blockquote><p>参考<a href="https://blog.csdn.net/xrinosvip/article/details/82019844" title="CSDN">网站</a></p></blockquote><h1 id="4-os"><a href="#4-os" class="headerlink" title="4. os"></a>4. os</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">main_path = <span class="string">&#x27;E:/os/&#x27;</span>    <span class="comment">#创建一个路径</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(main_path):   <span class="comment">#如果该路径不存在</span></span><br><span class="line">    os.makedirs(main_path)  <span class="comment">#则新建一个路径</span></span><br></pre></td></tr></table></figure><p><strong>删除文件:</strong><code>os.remove(path)</code></p><h2 id="os-path-abspath"><a href="#os-path-abspath" class="headerlink" title="os.path.abspath"></a>os.path.abspath</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(os.path.abspath(&#x27;.&#x27;))</span><br><span class="line"></span><br><span class="line">运行PS E:\BaiduSyncdisk\NeRF_Proj\NeRO&gt; &amp; F:/miniconda/envs/nero/python.exe e:/BaiduSyncdisk/NeRF_Proj/NeRO/blender_backend/relight_backend.py</span><br><span class="line"></span><br><span class="line">输出：E:\BaiduSyncdisk\NeRF_Proj\NeRO</span><br></pre></td></tr></table></figure><h1 id="5-将图片保存在文件夹中"><a href="#5-将图片保存在文件夹中" class="headerlink" title="5. 将图片保存在文件夹中"></a>5. 将图片保存在文件夹中</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dir</span> = <span class="string">&#x27;文件路径&#x27;</span> + name +<span class="string">&#x27;.jpg&#x27;</span>     <span class="comment">#文件名</span></span><br><span class="line">url_get = requests.get(url)         <span class="comment">#从url中获取图片信息</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="built_in">dir</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:           <span class="comment">#打开图片文件，保存图片信息</span></span><br><span class="line">    f.writer(url_get.content)</span><br></pre></td></tr></table></figure><h1 id="6-try…except-语句"><a href="#6-try…except-语句" class="headerlink" title="6. try…except 语句"></a>6. try…except 语句</h1><h2 id="6-1-语法"><a href="#6-1-语法" class="headerlink" title="6.1 语法"></a>6.1 <strong>语法</strong></h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#python 异常处理</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">&lt;语句&gt;        <span class="comment">#运行别的代码</span></span><br><span class="line"><span class="keyword">except</span> &lt;名字&gt;：</span><br><span class="line">&lt;语句&gt;        <span class="comment">#如果在try部份引发了&#x27;name&#x27;异常</span></span><br><span class="line"><span class="keyword">except</span> &lt;名字&gt;，&lt;数据&gt;:</span><br><span class="line">&lt;语句&gt;        <span class="comment">#如果引发了&#x27;name&#x27;异常，获得附加的数据</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">&lt;语句&gt;        <span class="comment">#如果没有异常发生</span></span><br></pre></td></tr></table></figure><h2 id="6-2-使用-except-而不带任何异常类型"><a href="#6-2-使用-except-而不带任何异常类型" class="headerlink" title="6.2 使用 except 而不带任何异常类型"></a>6.2 <strong>使用 except 而不带任何异常类型</strong></h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    正常的操作</span><br><span class="line">   ......................</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    发生异常，执行这块代码</span><br><span class="line">   ......................</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    如果没有异常执行这块代码</span><br></pre></td></tr></table></figure><p><strong>以上方式 try-except 语句捕获所有发生的异常。但这不是一个很好的方式，我们不能通过该程序识别出具体的异常信息。因为它捕获所有的异常</strong></p><h2 id="6-3-使用-except-而带多种异常类型"><a href="#6-3-使用-except-而带多种异常类型" class="headerlink" title="6.3 使用 except 而带多种异常类型"></a>6.3 <strong>使用 except 而带多种异常类型</strong></h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    正常的操作</span><br><span class="line">   ......................</span><br><span class="line"><span class="keyword">except</span>(Exception1[, Exception2[,...ExceptionN]]]):</span><br><span class="line">   发生以上多个异常中的一个，执行这块代码</span><br><span class="line">   ......................</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    如果没有异常执行这块代码</span><br></pre></td></tr></table></figure><h2 id="6-4-try-finally-语句"><a href="#6-4-try-finally-语句" class="headerlink" title="6.4 try-finally 语句"></a>6.4 <strong>try-finally 语句</strong></h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">&lt;语句&gt;</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">&lt;语句&gt;    <span class="comment">#退出try时总会执行</span></span><br><span class="line"><span class="keyword">raise</span></span><br></pre></td></tr></table></figure><h2 id="6-5-异常的参数"><a href="#6-5-异常的参数" class="headerlink" title="6.5 异常的参数"></a>6.5 <strong>异常的参数</strong></h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    正常的操作</span><br><span class="line">   ......................</span><br><span class="line"><span class="keyword">except</span> ExceptionType, Argument:</span><br><span class="line">    你可以在这输出 Argument 的值...</span><br></pre></td></tr></table></figure><h2 id="6-6-用户自定义异常"><a href="#6-6-用户自定义异常" class="headerlink" title="6.6 用户自定义异常"></a>6.6 <strong>用户自定义异常</strong></h2><ul><li>通过创建一个新的异常类，程序可以命名它们自己的异常。异常应该是典型的继承自 Exception 类，通过直接或间接的方式</li><li>以下为与 RuntimeError 相关的实例,实例中创建了一个类，基类为 RuntimeError，用于在异常触发时输出更多的信息</li><li>在 try 语句块中，用户自定义的异常后执行 except 块语句，变量 e 是用于创建 Networkerror 类的实例</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Networkerror</span>(<span class="title class_ inherited__">RuntimeError</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, arg</span>):</span><br><span class="line">        self.args = arg</span><br></pre></td></tr></table></figure><ul><li><strong>在你定义以上类后，你可以触发该异常，如下所示：</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">raise</span> Networkerror(<span class="string">&quot;Bad hostname&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> Networkerror,e:</span><br><span class="line">    <span class="built_in">print</span> e.args</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th style="text-align:center">异常名称</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">BaseException</td><td style="text-align:center">所有异常的基类</td></tr><tr><td style="text-align:center">SystemExit</td><td style="text-align:center">解释器请求退出</td></tr><tr><td style="text-align:center">KeyboardInterrupt</td><td style="text-align:center">用户中断执行(通常是输入^C)</td></tr><tr><td style="text-align:center">Exception</td><td style="text-align:center">常规错误的基类</td></tr><tr><td style="text-align:center">StopIteration</td><td style="text-align:center">迭代器没有更多的值</td></tr><tr><td style="text-align:center">GeneratorExit</td><td style="text-align:center">生成器(generator)发生异常来通知退出</td></tr><tr><td style="text-align:center">StandardError</td><td style="text-align:center">所有的内建标准异常的基类</td></tr><tr><td style="text-align:center">ArithmeticError</td><td style="text-align:center">所有数值计算错误的基类</td></tr><tr><td style="text-align:center">FloatingPointError</td><td style="text-align:center">浮点计算错误</td></tr><tr><td style="text-align:center">OverflowError</td><td style="text-align:center">数值运算超出最大限制</td></tr><tr><td style="text-align:center">ZeroDivisionError</td><td style="text-align:center">除(或取模)零 (所有数据类型)</td></tr><tr><td style="text-align:center">AssertionError</td><td style="text-align:center">断言语句失败</td></tr><tr><td style="text-align:center">AttributeError</td><td style="text-align:center">对象没有这个属性</td></tr><tr><td style="text-align:center">EOFError</td><td style="text-align:center">没有内建输入,到达 EOF 标记</td></tr><tr><td style="text-align:center">EnvironmentError</td><td style="text-align:center">操作系统错误的基类</td></tr><tr><td style="text-align:center">IOError</td><td style="text-align:center">输入/输出操作失败</td></tr><tr><td style="text-align:center">OSError</td><td style="text-align:center">操作系统错误</td></tr><tr><td style="text-align:center">WindowsError</td><td style="text-align:center">系统调用失败</td></tr><tr><td style="text-align:center">ImportError</td><td style="text-align:center">导入模块/对象失败</td></tr><tr><td style="text-align:center">LookupError</td><td style="text-align:center">无效数据查询的基类</td></tr><tr><td style="text-align:center">IndexError</td><td style="text-align:center">序列中没有此索引(index)</td></tr><tr><td style="text-align:center">KeyError</td><td style="text-align:center">映射中没有这个键</td></tr><tr><td style="text-align:center">MemoryError</td><td style="text-align:center">内存溢出错误(对于 Python 解释器不是致命的)</td></tr><tr><td style="text-align:center">NameError</td><td style="text-align:center">未声明/初始化对象 (没有属性)</td></tr><tr><td style="text-align:center">UnboundLocalError</td><td style="text-align:center">访问未初始化的本地变量</td></tr><tr><td style="text-align:center">ReferenceError</td><td style="text-align:center">弱引用(Weak reference)试图访问已经垃圾回收了的对象</td></tr><tr><td style="text-align:center">RuntimeError</td><td style="text-align:center">一般的运行时错误</td></tr><tr><td style="text-align:center">NotImplementedError</td><td style="text-align:center">尚未实现的方法</td></tr><tr><td style="text-align:center">SyntaxError Python</td><td style="text-align:center">语法错误</td></tr><tr><td style="text-align:center">IndentationError</td><td style="text-align:center">缩进错误</td></tr><tr><td style="text-align:center">TabError Tab</td><td style="text-align:center">和空格混用</td></tr><tr><td style="text-align:center">SystemError</td><td style="text-align:center">一般的解释器系统错误</td></tr><tr><td style="text-align:center">TypeError</td><td style="text-align:center">对类型无效的操作</td></tr><tr><td style="text-align:center">ValueError</td><td style="text-align:center">传入无效的参数</td></tr><tr><td style="text-align:center">UnicodeError Unicode</td><td style="text-align:center">相关的错误</td></tr><tr><td style="text-align:center">UnicodeDecodeError Unicode</td><td style="text-align:center">解码时的错误</td></tr><tr><td style="text-align:center">UnicodeEncodeError Unicode</td><td style="text-align:center">编码时错误</td></tr><tr><td style="text-align:center">UnicodeTranslateError Unicode</td><td style="text-align:center">转换时错误</td></tr><tr><td style="text-align:center">Warning</td><td style="text-align:center">警告的基类</td></tr><tr><td style="text-align:center">DeprecationWarning</td><td style="text-align:center">关于被弃用的特征的警告</td></tr><tr><td style="text-align:center">FutureWarning</td><td style="text-align:center">关于构造将来语义会有改变的警告</td></tr><tr><td style="text-align:center">OverflowWarning</td><td style="text-align:center">旧的关于自动提升为长整型(long)的警告</td></tr><tr><td style="text-align:center">PendingDeprecationWarning</td><td style="text-align:center">关于特性将会被废弃的警告</td></tr><tr><td style="text-align:center">RuntimeWarning</td><td style="text-align:center">可疑的运行时行为(runtime behavior)的警告</td></tr><tr><td style="text-align:center">SyntaxWarning</td><td style="text-align:center">可疑的语法的警告</td></tr><tr><td style="text-align:center">UserWarning</td><td style="text-align:center">用户代码生成的警告</td></tr></tbody></table></div><blockquote><p>参考<a href="https://www.runoob.com/python/python-exceptions.html">教程</a></p></blockquote><h1 id="7-python-小知识点"><a href="#7-python-小知识点" class="headerlink" title="7. python 小知识点"></a>7. python 小知识点</h1><h2 id="7-1-换行符-‘-n’-和-回车符-‘-r’-的区别？"><a href="#7-1-换行符-‘-n’-和-回车符-‘-r’-的区别？" class="headerlink" title="7.1 换行符 ‘\n’ 和 回车符 ‘\r’ 的区别？"></a>7.1 换行符 ‘\n’ 和 回车符 ‘\r’ 的区别？</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">换行符就是另起一新行，光标在新行的开头；</span><br><span class="line">\n</span><br><span class="line">回车符就是光标回到一旧行的开头；(即光标目前所在的行为旧行)</span><br><span class="line">\r</span><br><span class="line">在解析文本或其他格式的文件内容时，常常要碰到判定回车式换行的地方</span><br><span class="line">这个时候就要注意既要判定&quot;\r\n&quot;又要判定&quot;\n&quot;。</span><br></pre></td></tr></table></figure><h2 id="7-2-Python-中-import-from…import-import…as-的区别"><a href="#7-2-Python-中-import-from…import-import…as-的区别" class="headerlink" title="7.2 Python 中 import, from…import,import…as 的区别"></a>7.2 Python 中 import, from…import,import…as 的区别</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="built_in">print</span>(datetime.datetime.now())</span><br></pre></td></tr></table></figure><p>以上代码实现输出系统当前时间，是引入整个 datetime 包，然后再调用 datetime 这个类中的 now()方法</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="built_in">print</span>(datetime.now())</span><br></pre></td></tr></table></figure><p>这里是从 datetime 包中只导入 datetime 这个类，让后再调用 datetime 这个类中的 now()方法实现同样的目的</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime <span class="keyword">as</span> dt</span><br><span class="line"><span class="built_in">print</span>(dt.datetime.now())</span><br></pre></td></tr></table></figure><p>假如你嫌 datetime 这个包名称太长，想要给它取个别名，以后每次用到它的时候都用它的别名代替它，这时就需要用到 import…as</p><h2 id="7-3-Python-中-n-、-m-、-1-、-1-、-1-、-2-1-和-1-的含义"><a href="#7-3-Python-中-n-、-m-、-1-、-1-、-1-、-2-1-和-1-的含义" class="headerlink" title="7.3 Python 中[ : n]、[m : ]、[-1]、[:-1]、[::-1]、[2::-1]和[1:]的含义"></a>7.3 Python 中[ : n]、[m : ]、[-1]、[:-1]、[::-1]、[2::-1]和[1:]的含义</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[m : ] 代表列表中的第m+<span class="number">1</span>项到最后一项</span><br><span class="line"></span><br><span class="line">[ : n] 代表列表中的第一项到第n项</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3.4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">[ <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> ]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[-<span class="number">1</span>])     取最后一个元素</span><br><span class="line">结果：[<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[:-<span class="number">1</span>])     除了最后一个取全部</span><br><span class="line">结果：[ <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> ]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[::-<span class="number">1</span>])     取从后向前（相反）的元素</span><br><span class="line">结果：[ <span class="number">5</span> <span class="number">4</span> <span class="number">3</span> <span class="number">2</span> <span class="number">1</span> ]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">2</span>::-<span class="number">1</span>])     取从下标为<span class="number">2</span>的元素翻转读取</span><br><span class="line">结果：[ <span class="number">3</span> <span class="number">2</span> <span class="number">1</span> ]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">1</span>:])     取第二个到最后一个元素</span><br><span class="line">结果：[<span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]</span><br></pre></td></tr></table></figure><h2 id="7-4-python-浮点数保留几位小数"><a href="#7-4-python-浮点数保留几位小数" class="headerlink" title="7.4 python 浮点数保留几位小数"></a>7.4 python 浮点数保留几位小数</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">5.026</span></span><br><span class="line"></span><br><span class="line">b = <span class="number">5.000</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">round</span>(a,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 5.03</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">round</span>(b,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 5.0</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;%.2f&#x27;</span> % a</span><br><span class="line"><span class="comment"># &#x27;5.03&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;%.2f&#x27;</span> % b</span><br><span class="line"><span class="comment"># &#x27;5.00&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">float</span>(<span class="string">&#x27;%.2f&#x27;</span> % a)</span><br><span class="line"><span class="comment"># 5.03</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">float</span>(<span class="string">&#x27;%.2f&#x27;</span> % b)</span><br><span class="line"><span class="comment"># 5.0</span></span><br></pre></td></tr></table></figure><h2 id="7-5-除法和取模"><a href="#7-5-除法和取模" class="headerlink" title="7.5 除法和取模"></a>7.5 除法和取模</h2><p>Python中<br>// ：地板除，即向负无穷取整</p><ul><li>8//3 结果为2</li><li>-8//3 结果为-3</li></ul><p>Python中：% 取模（modulus）</p><ul><li>-10%3 结果为2</li><li>-90%8结果为6</li></ul><p>C++中：% 取余</p><ul><li>-10%3 结果为-1</li><li>-90%8结果为-2</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 取模，Python中可直接用%，计算模，r = a % b</span><br><span class="line">def mod(a, b):    </span><br><span class="line">    c = a // b</span><br><span class="line">    r = a - c * b</span><br><span class="line">    return r</span><br><span class="line"> </span><br><span class="line"># 取余 </span><br><span class="line">def rem(a, b):</span><br><span class="line">    c = int(a / b)</span><br><span class="line">    r = a - c * b</span><br><span class="line">    return r</span><br></pre></td></tr></table></figure><h1 id="8-python-中陌生的函数"><a href="#8-python-中陌生的函数" class="headerlink" title="8. python 中陌生的函数"></a>8. python 中陌生的函数</h1><p>_自己还不太熟悉的_</p><h2 id="8-1-str-split-对字符串进行切片—返回一个列表"><a href="#8-1-str-split-对字符串进行切片—返回一个列表" class="headerlink" title="8.1 str.split() 对字符串进行切片—返回一个列表"></a>8.1 <code>str.split()</code> <strong>对字符串进行切片—返回一个列表</strong></h2><ul><li>语法<code>str.split(str=&quot;&quot;, num=string.count(str)).</code></li></ul><ul><li>str：分隔符，默认为所有的空字符，包括空格、换行、指标</li><li>num：分割次数，默认为-1，即分割所有</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example</span></span><br><span class="line">txt = <span class="string">&quot;Google#Runoob#Taobao#Facebook&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二个参数为 1，返回两个参数列表</span></span><br><span class="line">x = txt.split(<span class="string">&quot;#&quot;</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x)         <span class="comment">#[&#x27;Google&#x27;, &#x27;Runoob#Taobao#Facebook&#x27;]</span></span><br></pre></td></tr></table></figure><blockquote><p>参考教程：<a href="https://www.runoob.com/python/att-string-split.html" title="runoob.com">RUNOOB</a></p></blockquote><h2 id="8-2-str-find-检测字符串中是否包含子字符串-str"><a href="#8-2-str-find-检测字符串中是否包含子字符串-str" class="headerlink" title="8.2 str.find() 检测字符串中是否包含子字符串 str"></a>8.2 <code>str.find()</code> <strong>检测字符串中是否包含子字符串 str</strong></h2><ul><li>_如果包含子字符串返回开始的索引值，否则返回-1_</li></ul><ul><li>语法<code>str.find(str, beg=0, end=len(string))</code></li><li>str — 指定检索的字符串</li><li>beg — 开始索引，默认为 0</li><li>end — 结束索引，默认为字符串的长度</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example</span></span><br><span class="line">str1 = <span class="string">&quot;this is string example....wow!!!&quot;</span>;</span><br><span class="line">str2 = <span class="string">&quot;exam&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> str1.find(str2);          <span class="comment">#15</span></span><br><span class="line"><span class="built_in">print</span> str1.find(str2, <span class="number">10</span>);      <span class="comment">#15</span></span><br><span class="line"><span class="built_in">print</span> str1.find(str2, <span class="number">40</span>);      <span class="comment">#-1,查不到返回-1</span></span><br></pre></td></tr></table></figure><h2 id="8-3-string-join"><a href="#8-3-string-join" class="headerlink" title="8.3 string.join()"></a>8.3 <code>string.join()</code></h2><ul><li>语法 <code>&#39;sep&#39;.join(seq)</code>_以 sep 作为分隔符，将 seq 所有的元素合并成一个新的字符串_ _返回值：返回一个以分隔符 sep 连接各个元素后生成的字符串_</li><li>sep：分隔符。可以为空</li><li>seq：要连接的元素序列、字符串、元组、字典</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>seq1 = [<span class="string">&#x27;hello&#x27;</span>,<span class="string">&#x27;good&#x27;</span>,<span class="string">&#x27;boy&#x27;</span>,<span class="string">&#x27;doiido&#x27;</span>] <span class="comment">#对序列操作</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> <span class="string">&#x27; &#x27;</span>.join(seq1)</span><br><span class="line">hello good boy doiido</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> <span class="string">&#x27;:&#x27;</span>.join(seq1)</span><br><span class="line">hello:good:boy:doiido</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对字符串操作</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>seq2 = <span class="string">&quot;hello good boy doiido&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> <span class="string">&#x27;:&#x27;</span>.join(seq2)</span><br><span class="line">h:e:l:l:o: :g:o:o:d: :b:o:y: :d:o:i:i:d:o</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对元组操作</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>seq3 = (<span class="string">&#x27;hello&#x27;</span>,<span class="string">&#x27;good&#x27;</span>,<span class="string">&#x27;boy&#x27;</span>,<span class="string">&#x27;doiido&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> <span class="string">&#x27;:&#x27;</span>.join(seq3)</span><br><span class="line">hello:good:boy:doiido</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对字典操作</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>seq4 = &#123;<span class="string">&#x27;hello&#x27;</span>:<span class="number">1</span>,<span class="string">&#x27;good&#x27;</span>:<span class="number">2</span>,<span class="string">&#x27;boy&#x27;</span>:<span class="number">3</span>,<span class="string">&#x27;doiido&#x27;</span>:<span class="number">4</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> <span class="string">&#x27;:&#x27;</span>.join(seq4)</span><br><span class="line">boy:good:doiido:hello</span><br></pre></td></tr></table></figure><ul><li><strong>另一个 <code>os.path.join()</code></strong></li></ul><ul><li>语法 <code>os.path.join(path1[,path2[,......]])</code></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 合并目录</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> os</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>os.path.join(<span class="string">&#x27;/hello/&#x27;</span>,<span class="string">&#x27;good/boy/&#x27;</span>,<span class="string">&#x27;doiido&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;/hello/good/boy/doiido&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>参考<a href="https://www.cnblogs.com/sui776265233/p/10755525.html">博客网站</a></p></blockquote><h2 id="8-4-ord-amp-chr"><a href="#8-4-ord-amp-chr" class="headerlink" title="8.4 ord() &amp; chr()"></a>8.4 ord() &amp; chr()</h2><p>ord()<br>将字符转化为ascii码</p><p>chr()<br>将ascii码转化为字母或实际数字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>)  <span class="comment"># 返回ASCII码 97</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">chr</span>(<span class="number">97</span>)   <span class="comment"># 返回 字母&#x27;a&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="9-python中类的相关知识"><a href="#9-python中类的相关知识" class="headerlink" title="9. python中类的相关知识"></a>9. python中类的相关知识</h1><h2 id="9-1-类的定义和创建"><a href="#9-1-类的定义和创建" class="headerlink" title="9.1 类的定义和创建"></a>9.1 类的定义和创建</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义类，类名为Cname</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Cname</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建cname1实例</span></span><br><span class="line">cname1 = Cname()</span><br></pre></td></tr></table></figure><h2 id="9-2-类中的实例属性与类属性"><a href="#9-2-类中的实例属性与类属性" class="headerlink" title="9.2 类中的实例属性与类属性"></a>9.2 类中的实例属性与类属性</h2><ul><li>实例属性：用于区分不同的实例，不同的类有不同的实例属性</li><li>类属性：是每个实例共有的属性，每个实例共有的属性</li></ul><h3 id="9-2-1-实例属性"><a href="#9-2-1-实例属性" class="headerlink" title="9.2.1 实例属性"></a>9.2.1 实例属性</h3><p><code>cname1.name = y</code> <code>cname2.name = q</code><br>每个实例有了name属性后就可以访问<br><code>print(cname1.name)</code> <code>print(cname2.name)</code></p><p>但是这样会比较麻烦，所以可以在每个实例中统一加上name属性<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Cname</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name</span>):  <span class="comment"># 初始化一个属性r</span></span><br><span class="line">        self.name = name</span><br></pre></td></tr></table></figure></p><p><strong>__init__() 方法的第一个参数必须是 self</strong><br><em>self 代表类的实例，是通过类创建的实例</em></p><p>然后创建实例，就可以直接带上参数 <code>cname1 = Cname(y)</code><br>然后访问实例属性 <code>print(cname1.name)</code></p><h3 id="9-2-2-类属性"><a href="#9-2-2-类属性" class="headerlink" title="9.2.2 类属性"></a>9.2.2 类属性</h3><p>类的属性绑定后，所有实例都可以访问，而且<strong>实例访问的类属性都相同</strong><br><strong>实例属性每个实例各自拥有，互相独立，而类属性有且只有一份</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Cname</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    zhongz = <span class="string">&#x27;people&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name</span>):</span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">cname1 = Cname(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">cname2 = Cname(<span class="string">&#x27;q&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;--------&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(cname1.zhongz)    <span class="comment"># people</span></span><br><span class="line"><span class="built_in">print</span>(cname2.zhongz)    <span class="comment"># people</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过类名修改了类属性后</span></span><br><span class="line">Cname.zhongz = <span class="string">&#x27;tenshi&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;--------&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(cname1.zhongz)    <span class="comment"># tenshi</span></span><br><span class="line"><span class="built_in">print</span>(cname2.zhongz)    <span class="comment"># tenshi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过实例名修改了类属性</span></span><br><span class="line">cname1.zhongz = <span class="string">&#x27;mea&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;--------&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(cname1.zhongz)    <span class="comment"># mea</span></span><br><span class="line"><span class="built_in">print</span>(cname2.zhongz)    <span class="comment"># tenshi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除了cname1的类属性zhongz后</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;--------&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(cname1.zhongz)    <span class="comment"># mea</span></span><br><span class="line"><span class="keyword">del</span> cname1.zhongz</span><br><span class="line"><span class="built_in">print</span>(cname1.zhongz)    <span class="comment"># tenshi</span></span><br></pre></td></tr></table></figure><p><strong>要修改类属性，不要再实例上修改，而是在类名上修改</strong></p><h2 id="9-3-类的实例方法"><a href="#9-3-类的实例方法" class="headerlink" title="9.3 类的实例方法"></a>9.3 类的实例方法</h2><p><strong>method</strong> is the <strong>function</strong> in <strong>class</strong> <del>英语四级差点没过的渣渣</del><br><strong>方法是表明这个类用是来做什么,方法就是类中的函数</strong></p><p><em>最简单的一个方法：打印属性</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Cname</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    zhongz = <span class="string">&#x27;people&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name</span>):</span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">printname</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(self.name) <span class="comment"># 打印名字</span></span><br><span class="line"></span><br><span class="line">cname1 = Cname(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">cname2 = Cname(<span class="string">&#x27;q&#x27;</span>)</span><br><span class="line">cname1.printname()  <span class="comment"># y</span></span><br><span class="line">cname2.printname()  <span class="comment"># q</span></span><br></pre></td></tr></table></figure><p><code>printname(self)</code>就是一个最简单的方法</p><h2 id="9-4-类中的访问限制"><a href="#9-4-类中的访问限制" class="headerlink" title="9.4 类中的访问限制"></a>9.4 类中的访问限制</h2><h3 id="9-4-1-属性的访问限制"><a href="#9-4-1-属性的访问限制" class="headerlink" title="9.4.1 属性的访问限制"></a>9.4.1 属性的访问限制</h3><p>python的类中的属性，如果有些属性不希望被外部访问，我们可以属性命名时以双下划线开头 <code>__</code>，如 <code>__age</code></p><blockquote><p>但，如果一个属性以”__xxx__“的形式定义，那么它可以被外部访问。以”__xxx__“定义的属性在Python的类中被称为特殊属性，有很多预定义的特殊属性是以“__xxx__”定义，所以我们不要把普通属性用”__xxx__“定义。</p><p><strong>加双下划线__xx 的属性，可以通过“ _类名__xx ”可以访问到属性的值 如<code>Cname._Cname__age</code></strong></p></blockquote><h3 id="9-4-2-方法的访问限制"><a href="#9-4-2-方法的访问限制" class="headerlink" title="9.4.2 方法的访问限制"></a>9.4.2 方法的访问限制</h3><p>双下划线，如<code>def __printage():</code></p><ul><li>此时，该方法只能在类的内部使用，而无法被外部调用</li></ul><p>单下划线，不能通过 from module import * 这种方式导入，可以通过其他方式：</p><ul><li>import A</li><li><code>b = A._B()</code></li></ul><h2 id="9-5-类中的装饰方法"><a href="#9-5-类中的装饰方法" class="headerlink" title="9.5 类中的装饰方法"></a>9.5 类中的装饰方法</h2><ul><li><code>@classmethod</code>    用来修饰类方法。使用在与类进行交互，但不和其实例进行交互的函数方法上</li><li><code>@staticmethod</code>   用来修饰静态方法。使用在有些与类相关函数，但不使用该类或该类的实例。如更改环境变量、修改其他类的属性等</li></ul><p><em>classmethod必须使用类的对象作为第一个参数，而staticmethod则可以不传递任何参数</em></p><h3 id="9-5-1-classmethod-修饰方法——类方法"><a href="#9-5-1-classmethod-修饰方法——类方法" class="headerlink" title="9.5.1 @classmethod 修饰方法——类方法"></a>9.5.1 @classmethod 修饰方法——类方法</h3><p>类方法，我们不用通过实例化类就能访问的方法。而且@classmethod 装饰的方法不能使用实例属性，只能是类属性。它主要使用在和类进行交互，但不和其实例进行交互的函数方法上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Cname</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    zhongz = <span class="string">&#x27;people&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name</span>):</span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">printname</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(self.name)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">printwe</span>(<span class="params">cls</span>):</span><br><span class="line">        <span class="built_in">print</span>(cls.zhongz)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cname.printname()   # 没有实例化 ，会发生错误</span></span><br><span class="line">Cname.printwe()     <span class="comment"># 没有实例化也可以访问</span></span><br></pre></td></tr></table></figure><blockquote><p>printwe(cls)中cls表示的是类，它和self类实例有一定的差别。类方法中都是使用cls，实例方法中使用self</p></blockquote><h3 id="9-5-2-staticmethod-修饰方法——静态方法"><a href="#9-5-2-staticmethod-修饰方法——静态方法" class="headerlink" title="9.5.2 @staticmethod 修饰方法——静态方法"></a>9.5.2 @staticmethod 修饰方法——静态方法</h3><p><code>@staticmethod</code> 不强制要求传递参数（它做的事与类方法或实例方法一样）<br><code>@staticmethod</code> 使用在有些和类相关函数，但不使用该类或者该类的实例。如更改环境变量、修改其他类的属性等<br><code>@staticmethod</code> 修饰的方法是放在类外的函数，我们为了方便将他移动到了类里面，它对类的运行无影响</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Date</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">   day = <span class="number">0</span></span><br><span class="line">   month = <span class="number">0</span></span><br><span class="line">   year = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, year=<span class="number">0</span>, month=<span class="number">0</span>, day=<span class="number">0</span></span>):</span><br><span class="line">       self.day = day</span><br><span class="line">       self.month = month</span><br><span class="line">       self.year = year</span><br><span class="line"></span><br><span class="line"><span class="meta">   @classmethod</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">from_string</span>(<span class="params">cls, date_as_string</span>):</span><br><span class="line">       year, month, day = date_as_string.split(<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">       date = cls(year, month, day)</span><br><span class="line">       <span class="keyword">return</span> date</span><br><span class="line">    <span class="comment"># 返回的是类的实例</span></span><br><span class="line"></span><br><span class="line"><span class="meta">   @staticmethod</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">is_date_valid</span>(<span class="params">date_as_string</span>):</span><br><span class="line">       <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">      用来校验日期的格式是否正确</span></span><br><span class="line"><span class="string">       &quot;&quot;&quot;</span></span><br><span class="line">       year, month, day = date_as_string.split(<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">       <span class="keyword">return</span> <span class="built_in">int</span>(year) &lt;= <span class="number">3999</span> <span class="keyword">and</span> <span class="built_in">int</span>(month) &lt;= <span class="number">12</span> <span class="keyword">and</span> <span class="built_in">int</span>(day) &lt;= <span class="number">31</span></span><br><span class="line"></span><br><span class="line">date1 = Date.from_string(<span class="string">&#x27;2012-05-10&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(date1.year, date1.month, date1.day)</span><br><span class="line">is_date = Date.is_date_valid(<span class="string">&#x27;2012-09-18&#x27;</span>) <span class="comment"># 格式正确 返回True</span></span><br></pre></td></tr></table></figure><p>is_date_valid(date_as_string) 只有一个参数，它的运行不会影响类的属性</p><blockquote><p>@staticmethod修饰方法 is_date_valid(date_as_string)中无实例化参数self或者cls；而@classmethod修饰的方法中有from_string(cls, date_as_string) 类参数cls</p></blockquote><h2 id="9-6-python中的property的使用"><a href="#9-6-python中的property的使用" class="headerlink" title="9.6 python中的property的使用"></a>9.6 python中的property的使用</h2><p>property的作用</p><ul><li>作为装饰器 @property将类方法转换为类属性（只读）</li><li>property重新实现一个属性的setter和getter方法</li></ul><h3 id="9-6-1-property将类方法转换为只读属性"><a href="#9-6-1-property将类方法转换为只读属性" class="headerlink" title="9.6.1 @property将类方法转换为只读属性"></a>9.6.1 @property将类方法转换为只读属性</h3><p>经常使用，将类的属性设置为不可修改</p><p>将一个类方法转变成一个类属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Circle</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">   __pi = <span class="number">3.14</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, r</span>):</span><br><span class="line">       self.r = r</span><br><span class="line"></span><br><span class="line"><span class="meta">   @property</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">pi</span>(<span class="params">self</span>):</span><br><span class="line">       <span class="keyword">return</span> self.__pi</span><br><span class="line"></span><br><span class="line">circle1 = Circle(<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(circle1.pi)</span><br><span class="line">circle1.pi=<span class="number">3.14159</span>  <span class="comment"># 出现AttributeError异常</span></span><br></pre></td></tr></table></figure><p>创建实例后我们可以使用circle1.pi 自己获取方法的返回值，而且他只能读不能修改</p><h3 id="9-6-2-property重新实现setter和getter方法"><a href="#9-6-2-property重新实现setter和getter方法" class="headerlink" title="9.6.2 property重新实现setter和getter方法"></a>9.6.2 property重新实现setter和getter方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Circle</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">   __pi = <span class="number">3.14</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, r</span>):</span><br><span class="line">       self.r = r</span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">get_pi</span>(<span class="params">self</span>):</span><br><span class="line">       <span class="keyword">return</span> self.__pi</span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">set_pi</span>(<span class="params">self, pi</span>):</span><br><span class="line">       Circle.__pi = pi</span><br><span class="line"></span><br><span class="line">   pi = <span class="built_in">property</span>(get_pi, set_pi)</span><br><span class="line"></span><br><span class="line">circle1 = Circle(<span class="number">2</span>)</span><br><span class="line">circle1.pi = <span class="number">3.14</span>  <span class="comment"># 设置 pi的值</span></span><br><span class="line"><span class="built_in">print</span>(circle1.pi)  <span class="comment"># 访问 pi的值</span></span><br></pre></td></tr></table></figure><p>当我们以这种方式使用属性函数时，它允许pi属性设置并获取值本身而不破坏原有代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Circle</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">   __pi = <span class="number">3.14</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, r</span>):</span><br><span class="line">       self.r = r</span><br><span class="line"></span><br><span class="line"><span class="meta">   @property</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">pi</span>(<span class="params">self</span>):</span><br><span class="line">       <span class="keyword">return</span> self.__pi</span><br><span class="line"></span><br><span class="line"><span class="meta">   @pi.setter</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">pi</span>(<span class="params">self, pi</span>):</span><br><span class="line">       Circle.__pi = pi</span><br><span class="line"></span><br><span class="line">circle1 = Circle(<span class="number">2</span>)</span><br><span class="line">circle1.pi = <span class="number">3.14</span>  <span class="comment"># 设置 pi的值</span></span><br><span class="line"><span class="built_in">print</span>(circle1.pi)  <span class="comment"># 访问 pi的值</span></span><br></pre></td></tr></table></figure><p>把一个getter方法变成属性，只需要加上@property就可以了，如上此时pi(self)方法，@property本身又创建了另一个装饰器@pi.setter，负责把一个setter方法变成属性赋值，于是，将@pi.setter加到pi(self, pi)上，我们就拥有一个可控的属性操作</p><blockquote><p>参考<a href="https://www.zhihu.com/people/lyzf">知乎大佬</a>的<a href="https://zhuanlan.zhihu.com/p/30223570">教程</a><br>感谢大佬让我搞懂了python的类，虽然最后的不太懂，但是基础是懂了</p></blockquote><h2 id="9-7-类的继承"><a href="#9-7-类的继承" class="headerlink" title="9.7 类的继承"></a>9.7 类的继承</h2><h3 id="9-7-1-类的继承"><a href="#9-7-1-类的继承" class="headerlink" title="9.7.1 类的继承"></a>9.7.1 类的继承</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Animal</span>(<span class="title class_ inherited__">object</span>):  <span class="comment">#  python3中所有类都可以继承于object基类</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, age</span>):</span><br><span class="line">       self.name = name</span><br><span class="line">       self.age = age</span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self</span>):</span><br><span class="line">       <span class="built_in">print</span>(self.name, <span class="string">&#x27;会叫&#x27;</span>)</span><br><span class="line"><span class="comment"># 现在我们需要定义一个Cat猫类继承于Animal，猫类比动物类多一个sex属性。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Cat</span>(<span class="title class_ inherited__">Animal</span>):</span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,name,age,sex</span>):</span><br><span class="line">       <span class="built_in">super</span>(Cat, self).__init__(name,age)  <span class="comment"># 不要忘记从Animal类引入属性</span></span><br><span class="line">       self.sex=sex</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:  <span class="comment"># 单模块被引用时下面代码不会受影响，用于调试</span></span><br><span class="line">    c = Cat(<span class="string">&#x27;喵喵&#x27;</span>, <span class="number">2</span>, <span class="string">&#x27;男&#x27;</span>)  <span class="comment">#  Cat继承了父类Animal的属性</span></span><br><span class="line">    c.call()  <span class="comment"># 输出 喵喵 会叫 ，Cat继承了父类Animal的方法 </span></span><br></pre></td></tr></table></figure><p>我悟了：类的继承一般都是object，然后如果想要继承自己的类，则可以把object继承对象改一下，原来类名后括号里的东西是继承对象</p><p>一定要用 <code>super(Cat, self).__init__(name,age)</code> 去初始化父类，否则，继承自 Animal的 Cat子类将没有 <code>name</code> 和 <code>age</code> 两个属性</p><p>函数<code>super(Cat, self)</code>将返回当前类继承的父类，即 Animal，然后调用<code>__init__()</code>方法，注意self参数已在<code>super()</code>中传入，在<code>__init__()</code>中将隐式传递，不能再写出self</p><h3 id="9-7-2-Python对子类方法的重构"><a href="#9-7-2-Python对子类方法的重构" class="headerlink" title="9.7.2 Python对子类方法的重构"></a>9.7.2 Python对子类方法的重构</h3><p>子类中的方法要求跟父类中的方法不同时，可以在子类中重构方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Cat</span>(<span class="title class_ inherited__">Animal</span>):</span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, age, sex</span>):</span><br><span class="line">       <span class="built_in">super</span>(Cat, self).__init__(name,age)</span><br><span class="line">       self.sex = sex</span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self</span>):</span><br><span class="line">       <span class="built_in">print</span>(self.name,<span class="string">&#x27;会“喵喵”叫&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">   c = Cat(<span class="string">&#x27;喵喵&#x27;</span>, <span class="number">2</span>, <span class="string">&#x27;男&#x27;</span>)</span><br><span class="line">   c.call()  <span class="comment"># 输出：喵喵 会“喵喵”叫</span></span><br></pre></td></tr></table></figure><p>当我们在子类中重构父类的方法后，Cat子类的实例先会在自己的类Cat中查找该方法，当找不到该方法时才会去父类Animal中查找对应的方法</p><h3 id="9-7-3-Python中子类与父类的关系"><a href="#9-7-3-Python中子类与父类的关系" class="headerlink" title="9.7.3 Python中子类与父类的关系"></a>9.7.3 Python中子类与父类的关系</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Animal</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">   <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Cat</span>(<span class="title class_ inherited__">Animal</span>):</span><br><span class="line">   <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">A= Animal()</span><br><span class="line">C = Cat()</span><br></pre></td></tr></table></figure><ul><li>“A”是Animal类的实例，但，“A”不是Cat类的实例。</li><li>“C”是Animal类的实例，“C”也是Cat类的实例。</li></ul><p>函数 <code>isinstance(变量,类型)</code><br>判断变量的类型，判断对象之间的关系</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;&quot;A&quot; IS Animal?&#x27;</span>, <span class="built_in">isinstance</span>(A, Animal))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;&quot;A&quot; IS Cat?&#x27;</span>, <span class="built_in">isinstance</span>(A, Cat))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;&quot;C&quot; IS Animal?&#x27;</span>, <span class="built_in">isinstance</span>(C, Animal))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;&quot;C&quot; IS Cat?&#x27;</span>, <span class="built_in">isinstance</span>(C, Cat))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="string">&quot;A&quot;</span> IS Animal? <span class="literal">True</span></span><br><span class="line"><span class="string">&quot;A&quot;</span> IS Cat? <span class="literal">False</span></span><br><span class="line"><span class="string">&quot;C&quot;</span> IS Animal? <span class="literal">True</span></span><br><span class="line"><span class="string">&quot;C&quot;</span> IS Cat? <span class="literal">True</span></span><br></pre></td></tr></table></figure><h3 id="9-7-4-python中多态"><a href="#9-7-4-python中多态" class="headerlink" title="9.7.4 python中多态"></a>9.7.4 python中多态</h3><p>类具有继承关系，并且子类类型可以向上转型看做父类类型，如果我们从 Animal派生出 Cat和Dog，并都写了一个 call() 方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Animal</span>(<span class="title class_ inherited__">object</span>):  </span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, age</span>):</span><br><span class="line">       self.name = name</span><br><span class="line">       self.age = age</span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self</span>):</span><br><span class="line">       <span class="built_in">print</span>(self.name, <span class="string">&#x27;会叫&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Cat</span>(<span class="title class_ inherited__">Animal</span>):</span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, age, sex</span>):</span><br><span class="line">       <span class="built_in">super</span>(Cat, self).__init__(name, age)</span><br><span class="line">       self.sex = sex</span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self</span>):</span><br><span class="line">       <span class="built_in">print</span>(self.name, <span class="string">&#x27;会“喵喵”叫&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dog</span>(<span class="title class_ inherited__">Animal</span>):</span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name, age, sex</span>):</span><br><span class="line">       <span class="built_in">super</span>(Dog, self).__init__(name, age)</span><br><span class="line">       self.sex = sex</span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self</span>):</span><br><span class="line">       <span class="built_in">print</span>(self.name, <span class="string">&#x27;会“汪汪”叫&#x27;</span>)</span><br></pre></td></tr></table></figure><p>我们定义一个do函数，接收一个变量 ‘all’,如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">do</span>(<span class="params"><span class="built_in">all</span></span>):</span><br><span class="line">   <span class="built_in">all</span>.call()</span><br><span class="line"></span><br><span class="line">A = Animal(<span class="string">&#x27;小黑&#x27;</span>,<span class="number">4</span>)</span><br><span class="line">C = Cat(<span class="string">&#x27;喵喵&#x27;</span>, <span class="number">2</span>, <span class="string">&#x27;男&#x27;</span>)</span><br><span class="line">D = Dog(<span class="string">&#x27;旺财&#x27;</span>, <span class="number">5</span>, <span class="string">&#x27;女&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> (A,C,D):</span><br><span class="line">   do(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="comment"># 小黑 会叫</span></span><br><span class="line"><span class="comment"># 喵喵 会“喵喵”叫</span></span><br><span class="line"><span class="comment"># 旺财 会“汪汪”叫</span></span><br></pre></td></tr></table></figure><p>这种行为称为多态。也就是说，方法调用将作用在 all 的实际类型上。C 是 Cat 类型，它实际上拥有自己的 call() 方法以及从 Animal 继承的 call 方法<br>而调用 C .call() 总是先查找它自身的定义，如果没有定义，则顺着继承链向上查找，直到在某个父类中找到为止</p><blockquote><p>注意事项</p><ul><li>在继承中基类的构造方法（<code>__init__()方法</code>）不会被自动调用，它需要在其派生类的构造方法中亲自专门调用。</li><li>在调用基类的方法时，需要加上基类的类名前缀，且需要带上self参数变量。而在类中调用普通函数时并不需要带上self参数</li><li>Python总是首先查找对应类的方法，如果它不能在派生类中找到对应的方法，它才开始到基类中逐个查找。（先在本类中查找调用的方法，找不到才去基类中找）</li></ul></blockquote><h1 id="10-几个python编写技巧"><a href="#10-几个python编写技巧" class="headerlink" title="10. 几个python编写技巧"></a>10. 几个python编写技巧</h1><h2 id="10-1-变量的交换"><a href="#10-1-变量的交换" class="headerlink" title="10.1 变量的交换"></a>10.1 变量的交换</h2><p><code>a,b = b,a</code></p><h2 id="10-2-字符串格式化"><a href="#10-2-字符串格式化" class="headerlink" title="10.2 字符串格式化"></a>10.2 字符串格式化</h2><p><code>print(&quot;Hi, I&#39;m %s . I&#39;m from %s . And I&#39;m %d&quot; % (name,country,age))</code><br><code>print(&quot;Hi, I&#39;m &#123;&#125; . I&#39;m from &#123;&#125; . And I&#39;m &#123;&#125;&quot;.format(name,country,age))</code><br><code>print(f&quot;Hi, I&#39;m &#123;name&#125; . I&#39;m from &#123;country&#125; . And I&#39;m &#123;age+1&#125;&quot;</code></p><h2 id="10-3-Yield语法"><a href="#10-3-Yield语法" class="headerlink" title="10.3 Yield语法"></a>10.3 Yield语法</h2><p>yield不需要整个列表生成完毕后再输出，可以一个一个输出</p><p>每当一个数据生成时，可以直接输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fibonacci</span>(<span class="params">n</span>):</span><br><span class="line">    a = <span class="number">0</span></span><br><span class="line">    b = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">yield</span> a</span><br><span class="line">        a, b = b, a+b</span><br><span class="line">    <span class="keyword">return</span> nums</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> fibonacci(<span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure><h2 id="10-4-map和zip的使用"><a href="#10-4-map和zip的使用" class="headerlink" title="10.4 map和zip的使用"></a>10.4 map和zip的使用</h2><p><code>map(function,sequence)</code><br>对序列sequence中每个元素都执行函数function操作，如<code>map(str,mylist)</code>：将列表中的每一项转换成字符串。<br>list()将每一项转换成列表</p><p><code>zip(*list)</code>返回的是一个元组，转置<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span> = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]]</span><br><span class="line">t = <span class="built_in">zip</span>(*<span class="built_in">list</span>)</span><br><span class="line"><span class="built_in">print</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[(<span class="number">1</span>, <span class="number">4</span>, <span class="number">7</span>), (<span class="number">2</span>, <span class="number">5</span>, <span class="number">8</span>), (<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>)]</span><br><span class="line"></span><br><span class="line">x = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">y = [<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br><span class="line">a = <span class="built_in">zip</span>(x,y)</span><br><span class="line"><span class="built_in">print</span> a</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">[(<span class="number">1</span>, <span class="number">6</span>), (<span class="number">2</span>, <span class="number">7</span>), (<span class="number">3</span>, <span class="number">8</span>), (<span class="number">4</span>, <span class="number">9</span>), (<span class="number">5</span>, <span class="number">10</span>)]</span><br></pre></td></tr></table></figure></p><p>eg：将多个列表合并创建json数组</p><p>new_list = list(map(list, zip(address, temp)))<br>jsonify({<br>    ‘data’: new_list<br>})</p><h1 id="11-装饰器"><a href="#11-装饰器" class="headerlink" title="11. 装饰器"></a>11. 装饰器</h1><blockquote><p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017451662295584">装饰器 - 廖雪峰的官方网站 (liaoxuefeng.com)</a></p></blockquote><p>eg：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import time </span><br><span class="line">import functools</span><br><span class="line"></span><br><span class="line">def metric(fn):</span><br><span class="line">    # print(&#x27;%s executed in %s ms&#x27; % (fn.__name__, 10.24))</span><br><span class="line">    @functools.wraps(fn)</span><br><span class="line">    def wrapper(*args, **kw):</span><br><span class="line">        start = time.time()</span><br><span class="line">        res = fn(*args, **kw)</span><br><span class="line">        end = time.time()</span><br><span class="line">        print(&#x27;&#123;:s&#125; executed in &#123;:5f&#125; ms, &#x27; &#x27;and the result is &#123;:d&#125;&#x27;.format(fn.__name__, (end - start), res))</span><br><span class="line">        return fn(*args, **kw)</span><br><span class="line">    return wrapper</span><br><span class="line"></span><br><span class="line"># 测试</span><br><span class="line">@metric</span><br><span class="line">def fast(x, y):</span><br><span class="line">    time.sleep(0.0012)</span><br><span class="line">    return x + y;</span><br><span class="line"></span><br><span class="line">@metric</span><br><span class="line">def slow(x, y, z):</span><br><span class="line">    time.sleep(0.1234)</span><br><span class="line">    return x * y * z;</span><br><span class="line"></span><br><span class="line">f = fast(11, 22)</span><br><span class="line">s = slow(11, 22, 33)</span><br><span class="line">if f != 33:</span><br><span class="line">    print(&#x27;测试失败!&#x27;)</span><br><span class="line">elif s != 7986:</span><br><span class="line">    print(&#x27;测试失败!&#x27;)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">print(fast.__name__)</span><br><span class="line">如果有@functools.wraps(fn)，则返回fast</span><br><span class="line">如果没有@functools.wraps(fn)，则返回wrapper</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">print(fast.__name__)</span><br><span class="line"></span><br><span class="line">output: </span><br><span class="line">fast executed in 0.013472 ms, and the result is 33</span><br><span class="line">slow executed in 0.124733 ms, and the result is 7986</span><br><span class="line">fast</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def log(func):</span><br><span class="line">    def wrapper(*args, **kw):</span><br><span class="line">        print(&#x27;call %s():&#x27; % func.__name__)</span><br><span class="line">        return func(*args, **kw)</span><br><span class="line">    return wrapper</span><br><span class="line"></span><br><span class="line">@log</span><br><span class="line">def now():</span><br><span class="line">    print(&#x27;2015-3-25&#x27;)</span><br><span class="line"></span><br><span class="line">执行：now()</span><br><span class="line">output:</span><br><span class="line">call now():</span><br><span class="line">2015-3-25</span><br></pre></td></tr></table></figure><p>把<code>@log</code>放到<code>now()</code>函数的定义处，相当于执行了语句：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">now = log(now)</span><br></pre></td></tr></table></figure><p>由于log()是一个decorator，返回一个函数，所以，原来的now()函数仍然存在，只是现在同名的now变量指向了新的函数，于是调用now()将执行新函数，即在log()函数中返回的wrapper()函数。</p><h2 id="装饰器需要参数"><a href="#装饰器需要参数" class="headerlink" title="装饰器需要参数"></a>装饰器需要参数</h2><p>如果decorator本身需要传入参数，那就需要编写一个返回decorator的高阶函数，写出来会更复杂。比如，要自定义log的文本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def log(text):</span><br><span class="line">    def decorator(func):</span><br><span class="line">        def wrapper(*args, **kw):</span><br><span class="line">            print(&#x27;%s %s():&#x27; % (text, func.__name__))</span><br><span class="line">            return func(*args, **kw)</span><br><span class="line">        return wrapper</span><br><span class="line">    return decorator</span><br></pre></td></tr></table></figure><p>这个3层嵌套的decorator用法如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">@log(&#x27;execute&#x27;)</span><br><span class="line">def now():</span><br><span class="line">    print(&#x27;2015-3-25&#x27;)</span><br></pre></td></tr></table></figure><p>执行结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; now()</span><br><span class="line">execute now():</span><br><span class="line">2015-3-25</span><br></pre></td></tr></table></figure><p>和两层嵌套的decorator相比，3层嵌套的效果是这样的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; now = log(&#x27;execute&#x27;)(now)</span><br></pre></td></tr></table></figure><h2 id="原始函数属性复制到装饰后的函数中"><a href="#原始函数属性复制到装饰后的函数中" class="headerlink" title="原始函数属性复制到装饰后的函数中"></a>原始函数属性复制到装饰后的函数中</h2><p>以上两种decorator的定义都没有问题，但还差最后一步。因为我们讲了函数也是对象，它有<code>__name__</code>等属性，但你去看经过decorator装饰之后的函数，它们的<code>__name__</code>已经从原来的<code>&#39;now&#39;</code>变成了<code>&#39;wrapper&#39;</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; now.__name__</span><br><span class="line">&#x27;wrapper&#x27;</span><br></pre></td></tr></table></figure><p>因为返回的那个<code>wrapper()</code>函数名字就是<code>&#39;wrapper&#39;</code>，所以，需要把原始函数的<code>__name__</code>等属性复制到<code>wrapper()</code>函数中，否则，有些依赖函数签名的代码执行就会出错。</p><p>不需要编写<code>wrapper.__name__ = func.__name__</code>这样的代码，Python内置的<code>functools.wraps</code>就是干这个事的，所以，一个完整的decorator的写法如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import functools</span><br><span class="line"></span><br><span class="line">def log(func):</span><br><span class="line">    @functools.wraps(func)</span><br><span class="line">    def wrapper(*args, **kw):</span><br><span class="line">        print(&#x27;call %s():&#x27; % func.__name__)</span><br><span class="line">        return func(*args, **kw)</span><br><span class="line">    return wrapper</span><br></pre></td></tr></table></figure><p>或者针对带参数的decorator：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import functools</span><br><span class="line"></span><br><span class="line">def log(text):</span><br><span class="line">    def decorator(func):</span><br><span class="line">        @functools.wraps(func)</span><br><span class="line">        def wrapper(*args, **kw):</span><br><span class="line">            print(&#x27;%s %s():&#x27; % (text, func.__name__))</span><br><span class="line">            return func(*args, **kw)</span><br><span class="line">        return wrapper</span><br><span class="line">    return decorator</span><br></pre></td></tr></table></figure><p><code>import functools</code>是导入<code>functools</code>模块。模块的概念稍候讲解。现在，只需记住在定义<code>wrapper()</code>的前面加上<code>@functools.wraps(func)</code>即可。</p>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown基本知识</title>
      <link href="/Learn/Learn-Markdown/"/>
      <url>/Learn/Learn-Markdown/</url>
      
        <content type="html"><![CDATA[<p>关于我使用 Markdown 以来学到的知识，即markdown的操作方法<br><span id="more"></span></p><h2 id="1-空格"><a href="#1-空格" class="headerlink" title="1. 空格"></a>1. 空格</h2><div class="table-container"><table><thead><tr><th style="text-align:center">语法</th><th style="text-align:center">example</th><th style="text-align:center">解释</th></tr></thead><tbody><tr><td style="text-align:center"><code>ok&amp;nbsp;ok</code>&nbsp;</td><td style="text-align:center">ok&nbsp;ok</td><td style="text-align:center">它是按下 space 键产生的空格,叫不换行空格，全称是 No-Break Space</td></tr><tr><td style="text-align:center"><code>ok&amp;ensp;ok</code>&ensp;</td><td style="text-align:center">ok&ensp;ok</td><td style="text-align:center">等同于字体度的一半,叫“半角空格”，全称是 En Space</td></tr><tr><td style="text-align:center"><code>ok&amp;emsp;ok</code>&emsp;</td><td style="text-align:center">ok&emsp;ok</td><td style="text-align:center">1 em 在 16px 的字体中就是 16px,它叫“全角空格”，全称是 Em Space</td></tr><tr><td style="text-align:center"><code>ok&amp;thinsp;ok</code>&thinsp;</td><td style="text-align:center">ok&thinsp;ok</td><td style="text-align:center">“窄空格”，全称是 Thin Space。占据的宽度比较小。它是 em 之六分之一宽</td></tr><tr><td style="text-align:center"><code>ok&amp;zwj;ok</code>&zwj;</td><td style="text-align:center">ok&zwj;ok</td><td style="text-align:center">它叫零宽连字，全称是 Zero Width Joiner，简称“ZWJ”，是一个不打印字符</td></tr></tbody></table></div><blockquote><p>参考<a href="https://www.jianshu.com/p/31eade263e7a" title="简书">网站</a></p></blockquote><h2 id="2-链接"><a href="#2-链接" class="headerlink" title="2. 链接"></a>2. 链接</h2><h3 id="2-1-行内式-方便简洁"><a href="#2-1-行内式-方便简洁" class="headerlink" title="2.1 行内式(方便简洁)"></a>2.1 行内式(方便简洁)</h3><p><code>[链接文字](链接网址 &quot;标题&quot;)</code><br><code>This is an [example link](https://www.jianshu.com/p/31eade263e7a)</code><br>会显示为:&nbsp;This is an <a href="https://www.jianshu.com/p/31eade263e7a">example link</a></p><h3 id="2-2-锚点（有了目录的话，基本不用了）"><a href="#2-2-锚点（有了目录的话，基本不用了）" class="headerlink" title="2.2 锚点（有了目录的话，基本不用了）"></a>2.2 锚点（有了目录的话，基本不用了）</h3><ul><li>第一种</li></ul><p><code>- [测试](#测试)</code><br><code>### &lt;a id=&quot;测试&quot;&gt;测试&lt;/a&gt;</code></p><ul><li><a href="#测试">测试</a></li></ul><ul><li>第二种</li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">### <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;#测试2&quot;</span>&gt;</span>测试2<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">h</span><br><span class="line">t</span><br><span class="line">m</span><br><span class="line">l</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">id</span>=<span class="string">&quot;测试2&quot;</span>&gt;</span>测试2<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><a href="#测试2">测试 2</a></li></ul><blockquote><p>参考<a href="https://blog.csdn.net/wangzhibo666/article/details/88731227" title="CSDN">link</a></p></blockquote><h2 id="3-图片"><a href="#3-图片" class="headerlink" title="3. 图片"></a>3. 图片</h2><h3 id="3-1-网络图片的添加"><a href="#3-1-网络图片的添加" class="headerlink" title="3.1 网络图片的添加"></a>3.1 网络图片的添加</h3><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">![<span class="string">alt 属性文本</span>](<span class="link">图片地址</span>)</span><br><span class="line"></span><br><span class="line">![<span class="string">alt 属性文本</span>](<span class="link">图片地址 &quot;可选标题&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![四月是你的谎言](https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=905665511,4125694826&amp;fm=26&amp;gp=0.jpg &quot;四谎&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=905665511,4125694826&amp;fm=26&amp;gp=0.jpg" alt="四月是你的谎言" title="四谎"></p><h3 id="3-2-本地图片的添加"><a href="#3-2-本地图片的添加" class="headerlink" title="3.2 本地图片的添加"></a>3.2 本地图片的添加</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">![avatar](/img/picture_exam.jpeg &quot;example&quot;)</span><br><span class="line">--or--</span><br><span class="line">&lt;!-- &lt;img src=&quot;/img/picture_exam.jpeg &quot; width = 10% height = 10% div align=right /&gt; --&gt;</span><br><span class="line">或者是</span><br><span class="line">&lt;!-- &lt;img src=&quot;url&quot; width = &quot;100&quot; height = &quot;100&quot; div align=center /&gt; --&gt;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/main/pictures/picture_exam.jpeg" alt="picture_exam.jpeg"></p><p><code>![avatar](/img/picture_exam.jpeg &quot;example&quot;)</code><br><code>&lt;img src=&quot;/img/picture_exam.jpeg &quot; width = 10% height = 10% div align=center /&gt;</code></p><h2 id="4-表格"><a href="#4-表格" class="headerlink" title="4. 表格"></a>4. 表格</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">| 左对齐 | 右对齐 | 居中对齐 |</span><br><span class="line">| :----- | -----: | :------: |</span><br><span class="line">| 单元格 | 单元格 |  单元格  |</span><br><span class="line">| 单元格 | 单元格 |  单元格  |</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th style="text-align:left">左对齐</th><th style="text-align:right">右对齐</th><th style="text-align:center">居中对齐</th></tr></thead><tbody><tr><td style="text-align:left">单元格</td><td style="text-align:right">单元格</td><td style="text-align:center">单元格</td></tr><tr><td style="text-align:left">单元格</td><td style="text-align:right">单元格</td><td style="text-align:center">单元格</td></tr></tbody></table></div><blockquote><p>参考教程:<a href="https://www.runoob.com/markdown/md-tutorial.html">markdown</a></p></blockquote><h2 id="5-设置字体大小颜色"><a href="#5-设置字体大小颜色" class="headerlink" title="5. 设置字体大小颜色"></a>5. 设置字体大小颜色</h2><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;黑体&quot;</span>&gt;</span>我是黑体字<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;微软雅黑&quot;</span>&gt;</span>我是微软雅黑<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;STCAIYUN&quot;</span>&gt;</span>我是华文彩云<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">#0099ff</span> <span class="attr">size</span>=<span class="string">7</span> <span class="attr">face</span>=<span class="string">&quot;黑体&quot;</span>&gt;</span>color=#0099ff size=72 face=&quot;黑体&quot;<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">#00ffff</span> <span class="attr">size</span>=<span class="string">72</span>&gt;</span>color=#00ffff<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">gray</span> <span class="attr">size</span>=<span class="string">72</span>&gt;</span>color=gray<span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br></pre></td></tr></table></figure><font face="黑体">我是黑体字</font><font face="微软雅黑">我是微软雅黑</font><font face="STCAIYUN">我是华文彩云</font><font color=#0099ff size=7 face="黑体">color=#0099ff size=72 face="黑体"</font><font color=#00ffff size=72>color=#00ffff</font><font color=gray size=72>color=gray</font><ul><li><p>测试用的锚点：</p><ul><li><a id="测试">锚点测试</a></li><li><a id="测试2">锚点测试 2</a></li></ul><blockquote><p>参考网站:<a href="https://blog.csdn.net/weixin_37998647/article/details/79428290" title="CSDN">CSDN</a></p></blockquote></li></ul><h2 id="6-待办清单"><a href="#6-待办清单" class="headerlink" title="6. 待办清单"></a>6. 待办清单</h2><p>可能是我的这个hexo主题不适配这种语法，但是再typora中就可以显示<br><em>有的markdown支持有的不支持</em></p><ul><li>[ ] 任务一</li><li>[x] 任务二</li></ul><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> [ ] 任务一</span><br><span class="line"><span class="bullet">-</span> [x] 任务二</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spider-Note</title>
      <link href="/Learn/Learn-Spider%E7%88%AC%E8%99%AB/"/>
      <url>/Learn/Learn-Spider%E7%88%AC%E8%99%AB/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考 Github 上的<a href="https://github.com/kingname/SourceCodeOfBook" title="Github">教程</a>学习<br>一个连 python 都没有完全学会的菜鸡来学爬虫</p></blockquote><span id="more"></span><h1 id="1-线程"><a href="#1-线程" class="headerlink" title="1. 线程"></a>1. 线程</h1><h2 id="1-1-线程-Pool"><a href="#1-1-线程-Pool" class="headerlink" title="1.1 线程 Pool"></a>1.1 线程 Pool</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing.dummy <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_power2</span>(<span class="params">num</span>):</span><br><span class="line"><span class="keyword">return</span> num*num</span><br><span class="line">pool = Pool(<span class="number">5</span>)</span><br><span class="line">origin_num = [x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">result = pool.<span class="built_in">map</span>(calc_power2,origin_num)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;0~9的平方分别为：<span class="subst">&#123;result&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><code>Pool(5)</code> &emsp;五个线程</p><hr/><h2 id="1-2-所用函数"><a href="#1-2-所用函数" class="headerlink" title="1.2 所用函数"></a>1.2 所用函数</h2><p><code>time.time()</code> &nbsp; 程序当前时间<br>eg：用来对比单线程和多线程访问 baidu 的速度</p><h1 id="2-request-库"><a href="#2-request-库" class="headerlink" title="2. request 库"></a>2. request 库</h1><h2 id="2-1-基础用法"><a href="#2-1-基础用法" class="headerlink" title="2.1 基础用法"></a>2.1 基础用法</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&#x27;&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&quot;UserAgent&quot;</span>:<span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like\ Gecko) Chrome/80.0.3987.87 Safari/537.36 Edg/80.0.361.48&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url,headers=headers)</span><br><span class="line">response.encoding = <span class="string">&#x27;utf-8&#x27;</span>  <span class="comment">#或者GBK</span></span><br><span class="line">html = response.text</span><br></pre></td></tr></table></figure><h2 id="2-2-进阶用法"><a href="#2-2-进阶用法" class="headerlink" title="2.2 进阶用法"></a>2.2 进阶用法</h2><ul><li><strong>使用 requests 模拟发送 get 请求</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://exercise.kingname.info/ajax_1_backend&#x27;</span></span><br><span class="line">html = requests.get(url).content.decode()</span><br><span class="line"><span class="built_in">print</span>(html)</span><br><span class="line"><span class="comment"># 如果你看到这一段文字，说明你已经成功访问了这个页面,并获取了GET方式的异步加载数据。</span></span><br></pre></td></tr></table></figure><ul><li><strong>使用 requests 模拟发送 post 请求</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://exercise.kingname.info/ajax_1_postbackend&#x27;</span></span><br><span class="line">html = requests.post(url,json=&#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;yunq&#x27;</span>,<span class="string">&#x27;age&#x27;</span>:<span class="number">24</span>&#125;).content.decode()</span><br><span class="line"><span class="built_in">print</span>(html)</span><br><span class="line"><span class="comment"># 如果你看到这一段文字，说明你已经成功访问了这个页面，并获取了POST方式的异步加载数据。你向服务器提交的两个参数，分别为name： yunq, age：24</span></span><br></pre></td></tr></table></figure><blockquote><p>参考<a href="http://exercise.kingname.info/exercise_ajax_1.html">学习网站</a>，(<a href="#异步GET与POST请求">异步 GET 与 POST 请求</a>)</p></blockquote><h1 id="3-re-库"><a href="#3-re-库" class="headerlink" title="3. re 库"></a>3. re 库</h1><h2 id="3-1-基础用法"><a href="#3-1-基础用法" class="headerlink" title="3.1 基础用法"></a>3.1 基础用法</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">re.findall(<span class="string">r&#x27;&#x27;</span>,html,re.S)   <span class="comment">#返回一个列表，这是一个列表所以可以取第一个数据</span></span><br><span class="line">                            <span class="comment">#re.findall(r&#x27;&#x27;,html,re.S)[0]</span></span><br><span class="line"></span><br><span class="line">re.search(<span class="string">r&#x27;&#x27;</span>,html,re.S)    <span class="comment">#返回一个re.Match类型数据</span></span><br><span class="line">                            <span class="comment">#&lt;re.Match object; span=(214, 297), match=&#x27;secret = \&#x27;&#123;&quot;code&quot;: &quot;\\u884c\\u52a8\\u4ee3\\u53f7&gt;</span></span><br><span class="line"></span><br><span class="line">re.search(<span class="string">r&#x27;href=&quot;sf&quot;&gt;(.*?)&lt;&#x27;</span>).group()</span><br><span class="line">                            <span class="comment">#返回一个字符串             #.*?是匹配到的内容</span></span><br><span class="line">                            <span class="comment">#group()返回的是&#x27;&#x27;内的字符串内容:href=&quot;sf&quot;&gt;(.*?)&lt;</span></span><br><span class="line">                            <span class="comment">#group(1)返回的是()中的字符串内容:.*?</span></span><br><span class="line">                            <span class="comment">#如果(.*?)有多个，则使用group(1),group(2)........</span></span><br></pre></td></tr></table></figure><h1 id="4-正则表达式"><a href="#4-正则表达式" class="headerlink" title="4. 正则表达式"></a>4. 正则表达式</h1><p><code>.*?</code></p><h1 id="5-Xpath—lxml-库"><a href="#5-Xpath—lxml-库" class="headerlink" title="5. Xpath—lxml 库"></a>5. Xpath—lxml 库</h1><ul><li>XPath 是一种查询语言，能从 XML\HTML 的树状结构中寻找节点</li></ul><h2 id="5-1-XPath-语法"><a href="#5-1-XPath-语法" class="headerlink" title="5.1 XPath 语法"></a>5.1 XPath 语法</h2><h3 id="5-1-1-XPath-语法解析"><a href="#5-1-1-XPath-语法解析" class="headerlink" title="5.1.1 XPath 语法解析"></a>5.1.1 XPath 语法解析</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">example_html</span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>测试<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;useful&quot;</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;info&quot;</span>&gt;</span>我需要的信息1<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;info&quot;</span>&gt;</span>我需要的信息2<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;info&quot;</span>&gt;</span>我需要的信息3<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;useless&quot;</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;info&quot;</span>&gt;</span>垃圾1<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;info&quot;</span>&gt;</span>垃圾2<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>info = selector.xpath(&#39;//div[@class=&quot;useful&quot;]/ul/li/text()&#39;)</code><br>就可以提取出 class=”userful”中的三句话，返回一个列表</p><h3 id="5-1-2-基本框架"><a href="#5-1-2-基本框架" class="headerlink" title="5.1.2 基本框架"></a>5.1.2 基本框架</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lxml.html</span><br><span class="line">selector = lxml.html.fromstring(<span class="string">&#x27;网页源代码&#x27;</span>)    <span class="comment">#网页源代码可用requests来获取</span></span><br><span class="line">info = selector.xpath(<span class="string">&#x27;一段XPath语句&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="5-1-3-example"><a href="#5-1-3-example" class="headerlink" title="5.1.3 example"></a>5.1.3 example</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lxml.html</span><br><span class="line"></span><br><span class="line">source = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;html&gt;</span></span><br><span class="line"><span class="string">    &lt;head&gt;</span></span><br><span class="line"><span class="string">         &lt;title&gt;测试&lt;/title&gt;</span></span><br><span class="line"><span class="string">    &lt;/head&gt;</span></span><br><span class="line"><span class="string">    &lt;body&gt;</span></span><br><span class="line"><span class="string">         &lt;div class=&quot;useful&quot;&gt;</span></span><br><span class="line"><span class="string"> &lt;ul&gt;</span></span><br><span class="line"><span class="string">                   &lt;li class=&quot;info&quot;&gt;我需要的信息1&lt;/li&gt;</span></span><br><span class="line"><span class="string">                   &lt;li class=&quot;info&quot;&gt;我需要的信息2&lt;/li&gt;</span></span><br><span class="line"><span class="string">                   &lt;li class=&quot;info&quot;&gt;我需要的信息3&lt;/li&gt;</span></span><br><span class="line"><span class="string"> &lt;/ul&gt;</span></span><br><span class="line"><span class="string">         &lt;/div&gt;</span></span><br><span class="line"><span class="string">         &lt;div class=&quot;useless&quot;&gt;</span></span><br><span class="line"><span class="string">              &lt;ul&gt;</span></span><br><span class="line"><span class="string">                   &lt;li class=&quot;info&quot;&gt;垃圾1&lt;/li&gt;</span></span><br><span class="line"><span class="string">                   &lt;li class=&quot;info&quot;&gt;垃圾2&lt;/li&gt;</span></span><br><span class="line"><span class="string">              &lt;/ul&gt;</span></span><br><span class="line"><span class="string">         &lt;/div&gt;</span></span><br><span class="line"><span class="string">     &lt;/body&gt;</span></span><br><span class="line"><span class="string">&lt;/html&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">selector = lxml.html.fromstring(source)</span><br><span class="line">info = selector.xpath(<span class="string">&#x27;//div[@class=&quot;useful&quot;]/ul/li/text()&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(info)         <span class="comment">#[&#x27;我需要的信息1&#x27;, &#x27;我需要的信息2&#x27;, &#x27;我需要的信息3&#x27;]</span></span><br></pre></td></tr></table></figure><hr/><ul><li>a.<strong>XPath 语句格式</strong><br><code>info = selector.xpath(&#39;一段XPath语句&#39;)</code>中’一段 XPath 语句’的格式<br>核心思想：XPath 就是写地址<br>获取文本：<code>//标签1[@属性1=&quot;属性值1&quot;]/标签2[@属性2=&quot;属性值2&quot;]/..../text()</code><br>获取属性值：<code>//标签1[@属性1=&quot;属性值1&quot;]/标签2[@属性2=&quot;属性值2&quot;]/..../@属性n</code><br>其中的<code>[@属性=&quot;属性值&quot;]</code>不是必需的，其作用是帮助过滤相同的标签，无相同标签可省略</li><li>b.<strong>标签 1 的选取</strong><br>标签 1 可以直接从 html 这个最外层的标签开始，一层一层往下找，这个时候，XPath 语句是这样的：<br><code>/html/body/div[@class=&quot;useful&quot;]/ul/li/text()</code><br>但是由于前面的’/html/body’是所有 HTML 通用的，而且没有属性，所以可不写，即带属性标签前的标签都可以省略</li><li>c.<strong>可以省略的属性</strong><br>1、本身标签没有属性<br>2、这个标签所有的属性值相同</li><li>d.<strong>XPath 的特殊情况</strong><br>1、以相同字符串开头<code>标签[starts-with(@属性,&quot;开头字符串&quot;)]</code></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="built_in">id</span>=<span class="string">&quot;test-1&quot;</span>&gt;需要的内容<span class="number">1</span>&lt;/div&gt;</span><br><span class="line">&lt;div <span class="built_in">id</span>=<span class="string">&quot;test-2&quot;</span>&gt;需要的内容<span class="number">2</span>&lt;/div&gt;</span><br><span class="line">&lt;div <span class="built_in">id</span>=<span class="string">&quot;testfault&quot;</span>&gt;需要的内容<span class="number">3</span>&lt;/div&gt;</span><br><span class="line">&lt;div <span class="built_in">id</span>=<span class="string">&quot;useless&quot;</span>&gt;这是我不需要的内容&lt;/div&gt;</span><br><span class="line">content = selector.xpath(<span class="string">&#x27;//div[starts-with(@id,&quot;test&quot;)]/text()&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)      <span class="comment">#[&#x27;需要的内容1&#x27;,&#x27;需要的内容3&#x27;,&#x27;需要的内容2&#x27;]</span></span><br></pre></td></tr></table></figure><p>2、属性值包含相同字符串<code>标签[contains(@属性,&quot;相同字符串&quot;)]</code><br>3、对 XPath 返回的对象执行 XPath</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">useful = selector.xpath(<span class="string">&#x27;//div[@class=&quot;useful&quot;]&#x27;</span>)   <span class="comment">#这里返回一个列表</span></span><br><span class="line">info_list = useful[<span class="number">0</span>].xpath(<span class="string">&#x27;ul/li/text()&#x27;</span>)         <span class="comment">#useful[0]即为列表中第一个数据</span></span><br><span class="line"><span class="built_in">print</span>(info_list)</span><br></pre></td></tr></table></figure><p>4、不同标签下的文字</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lxml.html</span><br><span class="line"></span><br><span class="line">html = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="string">&lt;html&gt;</span></span><br><span class="line"><span class="string">&lt;head lang=&quot;en&quot;&gt;</span></span><br><span class="line"><span class="string">    &lt;meta charset=&quot;UTF-8&quot;&gt;</span></span><br><span class="line"><span class="string">    &lt;title&gt;&lt;/title&gt;</span></span><br><span class="line"><span class="string">&lt;/head&gt;</span></span><br><span class="line"><span class="string">&lt;body&gt;</span></span><br><span class="line"><span class="string">&lt;div id=&quot;test3&quot;&gt;</span></span><br><span class="line"><span class="string">        我左青龙，</span></span><br><span class="line"><span class="string">        &lt;span id=&quot;tiger&quot;&gt;</span></span><br><span class="line"><span class="string">        右白虎，</span></span><br><span class="line"><span class="string">            &lt;ul&gt;上朱雀，</span></span><br><span class="line"><span class="string">                &lt;li&gt;下玄武。&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;/ul&gt;</span></span><br><span class="line"><span class="string">        老牛在当中，</span></span><br><span class="line"><span class="string">        &lt;/span&gt;</span></span><br><span class="line"><span class="string">        龙头在胸口。</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;/body&gt;</span></span><br><span class="line"><span class="string">&lt;/html&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#如果直接提取id=&quot;test3&quot;</span></span><br><span class="line">selector = lxml.html.fromstring(html)</span><br><span class="line">content_1 = selector.xpath(<span class="string">&#x27;//div[@id=&quot;test3&quot;]/text()&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content_1)   <span class="comment">#[&#x27;\n        我左青龙，\n        &#x27;, &#x27;\n        龙头在胸口。\n    &#x27;]</span></span><br><span class="line"><span class="comment">#只会提取到div标签中的文字信息，而不会自动提取子标签中的信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这时就需要用string(.)关键字了</span></span><br><span class="line">selector = lxml.html.fromstring(html)</span><br><span class="line">data = selector.xpath(<span class="string">&#x27;//div[@id=&quot;test3&quot;]&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">info = data.xpath(<span class="string">&#x27;string(.)&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(info)     <span class="comment">#就可以提取出所有的文本信息了</span></span><br></pre></td></tr></table></figure><h2 id="5-2-XPath-通过-chrome-辅助构造"><a href="#5-2-XPath-通过-chrome-辅助构造" class="headerlink" title="5.2 XPath 通过 chrome 辅助构造"></a>5.2 XPath 通过 chrome 辅助构造</h2><p>在一行源码单击右键，选择“Copy”→“Copy XPath”命令<br>把结果粘贴下来，可以看到如下的 XPath 语句：<br><code>//*[@id=&quot;thread_list&quot;]/li[2]/div/div[2]/div[1]/div[1]/a</code><br>_其中方括号中的数字，表示这是第几个该标签，但需要注意，这里的数字是从 1 开始_</p><h1 id="6-Beautiful-Soup4-库-BS4"><a href="#6-Beautiful-Soup4-库-BS4" class="headerlink" title="6. Beautiful Soup4 库(BS4)"></a>6. Beautiful Soup4 库(BS4)</h1><p>_BS4 在某些方面比 XPath 易懂，但是不如 XPath 简洁，而且由于它是使用 Python 开发的，因此速度比 XPath 慢。_<br>使用 Beautiful Soup4 提取 HTML 内容，一般要经过以下两步。</p><h2 id="6-1-bs4-处理步骤"><a href="#6-1-bs4-处理步骤" class="headerlink" title="6.1 bs4 处理步骤"></a>6.1 bs4 处理步骤</h2><ul><li>1)处理源代码生成 BeautifulSoup 对象。<br>解析源代码生成 BeautifulSoup 对象，使用以下代码：<br><code>soup = BeautifulSoup(网页源代码, &#39;解析器&#39;)</code><br>解析器：<br>这里的“解析器”，可以使用 html.parser：<br><code>soup = BeautifulSoup(source, &#39;html.parser&#39;)</code><br>如果安装了 lxml，还可以使用 lxml：<br><code>soup = BeautifulSoup(source, &#39;lxml&#39;)</code></li><li>2)使用 find<em>all()或者 find()来查找内容。<br>`soup.find(class</em>=’属性值’)`<br>_由于 HTML 中的 class 属性与 Python 的 class 关键字相同，因此为了不产生冲突，BS4 规定，如果遇到要查询 class 的情况，使用“class_”来代替_</li></ul><h2 id="6-2-example"><a href="#6-2-example" class="headerlink" title="6.2 example"></a>6.2 <a href="http://exercise.kingname.info/exercise_bs_1.html">example</a></h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = requests.get(<span class="string">&#x27;http://exercise.kingname.info/exercise_bs_1.html&#x27;</span>).content.decode()</span><br><span class="line"><span class="comment"># 1、解析源代码</span></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"><span class="comment"># 2、查找内容</span></span><br><span class="line">info = soup.find(class_= <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(info.string)      <span class="comment">#我需要的信息2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先抓大，再抓小</span></span><br><span class="line">useful = soup.find(class_=<span class="string">&#x27;useful&#x27;</span>)</span><br><span class="line">all_content = useful.find_all(<span class="string">&#x27;li&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> all_content:</span><br><span class="line">    <span class="built_in">print</span>(li.string)        <span class="comment">#我需要的信息1</span></span><br><span class="line">                            <span class="comment">#我需要的信息2</span></span><br><span class="line">                            <span class="comment">#我需要的信息3</span></span><br><span class="line">    <span class="built_in">print</span>(li)               <span class="comment">#&lt;li class=&quot;info&quot;&gt;我需要的信息1&lt;/li&gt;</span></span><br><span class="line">                            <span class="comment">#&lt;li class=&quot;test&quot;&gt;我需要的信息2&lt;/li&gt;</span></span><br><span class="line">                            <span class="comment">#&lt;li class=&quot;iamstrange&quot;&gt;我需要的信息3&lt;/li&gt;</span></span><br><span class="line">    <span class="built_in">print</span>(li[<span class="string">&#x27;class&#x27;</span>])      <span class="comment">#[&#x27;info&#x27;]</span></span><br><span class="line">                            <span class="comment">#[&#x27;test&#x27;]</span></span><br><span class="line">                            <span class="comment">#[&#x27;iamstrange&#x27;]</span></span><br></pre></td></tr></table></figure><ul><li>其他查找方法<br><strong>以‘我需要’为开头的信息</strong><br><code>content = soup.find_all(text = re.compile(&#39;我需要&#39;))</code><br><strong>对属性值搜素使用正则,即对 iamstrang 属性值搜索</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">content = soup.find_all(class_=re.<span class="built_in">compile</span>(<span class="string">&#x27;iam&#x27;</span>))[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(content.string)       <span class="comment">#我需要的信息3</span></span><br></pre></td></tr></table></figure><h1 id="7-异步加载与请求头"><a href="#7-异步加载与请求头" class="headerlink" title="7. 异步加载与请求头"></a>7. 异步加载与请求头</h1><h2 id="7-1-异步加载"><a href="#7-1-异步加载" class="headerlink" title="7.1 异步加载"></a>7.1 异步加载</h2><p>_异步加载：一个页面，点击后网址不变，页面改变_</p><h3 id="7-1-1-AJAX-技术"><a href="#7-1-1-AJAX-技术" class="headerlink" title="7.1.1 AJAX 技术"></a>7.1.1 AJAX 技术</h3><ul><li>AJAX 是 Asynchronous JavaScript And XML 的首字母缩写，意为异步 JavaScript 与 XML</li><li>使用 AJAX 技术，可以在不刷新网页的情况下更新网页数据。使用 AJAX 技术的网页，一般会使用 HTML 编写网页的框架。</li><li>在打开网页的时候，首先加载的是这个框架。剩下的部分将会在框架加载完成以后再通过 JavaScript 从后台加载。</li></ul><h3 id="7-1-2-JSON"><a href="#7-1-2-JSON" class="headerlink" title="7.1.2 JSON"></a>7.1.2 JSON</h3><ul><li>JSON 的全称是 JavaScript Object Notation，是一种轻量级的数据交换格式。网络之间使用 HTTP 方式传递数据的时候，绝大多数情况下传递的都是字符串。</li><li>因此，当需要把 Python 里面的数据发送给网页或者其他编程语言的时候，可以先将 Python 的数据转化为 JSON 格式的字符串，然后将字符串传递给其他语言，其他语言再将 JSON 格式的字符串转换为它自己的数据格式</li><li><strong>列表\字典与字符串相互转化</strong></li><li>_python 中字典 or 列表 与 json 格式字符串的相互转化_</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span> : <span class="string">&#x27;Connor&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sex&#x27;</span> : <span class="string">&#x27;boy&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span> : <span class="number">26</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">print</span>(data)     <span class="comment">#dict   #&#123;&#x27;name&#x27;: &#x27;Connor&#x27;, &#x27;sex&#x27;: &#x27;boy&#x27;, &#x27;age&#x27;: 26&#125;</span></span><br><span class="line">data1=json.dumps(data)</span><br><span class="line"><span class="built_in">print</span>(data1)    <span class="comment">#str    #&#123;&quot;name&quot;: &quot;Connor&quot;, &quot;sex&quot;: &quot;boy&quot;, &quot;age&quot;: 26&#125;</span></span><br><span class="line">data2=json.loads(data1)</span><br><span class="line"><span class="built_in">print</span>(data2)    <span class="comment">#dict   #&#123;&#x27;name&#x27;: &#x27;Connor&#x27;, &#x27;sex&#x27;: &#x27;boy&#x27;, &#x27;age&#x27;: 26&#125;</span></span><br><span class="line"><span class="comment"># 如果加上indent=4参数</span></span><br><span class="line">data3 = json.dumps(data,indent=<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(data3)    <span class="comment">#str</span></span><br><span class="line"><span class="comment">#结果更加的美观易读</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">    &quot;name&quot;: &quot;Connor&quot;,</span></span><br><span class="line"><span class="string">    &quot;sex&quot;: &quot;boy&quot;,</span></span><br><span class="line"><span class="string">    &quot;age&quot;: 26</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ul><li><strong>str=json.dumps(dict)</strong></li><li><strong>dict=json.loads(str)</strong></li></ul><h3 id="7-1-3-异步-GET-与-POST-请求"><a href="#7-1-3-异步-GET-与-POST-请求" class="headerlink" title="7.1.3 异步 GET 与 POST 请求"></a>7.1.3 异步 GET 与 POST 请求</h3><ul><li>使用异步加载技术的网站，被加载的内容是不能在源代码中找到的。</li><li>为了解决这个问题，就需要使用 Google Chrome 浏览器的开发者模式。在网页上单击右键，选择“检查”命令，然后定位到“Network”选项卡</li><li>接下来需要刷新网页。在 Windows 下，按 F5 键或者单击地址栏左边的“刷新”按钮</li><li>单击“Network”选项卡下面出现的“ajax_1_backend”和“ajax_1_postbackend”，并定位到“Response”选项卡，可以看到这里出现了网页上面的内容</li><li>再选择“Headers”选项卡，可以看到这个请求使用 GET 方式，发送到<a href="http://exercise.kingname.info/ajax_1_backend">http://exercise.kingname.info/ajax_1_backend</a></li><li>对于网页中的第 2 条内容，查看“Headers”选项卡，可以看到，这是使用 POST 方式向<a href="http://exercise.kingname.info/ajax_1_postbackend">http://exercise.kingname.info/ajax_1_postbackend</a> 发送请求，并以 JSON 格式提交数据<blockquote><p>具体代码实现看<em>request</em>&nbsp;&nbsp;的<strong><a href="#进阶用法">进阶用法</a></strong></p></blockquote></li></ul><h3 id="7-1-4-特殊的异步加载"><a href="#7-1-4-特殊的异步加载" class="headerlink" title="7.1.4 特殊的异步加载"></a>7.1.4 特殊的异步加载</h3><ul><li><strong><a href="http://exercise.kingname.info/exercise_ajax_2.html">练习页面</a></strong></li></ul><ul><li>伪装成异步加载的后端渲染,数据就在源代码里，但却不直接显示出来</li><li>源代码最下面的 JavaScript 代码，其中有一段：<br><code>&#123;&quot;code&quot;: &quot;\u884c\u52a8\u4ee3\u53f7\uff1a\u5929\u738b\u76d6\u5730\u864e&quot;&#125;</code></li><li>使用 Python 去解析，发现可以得到网页上面的内容</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">html_json = <span class="string">&#x27;&#123;&quot;code&quot;: &quot;\u884c\u52a8\u4ee3\u53f7\uff1a\u5929\u738b\u76d6\u5730\u864e&quot;&#125;&#x27;</span></span><br><span class="line">html_dic = json.loads(html_json)</span><br><span class="line"><span class="built_in">print</span>(html_dic)      <span class="comment">#&#123;&#x27;code&#x27;: &#x27;行动代号：天王盖地虎&#x27;&#125;</span></span><br></pre></td></tr></table></figure><ul><li><strong>这种假的异步加载页面，其处理思路一般是使用正则表达式从页面中把数据提取出来，然后直接解析</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://exercise.kingname.info/exercise_ajax_2.html&#x27;</span></span><br><span class="line">html = requests.get(url).content.decode()</span><br><span class="line">code_json = re.search(<span class="string">&quot;secret = &#x27;(.*?)&#x27;&quot;</span>, html, re.S).group(<span class="number">1</span>)</span><br><span class="line">code_dict = json.loads(code_json)</span><br><span class="line"><span class="built_in">print</span>(code_dict[<span class="string">&#x27;code&#x27;</span>])</span><br><span class="line"><span class="comment">#行动代号：天王盖地虎</span></span><br></pre></td></tr></table></figure><h3 id="7-1-5-多次请求的异步加载"><a href="#7-1-5-多次请求的异步加载" class="headerlink" title="7.1.5 多次请求的异步加载"></a>7.1.5 多次请求的异步加载</h3><ul><li><strong><a href="http://exercise.kingname.info/exercise_ajax_3.html">练习页面</a></strong></li></ul><ul><li>还有一些网页，显示在页面上的内容要经过多次异步请求才能得到。</li><li>第 1 个 AJAX 请求返回的是第 2 个请求的参数，第 2 个请求的返回内容又是第 3 个请求的参数，只有得到了上一个请求里面的有用信息，才能发起下一个请求</li><li>在“Headers”选项卡查看这个 POST 请求的具体参数，在 body 里面发现两个奇怪的参数 secret1 和 secret2</li><li>尝试修改 secret1 和 secret2，发现 POST 请求无法得到想要的结果<br><strong>奇怪的参数</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">name: &quot;xx&quot;</span><br><span class="line">age: 24</span><br><span class="line">secret1: &quot;kingname is genius.&quot;</span><br><span class="line">secret2: &quot;kingname&quot;</span><br></pre></td></tr></table></figure><p><strong>如果修改这两个参数</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://exercise.kingname.info/ajax_3_postbackend&#x27;</span></span><br><span class="line">return_json_1 = requests.post(url,json=&#123;<span class="string">&quot;name&quot;</span>:<span class="string">&quot;xx&quot;</span>,</span><br><span class="line"><span class="string">&quot;age&quot;</span>:<span class="string">&quot;24&quot;</span>,<span class="string">&quot;secret1&quot;</span>:<span class="string">&quot;123&quot;</span>,<span class="string">&quot;secret2&quot;</span>:<span class="string">&quot;456&quot;</span>&#125;)</span><br><span class="line">return_json_2 = requests.post(url,json=&#123;<span class="string">&quot;name&quot;</span> :<span class="string">&quot;xx&quot;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">23</span></span><br><span class="line">&#125;)</span><br><span class="line"><span class="built_in">print</span>(json.loads(return_json_1.content.decode()))   <span class="comment">#&#123;&#x27;success&#x27;: False, &#x27;reason&#x27;: &#x27;参数错误&#x27;&#125;</span></span><br><span class="line"><span class="built_in">print</span>(json.loads(return_json_2.content.decode()))   <span class="comment">#&#123;&#x27;success&#x27;: False, &#x27;reason&#x27;: &#x27;参数不全&#x27;&#125;</span></span><br></pre></td></tr></table></figure><ul><li>打开这个练习页的源代码，在源代码中可以找到 secret_2</li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>exercise ajax load<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">      <span class="keyword">var</span> secret_2 = <span class="string">&quot;kingname&quot;</span>;</span></span><br><span class="line"><span class="language-javascript">    </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;content&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;static/js/jquery-3.2.1.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;static/js/loaddata_3.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>虽然在 POST 参数中，名字是 secret2，而源代码中的名字是 secret_2，不过从值可以看出这就是同一个参数</li><li>源代码里面没有 secret1，因此就要考虑这个参数是不是来自于另一个异步请求</li><li>继续在开发者工具中查看其他请求，可以成功找到 secret1,注意，它的名字变为了“code”，但是从值可以看出这就是 secret1</li></ul><ul><li><strong>不少网站也会使用这种改名字的方式来迷惑爬虫开发者</strong></li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">code</span>: <span class="string">&quot;kingname is genius.&quot;</span>, <span class="attr">success</span>: <span class="literal">true</span>&#125;</span><br><span class="line"><span class="attr">code</span>: <span class="string">&quot;kingname is genius.&quot;</span></span><br><span class="line"><span class="attr">success</span>: <span class="literal">true</span></span><br></pre></td></tr></table></figure><ul><li>这一条请求就是一个不带任何参数的 GET 请求</li><li>_对于这种多次请求才能得到数据的情况，解决办法就是逐一请求，得到返回结果以后再发起下一个请求。具体到这个例子中，那就是先从源代码里面获得 secret2，再通过 GET 请求得到 secret1，最后使用 secret1 和 secret2 来获取页面上显示的内容_</li></ul><ul><li><strong>[爬取网站]<a href="http://exercise.kingname.info/exercise_ajax_3.html">http://exercise.kingname.info/exercise_ajax_3.html</a>)</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://exercise.kingname.info/exercise_ajax_3.html&#x27;</span></span><br><span class="line">first_ajax_url = <span class="string">&#x27;http://exercise.kingname.info/ajax_3_backend&#x27;</span></span><br><span class="line">second_ajax_url = <span class="string">&#x27;http://exercise.kingname.info/ajax_3_postbackend&#x27;</span></span><br><span class="line"></span><br><span class="line">page_html = requests.get(url).content.decode()</span><br><span class="line">secret_2 = re.search(<span class="string">&quot;secret_2 = &#x27;(.*?)&#x27;;&quot;</span>,page_html,re.S).group(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(secret_2)           <span class="comment">#kingname</span></span><br><span class="line"></span><br><span class="line">ajax_1_json = requests.get(first_ajax_url).content.decode()</span><br><span class="line"><span class="built_in">print</span>(ajax_1_json)        <span class="comment">#&#123;&quot;code&quot;: &quot;kingname is genius.&quot;, &quot;success&quot;: true&#125;</span></span><br><span class="line">ajax_1_dict = json.loads(ajax_1_json)</span><br><span class="line">secret_1 = ajax_1_dict[<span class="string">&#x27;code&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(secret_1)           <span class="comment">#kingname is genius.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取了secret_1和secret_2后post请求second_ajax_url</span></span><br><span class="line"></span><br><span class="line">ajax_2_json = requests.post(second_ajax_url,json=&#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;yq&#x27;</span>,<span class="string">&#x27;age&#x27;</span>:<span class="number">24</span>,<span class="string">&#x27;secret1&#x27;</span>:secret_1,<span class="string">&#x27;secret2&#x27;</span>:secret_2</span><br><span class="line">&#125;).content.decode()</span><br><span class="line"><span class="built_in">print</span>(ajax_2_json)          <span class="comment">#&#123;&quot;code&quot;: &quot;\u884c\u52a8\u4ee3\u53f7\uff1a\u54ce\u54df\u4e0d\u9519\u54e6&quot;, &quot;success&quot;: true&#125;</span></span><br><span class="line"></span><br><span class="line">ajax_2_dict = json.loads(ajax_2_json)</span><br><span class="line"><span class="built_in">print</span>(ajax_2_dict)          <span class="comment">#&#123;&#x27;code&#x27;: &#x27;行动代号：哎哟不错哦&#x27;, &#x27;success&#x27;: True&#125;</span></span><br><span class="line"></span><br><span class="line">code = ajax_2_dict[<span class="string">&#x27;code&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(code)                 <span class="comment">#行动代号：哎哟不错哦</span></span><br></pre></td></tr></table></figure><h3 id="7-1-6-基于异步加载的简单登录"><a href="#7-1-6-基于异步加载的简单登录" class="headerlink" title="7.1.6 基于异步加载的简单登录"></a>7.1.6 基于异步加载的简单登录</h3><ul><li><strong><a href="http://exercise.kingname.info/exercise_ajax_4.html">练习页面</a></strong></li></ul><ul><li>网站的登录方式有很多种，其中有一种比较简单的方式，就是使用 AJAX 发送请求来进行登录</li><li>在<a href="http://exercise.kingname.info/exercise_ajax_4.html">练习页面</a>中根据输入框中的提示，使用用户名“kingname”和密码“genius”进行登录,登录成功以后弹出提示框</li><li><strong>对于这种简单的登录功能，可以使用抓取异步加载网页的方式来进行处理</strong></li><li>在 Chrome 开发者工具中可以发现，当单击“登录”按钮时，网页向后台发送了一条请求<br><strong><code>&#123;&quot;code&quot;: &quot;kingname is genius&quot;, &quot;success&quot;: true&#125;</code></strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://exercise.kingname.info/ajax_4_backend&#x27;</span></span><br><span class="line">code_json = requests.post(url,json=&#123;</span><br><span class="line">    <span class="string">&#x27;username&#x27;</span>:<span class="string">&#x27;kingname&#x27;</span>,<span class="string">&#x27;password&#x27;</span>:<span class="string">&#x27;genius&#x27;</span>&#125;).content.decode()</span><br><span class="line">code__dict = json.loads(code_json)</span><br><span class="line"><span class="built_in">print</span>(code__dict[<span class="string">&#x27;code&#x27;</span>])</span><br><span class="line"><span class="comment"># kingname is genius</span></span><br></pre></td></tr></table></figure><ul><li>这就是使用 POST 方式的最简单的 AJAX 请求。使用获取 POST 方式的 AJAX 请求的代码，就能成功获取到登录以后返回的内容</li></ul><h2 id="7-2-请求头"><a href="#7-2-请求头" class="headerlink" title="7.2 请求头"></a>7.2 请求头</h2><h3 id="7-2-1-请求头的作用"><a href="#7-2-1-请求头的作用" class="headerlink" title="7.2.1 请求头的作用"></a>7.2.1 请求头的作用</h3><ul><li>使用计算机网页版外卖网站的读者应该会发现这样一个现象：第一次登录外卖网页的时候会让你选择当前所在的商业圈，一旦选定好之后关闭浏览器再打开，网页就会自动定位到先前选择的商业圈</li><li>又比如，例如携程的网站，使用计算机浏览器打开的时候，页面看起来非常复杂多样</li><li>同一个网址，使用手机浏览器打开时，网址会自动发生改变，而且得到的页面竟然完全不同<br><strong>同一个网址，PC 端和手机端页面不同</strong></li></ul><ul><li>Headers 称为请求头，浏览器可以将一些信息通过 Headers 传递给服务器，服务器也可以将一些信息通过 Headers 传递给浏览器，电商网站常常应用的 Cookies 就是 Headers 里面的一个部分</li></ul><h3 id="7-2-2-伪造请求头"><a href="#7-2-2-伪造请求头" class="headerlink" title="7.2.2 伪造请求头"></a>7.2.2 伪造请求头</h3><ul><li>打开<a href="http://exercise.kingname.info/exercise_headers.html">练习页</a>，使用 Chrome 的开发者工具监控这个页面的网页请求</li><li>页面看起来像是发起了一个普通的 GET 方式的异步请求给<a href="http://exercise.kingname.info/exercise_headers_backend">http://exercise.kingname.info/exercise_headers_backend</a></li><li>使用 requests 尝试获取这个网址的返回信息,结果发现失败</li><li>使用浏览器访问网站的时候，网站可以看到一个名称为 Headers（请求头）的东西</li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;</span><br><span class="line">            <span class="title class_">Accept</span>: *<span class="comment">/*</span></span><br><span class="line"><span class="comment">            Accept-Encoding: gzip, deflate</span></span><br><span class="line"><span class="comment">            Accept-Language: zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7</span></span><br><span class="line"><span class="comment">            anhao: kingname</span></span><br><span class="line"><span class="comment">            Content-Type: application/json; charset=utf-8</span></span><br><span class="line"><span class="comment">            Cookie: __cfduid=d513aff6c34f63c4c2971cdf1e19780051581303763</span></span><br><span class="line"><span class="comment">            Host: exercise.kingname.info</span></span><br><span class="line"><span class="comment">            Proxy-Connection: keep-alive</span></span><br><span class="line"><span class="comment">            Referer: http://exercise.kingname.info/exercise_headers.html</span></span><br><span class="line"><span class="comment">            User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.79 Safari/537.36</span></span><br><span class="line"><span class="comment">            X-Requested-With: XMLHttpRequest</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br></pre></td></tr></table></figure><ul><li>为了解决这个问题，就需要给爬虫“换头”。把浏览器的头安装到爬虫的身上，这样网站就不知道谁是谁了</li><li>要换头，首先就需要知道浏览器的头是什么样的。因此需要在 Chrome 浏览器开发者工具的“Network”选项卡的 Request Headers 里面观察这一次请求的请求头</li><li>在 requests 里面，设置请求头的参数名称为“headers”，它的值是一个字典<br><strong>带有请求头的请求，使用 requests 的发送格式为：</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">html = requests.get(url, headers=字典).content.decode()</span><br><span class="line">html = requests.post(url, json=xxx, headers=字典).content.decode()</span><br></pre></td></tr></table></figure><ul><li>代码中的字典就对应了浏览器中的请求头</li><li>在爬虫里面创建一个字典，将 Chrome 的请求头的内容复制进去，并调整好格式，发起一个带有 Chrome 请求头的爬虫请求，可以发现请求获得成功</li><li>虽然对于某些网站，在请求头里面只需要设置 User-Agent 就可以正常访问了，但是为了保险起见，还是建议把所有项目都带上，这样可以让爬虫更“像”浏览器</li></ul><h2 id="7-3-模拟浏览器"><a href="#7-3-模拟浏览器" class="headerlink" title="7.3 模拟浏览器"></a>7.3 模拟浏览器</h2><ul><li><strong><a href="http://exercise.kingname.info/exercise_advanced_ajax.html">练习页面</a></strong></li><li>_问题：_</li><li>有一些网站在发起 AJAX 请求的时候，会带上特殊的字符串用于身份验证。这种字符串称为 Token</li><li>打开练习页面，这个页面在发起 AJAX 请求的时候会在 Headers 中带上一个参数 ReqTime；在 POST 发送的数据中会有一个参数 sum</li><li>多次刷新页面，可以发现 ReqTime 和 sum 一直在变化</li><li>不难看出 ReqTime 是精确到毫秒的时间戳，即使使用 Python 生成了一个时间戳，也不能得到网页上面的内容</li></ul><h3 id="7-3-1-Selenium-介绍"><a href="#7-3-1-Selenium-介绍" class="headerlink" title="7.3.1 Selenium 介绍"></a>7.3.1 Selenium 介绍</h3><ul><li>虽然在网页的源代码中无法看到被异步加载的内容，但是在 Chrome 的开发者工具的“Elements”选项卡下却可以看到网页上的内容</li></ul><h3 id="7-3-2-selenium-安装"><a href="#7-3-2-selenium-安装" class="headerlink" title="7.3.2 selenium 安装"></a>7.3.2 selenium 安装</h3><ul><li>安装 selenium <code>pip install selenium</code></li><li>下载 ChromeDriver</li></ul><h3 id="7-3-3-selenium-的使用"><a href="#7-3-3-selenium-的使用" class="headerlink" title="7.3.3 selenium 的使用"></a>7.3.3 selenium 的使用</h3><h4 id="7-3-3-1-获取源代码"><a href="#7-3-3-1-获取源代码" class="headerlink" title="7.3.3.1 获取源代码"></a>7.3.3.1 获取源代码</h4><ul><li><strong>将 chromedriver 与代码放在同一个文件夹中以方便代码直接调用</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化selenium</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">driver = webdriver.Chrome(<span class="string">&#x27;./chromedriver&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li>指定了 Selenium 使用 ChromeDriver 来操作 Chrome 解析网页，括号里的参数就是 ChromeDriver 可执行文件的地址</li></ul><ul><li>如果要使用 PhantomJS，只需要修改第 3 行代码即可：driver = webdriver.PhantomJS(‘./phantomjs’)，需要将 PhantomJS 的可执行文件与代码放在一起</li><li>需要特别提醒的是，如果 chromedriver 与代码不在一起，可以通过绝对路径来指定，例如：driver = webdriver.Chrome(‘/usr/bin/chromedriver’)</li><li>使用 Windows 的读者可在路径字符串左引号的左边加一个“r”符号，将代码写为：driver = webdriver.Chrome(r’C:\server\chromedriver.exe’)</li><li>初始化完成以后，就可以使用 Selenium 打开网页了。要打开一个网页只需要一行代码：<br><code>driver.get(&#39;http://exercise.kingname.info/exercise_advanced_ajax.html&#39;)</code></li><li>代码运行以后会自动打开一个 Chrome 窗口，并在窗口里面自动进入这个网址对应的页面。一旦被异步加载的内容已经出现在了这个自动打开的 Chrome 窗口中，那么此时使用下列代码：<br><code>html = driver.page_source</code></li><li>就能得到在 Chrome 开发者工具中出现的 HTML 代码<br><strong>综合：</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome(<span class="string">r&#x27;C:\Program Files (x86)\Google\Chrome\Application\chromedriver&#x27;</span>)</span><br><span class="line">driver.get(<span class="string">&#x27;http://exercise.kingname.info/exercise_advanced_ajax.html&#x27;</span>)</span><br><span class="line">time.sleep(<span class="number">5</span>)</span><br><span class="line">html = driver.page_source</span><br><span class="line"><span class="built_in">print</span>(html)</span><br><span class="line"><span class="built_in">input</span>(<span class="string">&#x27;按任意键结束：&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>运行程序会出现以下界面</strong><br><img src="https://raw.githubusercontent.com/yq010105/Blog_images/master/img/selenium.png" alt="selenium" title="selenium"></p><h4 id="7-3-3-2-等待信息出现"><a href="#7-3-3-2-等待信息出现" class="headerlink" title="7.3.3.2 等待信息出现"></a>7.3.3.2 等待信息出现</h4><ul><li>设置了一个 5s 的延迟，这是由于 Selenium 并不会等待网页加载完成再执行后面的代码。它只是向 ChromeDriver 发送了一个命令，让 ChromeDriver 打开某个网页</li><li>至于网页要开多久，Selenium 并不关心。由于被异步加载的内容会延迟出现，因此需要等待它出现以后再开始抓取</li></ul><h4 id="7-3-3-3-在网页中获取元素"><a href="#7-3-3-3-在网页中获取元素" class="headerlink" title="7.3.3.3 在网页中获取元素"></a>7.3.3.3 在网页中获取元素</h4><p>_在网页中寻找需要的内容，可以使用类似于 Beautiful Soup4 的语法：_</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">element = driver.find_element_by_id(<span class="string">&quot;passwd-id&quot;</span>) <span class="comment">#如果有多个符合条件的，返回第1个</span></span><br><span class="line">element = driver.find_element_by_name(<span class="string">&quot;passwd&quot;</span>) <span class="comment">#如果有多个符合条件的，返回第1个</span></span><br><span class="line">element_list = driver.find_elements_by_id(<span class="string">&quot;passwd-id&quot;</span>) <span class="comment">#以列表形式返回所有的符合条件的element</span></span><br><span class="line">element_list = driver.find_elements_by_name(<span class="string">&quot;passwd&quot;</span>) <span class="comment">#以列表形式返回所有的符合条件的element</span></span><br></pre></td></tr></table></figure><p><strong>也可以使用 XPath</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">element = driver.find_element_by_xpath(<span class="string">&quot;//input[@id=&#x27;passwd-id&#x27;]&quot;</span>)</span><br><span class="line"><span class="comment">#如果有多个符合条件的，返回第1个</span></span><br><span class="line">element = driver.find_elements_by_xpath(<span class="string">&quot;//div[@id=&#x27;passwd-id&#x27;]&quot;</span>)</span><br><span class="line"><span class="comment">#以列表形式返回所有的符合条件的element</span></span><br></pre></td></tr></table></figure><p><a href="http://exercise.kingname.info/exercise_advanced_ajax.html">练习网站</a></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome(<span class="string">r&#x27;C:\Program Files (x86)\Google\Chrome\Application\chromedriver&#x27;</span>)</span><br><span class="line">driver.get(<span class="string">&#x27;http://exercise.kingname.info/exercise_advanced_ajax.html&#x27;</span>)</span><br><span class="line"></span><br><span class="line">time.sleep(<span class="number">5</span>)</span><br><span class="line"><span class="keyword">try</span> :</span><br><span class="line">    WebDriverWait(driver,<span class="number">30</span>).until(EC.text_to_be_present_in_element(By.CLASS_NAME,<span class="string">&quot;content&quot;</span>),<span class="string">&#x27;通关&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> _:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;网页加载太慢，爬&#x27;</span>)</span><br><span class="line"><span class="comment"># 但是也可能会爬，不知到原因</span></span><br><span class="line">element = driver.find_element_by_xpath(<span class="string">&#x27;//div[@class=&quot;content&quot;]&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;异步加载的内容是：<span class="subst">&#123;element.text&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 异步加载的内容是：通关成功，通关口令：这是最终数据。</span></span><br><span class="line"></span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure><h2 id="7-4-实例：乐视爬取视频评论"><a href="#7-4-实例：乐视爬取视频评论" class="headerlink" title="7.4 实例：乐视爬取视频评论"></a>7.4 实例：<a href="http://www.le.com">乐视</a>爬取视频评论</h2><ul><li>_1&gt;分析网站的异步加载请求_</li><li>_2&gt;使用 requests 发送请求_<hr/></li><li>通过使用 Chrome 的开发者工具分析页面的异步加载请求，可以发现评论所在的请求</li><li>可以使用 Python 来模拟这个请求，从而获取视频的评论信息</li><li><p>在请求的 URL 里面有两个参数：vid 和 pid,这两个参数在网页的源代码里面都可以找到</p><hr/></li><li><p>爬虫首先访问视频页面，通过正则表达式获取 vid 和 pid，并将结果保存到“necessary_info”这个类属性对应的字典中</p></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 核心代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_necessary_id</span>(<span class="params">self</span>):</span><br><span class="line">  source = self.get_source(self.url, self.HEADERS)</span><br><span class="line">  vid = re.search(<span class="string">&#x27;vid: (\d+)&#x27;</span>, source).group(<span class="number">1</span>)</span><br><span class="line">  pid = re.search(<span class="string">&#x27;pid: (\d+)&#x27;</span>, source).group(<span class="number">1</span>)</span><br><span class="line">  self.necessary_info[<span class="string">&#x27;xid&#x27;</span>] = vid</span><br><span class="line">  self.necessary_info[<span class="string">&#x27;pid&#x27;</span>] = pid</span><br></pre></td></tr></table></figure><ul><li>访问评论的接口，用 Python 发起请求，获得评论数据</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_comment</span>(<span class="params">self</span>):</span><br><span class="line">    url = self.COMMENT_URL.<span class="built_in">format</span>(xid=self.necessary_info[<span class="string">&#x27;xid&#x27;</span>],</span><br><span class="line">                             pid=self.necessary_info[<span class="string">&#x27;pid&#x27;</span>])</span><br><span class="line">    source = self.get_source(url, self.HEADERS)</span><br><span class="line">    source_json = source[source.find(<span class="string">&#x27;&#123;&quot;&#x27;</span>): -<span class="number">1</span>]</span><br><span class="line">    comment_dict = json.loads(source_json)</span><br><span class="line">    comments = comment_dict[<span class="string">&#x27;data&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> comment <span class="keyword">in</span> comments:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;发帖人： <span class="subst">&#123;comment[<span class="string">&quot;user&quot;</span>][<span class="string">&quot;username&quot;</span>]&#125;</span>, 评论内容：<span class="subst">&#123;comment[<span class="string">&quot;content&quot;</span>]&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li>代码中，提前定义的 self.COMMENT_URL 和 self.HEADERS</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 综合</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LetvSpider</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    COMMENT_URL = <span class="string">&#x27;http://api-my.le.com/vcm/api/list?jsonp=jQuery19100358 \</span></span><br><span class="line"><span class="string">    8935956887496_1581419682085&amp;type=video&amp;rows=20&amp;page=1&amp;sort=&amp;cid=2&amp;sourc\</span></span><br><span class="line"><span class="string">    e=1&amp;xid=27576461&amp;pid=10022394&amp;ctype=cmt%2Cimg%2Cvote&amp;listType=1&amp;_=1581419682087&#x27;</span></span><br><span class="line"></span><br><span class="line">    HEADERS = &#123;<span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;*/*&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Accept-Encoding&#x27;</span>: <span class="string">&#x27;gzip, deflate&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Cookie&#x27;</span>: <span class="string">&#x27;tj_lc=d551d3996ae75055e97c1f22ac9aa002; tj_uuid=-_15814196222976075472; tj_env=1; ssoCookieSynced=1; language=zh-cn; sso_curr_country=CN; vjuids=-75eba524.17033f49d1f.0.d645e0a5d3aa1; vjlast=1581419634.1581419634.30; tj_v2c=-27576461_2&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Host&#x27;</span>: <span class="string">&#x27;api-my.le.com&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Proxy-Connection&#x27;</span>: <span class="string">&#x27;keep-alive&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Referer&#x27;</span>:<span class="string">&#x27;http://www.le.com/ptv/vplay/27576461.html&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36&#x27;</span></span><br><span class="line">             &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,url</span>):</span><br><span class="line">        self.necessary_info = &#123;&#125;</span><br><span class="line">        self.url = url</span><br><span class="line">        self.get_necessary_id()</span><br><span class="line">        self.get_comment()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到request返回的html--str</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_source</span>(<span class="params">self,url,headers</span>):</span><br><span class="line">        <span class="keyword">return</span> requests.get(url,headers).content.decode()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_necessary_id</span>(<span class="params">self</span>):</span><br><span class="line">        source = self.get_source(self.url,self.HEADERS) <span class="comment"># 得到html</span></span><br><span class="line">        vid = re.search(<span class="string">&#x27;vid: (\d+)&#x27;</span>,source).group(<span class="number">1</span>)   <span class="comment">#re到vid、pid</span></span><br><span class="line">        pid = re.search(<span class="string">&#x27;pid: (\d+)&#x27;</span>,source).group(<span class="number">1</span>)</span><br><span class="line">        self.necessary_info[<span class="string">&#x27;xid&#x27;</span>] = vid        <span class="comment">#传入vid、pid到字典</span></span><br><span class="line">        self.necessary_info[<span class="string">&#x27;pid&#x27;</span>] = pid</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_comment</span>(<span class="params">self</span>):</span><br><span class="line">        url = self.COMMENT_URL.<span class="built_in">format</span>(xid=self.necessary_info[<span class="string">&#x27;xid&#x27;</span>],</span><br><span class="line">        pid=self.necessary_info[<span class="string">&#x27;pid&#x27;</span>])     <span class="comment"># format格式化  url 评论list的request_url</span></span><br><span class="line">        source = self.get_source(url,self.HEADERS)  <span class="comment"># 调用get_source()</span></span><br><span class="line">        source_json = source[source.find(<span class="string">&#x27;&#123;&quot;&#x27;</span>): -<span class="number">1</span>] <span class="comment"># str切片</span></span><br><span class="line">        comment_dict = json.loads(source_json)  <span class="comment">#一个字典</span></span><br><span class="line">        comments = comment_dict[<span class="string">&#x27;data&#x27;</span>]</span><br><span class="line">        <span class="keyword">for</span> comment <span class="keyword">in</span> comments:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;发帖人：<span class="subst">&#123;comment[<span class="string">&quot;user&quot;</span>][<span class="string">&quot;username&quot;</span>]&#125;</span>,评论内容:<span class="subst">&#123;comment[<span class="string">&quot;content&quot;</span>]&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spider = LetvSpider(<span class="string">&#x27;http://www.le.com/ptv/vplay/27576461.html&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">发帖人：福建乐迷,评论内容:好喜欢东华帝君</span></span><br><span class="line"><span class="string">发帖人：河北乐迷,评论内容:十</span></span><br><span class="line"><span class="string">发帖人：河北乐迷,评论内容:瑶光上神好漂亮。</span></span><br><span class="line"><span class="string">发帖人：河北乐迷,评论内容:太好看了。</span></span><br><span class="line"><span class="string">发帖人：河北乐迷,评论内容:真水无香。</span></span><br><span class="line"><span class="string">发帖人：河北乐迷,评论内容:喜欢白浅</span></span><br><span class="line"><span class="string">发帖人：天莫邪,评论内容:杨幂真不好看</span></span><br><span class="line"><span class="string">发帖人：呆萌小甜心,评论内容:爱幂幂</span></span><br><span class="line"><span class="string">发帖人：G_,评论内容:有谁是看了枕上书又来看十里桃花我浅浅的</span></span><br><span class="line"><span class="string">发帖人：黑名单,评论内容:我来啦</span></span><br><span class="line"><span class="string">发帖人：凉辰梦瑾空人心_702_210,评论内容:为啥只能隔乐视看了 好伤心�😭</span></span><br><span class="line"><span class="string">发帖人：上海乐迷,评论内容:产科医生</span></span><br><span class="line"><span class="string">发帖人：红_,评论内容:这个很好看</span></span><br><span class="line"><span class="string">发帖人：子璇,评论内容:墨渊霸气，白浅跟她在一起才不会受伤害</span></span><br><span class="line"><span class="string">发帖人：聂芳英,评论内容:为什么其他的APP上看不到</span></span><br><span class="line"><span class="string">发帖人：月色不错,评论内容:这个是玉帝还是王母</span></span><br><span class="line"><span class="string">发帖人：Myth橙子,评论内容:每个平台看一遍我是有多闲</span></span><br><span class="line"><span class="string">发帖人：上海乐迷,评论内容:怎么这么难找〈产科医生）的电视剧</span></span><br><span class="line"><span class="string">发帖人：上海乐迷,评论内容:我想看产科医生的电视剧</span></span><br><span class="line"><span class="string">发帖人：上海乐迷,评论内容:产科医生</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;网站名：&#123;name&#125;, 地址 &#123;url&#125;&quot;</span>.<span class="built_in">format</span>(name=<span class="string">&quot;菜鸟教程&quot;</span>, url=<span class="string">&quot;www.runoob.com&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过字典设置参数</span></span><br><span class="line">site = &#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;菜鸟教程&quot;</span>, <span class="string">&quot;url&quot;</span>: <span class="string">&quot;www.runoob.com&quot;</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;网站名：&#123;name&#125;, 地址 &#123;url&#125;&quot;</span>.<span class="built_in">format</span>(**site))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过列表索引设置参数</span></span><br><span class="line">my_list = [<span class="string">&#x27;菜鸟教程&#x27;</span>, <span class="string">&#x27;www.runoob.com&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;网站名：&#123;0[0]&#125;, 地址 &#123;0[1]&#125;&quot;</span>.<span class="built_in">format</span>(my_list))  <span class="comment"># &quot;0&quot; 是必须的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 网站名：菜鸟教程, 地址 www.runoob.com</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AssignValue</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, value</span>):</span><br><span class="line">        self.value = value</span><br><span class="line">my_value = AssignValue(<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;value 为: &#123;0.value&#125;&#x27;</span>.<span class="built_in">format</span>(my_value))  <span class="comment"># &quot;0&quot; 是可选的</span></span><br><span class="line"><span class="comment"># value 为: 6</span></span><br></pre></td></tr></table></figure><h1 id="8-模拟登录与验证码"><a href="#8-模拟登录与验证码" class="headerlink" title="8. 模拟登录与验证码"></a>8. 模拟登录与验证码</h1><ul><li>对于一个需要登录才能访问的网站，它的页面在登录前和登录后可能是不一样的</li><li>如果直接使用 requests 去获取源代码，只能得到登录以前的页面源代码</li></ul><h2 id="8-1-模拟登录"><a href="#8-1-模拟登录" class="headerlink" title="8.1 模拟登录"></a>8.1 模拟登录</h2><ul><li>1.使用 Selenium 操作浏览器登录和使用 Cookies 登录虽然简单粗暴，但是有效</li><li>2.使用模拟提交表单登录虽然较为麻烦，但可以实现自动化</li></ul><h3 id="8-1-1-使用-Selenium-模拟登录"><a href="#8-1-1-使用-Selenium-模拟登录" class="headerlink" title="8.1.1 使用 Selenium 模拟登录"></a>8.1.1 使用 Selenium 模拟登录</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">     使用Selenium来进行模拟登录，整个过程非常简单。流程如下。</span><br><span class="line">（1）初始化ChromeDriver。</span><br><span class="line">（2）打开知乎登录页面。</span><br><span class="line">（3）找到用户名的输入框，输入用户名。</span><br><span class="line">（4）找到密码输入框，输入用户名。</span><br><span class="line">（5）手动单击验证码。</span><br><span class="line">（6）按下Enter键。</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>程序首先打开知乎的登录页面，然后使用“find<em>element_by</em> name”分别找到输入账号和密码的两个输入框</li><li>这两个输入框的 name 属性值分别为“account”(我的是 username)和“password”</li><li>在 Selenium 中可以使用 send_keys()方法往输入框中输入字符串</li><li>在输入了密码以后，验证码框就会弹出来。知乎使用的验证码为点击倒立的文字，这种验证码不容易自动化处理，因此在这个地方让爬虫先暂停，手动点击倒立文字</li><li>爬虫中的 input()语句会阻塞程序，直到在控制台按下 Enter 键，爬虫才会继续运行</li></ul><h3 id="8-1-2-使用-Cookies-登录"><a href="#8-1-2-使用-Cookies-登录" class="headerlink" title="8.1.2 使用 Cookies 登录"></a>8.1.2 使用 Cookies 登录</h3><ul><li>_Cookie 是用户使用浏览器访问网站的时候网站存放在浏览器中的一小段数据_</li><li>Cookie 的复数形式 Cookies 用来表示各种各样的 Cookie。它们有些用来记录用户的状态信息；有些用来记录用户的操作行为；还有一些，具有现代网络最重要的功能：记录授权信息——用户是否登录以及用户登录哪个账号</li><li>为了不让用户每次访问网站都进行登录操作，浏览器会在用户第一次登录成功以后放一段加密的信息在 Cookies 中。下次用户访问，网站先检查 Cookies 有没有这个加密信息，如果有并且合法，那么就跳过登录操作，直接进入登录后的页面</li><li>通过已经登录的 Cookies，可以让爬虫绕过登录过程，直接进入登录以后的页面</li><li>在已经登录知乎的情况下，打开 Chrome 的开发者工具，定位到“Network”选项卡，然后刷新网页，在加载的内容中随便选择一项，然后看右侧的数据，从 Request Headers 中可以找到 Cookie</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cookie: _zap=56180d87-245a-4b79-83e2-711f4629644e; d_c0=&quot;AMAY69ZKzRCPTh5KJj9edoIQ4_BiQS3iqwM=|1581434842&quot;; _xsrf=jzLzeCfZignAw6qDdNqO85UOdCrRcB3C; Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49=1581485103,1581492629,1581492650,1581494278; capsion_ticket=&quot;2|1:0|10:1581494284|14:capsion_ticket|44:ZjQyY2FjMmZkZTJmNDJkNGI5NmYxMDNkMzc3MTVlNGI=|e2f4eb7e3652b2f1f3e439d7ff4275e4e15bdfbfbed8ce423dceded2da4235cf&quot;; z_c0=&quot;2|1:0|10:1581494646|4:z_c0|92:Mi4xY2R0cUJRQUFBQUFBd0JqcjFrck5FQ1lBQUFCZ0FsVk5kdjh3WHdBMEczY0dBVm5MNUFmV1V4cmtja0p1Rm1kMGtn|560b73b3b5f052f6151d4a02e62f1f645f01ad7826d8c183d7152fb2fcf8456d&quot;; Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49=1581494647; tst=r; KLBRSID=81978cf28cf03c58e07f705c156aa833|1581494650|1581494278</span><br></pre></td></tr></table></figure><ul><li><strong>请注意这里一定是“Request Headers”，不要选成了“Response Headers”</strong></li></ul><ul><li>只要把这个 Request Headers 的内容通过 requests 提交，就能直接进入登录以后的知乎页面了</li><li>可以看到，使用 Cookie 来登录网页，不仅可以绕过登录步骤，还可以绕过网站的验证码</li><li>Session，是指一段会话。网站会把每一个会话的 ID（Session ID）保存在浏览器的 Cookies 中用来标识用户的身份</li><li>requests 的 Session 模块可以自动保存网站返回的一些信息</li><li>其实在前面章节中使用的 requests.get()，在底层还是会先创建一个 Session，然后用 Session 去访问</li><li>对于 HTTPS 的网站，在 requests 发送请求的时候需要带上 verify=False 这个参数，否则爬虫会报错</li><li>带上这个参数以后，爬虫依然会报一个警告，这是因为没有 HTTPS 的证书</li><li>不过这个警告不会影响爬虫的运行结果。对于有强迫症的读者，可以参考相关内容为 requests 设置证书，从而解除这个警告</li></ul><h3 id="8-1-3-模拟表单登录"><a href="#8-1-3-模拟表单登录" class="headerlink" title="8.1.3 模拟表单登录"></a>8.1.3 模拟表单登录</h3><p><strong><a href="http://exercise.kingname.info/exercise_login?next=%2Fexercise_login_success">练习页面</a></strong></p><ul><li>这个登录页面多了一个“自动登录”复选框输入用户名 kingname，密码 genius，勾选“自动登录”复选框并单击“登录”按钮，可以看到登录成功后的页面</li><li>打开 Chrome 的开发者工具并监控登录过程</li><li>然而，仔细观察会发现登录请求的那个网址只会在“Network”选项卡中存在 1s，然后就消失了</li><li>Network”选项卡下面只剩下登录成功后的页面所发起的各种网络请求</li><li>这是因为表单登录成功以后会进行页面跳转，相当于开了一个新的网页，于是新的请求就会直接把旧的请求覆盖。为了避免这种情况，需要在 Chrome 的开发者工具的“Network”选项卡中勾选“Preserve log”复选框，再一次登录就可以看到登录过程</li><li>此时可以看到 Status Code 是 302，说明这里有一个网页跳转，也就证明了之前为什么登录以后看不到登录的请求</li><li><strong>使用 requests 的 Session 模块来模拟这个登录</strong></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">login_url = <span class="string">&#x27;http://exercise.kingname.info/exercise_login&#x27;</span></span><br><span class="line">login_sucess_url = <span class="string">&#x27;http://exercise.kingname.info/exercise_login_success&#x27;</span></span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;username&#x27;</span>:<span class="string">&#x27;kingname&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;password&#x27;</span>:<span class="string">&#x27;genius&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;remember&#x27;</span>:<span class="string">&#x27;Yes&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">session = requests.Session()</span><br><span class="line">before_login = session.get(login_sucess_url).text</span><br><span class="line"><span class="built_in">print</span>(before_login)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;============开始登陆==============&#x27;</span>)</span><br><span class="line">session.post(login_url,data=data).text</span><br><span class="line">after_login = session.get(login_sucess_url).text</span><br><span class="line"><span class="built_in">print</span>(after_login)</span><br></pre></td></tr></table></figure><p><strong>结果</strong></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>exercise login<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css">      <span class="selector-tag">label</span> &#123;</span></span><br><span class="line"><span class="language-css">        <span class="attribute">display</span>: block;</span></span><br><span class="line"><span class="language-css">        <span class="attribute">width</span>: <span class="number">100px</span>;</span></span><br><span class="line"><span class="language-css">        <span class="attribute">margin-left</span>: auto;</span></span><br><span class="line"><span class="language-css">        <span class="attribute">margin-right</span>: auto;</span></span><br><span class="line"><span class="language-css">      &#125;</span></span><br><span class="line"><span class="language-css">      <span class="selector-tag">body</span> &#123;</span></span><br><span class="line"><span class="language-css">        <span class="attribute">text-align</span>: center;</span></span><br><span class="line"><span class="language-css">      &#125;</span></span><br><span class="line"><span class="language-css">      <span class="selector-class">.content</span> &#123;</span></span><br><span class="line"><span class="language-css">        <span class="attribute">margin-right</span>: auto;</span></span><br><span class="line"><span class="language-css">        <span class="attribute">margin-left</span>: auto;</span></span><br><span class="line"><span class="language-css">        <span class="attribute">height</span>: <span class="number">200px</span>;</span></span><br><span class="line"><span class="language-css">      &#125;</span></span><br><span class="line"><span class="language-css">      <span class="selector-class">.login</span> &#123;</span></span><br><span class="line"><span class="language-css">        <span class="attribute">display</span>: block;</span></span><br><span class="line"><span class="language-css">        <span class="attribute">margin-right</span>: auto;</span></span><br><span class="line"><span class="language-css">        <span class="attribute">margin-left</span>: auto;</span></span><br><span class="line"><span class="language-css">        <span class="attribute">margin-top</span>: <span class="number">5px</span>;</span></span><br><span class="line"><span class="language-css">      &#125;</span></span><br><span class="line"><span class="language-css">    </span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;content&quot;</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">&quot;/exercise_login&quot;</span> <span class="attr">method</span>=<span class="string">&quot;POST&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;row&quot;</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;form-group&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">label</span>&gt;</span>用户名:<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">input</span></span></span><br><span class="line"><span class="tag">              <span class="attr">name</span>=<span class="string">&quot;username&quot;</span></span></span><br><span class="line"><span class="tag">              <span class="attr">type</span>=<span class="string">&quot;text&quot;</span></span></span><br><span class="line"><span class="tag">              <span class="attr">class</span>=<span class="string">&quot;form-control&quot;</span></span></span><br><span class="line"><span class="tag">              <span class="attr">placeholder</span>=<span class="string">&quot;请输入:kingname&quot;</span></span></span><br><span class="line"><span class="tag">              <span class="attr">value</span>=<span class="string">&quot;&quot;</span></span></span><br><span class="line"><span class="tag">            /&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;row&quot;</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;form-group&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">label</span>&gt;</span>密 码 :<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">input</span></span></span><br><span class="line"><span class="tag">              <span class="attr">name</span>=<span class="string">&quot;password&quot;</span></span></span><br><span class="line"><span class="tag">              <span class="attr">type</span>=<span class="string">&quot;password&quot;</span></span></span><br><span class="line"><span class="tag">              <span class="attr">class</span>=<span class="string">&quot;form-control&quot;</span></span></span><br><span class="line"><span class="tag">              <span class="attr">placeholder</span>=<span class="string">&quot;请输入:genius&quot;</span></span></span><br><span class="line"><span class="tag">              <span class="attr">value</span>=<span class="string">&quot;&quot;</span></span></span><br><span class="line"><span class="tag">            /&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;row&quot;</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">label</span> <span class="attr">class</span>=<span class="string">&quot;checkbox pull-right&quot;</span></span></span><br><span class="line"><span class="tag">            &gt;</span>自动登录</span><br><span class="line">            <span class="tag">&lt;<span class="name">input</span></span></span><br><span class="line"><span class="tag">              <span class="attr">type</span>=<span class="string">&quot;checkbox&quot;</span></span></span><br><span class="line"><span class="tag">              <span class="attr">name</span>=<span class="string">&quot;rememberme&quot;</span></span></span><br><span class="line"><span class="tag">              <span class="attr">value</span>=<span class="string">&quot;Yes&quot;</span></span></span><br><span class="line"><span class="tag">              <span class="attr">data-toggle</span>=<span class="string">&quot;checkbox&quot;</span></span></span><br><span class="line"><span class="tag">            /&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;row&quot;</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;col-md-12&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">button</span> <span class="attr">type</span>=<span class="string">&quot;submit&quot;</span> <span class="attr">class</span>=<span class="string">&quot;login&quot;</span>&gt;</span>登录<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;static/js/jquery-3.2.1.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br><span class="line">============开始登陆==============</span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Login Success<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;content&quot;</span>&gt;</span></span><br><span class="line">      如果你看到这一行内容，说明你已经登录成功。<span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;/exercise_logout&quot;</span></span></span><br><span class="line"><span class="tag">        &gt;</span>退出登录&lt;/a</span><br><span class="line">      &gt;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="8-2-验证码"><a href="#8-2-验证码" class="headerlink" title="8.2 验证码"></a>8.2 验证码</h2><h3 id="8-2-1-肉眼打码"><a href="#8-2-1-肉眼打码" class="headerlink" title="8.2.1 肉眼打码"></a>8.2.1 肉眼打码</h3><ul><li>对于一次登录就可以长时间使用的情况，只需要识别一次验证码即可</li><li>这种情况下，与其花时间开发一个自动识别验证码的程序，不如直接肉眼识别</li><li><p><strong>肉眼识别验证码有两种情况，借助浏览器与不借助浏览器</strong></p></li><li><p>1、借助浏览器<br>在模拟登录中讲到过 Cookies，通过 Cookies 能实现绕过登录，从而直接访问需要登录的网站。因此，对于需要输入验证码才能进行登录的网站，可以手动在浏览器登录网站，并通过 Chrome 获取 Cookies，然后使用 Cookies 来访问网站<br>这样就可以实现人工输入一次验证码，然后很长时间不再登录。</p></li><li>2、不借助浏览器<br>对于仅仅需要识别图片的验证码，可以使用这种方式——先把验证码下载到本地，然后肉眼去识别并手动输入给爬虫</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">手动输入验证码的一般流程如下:</span><br><span class="line">（1）爬虫访问登录页面</span><br><span class="line">（2）分析网页源代码，获取验证码地址</span><br><span class="line">（3）下载验证码到本地</span><br><span class="line">（4）打开验证码，人眼读取内容</span><br><span class="line">（5）构造POST的数据，填入验证码</span><br><span class="line">（6）POST提交</span><br></pre></td></tr></table></figure><ul><li>_需要注意的是，其中的（2）、（3）、（4）、（5）、（6）步是一气呵成的，是在爬虫运行的时候做的。绝对不能先把爬虫程序关闭，肉眼识别验证码以后再重新运行_</li></ul><p><strong><a href="http://exercise.kingname.info/exercise_captcha.html">练习页面</a></strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> lxml.html</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://exercise.kingname.info/exercise_captcha.html&#x27;</span></span><br><span class="line">url_check = <span class="string">&#x27;http://exercise.kingname.info/exercise_captcha_check&#x27;</span></span><br><span class="line"></span><br><span class="line">session = requests.session()</span><br><span class="line">html = session.get(url).content</span><br><span class="line">selector = lxml.html.fromstring(html)</span><br><span class="line">captcha_url = selector.xpath(<span class="string">&#x27;//img/@src&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">image = requests.get(<span class="string">&#x27;http://exercise.kingname.info/&#x27;</span>+captcha_url).content</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;captcha.png&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(image)</span><br><span class="line"></span><br><span class="line">captcha = <span class="built_in">input</span>(<span class="string">&#x27;请查看图片，然后输入在这里：&#x27;</span>)</span><br><span class="line">after_check = session.post(url_check,data=&#123;<span class="string">&#x27;captcha&#x27;</span>:captcha&#125;)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;请输入验证码后，网站返回：<span class="subst">&#123;after_check.content.decode()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>结果</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">请查看图片，然后输入在这里：1595</span><br><span class="line">请输入验证码后，网站返回：看到这个页面，说明你的验证码输入正确</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yq010105/Blog_images/master/img/captcha.png" alt="验证码"></p><h3 id="8-2-2-自动打码"><a href="#8-2-2-自动打码" class="headerlink" title="8.2.2 自动打码"></a>8.2.2 自动打码</h3><p>1、Python 图像识别</p><ul><li>对于验证码识别，Python 也有现成的库来使用</li><li>开源的 OCR 库 pytesseract 配合图像识别引擎 tesseract，可以用来将图片中的文字转换为文本</li><li>这种方式在爬虫中的应用并不多见。因为现在大部分的验证码都加上了干扰的纹理，已经很少能用单机版的图片识别方式来识别了。所以如果使用这种方式，只有两种情况：网站的验证码极其简单工整，使用大量的验证码来训练 tesseract<br>_安装 tesseract_<br>打开网页下载安装包：<a href="https://github.com/tesseract-ocr/tesseract/wiki/Downloads">https://github.com/tesseract-ocr/tesseract/wiki/Downloads</a> ,在“3rd party Windows exe’s/ installer”下面可以找到.exe 安装包<br>_安装 Python 库_<br>pip install Pillow<br>pip install pytesseract<br>其中，Pillow 是 Python 中专门用来处理图像的第三方库，pytesseract 是专门用来操作 tesseract 的第三方库<br>_tesseract 的使用_</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">          tesseract的使用非常简单。</span><br><span class="line">① 导入pytesseract和Pillow。</span><br><span class="line">② 打开图片。</span><br><span class="line">③ 识别。</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过以下代码来实现最简单的图片识别：</span></span><br><span class="line"><span class="keyword">import</span> pytesseract</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;验证码.png&#x27;</span>)</span><br><span class="line">code = pytesseract.image_to_string(image)</span><br><span class="line"><span class="built_in">print</span>(code)</span><br></pre></td></tr></table></figure><p>2、打码网站<br>在线验证码识别的网站，简称打码网站。这些网站有一些是使用深度学习技术识别验证码，有一些是雇佣了很多人来人肉识别验证码<br>网站提供了接口来实现验证码识别服务。使用打码网站理论上可以识别任何使用输入方式来验证的验证码</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这种打码网站的流程一般是这样的。</span><br><span class="line">① 将验证码上传到网站服务器。</span><br><span class="line">② 网站服务器将验证码分发给打码工人。</span><br><span class="line">③ 打码工人肉眼识别验证码并上传结果。</span><br><span class="line">④ 网站将结果返回。</span><br></pre></td></tr></table></figure><p>_使用在线打码_<br>在百度或者谷歌上面搜索“验证码在线识别”，就可以找到很多提供在线打码的网站。但是由于一般这种打码网站是需要交费才能使用的，所以要注意财产安全</p><h2 id="8-3-案例-自动登录果壳网"><a href="#8-3-案例-自动登录果壳网" class="headerlink" title="8.3 案例-自动登录果壳网"></a>8.3 案例-自动登录果壳网</h2><p><a href="https://www.guokr.com">目标网站</a><br>使用模拟登录与验证码识别的技术实现自动登录果壳网。 果壳网的登录界面有验证码，请使用人工或者在线打码的方式识别验证码，并让爬虫登录。登录以后可以正确显示“个人资料设置”界面的源代码</p><ul><li>涉及的知识点：</li><li>（1）爬虫识别验证码。</li><li>（2）爬虫模拟登录。<blockquote><p>来自<a href="https://github.com/kingname/SourceCodeOfBook/tree/master/%E7%AC%AC8%E7%AB%A0/program">第八章</a>，需要使用再来深度学习</p></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python Spider </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo搭建博客的基本知识</title>
      <link href="/Learn/Learn-Hexo/"/>
      <url>/Learn/Learn-Hexo/</url>
      
        <content type="html"><![CDATA[<p>Hexo博客基本操作，包括主题的修改和博客文件的备份等等</p><blockquote><p><strong>建立 hexo_blog，参考 b 站 up:<a href="https://space.bilibili.com/384068749">CodeSheep</a>的<a href="https://www.bilibili.com/video/av44544186">视频</a></strong><br><span id="more"></span></p></blockquote><iframe src="//player.bilibili.com/player.html?aid=44544186&bvid=BV1Yb411a7ty&cid=158772893&page=1" width="730px" height="500px" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><blockquote><blockquote><p><strong>所用主题 :<a href="https://github.com/litten/hexo-theme-yilia">yilia</a>，主要按照<a href="http://yansheng836.coding.me/">大佬博客</a>中的<a href="http://yansheng836.coding.me/tags/hexo/">这一分类</a>进行修改</strong></p></blockquote></blockquote><!-- more --><h2 id="1-hexo-基操"><a href="#1-hexo-基操" class="headerlink" title="1. hexo 基操"></a>1. hexo 基操</h2><h3 id="1-1-master-分支"><a href="#1-1-master-分支" class="headerlink" title="1.1 master 分支"></a>1.1 master 分支</h3><div class="table-container"><table><thead><tr><th style="text-align:center">描述</th><th style="text-align:center">代码</th></tr></thead><tbody><tr><td style="text-align:center">新建页面&emsp;type:’tags’</td><td style="text-align:center"><code>hexo new page &#39;title&#39;</code></td></tr><tr><td style="text-align:center">新建 md</td><td style="text-align:center"><code>hexo n &quot;title&quot;</code></td></tr><tr><td style="text-align:center">清理</td><td style="text-align:center"><code>hexo clean</code></td></tr><tr><td style="text-align:center">生成</td><td style="text-align:center"><code>hexo g</code></td></tr><tr><td style="text-align:center">部署到远端；推到 github</td><td style="text-align:center"><code>hexo d</code></td></tr><tr><td style="text-align:center">启动预览,blog</td><td style="text-align:center"><code>hexo s</code></td></tr></tbody></table></div><h3 id="1-2-blog-分支"><a href="#1-2-blog-分支" class="headerlink" title="1.2 blog 分支"></a>1.2 blog 分支</h3><ul><li><p>6、将当前目录下修改的所有代码从工作区添加到暂存区 . 代表当前目录</p><p><code>git add .</code></p></li><li><p>7、将缓存区内容添加到本地仓库</p><p><code>git commit -m &quot;提交信息&quot;</code></p></li><li><p>8、将本地版本库推送到远程服务器,将本地库如果设置了 blog 为默认分支，可以直接 git push</p><p><code>git push origin blog</code></p></li><li><p>9、先将远程仓库 master 中的信息同步到本地仓库 master 中</p><p><code>git pull origin master</code></p></li><li><p>10、查看工作区代码相对于暂存区的差别</p><p><code>git status</code></p></li></ul><blockquote><p>参考<a href="https://www.jianshu.com/p/2e1d551b8261" title="简书">简书网站</a></p></blockquote><h2 id="2-source-md-blog-目录下"><a href="#2-source-md-blog-目录下" class="headerlink" title="2. source/md-(blog 目录下)"></a>2. source/md-(blog 目录下)</h2><ul><li>没有跳过的会转化为 HTML，在 blog 中体现</li></ul><h2 id="3-添加背景图片和左侧图片"><a href="#3-添加背景图片和左侧图片" class="headerlink" title="3. 添加背景图片和左侧图片"></a>3. 添加背景图片和左侧图片</h2><p>主要在<code>source/main.0cf68a.css</code> 文件中修改</p><blockquote><p>具体修改参考<a href="http://yansheng836.coding.me/">大佬博客</a>中的<a href="http://yansheng836.coding.me/article/72a91df5.html">这一篇</a></p></blockquote><h3 id="3-1-左侧背景"><a href="#3-1-左侧背景" class="headerlink" title="3.1 左侧背景"></a>3.1 左侧背景</h3><p><code>themes/yilia/layout/_partial/left-col.ejs</code>文件中注释掉原来代码，添加新的无属性代码</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- &lt;div class=&quot;overlay&quot; style=&quot;background: &lt;%= theme.style &amp;&amp; theme.style.header ? theme.style.header : defaultBg %&gt;&quot;&gt; --&gt;</span><br><span class="line">&lt;!-- 左侧边栏（上半部分）不设置背景颜色 --&gt;</span><br><span class="line">&lt;div class=&quot;overlay&quot; &gt;</span><br></pre></td></tr></table></figure><p><code>themes\yilia\source\main.0cf68a.css</code>中修改添加背景图片</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.left-col</span> &#123;</span><br><span class="line">  <span class="comment">/* background:#fff; 注释掉原来的修改背景*/</span></span><br><span class="line">  <span class="attribute">background</span>: <span class="built_in">linear-gradient</span>(</span><br><span class="line">      <span class="built_in">rgba</span>(<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">0.5</span>),</span><br><span class="line">      <span class="built_in">rgba</span>(<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">0.5</span>)</span><br><span class="line">    ),</span><br><span class="line">    <span class="built_in">url</span>(<span class="string">&quot;http://bucket836.oss-cn-shenzhen.aliyuncs.com/wallpaper/381535373.jpeg&quot;</span>)</span><br><span class="line">      no-repeat <span class="number">0%</span> <span class="number">20%</span> / cover;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">300px</span>;</span><br><span class="line">  <span class="attribute">position</span>: fixed;</span><br><span class="line">  <span class="attribute">opacity</span>: <span class="number">1</span>;</span><br><span class="line">  <span class="attribute">transition</span>: all <span class="number">0.3s</span> ease-in;</span><br><span class="line">  -ms-<span class="attribute">transition</span>: all <span class="number">0.3s</span> ease-in;</span><br><span class="line">  <span class="attribute">height</span>: <span class="number">100%</span>;</span><br><span class="line">  <span class="attribute">z-index</span>: <span class="number">999</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果你的背景图片跟文字颜色不匹配(字看不清)，可以修改中文件</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.left-col</span> <span class="selector-id">#header</span> <span class="selector-tag">a</span> &#123;</span><br><span class="line">   <span class="attribute">color</span>:<span class="number">#696969</span></span><br><span class="line">   color:<span class="number">#673ab7</span>^M</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.left-col</span> <span class="selector-id">#header</span> <span class="selector-tag">a</span><span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">   <span class="attribute">color</span>:<span class="number">#b0a0aa</span></span><br><span class="line">   color: <span class="number">#03A9F4</span>^M</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.left-col</span> <span class="selector-id">#header</span> <span class="selector-class">.header-subtitle</span> &#123;</span><br><span class="line">    <span class="attribute">text-align</span>:center;</span><br><span class="line">   <span class="attribute">color</span>:<span class="number">#999</span>;</span><br><span class="line">   <span class="attribute">color</span>:<span class="number">#673ab7</span>;</span><br><span class="line">    <span class="attribute">font-size</span>:<span class="number">18px</span>;</span><br></pre></td></tr></table></figure><h3 id="3-2-文章背景"><a href="#3-2-文章背景" class="headerlink" title="3.2 文章背景"></a>3.2 文章背景</h3><p>先将文章背景调成透明色,搜索<code>.article &#123;</code></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.article</span> &#123;</span><br><span class="line">  <span class="attribute">margin</span>: <span class="number">30px</span>;</span><br><span class="line">  <span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ddd</span>;</span><br><span class="line">  <span class="attribute">border-top</span>: <span class="number">1px</span> solid <span class="number">#fff</span>;</span><br><span class="line">  <span class="attribute">border-bottom</span>: <span class="number">1px</span> solid <span class="number">#fff</span>;</span><br><span class="line">  <span class="attribute">background</span>: <span class="built_in">rgba</span>(<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">0.4</span>); <span class="comment">/*调成透明色，才能看清背景图片*/</span></span><br><span class="line">  <span class="attribute">transition</span>: all <span class="number">0.2s</span> ease-in;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后再添加背景图片，搜索<code>body &#123;</code></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">body</span> &#123;</span><br><span class="line">  <span class="attribute">margin</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">font-size</span>: <span class="number">14px</span>;</span><br><span class="line">  <span class="attribute">font-family</span>: Helvetica Neue, Helvetica, STHeiTi, Arial, sans-serif;</span><br><span class="line">  <span class="attribute">line-height</span>: <span class="number">1.5</span>;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#333</span>;</span><br><span class="line">  <span class="comment">/*background-color:rgb(85, 144, 161); */</span></span><br><span class="line">  <span class="attribute">min-height</span>: <span class="number">100%</span>;</span><br><span class="line">  <span class="attribute">background</span>: <span class="built_in">linear-gradient</span>(</span><br><span class="line">      <span class="built_in">rgba</span>(<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">0.5</span>),</span><br><span class="line">      <span class="built_in">rgba</span>(<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">0.5</span>)</span><br><span class="line">    ), <span class="built_in">url</span>(<span class="string">./img/1_.jpg</span>) no-repeat <span class="number">0%</span> <span class="number">20%</span> / cover; <span class="comment">/*添加背景图片*/</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>如果修改完代码，背景还没有变，可能你的 body 或者 article 定义了两次，前面有一个 body{},后面还有一个，只要删掉后面的就可以了</strong><br>_需要自己好好的找一找_</p><p><strong>如果你的 main.0cf68a.css,文件代码很乱，可以在<a href="https://github.com/yq010105/hexo_themes/blob/master/yilia/source/main.0cf68a.css">我的 Github</a>中复制</strong> _别问我怎么知道的_</p><h3 id="3-3-copy-代码块"><a href="#3-3-copy-代码块" class="headerlink" title="3.3 copy 代码块"></a>3.3 copy 代码块</h3><blockquote><p>按照<a href="http://yansheng836.coding.me/">大佬博客</a>中的<a href="http://yansheng836.coding.me/article/e9d1b881.html">这一篇</a></p></blockquote><h2 id="4-添加网站运行时间"><a href="#4-添加网站运行时间" class="headerlink" title="4. 添加网站运行时间"></a>4. 添加网站运行时间</h2><p>_简单配置_<br>修改<code>\themes\yilia\layout\_partial\footer.ejs</code>，在<code>&lt;/footer&gt;</code>上面添加如下内容</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--《添加网站运行时间 --&gt;</span><br><span class="line">&lt;!--<span class="language-xml"><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span>--&gt;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">&quot;timeDate&quot;</span>&gt;</span>载入天数...<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">&quot;times&quot;</span>&gt;</span>载入时分秒...<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">    <span class="keyword">var</span> now = <span class="keyword">new</span> <span class="title class_">Date</span>();</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml"></span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">    <span class="keyword">function</span> <span class="title function_">createtime</span>(<span class="params"></span>) &#123;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        <span class="keyword">var</span> grt = <span class="keyword">new</span> <span class="title class_">Date</span>(<span class="string">&quot;07/25/2019 12:00:00&quot;</span>); <span class="comment">//此处修改你的建站时间或者网站上线时间</span></span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        now.<span class="title function_">setTime</span>(now.<span class="title function_">getTime</span>() + <span class="number">250</span>);</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        days = (now - grt) / <span class="number">1000</span> / <span class="number">60</span> / <span class="number">60</span> / <span class="number">24</span>;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        dnum = <span class="title class_">Math</span>.<span class="title function_">floor</span>(days);</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        hours = (now - grt) / <span class="number">1000</span> / <span class="number">60</span> / <span class="number">60</span> - (<span class="number">24</span> * dnum);</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        hnum = <span class="title class_">Math</span>.<span class="title function_">floor</span>(hours);</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        <span class="keyword">if</span> (<span class="title class_">String</span>(hnum).<span class="property">length</span> == <span class="number">1</span>) &#123;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">            hnum = <span class="string">&quot;0&quot;</span> + hnum;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        &#125;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        minutes = (now - grt) / <span class="number">1000</span> / <span class="number">60</span> - (<span class="number">24</span> * <span class="number">60</span> * dnum) - (<span class="number">60</span> * hnum);</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        mnum = <span class="title class_">Math</span>.<span class="title function_">floor</span>(minutes);</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        <span class="keyword">if</span> (<span class="title class_">String</span>(mnum).<span class="property">length</span> == <span class="number">1</span>) &#123;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">            mnum = <span class="string">&quot;0&quot;</span> + mnum;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        &#125;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        seconds = (now - grt) / <span class="number">1000</span> - (<span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span> * dnum) - (<span class="number">60</span> * <span class="number">60</span> * hnum) - (<span class="number">60</span> * mnum);</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        snum = <span class="title class_">Math</span>.<span class="title function_">round</span>(seconds);</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        <span class="keyword">if</span> (<span class="title class_">String</span>(snum).<span class="property">length</span> == <span class="number">1</span>) &#123;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">            snum = <span class="string">&quot;0&quot;</span> + snum;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        &#125;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        <span class="variable language_">document</span>.<span class="title function_">getElementById</span>(<span class="string">&quot;timeDate&quot;</span>).<span class="property">innerHTML</span> = <span class="string">&quot; | 本站已安全运行 &quot;</span> + dnum + <span class="string">&quot; 天 &quot;</span>;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">        <span class="variable language_">document</span>.<span class="title function_">getElementById</span>(<span class="string">&quot;times&quot;</span>).<span class="property">innerHTML</span> = hnum + <span class="string">&quot; 小时 &quot;</span> + mnum + <span class="string">&quot; 分 &quot;</span> + snum + <span class="string">&quot; 秒&quot;</span>;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">    &#125;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">    <span class="built_in">setInterval</span>(<span class="string">&quot;createtime()&quot;</span>, <span class="number">250</span>);</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">&lt;!-- 添加网站运行时间》 --&gt;</span><br></pre></td></tr></table></figure><blockquote><p>参考<a href="http://yansheng836.coding.me/article/50902a4.html">这一篇</a></p></blockquote><h2 id="5-看板娘-かんばんむすめ"><a href="#5-看板娘-かんばんむすめ" class="headerlink" title="5. 看板娘(かんばんむすめ)"></a>5. 看板娘(かんばんむすめ)</h2><ul><li><strong>安装插件:</strong> <code>npm install --save hexo-helper-live2d</code> <strong><a href="https://github.com/EYHN/hexo-helper-live2d">github 项目</a></strong></li><li><strong>安装模型:</strong> <code>npm install live2d-widget-model-模型名</code> <strong><a href="https://huaji8.top/post/live2d-plugin-2.0/">模型名参考</a></strong></li><li><strong>在 blog/_config.yml 中添加</strong></li><li>_我在 yilia 中配置没有效果，但在 blog 中有效果_</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Live2D</span></span><br><span class="line"><span class="comment">## https://github.com/EYHN/hexo-helper-live2d</span></span><br><span class="line"><span class="attr">live2d:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">scriptFrom:</span> <span class="string">local</span></span><br><span class="line">  <span class="attr">pluginRootPath:</span> <span class="string">live2dw/</span></span><br><span class="line">  <span class="attr">pluginJsPath:</span> <span class="string">lib/</span></span><br><span class="line">  <span class="attr">pluginModelPath:</span> <span class="string">assets/</span></span><br><span class="line">  <span class="attr">tagMode:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">debug:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">model:</span></span><br><span class="line">    <span class="attr">use:</span> <span class="string">live2d-widget-model-模型名</span> <span class="comment"># 模型：https://huaji8.top/post/live2d-plugin-2.0/</span></span><br><span class="line">  <span class="attr">display:</span></span><br><span class="line">    <span class="attr">position:</span> <span class="string">right</span></span><br><span class="line">    <span class="attr">width:</span> <span class="number">150</span></span><br><span class="line">    <span class="attr">height:</span> <span class="number">300</span></span><br><span class="line">  <span class="attr">mobile:</span></span><br><span class="line">    <span class="attr">show:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><ul><li><strong>关闭 live2D:</strong> <code>enable:false</code></li><li><strong>卸载模型:</strong> <code>npm uninstall live2d-widget-model-模型名</code></li><li><strong>卸载插件:</strong> <code>npm uninstall hexo-helper-live2d</code></li><li><strong>删掉 yilia/_config.yml 中配置</strong></li><li><del>_过于占内存，已卸载_</del>、_33 真是太可爱了_<blockquote><p><a href="https://github.com/52cik/bilibili-haruna">2233 娘的 model</a><br>参考<a href="http://yansheng836.coding.me/article/e239dc63.html">这一篇</a></p></blockquote></li></ul><h2 id="6-网易云音乐插件"><a href="#6-网易云音乐插件" class="headerlink" title="6. 网易云音乐插件"></a>6. 网易云音乐插件</h2><ul><li>在<code>/yilia/layout/_partial/post/left-col.ejs</code>中最后的<code>&lt;/nav&gt;</code>标签上方添加</li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;% <span class="keyword">if</span> (theme.<span class="property">music</span> &amp;&amp; theme.<span class="property">music</span>.<span class="property">enable</span>)&#123; %&gt;</span><br><span class="line">&lt;%# <span class="string">&quot;网易云音乐插件&quot;</span> %&gt;</span><br><span class="line">&lt;%# <span class="string">&quot;bottom:120px; left:auto;position:absolute;  width:85%&quot;</span> %&gt;</span><br><span class="line">&lt;% <span class="keyword">var</span> defaultHeight = theme.<span class="property">music</span>.<span class="property">type</span> == <span class="number">1</span> ? <span class="string">&#x27;32&#x27;</span> : <span class="string">&#x27;66&#x27;</span>; %&gt;</span><br><span class="line">&lt;% <span class="keyword">var</span> defaultIframeHeight = theme.<span class="property">music</span>.<span class="property">type</span> == <span class="number">1</span> ? <span class="string">&#x27;52&#x27;</span> : <span class="string">&#x27;86&#x27;</span>; %&gt;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">iframe</span> <span class="attr">frameborder</span>=<span class="string">&quot;no&quot;</span> <span class="attr">border</span>=<span class="string">&quot;0&quot;</span> <span class="attr">marginwidth</span>=<span class="string">&quot;0&quot;</span> <span class="attr">marginheight</span>=<span class="string">&quot;0&quot;</span> <span class="attr">width</span>=<span class="string">&quot;240&quot;</span> <span class="attr">height</span>=<span class="string">&quot;&lt;%=defaultIframeHeight%&gt;&quot;</span> <span class="attr">src</span>=<span class="string">&quot;//music.163.com/outchain/player?type=2&amp;id=&lt;%=theme.music.id||1400594005%&gt;&amp;auto=&lt;%=theme.music.autoPlay?1:0%&gt;&amp;height=&lt;%=defaultHeight%&gt;&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">iframe</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line">&lt;% <span class="keyword">if</span> (theme.<span class="property">music</span>.<span class="property">text</span> || theme.<span class="property">music</span>.<span class="property">text</span> == <span class="literal">null</span>)&#123; %&gt;</span><br><span class="line">&lt;% <span class="keyword">var</span> musicText = ( theme.<span class="property">music</span>.<span class="property">text</span> == <span class="literal">null</span> || theme.<span class="property">music</span>.<span class="property">text</span> == <span class="literal">true</span> ) ? <span class="string">&quot;这似乎是首纯音乐，请尽情的欣赏它吧！&quot;</span> : theme.<span class="property">music</span>.<span class="property">text</span>; %&gt;</span><br><span class="line">&lt;p style=&quot;font-size: 24px;font-family: &#x27;Times New Roman&#x27;, Times, serif;&quot;&gt;&lt;%-musicText%&gt;&lt;p&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure><ul><li>然后在 yilia 配置文件<code>_config.yml</code>中添加</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网易云音乐插件</span></span><br><span class="line"><span class="attr">music:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># 播放器尺寸类型(1：长尺寸、2：短尺寸)</span></span><br><span class="line">  <span class="attr">type:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment">#id: 1334445174  # 网易云分享的音乐ID(更换音乐请更改此配置项)</span></span><br><span class="line">  <span class="attr">autoPlay:</span> <span class="literal">false</span> <span class="comment"># 是否开启自动播放</span></span><br><span class="line">  <span class="comment"># 提示文本(关闭请设置为false)</span></span><br><span class="line">  <span class="attr">text:</span> <span class="string">&quot;底部文字&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p>参考大佬做的<a href="https://github.com/JoeyBling/hexo-theme-yilia-plus">yilia-plus</a>中的配置</p></blockquote><h2 id="7-添加背景特效"><a href="#7-添加背景特效" class="headerlink" title="7. 添加背景特效"></a>7. 添加背景特效</h2><h3 id="7-1-点击爱心"><a href="#7-1-点击爱心" class="headerlink" title="7.1 点击爱心"></a>7.1 点击爱心</h3><ul><li>在<code>/yilia/source/js/</code>下添加<code>love.js</code>文件,书写代码</li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">(<span class="keyword">function</span>(<span class="params"><span class="variable language_">window</span>, <span class="variable language_">document</span>, <span class="literal">undefined</span></span>) &#123;</span><br><span class="line">  <span class="keyword">var</span> hearts = [];</span><br><span class="line">  <span class="variable language_">window</span>.<span class="property">requestAnimationFrame</span> = (<span class="keyword">function</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      <span class="variable language_">window</span>.<span class="property">requestAnimationFrame</span> ||</span><br><span class="line">      <span class="variable language_">window</span>.<span class="property">webkitRequestAnimationFrame</span> ||</span><br><span class="line">      <span class="variable language_">window</span>.<span class="property">mozRequestAnimationFrame</span> ||</span><br><span class="line">      <span class="variable language_">window</span>.<span class="property">oRequestAnimationFrame</span> ||</span><br><span class="line">      <span class="variable language_">window</span>.<span class="property">msRequestAnimationFrame</span> ||</span><br><span class="line">      <span class="keyword">function</span>(<span class="params">callback</span>) &#123;</span><br><span class="line">        <span class="built_in">setTimeout</span>(callback, <span class="number">1000</span> / <span class="number">60</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    );</span><br><span class="line">  &#125;)();</span><br><span class="line">  <span class="title function_">init</span>();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">function</span> <span class="title function_">init</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="title function_">css</span>(</span><br><span class="line">      <span class="string">&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &#x27;&#x27;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: absolute;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;</span></span><br><span class="line">    );</span><br><span class="line">    <span class="title function_">attachEvent</span>();</span><br><span class="line">    <span class="title function_">gameloop</span>();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">function</span> <span class="title function_">gameloop</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; hearts.<span class="property">length</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">if</span> (hearts[i].<span class="property">alpha</span> &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="variable language_">document</span>.<span class="property">body</span>.<span class="title function_">removeChild</span>(hearts[i].<span class="property">el</span>);</span><br><span class="line">        hearts.<span class="title function_">splice</span>(i, <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      hearts[i].<span class="property">y</span>--;</span><br><span class="line">      hearts[i].<span class="property">scale</span> += <span class="number">0.004</span>;</span><br><span class="line">      hearts[i].<span class="property">alpha</span> -= <span class="number">0.013</span>;</span><br><span class="line">      hearts[i].<span class="property">el</span>.<span class="property">style</span>.<span class="property">cssText</span> =</span><br><span class="line">        <span class="string">&quot;left:&quot;</span> +</span><br><span class="line">        hearts[i].<span class="property">x</span> +</span><br><span class="line">        <span class="string">&quot;px;top:&quot;</span> +</span><br><span class="line">        hearts[i].<span class="property">y</span> +</span><br><span class="line">        <span class="string">&quot;px;opacity:&quot;</span> +</span><br><span class="line">        hearts[i].<span class="property">alpha</span> +</span><br><span class="line">        <span class="string">&quot;;transform:scale(&quot;</span> +</span><br><span class="line">        hearts[i].<span class="property">scale</span> +</span><br><span class="line">        <span class="string">&quot;,&quot;</span> +</span><br><span class="line">        hearts[i].<span class="property">scale</span> +</span><br><span class="line">        <span class="string">&quot;) rotate(45deg);background:&quot;</span> +</span><br><span class="line">        hearts[i].<span class="property">color</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="title function_">requestAnimationFrame</span>(gameloop);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">function</span> <span class="title function_">attachEvent</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> old = <span class="keyword">typeof</span> <span class="variable language_">window</span>.<span class="property">onclick</span> === <span class="string">&quot;function&quot;</span> &amp;&amp; <span class="variable language_">window</span>.<span class="property">onclick</span>;</span><br><span class="line">    <span class="variable language_">window</span>.<span class="property">onclick</span> = <span class="keyword">function</span>(<span class="params">event</span>) &#123;</span><br><span class="line">      old &amp;&amp; <span class="title function_">old</span>();</span><br><span class="line">      <span class="title function_">createHeart</span>(event);</span><br><span class="line">    &#125;;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">function</span> <span class="title function_">createHeart</span>(<span class="params">event</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> d = <span class="variable language_">document</span>.<span class="title function_">createElement</span>(<span class="string">&quot;div&quot;</span>);</span><br><span class="line">    d.<span class="property">className</span> = <span class="string">&quot;heart&quot;</span>;</span><br><span class="line">    hearts.<span class="title function_">push</span>(&#123;</span><br><span class="line">      <span class="attr">el</span>: d,</span><br><span class="line">      <span class="attr">x</span>: event.<span class="property">clientX</span> - <span class="number">5</span>,</span><br><span class="line">      <span class="attr">y</span>: event.<span class="property">clientY</span> - <span class="number">5</span>,</span><br><span class="line">      <span class="attr">scale</span>: <span class="number">1</span>,</span><br><span class="line">      <span class="attr">alpha</span>: <span class="number">1</span>,</span><br><span class="line">      <span class="attr">color</span>: <span class="title function_">randomColor</span>()</span><br><span class="line">    &#125;);</span><br><span class="line">    <span class="variable language_">document</span>.<span class="property">body</span>.<span class="title function_">appendChild</span>(d);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">function</span> <span class="title function_">css</span>(<span class="params">css</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> style = <span class="variable language_">document</span>.<span class="title function_">createElement</span>(<span class="string">&quot;style&quot;</span>);</span><br><span class="line">    style.<span class="property">type</span> = <span class="string">&quot;text/css&quot;</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      style.<span class="title function_">appendChild</span>(<span class="variable language_">document</span>.<span class="title function_">createTextNode</span>(css));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ex) &#123;</span><br><span class="line">      style.<span class="property">styleSheet</span>.<span class="property">cssText</span> = css;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable language_">document</span>.<span class="title function_">getElementsByTagName</span>(<span class="string">&quot;head&quot;</span>)[<span class="number">0</span>].<span class="title function_">appendChild</span>(style);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">function</span> <span class="title function_">randomColor</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">      <span class="string">&quot;rgb(&quot;</span> +</span><br><span class="line">      ~~(<span class="title class_">Math</span>.<span class="title function_">random</span>() * <span class="number">255</span>) +</span><br><span class="line">      <span class="string">&quot;,&quot;</span> +</span><br><span class="line">      ~~(<span class="title class_">Math</span>.<span class="title function_">random</span>() * <span class="number">255</span>) +</span><br><span class="line">      <span class="string">&quot;,&quot;</span> +</span><br><span class="line">      ~~(<span class="title class_">Math</span>.<span class="title function_">random</span>() * <span class="number">255</span>) +</span><br><span class="line">      <span class="string">&quot;)&quot;</span></span><br><span class="line">    );</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)(<span class="variable language_">window</span>, <span class="variable language_">document</span>);</span><br></pre></td></tr></table></figure><ul><li>在<code>yilia/layout/layout.ejs</code>中添加代码(切记在&lt;/body&gt;标签前添加)</li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 《页面点击小红心 --&gt;</span><br><span class="line">&lt;% <span class="keyword">if</span> (theme.<span class="property">love</span>)&#123; %&gt;</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span> <span class="attr">src</span>=<span class="string">&quot;/js/love.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">&lt;% &#125; %&gt;</span><br><span class="line">&lt;!-- 页面点击小红心》 --&gt;</span><br></pre></td></tr></table></figure><ul><li>在<code>yilia/_config.yml</code>中添加配置</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#点击小红心</span></span><br><span class="line"><span class="attr">love:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h3 id="7-2-背景线条"><a href="#7-2-背景线条" class="headerlink" title="7.2 背景线条"></a>7.2 背景线条</h3><ul><li>在<code>yilia/layout/layout.ejs</code>中添加代码</li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--动态线条背景--&gt;</span><br><span class="line">&lt;% <span class="keyword">if</span> (theme.<span class="property">canvas_nest</span>.<span class="property">enable</span>)&#123; %&gt;</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span> <span class="attr">color</span>=<span class="string">&quot;&lt;%=theme.canvas_nest.color %&gt;&quot;</span> <span class="attr">opacity</span>=<span class="string">&quot;&lt;%=theme.canvas_nest.opacity %&gt;&quot;</span></span></span></span><br><span class="line"><span class="tag"><span class="language-xml">      <span class="attr">zIndex</span>=<span class="string">&quot;&lt;%=theme.canvas_nest.zIndex %&gt;&quot;</span> <span class="attr">count</span>=<span class="string">&quot;&lt;%=theme.canvas_nest.count %&gt;&quot;</span></span></span></span><br><span class="line"><span class="tag"><span class="language-xml">      <span class="attr">src</span>=<span class="string">&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">  <span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure><ul><li>在<code>yilia/_config.yml</code>中添加配置</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动态线条效果，会向鼠标集中</span></span><br><span class="line"><span class="attr">canvas_nest:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">color:</span> <span class="string">&quot;255, 235, 59&quot;</span> <span class="comment"># color of lines, default: &#x27;0,0,0&#x27;; RGB values: (R,G,B).(<span class="doctag">note:</span> use &#x27;,&#x27; to separate.)</span></span><br><span class="line">  <span class="attr">pointColor:</span> <span class="string">&quot;156,39,176&quot;</span> <span class="comment"># color of points, default: &#x27;0,0,0&#x27;; RGB values: (R,G,B).(<span class="doctag">note:</span> use &#x27;,&#x27; to separate.)</span></span><br><span class="line">  <span class="attr">opacity:</span> <span class="string">&quot;0.8&quot;</span> <span class="comment"># the opacity of line (0~1), default: 0.5.</span></span><br><span class="line">  <span class="attr">count:</span> <span class="string">&quot;99&quot;</span> <span class="comment"># the number of lines, default: 99.</span></span><br><span class="line">  <span class="attr">zIndex:</span> <span class="string">&quot;-1&quot;</span> <span class="comment"># z-index property of the background, default: -1.</span></span><br></pre></td></tr></table></figure><h3 id="7-3-背景点击文字"><a href="#7-3-背景点击文字" class="headerlink" title="7.3 背景点击文字"></a>7.3 背景点击文字</h3><ul><li>在<code>yilia/source/js/</code>下添加<code>click_show_text.js</code>文件，添加代码</li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> a_idx = <span class="number">0</span>;</span><br><span class="line"><span class="title function_">jQuery</span>(<span class="variable language_">document</span>).<span class="title function_">ready</span>(<span class="keyword">function</span>(<span class="params">$</span>) &#123;</span><br><span class="line">  $(<span class="string">&quot;body&quot;</span>).<span class="title function_">click</span>(<span class="keyword">function</span>(<span class="params">e</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> a = <span class="keyword">new</span> <span class="title class_">Array</span>(</span><br><span class="line">      <span class="string">&quot;富强&quot;</span>,</span><br><span class="line">      <span class="string">&quot;民主&quot;</span>,</span><br><span class="line">      <span class="string">&quot;文明&quot;</span>,</span><br><span class="line">      <span class="string">&quot;和谐&quot;</span>,</span><br><span class="line">      <span class="string">&quot;自由&quot;</span>,</span><br><span class="line">      <span class="string">&quot;平等&quot;</span>,</span><br><span class="line">      <span class="string">&quot;公正&quot;</span>,</span><br><span class="line">      <span class="string">&quot;法治&quot;</span>,</span><br><span class="line">      <span class="string">&quot;爱国&quot;</span>,</span><br><span class="line">      <span class="string">&quot;敬业&quot;</span>,</span><br><span class="line">      <span class="string">&quot;诚信&quot;</span>,</span><br><span class="line">      <span class="string">&quot;友善&quot;</span></span><br><span class="line">    );</span><br><span class="line">    <span class="keyword">var</span> $i = $(<span class="string">&quot;&lt;span/&gt;&quot;</span>).<span class="title function_">text</span>(a[a_idx]);</span><br><span class="line">    a_idx = (a_idx + <span class="number">1</span>) % a.<span class="property">length</span>;</span><br><span class="line">    <span class="keyword">var</span> x = e.<span class="property">pageX</span>,</span><br><span class="line">      y = e.<span class="property">pageY</span>;</span><br><span class="line">    $i.<span class="title function_">css</span>(&#123;</span><br><span class="line">      <span class="string">&quot;z-index&quot;</span>: <span class="number">5</span>,</span><br><span class="line">      <span class="attr">top</span>: y - <span class="number">20</span>,</span><br><span class="line">      <span class="attr">left</span>: x,</span><br><span class="line">      <span class="attr">position</span>: <span class="string">&quot;absolute&quot;</span>,</span><br><span class="line">      <span class="string">&quot;font-weight&quot;</span>: <span class="string">&quot;bold&quot;</span>,</span><br><span class="line">      <span class="attr">color</span>: <span class="string">&quot;#FF0000&quot;</span></span><br><span class="line">    &#125;);</span><br><span class="line">    $(<span class="string">&quot;body&quot;</span>).<span class="title function_">append</span>($i);</span><br><span class="line">    $i.<span class="title function_">animate</span>(</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">top</span>: y - <span class="number">180</span>,</span><br><span class="line">        <span class="attr">opacity</span>: <span class="number">0</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="number">3000</span>,</span><br><span class="line">      <span class="keyword">function</span>(<span class="params"></span>) &#123;</span><br><span class="line">        $i.<span class="title function_">remove</span>();</span><br><span class="line">      &#125;</span><br><span class="line">    );</span><br><span class="line">  &#125;);</span><br><span class="line">  <span class="built_in">setTimeout</span>(<span class="string">&quot;delay()&quot;</span>, <span class="number">2000</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">delay</span>(<span class="params"></span>) &#123;</span><br><span class="line">  $(<span class="string">&quot;.buryit&quot;</span>).<span class="title function_">removeAttr</span>(<span class="string">&quot;onclick&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在<code>yilia/layout/layout.ejs</code>中添加代码</li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--单击显示文字--&gt;</span><br><span class="line">&lt;% <span class="keyword">if</span> (theme.<span class="property">click_show_text</span>)&#123; %&gt;</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span> <span class="attr">src</span>=<span class="string">&quot;/js/click_show_text.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure><ul><li>在<code>yilia/_config.yml</code>中添加配置</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 鼠标点击显示文字</span></span><br><span class="line"><span class="attr">click_show_text:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><blockquote><p><a href="http://yansheng836.coding.me/article/cf9c6a5e.html">参考博客</a></p></blockquote><h2 id="8-修改手机端的页面背景颜色，文章的背景颜色以及头像上方颜色"><a href="#8-修改手机端的页面背景颜色，文章的背景颜色以及头像上方颜色" class="headerlink" title="8. 修改手机端的页面背景颜色，文章的背景颜色以及头像上方颜色"></a>8. 修改手机端的页面背景颜色，文章的背景颜色以及头像上方颜色</h2><p>_在<code>main.0cf68a.css</code>中修改@media 下的模块_</p><h3 id="8-1-页面背景颜色-图片"><a href="#8-1-页面背景颜色-图片" class="headerlink" title="8.1 页面背景颜色(图片)"></a>8.1 页面背景颜色(图片)</h3><p>添加下列代码，即为修改页面的背景托 i 按</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">@media</span> screen <span class="keyword">and</span> (<span class="attribute">max-width</span>: <span class="number">800px</span>) &#123;</span><br><span class="line">  <span class="selector-tag">body</span> &#123;</span><br><span class="line">    <span class="attribute">background</span>: <span class="built_in">linear-gradient</span>(</span><br><span class="line">        <span class="built_in">rgba</span>(<span class="number">255</span>, <span class="number">127</span>, <span class="number">127</span>, <span class="number">0.212</span>),</span><br><span class="line">        <span class="built_in">rgba</span>(<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">0.2</span>)</span><br><span class="line">      ), <span class="built_in">url</span>(<span class="string">./img/phone2.jpg</span>) no-repeat <span class="number">0%</span> <span class="number">20%</span> / cover;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="8-2-文章背景颜色"><a href="#8-2-文章背景颜色" class="headerlink" title="8.2 文章背景颜色"></a>8.2 文章背景颜色</h3><p>找到<code>@media</code>下的 article</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.article</span> &#123;</span><br><span class="line">  <span class="attribute">margin</span>: <span class="number">30px</span>;</span><br><span class="line">  <span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#ddd</span>;</span><br><span class="line">  <span class="attribute">border-top</span>: <span class="number">1px</span> solid <span class="number">#fff</span>;</span><br><span class="line">  <span class="attribute">border-bottom</span>: <span class="number">1px</span> solid <span class="number">#fff</span>;</span><br><span class="line">  <span class="attribute">background</span>: <span class="built_in">rgba</span>(<span class="number">85</span>, <span class="number">185</span>, <span class="number">185</span>, <span class="number">0.425</span>);</span><br><span class="line">  <span class="attribute">transition</span>: all <span class="number">0.2s</span> ease-in;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>根据自己喜好改变<code>background</code>属性值</p><h3 id="8-3-头像图片"><a href="#8-3-头像图片" class="headerlink" title="8.3 头像图片"></a>8.3 头像图片</h3><p>直接在<code>_config.yml</code>中修改，我改成了透明即<code>header: &#39;rgba(255, 127, 127, 0)&#39;</code></p><h2 id="9-给你的网页添加动态标题"><a href="#9-给你的网页添加动态标题" class="headerlink" title="9. 给你的网页添加动态标题"></a>9. 给你的网页添加动态标题</h2><p>在网站的开头或者结尾添加代码，我是在<code>layout.ejs</code>上添加的</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;script&gt;</span><br><span class="line">  <span class="keyword">var</span> <span class="title class_">OriginTitile</span>=<span class="variable language_">document</span>.<span class="property">title</span>;</span><br><span class="line">  <span class="keyword">var</span> st;</span><br><span class="line">  <span class="variable language_">document</span>.<span class="title function_">addEventListener</span>(<span class="string">&#x27;visibilitychange&#x27;</span>,<span class="keyword">function</span>(<span class="params"></span>)&#123;</span><br><span class="line">  <span class="keyword">if</span>(<span class="variable language_">document</span>.<span class="property">hidden</span>)&#123;</span><br><span class="line">      <span class="variable language_">document</span>.<span class="property">title</span>=<span class="string">&quot;(つェ⊂)看不惹~&quot;</span>+<span class="title class_">OriginTitile</span>;</span><br><span class="line">      <span class="built_in">clearTimeout</span>(st);</span><br><span class="line">      <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;hide&#x27;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span>&#123;</span><br><span class="line">      <span class="variable language_">document</span>.<span class="property">title</span>=<span class="string">&#x27;(*´∇｀*)被你发现了~ &#x27;</span>+<span class="title class_">OriginTitile</span>;</span><br><span class="line">      <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;show&#x27;</span>);</span><br><span class="line">      st=<span class="built_in">setTimeout</span>(<span class="keyword">function</span>(<span class="params"></span>)&#123;</span><br><span class="line">      <span class="variable language_">document</span>.<span class="property">title</span>=<span class="title class_">OriginTitile</span>;</span><br><span class="line">      &#125;,<span class="number">6000</span>);</span><br><span class="line">      <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;endChange=&#x27;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Learn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
