---
title: 2K2K
date: 2023-09-26 14:06:09
tags:
  - DepthEstimation
  - ClothedHumans
categories: HumanBodyReconstruction/Depth&Normal Estimation
---

| Title     | High-fidelity 3D Human Digitization from Single 2K Resolution Images                                                                                                                 |
| --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Author    | Sang-Hun Han1, Min-Gyu Park2, Ju Hong Yoon2,Ju-Mi Kang2, Young-Jae Park1, and Hae-Gon Jeon1                                                                                          |
| Conf/Jour | CVPR 2023 Highlight                                                                                                                                                                  |
| Year      | 2023                                                                                                                                                                                 |
| Project   | [High-fidelity 3D Human Digitization from Single 2K Resolution Images Project Page (sanghunhan92.github.io)](https://sanghunhan92.github.io/conference/2K2K/)                        |
| Paper     | [High-fidelity 3D Human Digitization from Single 2K Resolution Images (readpaper.com)](https://readpaper.com/pdf-annotate/note?pdfId=4738290373604950017&noteId=1970916692663980032) |

![image.png](https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921160120.png)

**可以看成一种估计深度图的方法**
缺点：需要好的数据集
- 需要提供法线图、mask、深度图(低分辨率+高分辨率)
- 需要人体模型的关节点信息
- 无法预测自遮挡部位
- 对低分辨率重建效果不好

<!-- more -->

# Abstract
高质量的3D人体重建需要高保真度和大规模的训练数据，以及有效利用**高分辨率输入图像**的适当网络设计。为了解决这些问题，我们提出了一种简单而有效的3D人体数字化方法，称为2K2K，它构建了一个大规模的2K人体数据集，并从2K分辨率图像推断3D人体模型。所提出的方法分别恢复了人体的全局形状和细节。低分辨率深度网络从低分辨率图像中预测全局结构，而部分图像到法线网络则预测3D人体结构的细节。高分辨率深度网络合并全局3D形状和详细结构，以推断高分辨率的前后侧深度图。最后，一个现成的网格生成器重建完整的3D人体模型，可在获得。此外，我们还提供了2,050个3D人体模型，包括纹理地图、3D关节和SMPL参数，供研究目的使用。在实验中，我们展示了在各种数据集上与最新工作相比的竞争性性能。[https://github.com/SangHunHan92/2K2K](https://github.com/SangHunHan92/2K2K)

# Method
分为两个阶段：
- Stage1 Loss：
    - 使用 L1 损失和 LSSIMloss 来优化图像到法线网络
        - GT：低分辨率法线图，高分辨率法线图
    - 通过最小化平滑 L1 损失 Ls1 和二元交叉熵损失 LBCE 的线性组合来训练深度网络
        - GT：低分辨率深度图，低分辨率法线图，低分辨率Mask
- Stage2 Loss：冻结阶段1训练的网络，并训练高分辨率深度生成器
    - 使用 Lsl1和 LBCE 来优化高分辨率深度网络
        - GT：高分辨率深度图，高分辨率法线图，高分辨率Mask

***最终得到的是双面的高分辨率深度图，通过深度图获取点云，然后预测法线，最后通过a screened Poisson surface construction来得到mesh***

**低分辨率深度网络**分别预测低分辨率深度和法向图，使用dual-encoder AU-Net (D-AU-Net)网络, ref: [Monocular Human Digitization via Implicit Re-projection Networks (readpaper.com)](https://readpaper.com/pdf-annotate/note?pdfId=4624854467519463425&noteId=1997087774731159296)
**Body part extraction和Part-wise normal prediction**预测高分辨率双面法向map
![image.png](https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921163941.png)
- 将人体按照keypoint关节分成patches，每个patches和low分辨率法线图来预测high分辨率双面法线 map
    - 只需训练头、躯干、手臂、腿和脚这5个法线预测网络AU-Net，然后即可预测每个patch
    - [Attention U-Net: Learning Where to Look for the Pancreas (readpaper.com)](https://readpaper.com/pdf-annotate/note?pdfId=4500191380932026369&noteId=1997072200139556608)

**高分辨率深度预测网络**
![image.png](https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921164612.png)
    **网格生成**：从深度图生成 3D 模型有多种方法。在这项工作中。我们采用与[13]类似的方法。我们将深度图转换为 3D 点云，然后从相邻点计算每个点的表面法线。之后，我们运行一个筛选的泊松曲面构造[22]来获得平滑的人体网格V。

# Datasets

- 新的数据集：我们的数据集提供了高保真的 3D 人体模型，由 80 个 DSLR 相机、纹理贴图、3D 姿势（openpifpafw全身）和 SMPL 模型参数捕获。
    - 2,050 个scan 3D模型
    - Skinned Multi-Person Linear (SMPL)

booth中一共80个DSLR：每个条上5个相机，从上到下依次对齐：人头、上身、中部、下身、膝盖。

![image.png](https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921162402.png)

使用商业化软件RealityCapture生成初始粗糙的人体模型，然后专家手动对模型进行后处理(填充孔洞、头发几何细节)。已发布模型的顶点数约为 1M，其示例如图 2 (b) 所示。扫描模型正确地保留了手指和皱纹等几何细节，主要是因为在受控环境中捕获的高质量图像。

![image.png](https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20230921162858.png)

合成的Rendered Image(使用Unity、Unreal等软件)

# Experiments

在四个RTX A6000 GPU上训练3天

Ubuntu 20.04 with Python 3.8, PyTorch 1.9.1 and CUDA 11.1
```bash
apt-get install -y freeglut3-dev libglib2.0-0 libsm6 libxrender1 libxext6 openexr libopenexr-dev libjpeg-dev zlib1g-dev
apt install -y libgl1-mesa-dri libegl1-mesa libgbm1 libgl1-mesa-glx libglib2.0-0
pip install -r requirements.txt
```

## fast test

```bash
cd checkpoints && wget https://github.com/SangHunHan92/2K2K/releases/download/Checkpoint/ckpt_bg_mask.pth.tar && cd ..
```

```shell
python test_02_model.py --load_ckpt ckpt_bg_mask.pth.tar --save_path ./exp

terminal output：
u2net.onnx model下载'https://github.com/danielgatis/rembg/releases/download/v0.0.0/u2net  
.onnx'

python test_03_poisson.py --save_path ./exp
```

custom test
```shell
bin\OpenPoseDemo.exe --image_dir test_folder --write_json test_folder --hand --write_images test_folder\test --write_images_format jpg

python test_02_model.py --load_ckpt ckpt_bg_mask.pth.tar --save_path ./exp --data_path ./test_folder --save_name ???
python test_03_poisson.py --save_path ./exp --save_name ???

bin\OpenPoseDemo.exe --image_dir test_2 --write_json test_2 --hand --write_images test_2\test --write_images_format jpg

...
```

## render

```
data
├ IndoorCVPR09
│  ├ airport_inside
│  └ ⋮
├ Joint3D
│  ├ RP
│  └ THuman2
├ list
├ obj
│  ├ RP
│  │  ├ rp_aaron_posed_013_OBJ
│  │  └ ⋮
│  └ THuman2
│     ├ data
│     └ smplx
│  ├ 2K2K
│  │  ├ 00003
│  │  └ ⋮
├ PERS
│  ├ COLOR
│  ├ DEPTH
│  └ keypoint
└ (ORTH)
```

### data folder

- obj
  - 2K2K
    - 00003
      - 00003.ply
- PERS
  - COLOR
    - NOSHADING
      - `2K2K_0_y_-30_x`
        - 00003_front.png
        - 00003_back.png
      - `2K2K*0_y*-20_x`
      - ...
      - `2K2K_0_y_30_x`
    - SHADED
      - `2K2K_0_y_-30_x`
        - 00003_front.png
      - ...
  - DEPTH
    - `2K2K_0_y_-30_x`
      - 00003_front.png
      - 00003_back.png
    - ...
- ORTH
  - COLOR
    - NOSHADING
    - SHADED
  - DEPTH

如果不算ORTH：共21+14=35张图片
### render image

```python
pers2pc(image_front, front_depth.astype(np.float64) / 32.0, 2048, 50)

def pers2pc(pers_color, pers_depth, res, fov):
    focal = res / (2 * np.tan(np.radians(fov) / 2.0))

res = 2048
fov = 50
focal = 2195.975086601788
```

```bash
python render/render.py --data_path ./data --data_name 2K2K

python render/render.py --data_path ./data --data_name RP
python render/render.py --data_path ./data --data_name THuman2 --smpl_model_path {smpl_model_path}
```

Get ： PERS/COLOR and PERS/DEPTH

### render keypoint

```bash
unzip data/Joint3D.zip -d data/Joint3D/
python render/render_keypoint.py --data_path ./data --data_name RP
python render/render_keypoint.py --data_path ./data --data_name THuman2


# openpose
## input image
bin\OpenPoseDemo.exe --image_dir G:/2K2K_Datasets/230831/train/data/PERS/COLOR/FOR_POSE/2K2K_0_y_0_x --write_json G:/2K2K_Datasets/230831/train/data/PERS/keypoint_json/2K2K_0_y_0_x --face --hand --write_images_format png

G:/2K2K_Datasets/230831/train/data/PERS/COLOR/FOR_POSE/2K2K_0_y_0_x
    2K2K_0_y_10_x
    2K2K_0_y_20_x
    2K2K_0_y_30_x
    2K2K_0_y_-10_x
    2K2K_0_y_-20_x
    2K2K_0_y_-30_x

## output keypoint_json
G:/2K2K_Datasets/230831/train/data/PERS/keypoint_json/2K2K_0_y_0_x ...

## output keypoint_npy
python render_keypoint_2k.py # use json2npy function

G:/2K2K_Datasets/230831/train/data/PERS/keypoint/2K2K_0_y_0_x ...
```

Get ： PERS/keypoint

## train

phase1 大概需要7天, (单张3090)

```bash
python train.py --data_path ./data --phase 1 --batch_size 1
python train.py --data_path ./data --phase 2 --batch_size 1 --load_ckpt {checkpoint_file_name}
```

# Limitations

由于我们明确地预测每个身体部位的法线贴图，我们的方法没有考虑严重的自遮挡，例如，when a lower arm is behind the back。我们声称这种现象本质上是模棱两可的，可能的补救措施要么是预测遮挡像素的语义，要么是在[34]之前使用人体来指导深度预测。由于空间限制，我们在补充材料中提供了几个失败案例。 

# Conclusion

我们提出了 2K2K，这是一个从高分辨率单幅图像中数字化人类的有效框架。为此，我们首先通过扫描 2,050 个人体模型构建了一个大规模的人体模型数据集，并使用它们来训练我们的网络，由部分正态预测、低分辨率和高分辨率深度预测网络组成。为了有效地处理高分辨率输入，我们裁剪和弱对齐每个身体部位不仅可以处理姿势变化，还可以更好地恢复人体的精细细节，如面部表情。我们证明了所提出的方法适用于各种数据集的高分辨率图像有效工作。

# Code

## Human Body Part(2K2K)

将人体分为 12 个部分，可以只用头、躯干、手臂(4 part)、腿(4 part)和脚(2 part) 五个网络来预测

[CMU-Perceptual-Computing-Lab/openpose: OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation (github.com)](https://github.com/CMU-Perceptual-Computing-Lab/openpose)

- **openpose** ... to get keypoint(json) ,then convert json to npy (shape: 31,3)
- 输入原图 image，根据 pose(.npy)得到仿射变换矩阵(init_affine_2048)，仿射变换矩阵将目标部位移动到相机中心，然后通过 centercrop 得到 part image
  - 原图 image 下采样后通过网络得到低分辨率法向量图，low normal 插值到 2k 后同样变换得到 part low normal
  - part image 和 part low normal 通过 5 个 part network 得到每个部分的 part high normal
  - part high normal 通过 occupy 方式得到的权重，求和拼接成 high normal

![image.png|222](https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures/20231011103025.png)

Note：pose 中脸部只有 2eye、2ear 和 1nose，手部为 4 个 finger

```python
[0, 1, 2, 3, 4] = [nose, L eye, R eye, L ear, R ear]
[5, 6, 7, 8, 9, 10] = [L shoudler, R shoudler, L elbow, R elbow, L wrist, R wrist]
[11, 12, 13, 14, 15, 16] = [L hip, R hip, L knee, R knee, L ankle, R ankle]
[17, 18, 19, 20, 21, 22] = [L big toe, L little toe, L sole, R big toe, R little toe, R sole]
[23, 24, 25, 26, 27, 28, 29, 30] = [L finger 2, 3, 4, 5, R finger 2, 3, 4, 5]
```

根据 pose 将人体分为 12 个部分用 5 个网络预测:

```python
face[4, 3]
body[6, 5, 12, 11]
arm[5, 7], [7, 9, 23, 24, 25, 26], [6, 8], [8, 10, 27, 28, 29, 30]
leg[11, 13], [13, 15], [12, 14], [14, 16]
foot[17, 18, 19, 15], [20, 21, 22, 16]
```