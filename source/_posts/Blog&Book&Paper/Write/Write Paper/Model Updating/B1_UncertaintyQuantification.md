---
title: B1_UncertaintyQuantification
date: 2026-01-01 16:21:39
tags: 
categories: Blog&Book&Paper/Write/Write Paper/Model Updating
Year: 
Journal:
---

<!-- more -->

20~30w words

25000 words each chapter

```ad-note
《数字模型的不确定性量化方法》三级目录

第1章 数字模型与工程不确定性问题
1.1 数字模型的工程内涵
1.1.1 数字模型、数值模型、与数字孪生的关系
1.1.2 数字模型在工程分析与决策中的作用
1.2 工程系统中不确定性的来源
1.2.1 参数、建模、与环境不确定性
1.2.2 模型结构与系统偏差
1.3 不确定性对模型预测的影响
1.3.1 预测偏差与可信性问题
1.3.2 不确定性驱动的风险与决策
1.4 数字孪生背景下的不确定性量化需求
1.4.1 数字孪生对数值模型的新要求
1.4.2 数字模型可靠性的关键挑战

2章 工程不确定性的分类与数学表征
2.1 不确定性的基本分类
2.1.1 随机不确定性与认知不确定性
2.1.2 单源与多源不确定性
2.2 不确定性的数学描述方法
2.2.1 随机变量与随机过程
2.2.2 区间变量与集合模型
2.3 非精确概率模型
2.3.1 证据理论的基本概念
2.3.2 概率盒与区间概率
2.4 多源混合不确定性的统一建模
2.4.1 混合不确定性建模框架
2.4.2 工程建模中的实践原则

第3章 概率不确定性量化基础方法
3.1 概率模型构建
3.1.1 随机参数的统计建模
3.1.2 样本数据与先验信息
3.2 不确定性传播方法
3.2.1 Monte Carlo 模拟
3.2.2 改进采样与降维方法
3.3 基于代理模型的概率分析
3.3.1 多项式混沌展开
3.3.2 高斯过程与响应面方法
3.4 概率方法的工程适用性
3.4.1 计算效率与精度问题
3.4.2 方法选择与工程约束

第4章 区间与不精确概率不确定性量化方法
4.1 区间不确定性模型
4.1.1 区间变量的定义与运算
4.1.2 区间传播的基本方法
4.2 基于证据理论的不确定性量化
4.2.1 基本概率分配
4.2.2 合成规则与冲突处理
4.3 非精确概率传播分析
4.3.1 概率盒的传播方法
4.3.2 区间概率分析
4.4 区间与概率方法的比较与融合
4.4.1 方法优缺点分析
4.4.2 混合建模策略


第5章 数字模型中的不确定性传播分析
5.1 不确定性正向传播问题
5.1.1 模型输入到输出的映射
5.1.2 非线性系统的传播特性
5.2 多维输出与相关性分析
5.2.1 输出相关性建模
5.2.2 高维响应的处理方法
5.3 动力学系统的不确定性传播
5.3.1 时域动力学响应
5.3.2 频域与模态不确定性
5.4 工程计算中的数值稳定性问题
5.4.1 数值误差与不确定性的区分
5.4.2 面向工程应用的误差控制与计算资源配置

第6章 不确定性驱动的数字模型修正
6.1 模型修正与确认的基本概念
6.1.1 参数校准与模型更新
6.1.2 校准与预测的一致性
6.2 基于概率的随机模型修正方法
6.2.1 随机模型修正框架
6.2.2 似然函数与贝叶斯推断
6.3 基于区间与非精确概率的随机模型修正
6.3.1 区间模型修正思想
6.3.2 混合不确定性下的修正策略
6.4 多源不确定性下的模型修正与确认流程
6.4.1 数据融合与权重分配
6.4.2 工程实现要点


第7章 数字模型验证与确认中的不确定性问题
7.1 模型验证与确认基础
7.1.1 验证与确认的定义
7.1.2 不确定性在 V&V 中的作用
7.2 基于不确定性的模型一致性评价
7.2.1 一致性判据设计
7.2.2 不确定性区间对比方法
7.3 验证与确认中的实验设计问题
7.3.1 实验数据的不确定性
7.3.2 数据质量与可信度评估
7.4 工程模型验证流程与规范
7.4.1 工程实施流程
7.4.2 常见问题与对策

第8章 面向可信数字孪生的不确定性量化方法
8.1 数字孪生中的模型体系
8.1.1 多模型与多保真框架
8.1.2 数据与模型协同机制
8.2 在线不确定性修正方法
8.2.1 实时数据驱动更新
8.2.2 在线模型修正策略
8.3 数据驱动与物理模型融合
8.3.1 混合建模方法
8.3.2 融合模型的不确定性分析
8.4 面向决策的可信数字模型
8.4.1 可信性指标设计
8.4.2 不确定性驱动决策支持


第9章 典型工程应用案例
9.1 航空结构数字模型不确定性分析
9.1.1 结构动力学建模
9.1.2 不确定性传播与修正
9.2 复杂系统模型修正工程实例
9.2.1 实验数据获取
9.2.2 模型校准与验证
9.3 实验与仿真融合的可信性评估
9.3.1 数据一致性分析
9.3.2 结果可信性评价
9.4 工程应用中的经验与挑战
9.4.1 实践经验总结
9.4.2 未来发展方向

```


你是一名不确定性量化领域的专家，目前需要你写一本中文专著，第三章为概率不确定性量化基础方法，大纲如下：
第3章 概率不确定性量化基础方法
3.1 概率模型构建
3.1.1 随机参数的统计建模
3.1.2 样本数据与先验信息
3.2 不确定性传播方法
3.2.1 Monte Carlo 模拟
3.2.2 改进采样与降维方法
3.3 基于代理模型的概率分析
3.3.1 多项式混沌展开
3.3.2 高斯过程与响应面方法
3.4 概率方法的工程适用性
3.4.1 计算效率与精度问题
3.4.2 方法选择与工程约束

需要你进行本章内容的撰写，要求如下：
该章不少于25000字，使用学术界专业术语，逻辑通顺。并附带20篇带doi的参考文献。
保证使用分段论述，不要列表叙述，不要加粗、斜体等markdown格式。
第一小节3.1可以叙述理论，但方法介绍部分例如3.2和3.3必须要有公式说明。
每一小节要有一段总纲，讲述大致这节要讲什么，每章也要有几段总纲，讲述这章讲什么。


# 第3章 概率不确定性量化基础方法

不确定性量化（Uncertainty Quantification, UQ）是现代科学与工程领域中不可或缺的关键技术，旨在建立从输入参数的不确定性到系统响应不确定性的数学映射关系。在众多UQ范式中，概率不确定性量化方法因其成熟的理论基础和广泛的工程适用性，成为了处理随机不确定性最为主流的手段[Smith, Uncertainty Quantification: Theory, Implementation, and Applications, 01/2013](zotero://select/library/items/LJDCGDIC)。本章内容涵盖了从不确定性表征到不确定性传播，再到基于代理模型的高效分析技术的完整流程，以及其在复杂系统分析中的应用。概率UQ的核心思想是将系统输入中的不确定性视为随机变量，通过概率分布函数或累积分布函数进行精确描述，进而利用数学工具将这些输入不确定性传播至系统响应，最终获得输出响应的概率分布、统计矩（如均值、方差）以及可靠性指标。本章的论述将围绕三个核心环节展开：首先是概率模型构建，即如何基于有限的样本数据和先验知识，对随机输入参数进行统计建模；其次是不确定性传播，重点介绍以蒙特卡洛（Monte Carlo, MC）模拟为代表的经典方法，以及为提高计算效率而开发的改进采样与降维技术；最后是探讨了如何利用多项式混沌展开和高斯过程等高效的代理模型，克服复杂计算模型带来的高昂成本问题。此外，本章还从计算效率与精度权衡的角度，讨论这些方法在实际工程中的适用性准则。通过对这些基础方法的深入剖析，本章旨在为读者建立一个坚实的概率UQ理论框架，并指导其在实际工程问题中进行合理的方法选择与应用。
 
## 3.1 概率模型构建

概率模型构建是概率不确定性量化的基石，其质量直接决定了后续不确定性传播与分析结果的准确性与可靠性。本节将深入探讨如何将工程系统中的固有的随机不确定性和知识缺乏的认知不确定性转化为严谨的数学概率模型。核心在于对随机输入参数进行统计特征描述，并结合可用的样本数据和领域专家的先验信息，构建出能够准确反映系统输入状态的概率分布模型。

### 3.1.1 随机参数的统计建模

在工程科学的数值建模中，输入参数通常被理想化为确定性的数值，然而在物理现实中，不确定性是普遍存在的。材料属性的微观结构异质性、制造加工过程中的公差波动、边界条件的随机扰动以及测量数据的误差，均导致了系统输入表现出显著的随机特征。统计建模的核心任务，即是在概率论公理化体系下，建立从物理观测数据到数学随机变量的映射，从而量化这些输入参数的内在变异性。

对于连续型随机变量，最基础且应用最广泛的是正态分布（Normal Distribution），亦称高斯分布。正态分布在统计建模中占据核心地位，其理论基础在于中心极限定理：当一个物理量是由大量相互独立的、微小的随机因素叠加而成时，无论这些微小因素服从何种分布，该物理量在总体上将近似服从正态分布。这一特性使得正态分布非常适合描述测量误差、加工尺寸偏差等双侧对称波动的参数。若随机变量 $X$ 服从均值为 $\mu$、方差为 $\sigma^2$ 的正态分布，记为 $X \sim \mathcal{N}(\mu, \sigma^2)$，其概率密度函数为：
$$ f_X(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(x - \mu)^2}{2\sigma^2} \right), \quad -\infty < x < +\infty $$
正态分布具有优良的数学性质，其各阶统计矩完全由 $\mu$ 和 $\sigma$ 决定。然而，正态分布的取值范围覆盖整个实数轴，这意味着它在理论上允许负值的出现。对于某些严格非负的物理参数（如质量、时间、强度），若变异系数较大，使用正态分布建模可能会导致非物理的负值采样，从而引入建模误差。

为了克服正态分布在描述非负物理量时的局限性，对数正态分布（Lognormal Distribution）被广泛应用于工程中。对数正态分布通常用于描述由多个随机因素相乘作用产生的物理量，或者那些取值跨越多个数量级的参数，如材料的疲劳寿命等。若随机变量 $X$ 的自然对数 $\ln X$ 服从正态分布 $\mathcal{N}(\lambda, \zeta^2)$，则称 $X$ 服从对数正态分布，记为 $X \sim \mathcal{LN}(\lambda, \zeta^2)$。其概率密度函数由下式给出：
$$ f_X(x) = \frac{1}{\sqrt{2\pi}\zeta x} \exp\left( -\frac{(\ln x - \lambda)^2}{2\zeta^2} \right), \quad x > 0 $$
其中，$\lambda$ 和 $\zeta$ 分别是对数均值和对数标准差。对数正态分布是右偏分布，其长尾特性能够较好地描述那些具有偶尔出现极大值倾向的数据。

在可靠性工程、寿命分析及材料强度建模领域，威布尔分布（Weibull Distribution）是更为常用的分布类型。威布尔分布的理论基础源于极值理论中的最小值分布，适用于描述系统中多个弱链条的失效行为，即链条的强度取决于其最薄弱环节的强度。这使得它特别适合描述脆性材料的断裂强度、电子元器件的失效时间以及风速分布。双参数威布尔分布的概率密度函数为：
$$ f_X(x) = \frac{k}{\lambda} \left( \frac{x}{\lambda} \right)^{k-1} \exp\left[ -\left( \frac{x}{\lambda} \right)^k \right], \quad x \ge 0 $$
式中，$k > 0$ 为形状参数，$\lambda > 0$ 为尺度参数。形状参数 $k$ 决定了分布的形态及失效率函数的特征：当 $k < 1$ 时，失效率随时间递减（早期失效）；当 $k = 1$ 时，退化为指数分布，失效率恒定（随机失效）；当 $k > 1$ 时，失效率随时间递增（磨损失效）。这种灵活性使得威布尔分布能够涵盖多种不同类型的随机现象。

伽马分布（Gamma Distribution）是另一类重要的非负偏态分布，广泛应用于贝叶斯统计中。它是指数分布的推广，常用于描述多个独立指数分布变量之和，即完成 $k$ 个独立随机事件所需的总等待时间。若随机变量 $X$ 服从参数为 $\alpha$ 和 $\beta$ 的伽马分布，其概率密度函数表示为：
$$ f_X(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} \exp(-\beta x), \quad x > 0 $$
其中，$\alpha > 0$ 为形状参数，$\beta > 0$ 为率参数（有时用尺度参数 $\theta = 1/\beta$ 表示），$\Gamma(\cdot)$ 为伽马函数。伽马分布具有极强的适应性，通过调整参数，它可以逼近正态分布或退化为指数分布及卡方分布，常被用作方差或精度的共轭先验分布。

当系统输入包含多个随机参数时，必须考虑它们之间的相关性。如果随机参数 $X_1, X_2, \dots, X_n$ 相互独立，则它们的联合概率密度函数 $f_{\mathbf{X}}(\mathbf{x})$ 可以简单地分解为边缘概率密度函数的乘积：
$$f_{\mathbf{X}}(\mathbf{x}) = \prod_{i=1}^n f_{X_i}(x_i)$$

然而，在多数实际工程问题中，输入参数之间存在复杂的依赖关系。例如，同一批次生产的材料，其弹性模量和屈服强度往往是正相关的。在这种情况下，需要构建联合概率模型来捕获这种依赖性。Copula函数理论[Nelsen, Copulas and quasi-copulas: An introduction to their properties and applications, 2005, Logical, Algebraic, Analytic and Probabilistic Aspects of Triangular Norms](zotero://select/library/items/4YX8E6PF)提供了一种处理多变量复杂相关性的通用框架。根据Sklar定理，任意 $n$ 维联合累积分布函数 $F_{\mathbf{X}}(\mathbf{x})$ 均可分解为边缘累积分布函数 $F_{X_i}(x_i)$ 与一个Copula函数 $C$ 的组合：
$$ F_{\mathbf{X}}(x_1, \dots, x_n) = C(F_{X_1}(x_1), \dots, F_{X_n}(x_n)) $$

Copula函数 $C: [0, 1]^n \to [0, 1]$ 将边缘分布的信息与变量间的依赖结构完全解耦。通过选择不同类型的Copula函数，可以灵活地构建具有特定相关特性的多维概率模型，而不受边缘分布类型的限制，如高斯Copula描述线性相关，Clayton或Gumbel Copula描述非对称的尾部依赖。

在某些情况下，参数的物理机理不明确且先验知识匮乏，强行假设其服从某种参数化分布可能引入较大的模型偏差。此时，非参数化统计建模方法，特别是核密度估计（Kernel Density Estimation, KDE）[Chen, A Tutorial on Kernel Density Estimation and Recent Advances, 2017-09-12](zotero://select/library/items/N32PXACK)，提供了另一种途径。KDE不依赖于任何特定的函数形式假设，而是直接利用样本数据重构总体的概率密度。其基本思想是在每一个观测数据点处放置一个核函数，如高斯核函数，然后通过叠加这些核函数来获得整体的密度估计：
$$ \hat{f}_h(x) = \frac{1}{n h} \sum_{i=1}^n K\left( \frac{x - x_i}{h} \right) $$
其中 $h$ 为带宽参数，控制估计曲线的光滑程度。KDE能够有效捕捉多峰、偏斜等复杂的数据特征，真实还原数据的分布形态。

最后，对于涉及空间分布或时间演化的不确定性参数，单一的随机变量已不足以描述其特性，必须引入随机场或随机过程模型。为了在数值计算中处理这类无限维对象，Karhunen-Loève（KL）展开[Zhou et al., 不确定性量化的高精度数值方法和理论, 2015-07-01, SCIENTIA SINICA Mathematica](zotero://select/library/items/IBZNBC4L)提供了一种最优的离散化手段。KL展开基于随机场协方差函数的谱分解，将随机过程 $\omega(\mathbf{x}, \theta)$ 表示为确定性正交基函数 $\phi_k(\mathbf{x})$ 与互不相关的随机系数 $\xi_k(\theta)$ 的级数和：
$$ \omega(\mathbf{x}, \theta) \approx \mu(\mathbf{x}) + \sum_{k=1}^M \sqrt{\lambda_k} \xi_k(\theta) \phi_k(\mathbf{x}) $$
通过截取前 $M$ 项，KL展开将无限维的随机场问题转化为有限维的随机变量问题，且在均方误差意义下具有最优的收敛性。

### 3.1.2 样本数据与先验信息

概率模型的构建过程本质上是一个信息融合与推断的过程，其核心在于如何利用现有的观测数据和潜在的先验知识来量化模型参数的不确定性。根据对参数性质的不同认知，参数估计理论主要分为频率学派和贝叶斯学派两大流派。频率学派将参数视为客观存在的、未知但固定的常数，推断完全依赖于观测样本的统计特性；而贝叶斯学派则将参数视为随机变量，认为参数本身具有不确定性，这种不确定性可以用概率分布来描述，并且随着信息的累积而不断更新。在实际工程的不确定性量化中，数据的稀缺性往往是常态，如何在这两种范式之间进行选择或融合，直接决定了模型的置信度与鲁棒性。

在经典统计学框架下，应用最为广泛的点估计方法为极大似然估计（Maximum Likelihood Estimation, MLE），又称最大似然估计。其思想最早由德国数学家高斯（C. F. Gauss）在1821年研究最小二乘法时萌生，后由英国统计学家费舍尔（R. A. Fisher）在20世纪初进行了系统性的理论奠基与推广[Stigler, The Epic Story of Maximum Likelihood, 2007-11-01, Statistical Science](zotero://select/library/items/9N9YAARZ)。极大似然估计的基本认识论假设为既然某个随机事件（即观测样本）已经发生，那么推断出的模型参数应当使得该事件发生的概率最大。具体而言，假设给定一组来自总体$X$的独立同分布的样本数据$\mathbf{x} = (x_1, x_2, \dots, x_m)$，其概率密度函数形式已知为$f_X(x; \boldsymbol{\theta})$，但参数向量$\boldsymbol{\theta}$未知。似然函数$L(\boldsymbol{\theta} | \mathbf{x})$定义为在给定参数$\boldsymbol{\theta}$下观测到这组样本的联合概率密度，由于样本的独立性，似然函数可写为边缘密度的乘积：

$$L(\boldsymbol{\theta} | \mathbf{x}) = \prod_{i=1}^m f_{X}(x_i | \boldsymbol{\theta})$$

极大似然估计的目标是在参数空间内寻找一个估计值$\hat{\boldsymbol{\theta}}_{\text{MLE}}$，使得观测数据出现的可能性最大，即：

$$\hat{\boldsymbol{\theta}}_{\text{MLE}} = \arg \max_{\boldsymbol{\theta}} L(\boldsymbol{\theta} | \mathbf{x})$$

在计算上，为了处理乘积形式带来的数值溢出风险并简化求导运算，通常引入对数似然函数：
$$\ln L(\boldsymbol{\theta} | \mathbf{x}) = \ln\left(\prod_{i=1}^m f_{X}(x_i | \boldsymbol{\theta})\right) = \sum_{i=1}^m \ln f_{X}(x_i | \boldsymbol{\theta})$$

由于对数函数的单调递增性，极值点的位置保持不变。于是，优化问题转化为求解对数似然函数关于参数的梯度方程，即似然方程：

$$\frac{\partial \ln L(\boldsymbol{\theta} | \mathbf{x})}{\partial \boldsymbol{\theta}} = \sum_{i=1}^m \frac{\partial \ln f_{X}(x_i | \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0$$

极大似然估计之所以成为工程统计的标准工具，归功于其在大样本条件下表现出的优良统计性质，包括一致性，即随着样本量趋于无穷，估计值收敛于真实参数值；渐近正态性，即估计值的分布趋近于正态分布；以及有效性即在所有无偏估计中具有最小方差。然而，极大似然估计也存在显著的局限性。首先它是一种纯数据驱动的方法，在小样本情况下，估计结果极易受到异常值的干扰，导致较大的方差和偏差，其次它无法处理参数本身的不确定性，给出的仅是一个确定的点估计，难以直接量化估计值的置信程度。

当工程现场的观测数据稀疏甚至缺失，或者领域专家拥有关于物理参数的丰富经验知识时，贝叶斯估计方法（Bayesian Estimation）提供了将这些先验信息融入统计建模的严密数学框架。贝叶斯理论源于18世纪英国数学家托马斯·贝叶斯（Thomas Bayes）提出的逆概率思想，并由法国数学家拉普拉斯（Pierre-Simon Laplace）在19世纪初将其发展为完整的概率归纳逻辑[Gelman et al., Bayesian Data Analysis, Third Edition](zotero://select/library/items/PW8DGTV9)。与频率学派不同，贝叶斯学派的核心观点是概率不仅仅是对随机事件发生频率的描述，更是一种对不确定性和认知状态的量化。参数$\boldsymbol{\theta}$不再是一个未知的常量，而是一个服从某种概率分布的随机变量。贝叶斯推断的过程，本质上是利用观测数据对参数的先验认知进行修正的过程。

设$\pi(\boldsymbol{\theta})$为在获取数据之前基于经验或物理约束对参数设定的先验分布，$L(\boldsymbol{\theta} | \mathbf{x})$为反映数据信息的似然函数。根据贝叶斯定理，结合先验信息与样本信息后的后验分布$\pi(\boldsymbol{\theta} | \mathbf{x})$可表示为：

$$\pi(\boldsymbol{\theta} | \mathbf{x}) = \frac{L(\boldsymbol{\theta} | \mathbf{x}) \pi(\boldsymbol{\theta})}{p(\mathbf{x})} = \frac{L(\boldsymbol{\theta} | \mathbf{x}) \pi(\boldsymbol{\theta})}{\int_{\Theta} L(\boldsymbol{\theta} | \mathbf{x}) \pi(\boldsymbol{\theta}) d\boldsymbol{\theta}}$$

式中，分母$p(\mathbf{x})$被称为边际似然或证据因子，它是一个归一化常数，确保后验分布积分为1。贝叶斯公式清晰地展示了后验分布正比于似然函数与先验分布的乘积。当观测数据较少时，似然函数较为平坦，后验分布主要由先验分布主导，体现了专家经验在小样本下的价值。随着数据量的增加，似然函数变得尖峰化，数据的影响力逐渐压倒先验信息，后验分布将收敛于极大似然估计的结果。这种自适应的加权机制使得贝叶斯方法在处理小样本问题时具有极强的鲁棒性。

先验分布的选择是贝叶斯建模中的关键环节，也是争议的焦点。为了减少主观性对推断结果的影响，工程中常采用共轭先验或无信息先验。共轭先验是指先验分布与后验分布属于同一分布族，例如二项分布的共轭先验是Beta分布，正态分布均值的共轭先验是正态分布。这种选择使得后验分布具有解析表达式，极大地简化了计算。而在缺乏明确先验知识时，基于最大熵原理构造的先验分布被认为是最客观的，它在满足已知约束的所有分布中选取熵最大的分布，从而引入最少的主观假设。

尽管贝叶斯方法理论优美，但在实际应用中长期受制于计算瓶颈。后验分布公式中的分母涉及对高维参数空间的多重积分，这在解析上通常是不可积的。直到20世纪中叶，随着计算机技术的发展，马尔可夫链蒙特卡洛（Markov Chain Monte Carlo, MCMC）方法[Speagle, A Conceptual Introduction to Markov Chain Monte Carlo Methods, 2020-03-07](zotero://select/library/items/6UKAHI8E)的出现才彻底解决了这一难题。MCMC的基本思想是构造一个马尔可夫链，使其平稳分布收敛于目标后验分布$\pi(\boldsymbol{\theta} | \mathbf{x})$，通过在链上进行采样来模拟后验分布，进而计算参数的均值、方差或置信区间。经典的MCMC算法包括Metropolis-Hastings算法和Gibbs采样算法。Metropolis-Hastings算法通过引入建议分布和接受-拒绝机制，使得马尔可夫链能够遍历复杂的参数空间；而Gibbs采样则是其特例，适用于多维参数的条件分布易于采样的情形。

## 3.2 不确定性传播方法

不确定性传播（Uncertainty Propagation）是概率UQ的核心任务，旨在研究输入随机变量的概率密度函数如何通过计算模型映射为输出响应的统计特征。这一过程在数学上本质上是一个高维积分问题。无论是计算输出响应的均值、方差，还是估计失效概率，都需要对输入空间进行积分运算。由于工程模型通常表现为复杂的非线性黑箱函数，且输入维数可能很高，解析求解往往是不可能的。因此，数值模拟方法成为了解决这一问题的主要手段。本节将详细阐述最经典的蒙特卡洛模拟方法及其数学原理，并针对其收敛速度慢的问题，介绍拉丁超立方采样、重要性采样等改进策略，以及针对高维问题的降维处理思路。

### 3.2.1 Monte Carlo 模拟

蒙特卡洛（Monte Carlo, MC）模拟[Fishman, Monte Carlo, 1996](zotero://select/library/items/93ZK4K25)作为不确定性传播领域中最经典、最通用且直观的方法，其核心思想是构建一个与原工程问题具有相同概率特征的随机过程，通过对该过程进行大量的统计实验，利用实验结果的统计特性来逼近问题的解析解。在工程不确定性量化中，蒙特卡洛方法不需要了解物理模型内部的控制方程或刚度矩阵结构，仅需将确定性求解器作为一个能够将输入映射为输出的函数算子。这种特性使得蒙特卡洛方法具有极强的普适性，能够毫无障碍地处理高度非线性、不连续甚至非光滑的复杂工程系统，也正是这种特性使其成为处理高维不确定性传播问题的基石。

从数学角度形式化描述，假设工程系统的输入参数由一个$d$维随机向量$\mathbf{X} = (X_1, X_2, \dots, X_d)^T$表示，其联合概率密度函数为$f_{\mathbf{X}}(\mathbf{x})$，定义域为$\mathcal{D}_{\mathbf{X}} \subseteq \mathbb{R}^d$。系统的输出响应$Y$通过物理模型计算得到，即$Y = g(\mathbf{X})$。不确定性传播的主要任务通常涉及计算输出响应的统计矩（如均值、方差）或特定事件发生的概率（如可靠性分析中的失效概率）。这在本质上等价于计算高维加权积分。以输出响应的期望值$\mu_Y$为例，其定义为：

$$I = \mu_Y = \mathbb{E}[g(\mathbf{X})] = \int_{\mathcal{D}_{\mathbf{X}}} g(\mathbf{x})f_{\mathbf{X}}(\mathbf{x}) d\mathbf{x}$$

对于实际工程问题，响应函数$g(\mathbf{x})$往往极其复杂且无法获得显式解析表达式，加之积分维数$d$可能高达数十甚至上百，传统的基于网格的数值积分方法（如梯形法则或Simpson法则）会遭遇维数灾难，计算量随维数呈指数增长，从而无法实施。蒙特卡洛方法巧妙地规避了这一难题，它将积分问题转化为统计推断问题。首先，依据输入概率密度函数$f_{\mathbf{X}}(\mathbf{x})$，利用伪随机数生成器抽取$N$个独立同分布的样本点$\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(N)}\}$。其次，将这些样本逐一输入到确定性物理模型中进行仿真计算，获得对应的输出样本集$\{y^{(1)}, y^{(2)}, \dots, y^{(N)}\}$，其中$y^{(i)} = g(\mathbf{x}^{(i)})$。最后，利用样本统计量来估计总体统计特性。

对于输出均值$\mu_Y$，蒙特卡洛估计量$\hat{I}_{MC}$为样本均值：

$$\hat{I}_{MC} = \frac{1}{N} \sum_{i=1}^{N} g(\mathbf{x}^{(i)})$$

该估计量是无偏的，即$\mathbb{E}[\hat{I}_{MC}] = I$。对于输出方差$\sigma_Y^2$，其无偏估计量$\hat{\sigma}_Y^2$为：

$$\hat{\sigma}_Y^2 = \frac{1}{N-1} \sum_{i=1}^{N} \left( g(\mathbf{x}^{(i)}) - \hat{I}_{MC} \right)^2$$

在可靠性分析中，若关注系统失效概率$P_f = P(g(\mathbf{X}) \le 0)$，则可引入指示函数$I_F(\mathbf{x})$，当$g(\mathbf{x}) \le 0$时取值为1，否则为0。此时失效概率的蒙特卡洛估计转化为计算指示函数的均值：

$$\hat{P}_{f, MC} = \frac{1}{N} \sum_{i=1}^{N} I_F(\mathbf{x}^{(i)}) = \frac{N_f}{N}$$

其中$N_f$为落入失效域的样本数量。

蒙特卡洛方法的理论收敛性由概率论中的两大极限定理保证。根据柯尔莫哥洛夫强大数定律，当样本量$N$趋向于无穷时，估计量$\hat{I}_{MC}$以概率1收敛于真实积分值$I$。更重要的是，中心极限定理揭示了估计误差的分布规律。当$N$充分大时，估计误差$\epsilon_N = \hat{I}_{MC} - I$渐近服从均值为0、方差为$\sigma_Y^2/N$的正态分布。这意味着蒙特卡洛积分的标准差为：

$$\text{Error}_{MC} \propto \frac{\sigma_Y}{\sqrt{N}}$$

这一误差表达式揭示了蒙特卡洛方法的两个核心特性。第一，其收敛速度为$O(N^{-1/2})$。这一收敛阶完全独立于输入空间的维数$d$。这一特性赋予了蒙特卡洛方法在高维问题中无可替代的优势——无论输入参数有10个还是1000个，只要样本量相同，估计精度的量级就基本保持不变，从而彻底摆脱了数值积分中的“维数灾难”。第二，其收敛速度是缓慢的。误差与样本量的平方根成反比，意味着为了将估计精度提高一个数量级（即误差减小为原来的1/10），计算样本量必须增加100倍。

这种缓慢的收敛特性构成了蒙特卡洛方法在工程应用中的主要瓶颈。在现代工程设计中，单次高保真仿真可能耗时数小时甚至数天。若要获得具有统计显著性的结果，往往需要成千上万次计算，这在计算预算上通常是不可接受的。这种矛盾在小概率事件的评估中尤为尖锐。例如，在结构可靠性分析中，目标失效概率往往在$10^{-3}$到$10^{-6}$量级。为了以10%的相对误差估计$10^{-4}$的失效概率，根据公式$CoV \approx 1/\sqrt{N P_f}$，大约需要$10^6$次仿真计算。如此巨大的计算代价使得直接蒙特卡洛模拟在处理极低概率事件或极度耗时的物理模型时变得不切实际。因此，尽管蒙特卡洛方法在理论上严谨且易于并行化实现，但在计算资源受限的实际工程约束下，必须寻求包括方差缩减技术、代理模型方法在内的更高效的不确定性量化策略。

### 3.2.2 改进采样与降维方法

尽管标准蒙特卡洛模拟具有实现简便和普适性强的优点，但其收敛速度仅为样本量的平方根倒数，这意味着为了获得高精度的统计矩或失效概率估计，往往需要付出巨大的计算代价。为了缓解计算效率与精度之间的矛盾，不确定性量化领域发展了一系列方差缩减技术。这些技术的核心思想是在保持估计量无偏或渐近无偏的前提下，通过利用输入变量的结构信息或输出响应的特征信息，显著降低估计量的方差，从而以较小的样本规模逼近真实值。本节将重点阐述三种在工程中应用最为广泛的改进采样策略：拉丁超立方采样、重要性采样以及准蒙特卡洛方法。

拉丁超立方采样（Latin Hypercube Sampling, LHS）[Shields et al., The generalization of Latin hypercube sampling, 2016-04, Reliability Engineering & System Safety](zotero://select/library/items/EJ3QIF6I)是一种基于分层抽样原理的高效采样技术，最初由McKay等人提出[McKay, Latin hypercube sampling as a tool in uncertainty analysis of computer models, 1992, Proceedings of the 24th conference on Winter simulation  - WSC '92](zotero://select/library/items/85F3RE8N)，旨在解决多维空间投影的均匀性问题。与简单随机采样不同，LHS强制样本在每一个维度的边缘分布上都实现分层均匀覆盖。

对于一个$n$维随机向量$\mathbf{X}$，LHS首先将每个随机变量$X_i$的累积分布函数$F_{X_i}(x)$的值域$[0, 1]$划分为$N$个互不重叠且概率相等的子区间，区间长度均为$1/N$。随后，在每个维度的每个子区间内独立抽取一个样本点，确保了边缘分布的每个概率层级都有且仅有一个样本被选取。为了构建$n$维联合样本，需要将各维度的样本进行随机配对。数学上，这通过生成$n$个独立的随机排列来实现。设$\pi_i(\cdot)$为集合$\{1, 2, \dots, N\}$上的一个随机置换，第$j$个样本点在第$i$维上的取值由下式确定：

$$x_{i}^{(j)} = F_{X_i}^{-1}\left( \frac{\pi_i(j) - 1 + \xi_{i}^{(j)}}{N} \right)$$

其中，$\xi_{i}^{(j)}$是服从$[0, 1]$均匀分布的独立随机变量，用于在子区间内部引入随机扰动。LHS的一个显著优势在于它能够完全过滤掉由输入变量的加性主效应引起的方差。如果系统响应函数可以近似表示为各输入变量函数的和，LHS的估计方差将远小于简单蒙特卡洛模拟，其收敛速度在最优情况下可接近$O(N^{-1})$。此外，由于LHS保持了样本的随机性，统计学中的中心极限定理依然适用，使得误差估计成为可能。

当分析的重点从全局统计矩转向尾部极值或小概率失效事件时，重要性采样（Importance Sampling, IS）[Tokdar et al., Importance sampling: a review, 2010-01, WIREs Computational Statistics](zotero://select/library/items/88D2Q752)则成为了更为合适的选择。在可靠性分析中，失效事件通常发生在输入空间的局部区域，且发生概率极低（如$10^{-3}$至$10^{-6}$）。标准蒙特卡洛模拟在安全域内浪费了大量计算资源，而落入失效域的样本寥寥无几，导致估计结果具有极高的变异系数。重要性采样的核心思想是通过测度变换，人为地修改采样分布，使得样本更多地出现在对积分贡献较大的区域（即失效域或高概率密度区域），随后通过引入似然比权重来修正由此产生的偏差。设原概率密度函数为$f_{\mathbf{X}}(\mathbf{x})$，引入一个易于采样的辅助概率密度函数$h(\mathbf{x})$，使得在失效域内$h(\mathbf{x}) > f_{\mathbf{X}}(\mathbf{x})$。则目标积分$I = \mathbb{E}[g(\mathbf{X})]$可重写为关于$h(\mathbf{x})$的期望：

$$I = \int g(\mathbf{x}) f_{\mathbf{X}}(\mathbf{x}) d\mathbf{x} = \int g(\mathbf{x}) \frac{f_{\mathbf{X}}(\mathbf{x})}{h(\mathbf{x})} h(\mathbf{x}) d\mathbf{x} = \mathbb{E}_h \left[ g(\mathbf{X}) \frac{f_{\mathbf{X}}(\mathbf{X})}{h(\mathbf{X})} \right]$$

基于此，重要性采样的估计量构造为：

$$\hat{I}_{IS} = \frac{1}{N} \sum_{j=1}^{N} g(\mathbf{x}^{(j)}) w(\mathbf{x}^{(j)})$$

其中，样本$\mathbf{x}^{(j)}$是从辅助分布$h(\mathbf{x})$中抽取的，权重$w(\mathbf{x}) = f_{\mathbf{X}}(\mathbf{x}) / h(\mathbf{x})$称为似然比。重要性采样的成败完全取决于辅助分布$h(\mathbf{x})$的选择。理论上存在一个最优的辅助分布$h_{opt}(\mathbf{x}) \propto |g(\mathbf{x})| f_{\mathbf{X}}(\mathbf{x})$，使得估计方差为零。然而，构造该分布需要预先知道目标积分值，这在实际中是不可能的。因此，工程实践中常采用基于设计点的移动分布、核密度估计或自适应重要性采样策略来逼近最优分布，从而在保证无偏性的同时大幅降低方差。

除了上述基于概率论的改进方法，准蒙特卡洛（Quasi-Monte Carlo, QMC）方法[Hou et al., Quasi-Monte Carlo based uncertainty analysis: Sampling efficiency and error estimation in engineering applications, 2019-11-01, Reliability Engineering & System Safety](zotero://select/library/items/XJUXZ37G)通过引入数论中的低差异序列，彻底摒弃了伪随机数的概念。QMC的基本前提是，为了数值积分的准确性，样本点应当尽可能均匀地填满整个积分域，避免出现随机采样中常见的团簇或空隙现象。这种均匀性通过差异度来量化。常用的低差异序列包括Sobol序列、Halton序列和Faure序列等，它们是通过确定性的算法生成的，旨在最小化星差异度$D_N^*(\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(N)})$。根据Koksma-Hlawka不等式，对于有界变差函数$g(\mathbf{x})$，QMC的积分误差上界由下式给出：

$$|\hat{I}_{QMC} - I| \le V(g) \cdot D_N^*(\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(N)})$$

其中，$V(g)$是函数$g$的Hardy-Krause变差。由于低差异序列的差异度量级为$O((\log N)^n / N)$，因此QMC在理论上具有比标准蒙特卡洛更优的渐近收敛阶，接近$O(N^{-1})$。这意味着在相同精度下，QMC所需的样本量显著少于随机模拟。然而，需要注意的是，随着维数$n$的增加，$(\log N)^n$项会迅速增长，导致在极高维问题中QMC的优势不再明显，这种现象并未完全摆脱维数灾难。为了兼顾QMC的高效性和蒙特卡洛的误差评估能力，研究者还提出了随机准蒙特卡洛，如加扰Sobol序列，通过引入随机位移保留了低差异特性，同时允许进行方差估计。

在处理复杂工程系统的不确定性量化问题时，随着输入随机变量维度的增加，计算模型的输入空间体积呈指数级膨胀，这便是著名的维数灾难问题。在高维空间中，样本数据的稀疏性使得常规的蒙特卡洛模拟难以在有限的计算资源下覆盖整个概率空间，同时也严重阻碍了高精度代理模型的构建。因此，除了采用改进的采样策略外，对高维输入空间进行有效的降维处理，是提升计算效率、保证分析精度的另一条关键途径。降维技术在本质上可以分为两类，一类是基于特征提取的变换方法，旨在通过数学变换将原始相关变量映射为一组低维的无关变量；另一类是基于特征选择的筛选方法，旨在识别并剔除对输出响应影响微弱的非重要变量。

主成分分析（Principal Component Analysis, PCA）[Shlens, A Tutorial on Principal Component Analysis, 2014-04-03](zotero://select/library/items/MSL58JF3)是特征提取方法中最具代表性的技术，尤其适用于处理输入参数之间存在强线性相关性的情形。在工程实际中，许多几何参数或材料属性并非独立变化，而是表现出某种统计相关结构。PCA通过正交变换，将一组可能相关的变量转换为一组线性不相关的变量，这些新变量称为主成分。从数学角度看，假设输入随机向量为$\mathbf{X} = [X_1, X_2, \dots, X_d]^T$，其协方差矩阵为$\mathbf{\Sigma}_{\mathbf{X}}$。PCA的目标是寻找一组正交基向量$\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_d$，使得数据在这些基向量方向上的投影方差最大化。这等价于求解协方差矩阵的特征值问题：
$$\mathbf{\Sigma}_{\mathbf{X}} \mathbf{v}_k = \lambda_k \mathbf{v}_k$$
其中，$\lambda_k$为特征值，$\mathbf{v}_k$为对应的特征向量。将特征值按降序排列$\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_d$，前$k$个主成分解释了数据总方差的大部分。通过截断较小的特征值对应的成分，可以将原始$d$维空间降至$k$维子空间（$k < d$），同时保留了系统输入的主要统计变异信息。

然而，传统的PCA仅能处理线性相关结构，对于工程中广泛存在的非线性依赖关系，其降维效果往往受限。为此，基于核技巧的核主成分分析（Kernel PCA, KPCA）[Scholkopf et al., Kernel Principal Component Analysis](zotero://select/library/items/TRS83GT5)应运而生。KPCA的基本思想是将原始输入空间通过非线性映射$\phi(\cdot)$映射到一个高维甚至无穷维的特征空间$\mathcal{F}$中，并在该特征空间内执行线性的PCA。根据Mercer定理，无需显式知道映射函数$\phi(\cdot)$的具体形式，只需定义一个满足正定性的核函数$k(\mathbf{x}_i, \mathbf{x}_j) = \langle \phi(\mathbf{x}_i), \phi(\mathbf{x}_j) \rangle$来计算内积即可。常用的核函数包括高斯径向基核（RBF）和多项式核。通过构建核矩阵$\mathbf{K}$并对其进行特征分解，可以提取出非线性主成分。KPCA能够有效揭示输入数据在高维流形上的非线性结构，从而实现更为紧致的参数化表达。

不同于PCA等无监督的变换方法，基于灵敏度分析的降维方法[Nikishova et al., Sensitivity analysis based dimension reduction of multiscale models, 2020-04, Mathematics and Computers in Simulation](zotero://select/library/items/J5IRQ3CC)属于有监督的特征选择范畴，它直接利用系统输出响应的信息来评估输入变量的重要性。在众多灵敏度分析方法中，基于方差分解的Sobol全局灵敏度分析因其能够定量评估变量的主效应及其与其他变量的交互效应，被公认为最严谨的方法之一。Sobol方法基于方差分析分解原理，将模型响应$Y = g(\mathbf{X})$的方差$D$分解为各项子方差之和：
$$D = \sum_{i=1}^{d} D_i + \sum_{1 \le i < j \le d} D_{ij} + \dots + D_{1,2,\dots,d}$$
其中，$D_i$表示第$i$个变量单独作用引起的方差贡献，$D_{ij}$表示第$i$和第$j$个变量交互作用引起的方差贡献。在此基础上，定义一阶灵敏度指数$S_i = D_i / D$，用于衡量单个变量对输出方差的直接贡献；以及总效应灵敏度指数$S_{Ti}$，用于衡量第$i$个变量及其涉及的所有高阶交互项对总方差的贡献总和。总效应指数的计算公式为：
$$S_{Ti} = 1 - \frac{D_{\sim i}}{D}$$
其中，$D_{\sim i}$表示除变量$X_i$以外所有其他变量引起的方差。在降维实践中，总效应指数$S_{Ti}$具有决定性的判别意义。若某个变量的$S_{Ti}$接近于零，说明该变量无论是单独作用还是与其他变量耦合，对系统响应的不确定性贡献均可忽略不计。基于此准则，可以将该变量固定为确定性常数，从而将原本的高维随机问题简化为低维问题。

这种基于灵敏度分析的降维策略通常作为一个预处理步骤，在构建高精度的代理模型之前实施。通过剔除大量的非重要变量，可以显著降低代理模型训练所需的样本量，缓解“维数灾难”对代理模型收敛速度和泛化能力的影响。例如，在一个包含50个随机参数的结构动力学模型中，经过Sobol分析可能发现仅有8个参数的总效应指数显著大于阈值。此时，后续的复杂概率分析只需在这8维子空间内进行，这将使计算效率提升数个数量级。需要指出的是，灵敏度指数本身的计算也需要一定的计算成本，因此在实际操作中，常采用基于筛选的Morris方法作为初筛，或利用低阶多项式混沌展开快速估算Sobol指数，以实现降维成本与效益的最佳平衡。

## 3.3 基于代理模型的概率分析

尽管改进采样方法在一定程度上缓解了计算压力，但对于极度耗时的复杂物理模型，直接调用求解器进行数千次计算仍然是不切实际的。基于代理模型的方法因此应运而生。代理模型本质上是一种基于数据驱动的近似数学模型，它通过少量的高保真模型数据构建起输入与输出之间的快速映射关系。一旦构建完成，代理模型可以在几乎忽略不计的时间内代替原模型进行预测，从而使得蒙特卡洛模拟等需要大量评估的方法变得可行。本节将重点介绍两类在不确定性量化中应用最广的代理模型，即基于谱展开的多项式混沌展开方法和基于统计学习的高斯过程方法。它们分别代表了全局逼近和局部统计插值的两种不同思路。

### 3.3.1 多项式混沌展开

多项式混沌展开（Polynomial Chaos Expansion, PCE）[Crestaux et al., Polynomial chaos expansion for sensitivity analysis, 2009-07-01, Reliability Engineering & System Safety](zotero://select/library/items/TVUHQU3S)作为不确定性量化领域中基于谱方法的代表性技术，为随机系统的建模与分析提供了一种高效且严密的数学框架。与蒙特卡洛模拟关注随机样本点的统计特性不同，PCE旨在寻找随机响应函数在特定概率空间下的最佳函数逼近。其核心思想源于随机过程的谱表示理论，即任何有限方差的随机变量（即属于$L^2$希尔伯特空间的随机函数）均可展开为关于输入随机变量的正交多项式基函数的级数和。这种方法将原本复杂的随机输入输出映射关系，转化为一组确定性系数的求解问题，从而在保持高精度的同时，极大地降低了计算成本，尤其适用于响应面较为光滑的工程问题。

在数学表述上，假设工程系统的输入参数由$d$维独立随机向量$\mathbf{X}$表示，经过适当的等概率变换，通常将其映射为一组服从标准分布（如标准正态分布或均匀分布）的独立随机变量$\boldsymbol{\xi} = (\xi_1, \xi_2, \dots, \xi_d)^T$。系统的输出响应$Y = g(\boldsymbol{\xi})$可以表示为如下的多项式混沌展开式：

$$Y = \sum_{\boldsymbol{\alpha} \in \mathbb{N}^d} c_{\boldsymbol{\alpha}} \Psi_{\boldsymbol{\alpha}}(\boldsymbol{\xi})$$

其中，$\boldsymbol{\alpha} = (\alpha_1, \dots, \alpha_d)$是一个$d$维多重索引，用于标识多项式的阶数；$c_{\boldsymbol{\alpha}}$是待求的确定性展开系数，包含了系统响应的统计特征信息；$\Psi_{\boldsymbol{\alpha}}(\boldsymbol{\xi})$是多元正交多项式基函数。为了保证展开式的收敛性与计算的稳定性，基函数的选择必须遵循Wiener-Askey方案。该方案建立了输入变量的概率分布与正交多项式族之间的一一对应关系，其依据在于多项式正交性的权函数应与输入变量的概率密度函数一致。具体而言，若输入变量服从高斯分布，应选取Hermite多项式作为基函数；若服从均匀分布，则选取Legendre多项式；若服从Beta分布，则选取Jacobi多项式。这种匹配确保了基函数在相应的概率测度下满足如下正交性质：

$$\mathbb{E}[\Psi_{\boldsymbol{\alpha}}(\boldsymbol{\xi}) \Psi_{\boldsymbol{\beta}}(\boldsymbol{\xi})] = \int_{\Omega} \Psi_{\boldsymbol{\alpha}}(\boldsymbol{\xi}) \Psi_{\boldsymbol{\beta}}(\boldsymbol{\xi}) f_{\boldsymbol{\xi}}(\boldsymbol{\xi}) d\boldsymbol{\xi} = \delta_{\boldsymbol{\alpha}\boldsymbol{\beta}} \gamma_{\boldsymbol{\alpha}}$$

式中，$\delta_{\boldsymbol{\alpha}\boldsymbol{\beta}}$为Kronecker符号，$\gamma_{\boldsymbol{\alpha}}$为归一化常数。多元基函数$\Psi_{\boldsymbol{\alpha}}(\boldsymbol{\xi})$通常通过一元正交多项式的张量积构建，即$\Psi_{\boldsymbol{\alpha}}(\boldsymbol{\xi}) = \prod_{i=1}^d \psi_{\alpha_i}^{(i)}(\xi_i)$，其中$\psi_k^{(i)}$是对应于第$i$个变量的$k$阶一元正交多项式。

理论上，PCE是一个无穷级数，但在实际计算中必须进行截断。最常用的截断策略是全阶截断，即保留所有总阶数不超过$p$的项，满足$|\boldsymbol{\alpha}| = \sum \alpha_i \le p$。此时，展开项的总数$P$与输入维数$d$和多项式阶数$p$之间遵循二项式关系$P = \binom{d+p}{p}$。这一公式揭示了PCE面临的“维数灾难”挑战，即随着输入维数$d$的增加，待求系数的数量$P$呈阶乘级增长。因此，标准PCE通常仅适用于中低维问题（$d < 20$）。

构建PCE代理模型的关键任务在于高效、准确地计算展开系数$c_{\boldsymbol{\alpha}}$。目前主流的方法分为非侵入式谱投影法和线性回归法。谱投影法直接利用基函数的正交性，将系数计算转化为高维积分问题：

$$c_{\boldsymbol{\alpha}} = \frac{1}{\gamma_{\boldsymbol{\alpha}}} \mathbb{E}[Y \Psi_{\boldsymbol{\alpha}}(\boldsymbol{\xi})] = \frac{1}{\gamma_{\boldsymbol{\alpha}}} \int_{\Omega} g(\boldsymbol{\xi}) \Psi_{\boldsymbol{\alpha}}(\boldsymbol{\xi}) f_{\boldsymbol{\xi}}(\boldsymbol{\xi}) d\boldsymbol{\xi}$$

该积分可以通过全张量积高斯求积公式或稀疏网格积分来数值求解。谱投影法的优点是系数计算相互独立，且精度较高，但随着维数增加，所需积分点的数量会迅速超出计算预算。相比之下，回归法将系数求解视为一个最小二乘拟合问题。通过抽取$N$个训练样本点（通常采用拉丁超立方采样或准蒙特卡洛序列），构建设计矩阵$\mathbf{\Psi}$和响应向量$\mathbf{y}$，目标是寻找使模型预测误差平方和最小的系数向量$\mathbf{c}$：

$$\hat{\mathbf{c}} = \arg \min_{\mathbf{c}} \sum_{i=1}^N \left( g(\boldsymbol{\xi}^{(i)}) - \sum_{\boldsymbol{\alpha} \in \mathcal{A}} c_{\boldsymbol{\alpha}} \Psi_{\boldsymbol{\alpha}}(\boldsymbol{\xi}^{(i)}) \right)^2$$

线性回归法的核心优势在于其灵活性，它解耦了样本数量与求积规则的约束。根据经验法则，为了保证回归问题的适定性和数值稳定性，样本量$N$通常需要达到待求系数个数$P$的2至3倍（即$N \approx 2P \sim 3P$）。

一旦展开系数$c_{\boldsymbol{\alpha}}$被确定，PCE模型便成为了原物理模型的显式解析替代。PCE最显著的优势在于，它使得对系统响应的统计分析变得异常简单，无需再进行额外的蒙特卡洛模拟。利用正交多项式的性质，响应的均值直接等于常数项系数$c_{\mathbf{0}}$（其中$\mathbf{0}$表示全零索引）。根据Parseval恒等式，响应的方差可以解析地表示为除常数项外所有系数平方的加权和：

$$\mu_Y = c_{\mathbf{0}}, \quad \sigma_Y^2 = \sum_{\boldsymbol{\alpha} \in \mathcal{A}, \boldsymbol{\alpha} \neq \mathbf{0}} c_{\boldsymbol{\alpha}}^2 \gamma_{\boldsymbol{\alpha}}$$

更为重要的是，PCE天然支持全局灵敏度分析。Sobol灵敏度指数可以直接通过系数的组合运算获得。对于任意输入变量子集$u \subseteq \{1, \dots, d\}$，其对输出方差的贡献$D_u$可以表示为所有仅与该子集相关的多项式项的方差之和。例如，第$i$个变量的一阶Sobol指数$S_i$计算如下：

$$S_i = \frac{D_i}{\sigma_Y^2} = \frac{1}{\sigma_Y^2} \sum_{\boldsymbol{\alpha} \in \mathcal{A}_i} c_{\boldsymbol{\alpha}}^2 \gamma_{\boldsymbol{\alpha}}$$

其中，集合$\mathcal{A}_i$包含了仅在第$i$维上有非零阶数的多重索引。这种一次建模即可全面分析的特性，使得多项式混沌展开成为了概率不确定性量化中兼具效率与深度的核心工具，广泛应用于各类工程系统的可靠性分析、灵敏度分析及优化设计中。

### 3.3.2 高斯过程与响应面方法

与多项式混沌展开致力于构建输入与输出之间的全局显式多项式逼近不同，高斯过程（Gaussian Process, GP）回归[Wang, An Intuitive Tutorial to Gaussian Process Regression, 7/2023, Computing in Science & Engineering](zotero://select/library/items/ZMBL33EA)，又被称为Kriging模型。它属于非参数贝叶斯统计推断的范畴，其核心理念不再是寻找一个固定的函数形式来拟合数据，而是通过定义函数的概率分布来描述模型。简而言之，高斯过程假设系统响应$y(\mathbf{x})$是定义在连续输入空间上的一个随机过程的特定实现，且该过程的任意有限个采样点的联合分布均服从多元正交分布。这种方法不仅能够提供目标响应的预测均值，更能给出一个随输入位置变化的预测方差。这一独特的双重输出特性，量化了由于训练数据稀疏而导致的认知不确定性，使得高斯过程成为小样本条件下构建高精度代理模型以及实施序列自适应采样的首选工具。

在数学架构上，Kriging模型通常被表述为确定性趋势项与随机偏差项的叠加。对于$d$维输入向量$\mathbf{x} \in \mathbb{R}^d$，响应函数$y(\mathbf{x})$表示为：

$$y(\mathbf{x}) = \mathbf{f}(\mathbf{x})^T \boldsymbol{\beta} + Z(\mathbf{x})$$

其中第一部分$\mathbf{f}(\mathbf{x})^T \boldsymbol{\beta}$描述了系统的全局均值趋势，$\mathbf{f}(\mathbf{x})$为回归基函数向量，$\boldsymbol{\beta}$为对应的回归系数。根据基函数的选择不同，模型可分为普通克里金（Ordinary Kriging）和泛克里金（Universal Kriging）。第二部分$Z(\mathbf{x})$是核心所在，它是一个零均值的平稳高斯随机过程，用于捕捉局部非线性偏差。该过程的统计特性完全由协方差函数或核函数$k(\mathbf{x}, \mathbf{x}')$决定，即$\mathbb{E}[Z(\mathbf{x})Z(\mathbf{x}')] = \sigma^2 R(\mathbf{x}, \mathbf{x}'; \boldsymbol{\theta})$，其中$\sigma^2$是过程方差，$R(\cdot, \cdot)$是相关函数，$\boldsymbol{\theta}$是超参数。

核函数的选择决定了代理模型对响应曲面光滑度和波动性的先验假设。在工程应用中，最常用的核函数包括高斯核和Matern核。高斯核假设响应函数是无限可微的，适用于极其光滑的物理过程，而Matern核则允许对函数的可微性进行精细控制，更能反映实际工程中可能存在的粗糙性或快速变化特征。为了处理多维输入参数在敏感度上的差异，通常采用各向异性核函数，即为每个输入维度赋予一个独立的特征长度尺度。特征长度尺度$\theta_i$衡量了在第$i$个维度上，输入变量变化多大范围时，输出响应之间的相关性显著衰减。$\theta_i$越小，意味着响应随该输入变化越剧烈；反之则意味着变化平缓。

Kriging模型的训练过程，本质上是一个基于观测数据的超参数优化过程。设训练数据集为$\mathcal{D} = \{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^N$，包含$N$个样本点。模型训练通过极大似然估计或交叉验证来确定最优超参数$\boldsymbol{\theta}$。由于对于给定的相关参数，回归系数$\boldsymbol{\beta}$和过程方差$\sigma^2$可以通过解析方法求出，因此优化问题通常转化为对相关参数$\boldsymbol{\theta}$的集中对数似然函数的最小化问题。这一过程虽然计算量适中，但似然函数往往是非凸的，可能存在多个局部极值，因此通常需要借助遗传算法或多起点梯度优化算法来求解。

一旦模型训练完成，对于任意未知的预测点$\mathbf{x}^*$，Kriging模型通过计算后验分布进行预测。根据多元正态分布的条件概率公式，预测值$Y(\mathbf{x}^*)$服从正态分布$\mathcal{N}(\hat{y}(\mathbf{x}^*), \hat{s}^2(\mathbf{x}^*))$。其中，预测均值$\hat{y}(\mathbf{x}^*)$即为最佳线性无偏估计：

$$\hat{y}(\mathbf{x}^*) = \mathbf{f}(\mathbf{x}^*)^T \hat{\boldsymbol{\beta}} + \mathbf{r}(\mathbf{x}^*)^T \mathbf{R}^{-1} (\mathbf{y} - \mathbf{F}\hat{\boldsymbol{\beta}})$$

其中$\mathbf{y}$是训练样本响应向量，$\mathbf{F}$是训练点的回归矩阵，$\mathbf{R}$是$N \times N$的样本相关矩阵，$\mathbf{r}(\mathbf{x}^*)$是预测点与所有训练样本点之间的相关向量。该均值预测具有插值特性，即在训练点处预测值严格等于观测值。更为关键的是预测方差$\hat{s}^2(\mathbf{x}^*)$：

$$\hat{s}^2(\mathbf{x}^*) = \sigma^2 \left( 1 - \mathbf{r}(\mathbf{x}^*)^T \mathbf{R}^{-1} \mathbf{r}(\mathbf{x}^*) + u(\mathbf{x}^*)^T (\mathbf{F}^T \mathbf{R}^{-1} \mathbf{F})^{-1} u(\mathbf{x}^*) \right)$$

这里$u(\mathbf{x}^*)$是一个辅助向量。预测方差直观地展示了置信区间在观测点附近，方差收缩为零，而在远离观测点的稀疏区域，方差显著增大。

这种能够量化自身预测误差的能力，赋予了Kriging模型在主动学习策略中不可替代的地位。在基于代理模型的全局优化或可靠性分析中[Echard et al., AK-MCS: An active learning reliability method combining Kriging and Monte Carlo Simulation, 2011-03, Structural Safety](zotero://select/library/items/5FKQKPQM)，研究者利用预测均值和预测方差构造学习函数，如期望改进或U函数指导计算机自动寻找对改善模型精度最有价值的下一个采样点。这种序列加点策略使得Kriging模型能够以极少的样本量（通常几十到几百个）精确捕捉复杂的非线性响应面，尤其是在处理具有多峰特性或不连续失效边界的问题时表现优异。然而，Kriging的主要局限在于其计算复杂度，预测公式中涉及相关矩阵$\mathbf{R}$的求逆或Cholesky分解，其运算量随样本量$N$呈$O(N^3)$增长。因此，当样本量超过数千时，通常需要采用稀疏高斯过程近似或局部Kriging方法来降低计算负担。

RSM是一种更早期的代理模型方法，它使用低阶多项式（通常是二次多项式）来近似系统响应函数 $\mathcal{M}(\mathbf{X})$。
$$\tilde{\mathcal{M}}(\mathbf{X}) = \beta_0 + \sum_{i=1}^n \beta_i X_i + \sum_{i=1}^n \beta_{ii} X_i^2 + \sum_{1 \le i < j \le n} \beta_{ij} X_i X_j + \epsilon \quad \text{(公式 3.3.10)}$$
其中 $\beta$ 是多项式系数，通过最小二乘回归从一组实验设计点处的模型响应中确定。RSM的优点是简单、易于实现，但其精度受限于多项式的阶数，难以捕捉高度非线性的系统响应。在现代UQ中，PCE和GP因其更高的精度和更强的理论基础，已在很大程度上取代了传统的RSM。

## 3.4 概率方法的工程适用性

前文所述的各种概率不确定性量化方法各有优劣，在将其应用于实际工程问题时，不能盲目套用，而必须结合具体问题的特征进行适用性分析。工程适用性主要体现在计算效率与精度的权衡，以及面对复杂工程约束时的方法选择策略。本节将总结如何在有限的计算资源下，针对不同维度、不同非线性程度的问题，选择最合适的量化手段。

### 3.4.1 计算效率与精度问题

在不确定性量化的工程实践中，计算效率与估计精度构成了一对永恒且核心的矛盾。随着现代工程系统复杂度的提升，高保真度物理模型（的单次求解往往需要消耗巨大的计算资源和时间。在这种背景下，不确定性传播所需的成千上万次模型评估变得极具挑战性。因此，评估并选择一种量化方法，不再仅仅是基于其理论上的渐近收敛性，更取决于其在有限计算预算约束下，能否以最小的计算代价达成工程所允许的误差容限。这一权衡过程涉及对模型维数、非线性程度、采样策略以及代理模型训练成本的综合考量。

首先，必须从收敛阶的角度审视直接模拟方法与代理模型方法的效率差异。蒙特卡洛模拟作为基准方法，其无偏估计的特性使其精度完全独立于物理模型的数学形式，且具有鲁棒的误差收敛行为。然而，其$O(N^{-1/2})$的收敛速度意味着为了将统计误差减半，计算量必须增加四倍。对于高保真模型，若单次计算耗时1小时，进行$10^4$次模拟以获取$10^{-2}$量级的精度显然是不切实际的。尽管拉丁超立方采样和准蒙特卡洛等改进策略通过提高空间填充性，在一定程度上将收敛速度提升至接近$O(N^{-1})$，但这种优势在高维空间（维数$d > 20$）中会迅速衰减，依然难以从根本上解决大规模计算的瓶颈。

相比之下，基于谱展开的多项式混沌展开在处理低维且响应光滑的问题时，展现出谱收敛或指数级收敛特性。即随着多项式阶数的增加，近似误差呈指数速率下降。这意味着在理想情况下，仅需少量的模型评估即可获得极高的精度。然而，PCE面临着维数灾难挑战，当维数较高时，待求系数的数量急剧膨胀，导致求解所需的样本量甚至超过了直接蒙特卡洛模拟的需求。为了解决这一矛盾，稀疏PCE技术[Lüthen et al., Sparse Polynomial Chaos Expansions: Literature Survey and Benchmark, 2021-01, SIAM/ASA Journal on Uncertainty Quantification](zotero://select/library/items/E3N8MZR6)应运而生。该技术基于稀疏性假设，即在高维空间中，绝大多数基函数项对输出方差的贡献是微不足道的，只有少数低阶项或主交互项起主导作用。通过引入$L_1$范数正则化，稀疏PCE能够从庞大的候选基函数库中自动筛选出非零系数项，从而将求解问题的自由度从组合数级别降低至与有效系数个数相当的线性级别。这使得PCE在处理中高维问题时，依然能够保持较高的计算效率。

对于具有强非线性、多峰或非光滑特征的响应面，高斯过程回归通常表现出比PCE更优的适应性。Kriging模型作为一种插值技术，能够精确穿过所有训练样本点，非常适合样本数据极其昂贵且稀缺的场景。然而，Kriging的计算效率瓶颈在于模型训练阶段。模型训练涉及对$N \times N$维相关矩阵的求逆或Cholesky分解，其时间复杂度为$O(N^3)$。当训练样本量增加时，训练时间呈立方级增长。此外，当样本点在空间中距离过近时，相关矩阵的条件数会急剧恶化，导致数值不稳定甚至求解失败。因此，Kriging并不适合大数据量建模。在评估效率时，不仅要考虑代理模型预测的时间，更必须权衡代理模型构建本身的时间成本。如果构建高精度代理模型所需的训练样本量过大，以至于训练总耗时接近于直接运行蒙特卡洛模拟的时间，那么代理模型方法便失去了其工程价值。

在精度评估方面，必须严格区分“训练误差”与“泛化误差”。极小的训练误差并不意味着模型在未知区域具有高精度，反而可能是过拟合的表现。为了真实评估代理模型的泛化能力，必须采用交叉验证技术。留一法是一种严谨的验证手段，它依次剔除一个样本点，利用剩余$N-1$个点构建模型来预测被剔除点的响应，遍历所有点后计算平均误差。对于PCE和Kriging，幸运的是，存在解析公式可以直接基于完整模型的某些矩阵分量计算误差，而无需重复训练$N$次模型，这极大地降低了验证成本。

最后，计算效率与精度的权衡还取决于分析的具体目标是关注全局统计特性还是局部极值特性。若目标是均值估计，要求代理模型在整个参数空间内具有良好的全局逼近能力，若目标是可靠性分析，关注的是尾部极小概率事件，此时要求模型在整个域内都精确是极其浪费的，我们只关心模型在极限状态曲面附近的精度。在这种情形下，结合主动学习的策略代表了当前效率与精度的最佳平衡。该策略利用Kriging提供的预测方差构建学习函数，指导算法仅在预测不确定性大且可能导致误判的失效边界区域进行采样。这种按需加点的机制，使得往往只需数百次物理模型计算，即可在局部区域达到极高精度，从而准确估计出原本需要数百万次蒙特卡洛模拟才能得到的失效概率，实现了计算效率数量级的飞跃。

### 3.4.2 方法选择与工程约束

在将概率不确定性量化理论转化为实际工程生产力的过程中，分析人员面临的挑战往往超越了单纯的数学算法范畴，更多地体现为如何在复杂的现实约束条件下做出最优的策略选择。这些约束条件主要来源于现有商业仿真软件的封闭性架构、物理问题本身的高维与非线性特征以及有限的项目周期与计算资源。因此，工程适用性的核心在于深入理解各类方法的边界条件，并能根据具体问题的特征，在计算效率、估计精度与实施难度之间找到最佳的平衡点。

首要的工程约束来自于仿真软件的架构封闭性，这直接决定了是采用非侵入式还是侵入式的实施路线。在航空航天、核能及汽车工业中，绝大多数高保真仿真依赖于成熟的商业代码如Ansys、Abaqus和Fluent，或者经过长期验证的遗留代码。这些求解器通常不公开源代码，或者其代码规模极其庞大且逻辑复杂，使得任何试图修改其内部数据结构或求解流程的尝试都变得异常困难且风险极高。因此，非侵入式方法成为了工程界的绝对主导选择。此类方法将确定性求解器视为一个输入输出映射算子，仅需通过脚本语言编写外部包装器来自动化地生成输入文件、驱动求解器运行并提取结果数据。蒙特卡洛模拟、拉丁超立方采样、回归型多项式混沌展开以及高斯过程回归模型均属于此类。非侵入式方法实现了不确定性量化算法与物理求解器的完全解耦，具有极强的通用性和软件独立性，且天然适合在高性能计算集群上进行粗粒度并行计算。相比之下，侵入式方法如随机有限元法，要求将输入随机变量的Karhunen-Loève展开直接代入控制方程，导致刚度矩阵由确定性矩阵变为随机矩阵的展开形式。这将导致最终的线性方程组规模扩大数倍，且方程组呈现高度耦合状态，极大地破坏了原有的稀疏性结构。尽管侵入式方法在单次求解中能直接获得响应的统计系数，理论效率较高，但其巨大的开发成本和极差的可移植性使其难以在工业界大规模推广。

输入参数空间的维度是决定方法选择的第二个关键指标，必须建立基于维度的分级处理流程。对于随机变量个数小于十五的低维问题，不仅计算复杂度的指数级增长尚未显现，而且参数间的交互作用易于捕捉。此时，全阶多项式混沌展开或基于各向异性核的高斯过程回归均为理想选择。它们能在极少的样本量下提供全局高精度的显式表达，且多项式混沌展开便于进行后续的全局灵敏度分析。若训练样本极度稀缺，基于克里金插值的高斯过程模型则更具优势。对于变量个数介于十五至五十之间的中等维度问题，全阶多项式混沌展开的基函数项数将呈爆炸式增长，导致回归问题变成欠定问题。此时必须引入稀疏化技术，如最小角回归或压缩感知技术。稀疏多项式混沌展开利用大多数物理系统仅受少数低阶交互作用主导的稀疏性原理，能够在欠定方程组下自动筛选出关键基函数并求解展开系数，是处理中等维数问题的利器。对于变量个数超过五十甚至上百的高维问题，基于距离度量的高斯过程回归模型会因高维空间中样本距离的均匀化而失效，直接构建代理模型往往面临过拟合或训练失败的风险。此时标准的策略是分两步走，首先利用计算成本低廉的筛选法如Morris方法或基于小样本的Sobol灵敏度分析进行特征选择，剔除大量非重要变量，将问题降维至可处理的子空间。若降维后维数依然很高，或者无法进行有效降维，则应退回到对维数不敏感的拟蒙特卡洛采样或改进的拉丁超立方采样，以保证结果的稳健性，尽管这可能需要付出更多的计算时间。

物理响应函数的正则性，即响应面的光滑程度和拓扑特征，是影响代理模型选择的第三大因素。多项式混沌展开本质上是基于全局多项式基函数的逼近，其最佳收敛性依赖于目标函数的解析性或高度光滑性。如果工程系统表现出强非线性但依然光滑，例如流体阻力随速度的连续变化，多项式混沌展开依然有效。然而，如果系统存在分岔、接触撞击、断裂失效或开关切换等物理机制，导致输出响应面出现不连续、尖点或梯度突变，全局多项式逼近将在不连续点附近产生严重的吉布斯振荡现象，导致局部误差剧增甚至产生非物理的预测值。在这种情况下，基于局部相关性的高斯过程回归模型表现更为稳健，特别是配合Matern类核函数时，能够更好地适应粗糙的响应面。此外，支持向量回归或近年来兴起的深度神经网络代理模型，因其强大的非线性映射能力和对复杂拓扑结构的适应性，在处理此类非光滑及高度非线性问题时往往能取得比传统谱方法更好的效果。

最后，计算预算的硬性约束促使工程界转向多保真度协同分析。在实际项目中，往往存在同一物理对象的多个不同精度的模型，包括高精度但耗时的精细网格模型和低精度但极快的粗糙网格或简化物理模型。协同克里金或多级蒙特卡洛方法能够利用大量的低保真度样本来捕捉响应面的整体趋势，同时利用少量的高保真度样本来校正局部偏差。这种策略在保证最终精度的前提下，能够将计算成本降低数个数量级，是解决超大规模工程系统如全机气动弹性分析或反应堆堆芯模拟不确定性量化的关键路径。

综上所述，并不存在一种万能的不确定性量化方法。最优的工程实践方案往往是混合式的。在非侵入式的框架下，首先通过灵敏度分析进行参数降维，随后根据物理问题的光滑性和非线性程度选择多项式混沌展开或高斯过程回归构建代理模型，并在必要时引入多保真度数据融合或主动学习策略，以在有限的计算预算内实现对模型预测精度的最大化挖掘。

