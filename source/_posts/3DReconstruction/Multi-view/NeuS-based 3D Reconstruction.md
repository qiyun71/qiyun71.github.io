---
title: Multi-view 3D Reconstruction based on SDF and volume rendering
date: 2024-06-17 17:11:22
tags: 
categories: 3DReconstruction/Multi-view
top: true
---

- NeRFç³»åˆ—çš„å¤šè§†å›¾ä¸‰ç»´é‡å»ºæ–¹æ³•å…³é”®æœ‰ä¸¤ç‚¹:
  - å¦‚ä½•è¡¨ç¤º3D model: mesh, voxel, pointcloud, RGBD, Implicit Density Field, SDF or Other Primitive(é«˜æ–¯ä½“, æ¤­åœ†ä½“, çƒ)
  - å¦‚ä½•å°†3D model å¯å¾®æ¸²æŸ“æˆ2Då›¾åƒ: Volume Rendering or Rasterization?


- **Accuracy** é‡å»ºçš„æ¨¡å‹ç²¾åº¦ä¸å¥½ï¼Œå½±å“å› ç´ ï¼š
  - æ•°æ®é›†è´¨é‡ï¼šç…§ç‰‡æ‹æ‘„è´¨é‡(è®¾å¤‡)ã€ç›¸æœºä½å§¿ä¼°è®¡ç²¾åº¦(COLMAP)
    - ç…§ç‰‡è´¨é‡é—®é¢˜ï¼šæ··å ã€æ¨¡ç³Šã€æ»šåŠ¨å¿«é—¨ (RS) æ•ˆåº”ã€HDR/LDRã€è¿åŠ¨æ¨¡ç³Šã€ä½å…‰ç…§
  - NeuSæ–¹æ³•çš„é—®é¢˜ï¼šLosså‡½æ•°çº¦æŸã€ä½“æ¸²æŸ“çš„è¿‡åº¦ç®€åŒ–ã€ç¼ºå°‘ç›‘ç£(*æ·±åº¦oræ³•å‘é‡*)
  - ç½‘æ ¼æå–æ–¹æ³•(Marching Cube)
- **Efficiency** è®­ç»ƒ/æ¸²æŸ“çš„é€Ÿåº¦å¤ªæ…¢ï¼Œå½±å“å› ç´ ï¼š
  - MLPè®¡ç®—æ¬¡æ•°å¤š --> MIMO MLPã€NGP-RTã€
  - MLPå±‚æ•°å¤šè®¡ç®—æ…¢--> InstantNGP


<!-- more -->

*Other link about 3D Reconstruction: (need to add " ../ " in obsidian)*
- [Paper About 3D Reconstruction](../../Paper%20About%203D%20Reconstruction)
  - [Finite Element Model 3D Reconstruction](../../Finite%20Element%20Model%203D%20Reconstruction)
  - [Anime Image 3D Reconstruction](../../Anime%20Image%203D%20Reconstruction)
  - [Multi-view Human Body Reconstruction](Multi-view%20Human%20Body%20Reconstruction)
- [Basics about 3D Reconstruction](../../Basics%20about%203D%20Reconstruction)
- [Datasets](../../Datasets)
- [Code of Multi-view 3D Reconstruction based on SDF and volume rendering](Code%20of%20Multi-view%203D%20Reconstruction%20based%20on%20SDF%20and%20volume%20rendering)

[Master Paper(3DReconstruction)](../../Blog&Book&Paper/Write/Write%20Paper/3D%20Reconstruction/Master%20Paper(3DReconstruction).md)

# Accuracy

## Camera Pose Estimation

## 3D Point Sampling

### NerfAcc

NerfAccï¼šå æ®+é€†å˜æ¢é‡‡æ · æ··åˆé‡‡æ ·æ–¹å¼
å…ˆä½¿ç”¨å æ®ç½‘æ ¼ç¡®å®šå“ªäº›åŒºåŸŸéœ€è¦é‡‡æ ·ï¼Œå†é€šè¿‡ç²—é‡‡æ ·å¾—åˆ°çš„æƒé‡ä½¿ç”¨é€†å˜æ¢é‡‡æ ·è¿›è¡Œç²¾é‡‡æ ·å¾—åˆ°é‡‡æ ·ç‚¹

## Loss Function

### S3IM

> [S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields](https://arxiv.org/pdf/2308.07032)

MSE loss æ˜¯ point-wise çš„ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°ä¸€ç»„pixelçš„ç»“æ„ç‰¹å¾ï¼Œè€ŒSSIMé€šè¿‡ä¸€ä¸ªKxKçš„æ ¸å’Œstrideæ‰«è¿‡æ•´ä¸ªå›¾åƒï¼Œæœ€åæ±‚å¹³å‡MSSIMï¼Œå¯ä»¥è€ƒè™‘å›¾åƒçš„ç»“æ„ä¿¡æ¯ã€‚
- $\mathrm{SSIM}(a,b)=l(\boldsymbol{a},\boldsymbol{b})c(\boldsymbol{a},\boldsymbol{b})s(\boldsymbol{a},\boldsymbol{b}).$  ä¸€ä¸ªæ ¸è¦†ç›–çš„å›¾åƒï¼Œ$C_{1},C_{2},C_{3}$ æ˜¯å°çš„å¸¸æ•°
  - luminanceäº®åº¦ï¼š$l(\boldsymbol{a},\boldsymbol{b})=\frac{2\mu_a\mu_b+C_1}{\mu_a^2+\mu_b^2+C_1},$
  - contrastå¯¹æ¯”åº¦ï¼š$c(\boldsymbol{a},\boldsymbol{b})=\frac{2\sigma_a\sigma_b+C_2}{\sigma_a^2+\sigma_b^2+C_2},$
  - structureç»“æ„ï¼š$s(\boldsymbol{a},\boldsymbol{b})=\frac{\sigma_{ab}+C_3}{\sigma_a\sigma_b+C_3}.$
*ä½†æ˜¯åœ¨NeRFè®­ç»ƒè¿‡ç¨‹ä¸­pixelåœ¨ä¸€ä¸ªbatchæ˜¯éšæœºçš„ï¼Œä¸¢å¤±äº†å±€éƒ¨patchçš„åƒç´ ä¸­ä½ç½®ç›¸å…³çš„ä¿¡æ¯*ï¼Œæœ¬æ–‡æå‡ºçš„S3IMï¼Œæ˜¯SSIMçš„éšæœºå˜ä½“ã€‚æ¯ä¸ªminibatchæœ‰Bä¸ªåƒç´ ï¼Œæ ¸å¤§å°KxKï¼Œæ­¥é•¿s=K(å› ä¸ºåœ¨minibatchä¸­çš„éšæœºpatchæ˜¯ç‹¬ç«‹çš„ï¼Œè€Œä¸”ä¸éœ€è¦é‡å çš„æƒ…å†µ)
- å°†Bä¸ªåƒç´ /å…‰çº¿æ„æˆä¸€ä¸ªrendered patch $\mathcal{P}(\hat{\mathcal{C}})$ï¼ŒåŒæ—¶æœ‰ä¸€ä¸ªgt image patch $\mathcal{P}(\mathcal{C})$
- è®¡ç®—renderedå’Œgt patchä¹‹é—´çš„$SSIM(\mathcal{P}(\hat{\mathcal{C}}),\mathcal{P}(\mathcal{C}))$ with kernel size KxK and stride size s =K
- ç”±äºpatchæ˜¯éšæœºçš„ï¼Œé‡å¤Mæ¬¡ä¸Šè¿°ä¸¤æ­¥ï¼Œå¹¶è®¡ç®—Mæ¬¡SSIMçš„å¹³å‡å€¼

$\mathrm{S3IM}(\hat{\mathcal{R}},\mathcal{R})=\frac{1}{M}\sum_{m=1}^{M}\mathrm{SSIM}(\mathcal{P}^{(m)}(\hat{\mathcal{C}}),\mathcal{P}^{(m)}(\mathcal{C}))$

$L_{\mathrm{S3IM}}(\Theta,\mathcal{R})=1-\mathrm{S3IM}(\hat{\mathcal{R}},\mathcal{R}) = =1-\frac1M\sum_{m=1}^M\mathrm{SSIM}(\mathcal{P}^{(m)}(\hat{\mathcal{C}}),\mathcal{P}^{(m)}(\mathcal{C}))$

## Volume Rendering


### NeuS

> [NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction](https://arxiv.org/pdf/2106.10689)

SDFå†…éƒ¨-1ï¼Œå¤–éƒ¨1ï¼Œè¡¨é¢0
NeuSæ²¡æœ‰ä¸NeRFä¸€æ ·ç›´æ¥ä½¿ç”¨MLPè¾“å‡ºçš„ä¸é€æ˜åº¦$\sigma$ä½œä¸º$\rho$ï¼Œè€Œæ˜¯ä½¿ç”¨é¢„æµ‹çš„sdfè¿›è¡Œç›¸åº”è®¡ç®—å¾—åˆ°$\rho$ï¼Œä»¥åŠæƒé‡
- $C(\mathbf{o},\mathbf{v})=\int_{0}^{+\infty}w(t)c(\mathbf{p}(t),\mathbf{v})\mathrm{d}t$  $\omega(t)=T(t)\rho(t),\text{where}T(t)=\exp\left(-\int_0^t\rho(u)\mathrm{d}u\right)$
- $\rho(t)=\max\left(\frac{-\frac{\mathrm{d}\Phi_s}{\mathrm{d}t}(f(\mathbf{p}(t)))}{\Phi_s(f(\mathbf{p}(t)))},0\right)$ , MLPé¢„æµ‹çš„sdfå³$f(\mathbf{p}(t))$ 
- $\phi_s(x) =\frac{se^{-sx}}{(1+e^{-sx})^{2}}$, $\Phi_s(x)=(1+e^{-sx})^{-1},\text{i.e.,}\phi_s(x)=\Phi_s'(x)$

ç¦»æ•£åŒ–ï¼š
- $\hat{C}=\sum_{i=1}^nT_i\alpha_ic_i,$ $\alpha_i=1-\exp\left(-\int_{t_i}^{t_{i+1}}\rho(t)\mathrm{d}t\right),$ $T_i=\prod_{j=1}^{i-1}(1-\alpha_j)$
- $\alpha_i=\max\left(\frac{\Phi_s(f(\mathbf{p}(t_i)))-\Phi_s(f(\mathbf{p}(t_{i+1})))}{\Phi_s(f(\mathbf{p}(t_i)))},0\right).$
- $\alpha_{i}=max()$

é™¤äº†$\mathcal{L}1$å’Œ$\mathcal{L}_{mask}$æŸå¤±ä¹‹å¤–è¿˜ä½¿ç”¨äº†$\mathcal{L}_{r e g}=\frac{1}{n m}\sum_{k,i}(\|\nabla f(\hat{\mathbf{p}}_{k,i})\|_{2}-1)^{2}.$ (Eikonal term)
- where m is batch size(ray scalar), n is the point sampling size



### NeuRodin

> [NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface Reconstruction](https://open3dvlab.github.io/NeuRodin/)

å®¤å†…å¤–å¤§åœºæ™¯ï¼Œä¹‹å‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼š
- è¿‡åº¦å‡ ä½•æ­£åˆ™åŒ–a); 
- æ²¡æœ‰å¯¹å‡ ä½•æ‹“æ‰‘çº¦æŸb); 
- æœ¬æ–‡Two-stageçš„æƒ³æ³•ï¼šé¦–å…ˆå¯¹SDFä¸è¿›è¡Œçº¦æŸ(ä¸ç”¨$\mathcal{L}_{eik}$ï¼Œç±»ä¼¼densityè¿›è¡Œè®­ç»ƒ)ï¼Œç„¶åä½¿ç”¨å‡ ä½•æ­£åˆ™åŒ–æ¥refineå…‰æ»‘è¡¨é¢

![image.png|666](https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240902141345.png)


å…ˆå‰ SDF-based volume rendering æ–¹æ³•çš„ä¸è¶³ï¼š
- SDFåˆ°å¯†åº¦è½¬æ¢çš„ä¸åˆé€‚å‡è®¾ï¼š$\sigma(\mathbf{r}(t))=\Psi_s(f(\mathbf{r}(t)))=\begin{cases}\frac{1}{2s}\exp\left(\frac{-f(\mathbf{r}(t))}{s}\right)&\text{if }f(\mathbf{r}(t))\geq0,\\\frac{1}{s}\left(1-\frac{1}{2}\exp\left(\frac{f(\mathbf{r}(t))}{s}\right)\right)&\text{if }f(\mathbf{r}(t))<0.\end{cases}$
  - SDFå€¼ç›¸åŒçš„åŒºåŸŸå¯†åº¦å€¼ä¹Ÿæ˜¯ç›¸åŒçš„ï¼Œé™åˆ¶äº†å¯†åº¦åœº(derived from SDF)çš„è¡¨è¾¾èƒ½åŠ›ã€‚
  - åŸå…ˆçš„å¯†åº¦åœºæ–¹æ³•(NeRF)çš„å¯†åº¦å€¼èŒƒå›´å¯ä»¥æ˜¯$[0,+\infty]$ï¼Œè€ŒSDFè®¡ç®—å¾—åˆ°çš„å¯†åº¦èŒƒå›´åœ¨$\left(0,\frac{1}{s} \right]$ [Function Desmos](https://www.desmos.com/calculator/u8gnwtp7jf?lang=zh-CN)
- å¯†åº¦åå·®(SDF to Densityè¿‡ç¨‹ä¸­)ï¼Œè™½æœ‰å¾ˆå¤šæ”¹è¿›ä½†æ˜¯åå·®ä»å­˜åœ¨ï¼Œ**ä¸”å‡ ä½•æ­£åˆ™åŒ–ä¼šäº§ç”Ÿä¸€äº›ä¸å¥½å½±å“** (exacerbates this bias, complicating model convergence and resulting in the creation of inaccurate surfaces)ï¼Œä¸€äº›æ”¹è¿›çš„æ–¹æ³•ï¼š
  - [TUVR](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Towards_Unbiased_Volume_Rendering_of_Neural_Implicit_Surfaces_With_Geometry_CVPR_2023_paper.pdf): Towards Unbiased Volume Rendering of Neural Implicit Surfaces with Geometry Priors
  - Debsdf: Delving into the details and bias of neural indoor scene reconstruction
  - Recovering fine details for neural implicit surface reconstruction.
- å‡ ä½•è¿‡åº¦æ­£åˆ™åŒ–ï¼Œsuch as Eikonal loss or smoothness constraintsï¼Œ**å¯¼è‡´ç¼ºé™·ï¼š**
  - åœ¨æ‰€æœ‰åŒºåŸŸè¿‡åº¦å…‰æ»‘, both flat and intricate, leading to a loss of fine details)ã€‚
  - å½“ä¼˜åŒ–Normaläº§ç”Ÿçš„é¢œè‰²å’Œé€šè¿‡å‡ ä½•æ­£åˆ™åŒ–æ˜¾å¼åœ°çº¦æŸSDFæ—¶ï¼Œä¼˜åŒ–è¿‡ç¨‹ä¼šé˜»ç¢æ‹“æ‰‘ç»“æ„çš„äº§ç”Ÿã€‚

æœ¬æ–‡è§£å†³æ–¹æ³•ï¼š
- **Uniform SDF, Diverse Densities** 
  - ç©ºé—´ä¸­æ¯ä¸€ç‚¹éƒ½æœ‰ç‹¬è‡ªçš„ç¼©æ”¾å› å­ï¼šä½¿ç”¨éçº¿æ€§æ˜ å°„æ¥æ ¹æ®ä¸‰ç»´ç©ºé—´ä¸­ä¸€ç‚¹åæ ‡è·å–ç‹¬ä¸€æ— äºŒçš„ç¼©æ”¾å› å­s (local scale $s(t)$)  (ç±»ä¼¼Adaptive shells for efficient neural radiance field rendering.çš„å·¥ä½œï¼Œéœ€è¦ç»“åˆSDFå’Œdensityçš„ç‰¹æ€§)
  - $(f(\mathbf{r}(t)),s(\mathbf{r}(t)),\mathbf{z}(\mathbf{r}(t)))=\phi_\text{geo}(\mathbf{r}(t)),\quad\sigma(\mathbf{r}(t))=\Psi_{s(\mathbf{r}(t))}\left(f(\mathbf{r}(t))\right).$ è§£é‡Šï¼š**(SDF, s, å‡ ä½•ç‰¹å¾)=éçº¿æ€§æ˜ å°„(ç‚¹åæ ‡)** and **å¯†åº¦=å‡½æ•°(SDF, s)**
- **Explicit Bias Correction** 
  - å­˜åœ¨çš„åå·® ![image.png|333](https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240902132257.png)
    - A: maximum probability distance$\hat{D}_{\text{prob}}(\mathbf{r})=\arg\max_{t\in(0,+\infty)}w(t)=\arg\max_{t\in(0,+\infty)}T(t)\sigma(\mathbf{r}(t)).$
    - B: rendered distance $\hat{D}_{\text{rendered}}(\mathbf{r})=\int_0^{+\infty}T(t)\sigma(\mathbf{r}(t))t \mathrm{d}t.$ ç›¸å½“äºå¯¹æƒé‡æ±‚äº†å‡å€¼
    - C: SDF zero level set
  - æœ¬æ–‡è§£å†³æ–¹æ³•:  $$\mathcal{L}_{\mathrm{bias}}=\frac1m\sum_{\mathbf{r}\in\mathcal{R}}\max\left(f(\mathbf{r}(t^*+\epsilon_{\mathrm{bias}})),0\right),\quad t^{*}=\arg\max_{t\in(0,+\infty)}T(t)\sigma(\mathbf{r}(t))$$ 
    - é€šè¿‡çº¦æŸæ¯æ¡å…‰çº¿ä¸ŠA($t^*$ with bias correction factor $\epsilon_{\mathrm{bias}}$)ä¸Cçš„å·®å¼‚ï¼Œä¸”ä»…çº¦æŸsdfä¸ºæ­£(å³æ¨¡å‹å¤–éƒ¨)çš„éƒ¨åˆ† 
    - ***ä¸ºä»€ä¹ˆä¸çº¦æŸæ¨¡å‹å†…éƒ¨å‘¢ï¼Ÿï¼šé¼“åŠ±SDFåœ¨Aä½ç½®ä¹‹åå–è´Ÿå€¼ï¼Œç»éªŒæµ‹è¯•å‡ºæ¥çš„(é™„å½•C)ï¼Œä¸”æä¾›äº†[æ•°å­¦è§£é‡Š](https://www.desmos.com/calculator/k1jklfvd5y?lang=zh-CN):*** å½“Aåœ¨Cä¹‹å‰æ—¶ï¼Œéšç€$\theta$çš„å˜åŒ–ACä¹‹é—´å·®å¼‚å˜åŒ–çš„æ›´å¤§ï¼›å½“Aåœ¨Cä¹‹åæ—¶ï¼Œéšç€$\theta$çš„å˜åŒ–ACä¹‹é—´å·®å¼‚å˜åŒ–è¾ƒå° 
    - $\epsilon_{\mathrm{bias}}$ æ˜¯ç”±äºé€‰å–maximumçš„æ–¹æ³•å¯¼è‡´çš„ï¼šç›´æ¥ä½¿ç”¨é‡‡æ ·ç‚¹çš„æœ€å¤§æƒé‡æ¥è¿‘ä¼¼$t^*$
- **Two-Stage Optimization to Tackle Geometry Over-Regularization**
  - **Stage 1**â€”â€”Geometry Over-Regularization (estimated gradients + local scale s(VolSDF SDF2Density) + $\mathcal{L}_{\mathrm{bias}}$)
    - ç›´è§‚è§£æ³•ï¼šæ¶ˆé™¤å‡ ä½•çº¦æŸæˆ–è€…é™ä½æƒé‡ï¼Œä¸”é¿å…conditioné¢œè‰²(predicted normal)ã€‚ ä½†æ˜¯ä¼šäº§ç”Ÿéè‡ªç„¶çš„SDF zero-level set
    - ç®€å•é«˜æ•ˆçš„è§£æ³•æ˜¯ï¼šä¸ç›´æ¥ä½¿ç”¨æ¢¯åº¦${\nabla f(\mathbf{r}(t))}$è¿›è¡Œå‡ ä½•æ­£åˆ™ï¼Œè€Œæ˜¯ä½¿ç”¨ä¼°è®¡æ¢¯åº¦$\hat{\nabla}f(\mathbf{r}(t))$é€šè¿‡ç‰¹æ®Šè®¾è®¡æ¥å¼•å…¥ä¸ç¡®å®šæ€§ï¼Œ
      - xåˆ†é‡çš„ä¼°è®¡æ¢¯åº¦ä¸ºï¼š$\hat{\nabla}_xf(\mathbf{r}(t))=\frac{f\left(\mathbf{r}(t)+\boldsymbol{\epsilon}_x\right)-f\left(\mathbf{r}(t)-\boldsymbol{\epsilon}_x\right)}{2\epsilon},\quad\text{where }\epsilon_x=(\epsilon,0,0)\text{ and }\epsilon\sim U(0,\epsilon_{\max}).$
      - é€šè¿‡æœ‰é™å·®åˆ†æ³•ä¼°è®¡æ¢¯åº¦çš„step size $\epsilon$æ˜¯ä¸€ä¸ªéšæœºé‡‡æ ·çš„æ•°ï¼Œè¿™æ ·åœ¨æ›´å¤§çš„åœºæ™¯çš„estimated normalä¸­æœ‰å¾ˆå°çš„varianceï¼Œåœ¨fine detailsçš„normalä¸­æœ‰æ›´å¤§çš„varianceã€‚**è¿™æ ·çš„ä¸ç¡®å®šæ€§ç¡®ä¿äº†æ›´å¤§ç‰¹å¾çš„ç¨³å®šæ€§å’Œå¤æ‚ç»†èŠ‚çš„çµæ´»æ€§**
    - æ€»ç»“:
      - $\mathcal{L}_{\mathrm{coarse}}=\mathcal{L}_{\mathrm{color}}+\lambda_{\mathrm{eik}}\mathcal{L}_{\mathrm{eik}}(\hat{\nabla}f)+\lambda_{\mathrm{bias}}\mathcal{L}_{\mathrm{bias}}.$
      - VolSDFçš„SDF-to-densityæ–¹æ³•ï¼š
        - $\sigma(\mathbf{r}(t))=\Psi_s(f(\mathbf{r}(t)))=\begin{cases}\frac{1}{2s}\exp\left(\frac{-f(\mathbf{r}(t))}{s}\right)&\text{if }f(\mathbf{r}(t))\geq0,\\\frac{1}{s}\left(1-\frac{1}{2}\exp\left(\frac{f(\mathbf{r}(t))}{s}\right)\right)&\text{if }f(\mathbf{r}(t))<0.\end{cases}$
  - **Stage 2**â€”â€”Refinement (estimated gradients + TUVR SDF2Density + $\mathcal{L}_{\mathrm{smooth}}$)
    - ä½¿ç”¨PermutoSDFçš„æŸå¤±$\mathcal{L}_{\mathrm{smooth}}=\frac1{mn}\sum_{\mathbf{r},t}\left(\mathbf{n}\left(\mathbf{r}(t)\right)\cdot\mathbf{n}\left(\mathbf{r}(t)+\epsilon_s\boldsymbol{\eta}(\mathbf{r}(t))\right)-1\right)^2,$ æ¥å¢åŠ å±€éƒ¨å…‰æ»‘åº¦ï¼Œ$\boldsymbol{\eta}(\mathbf{r}(t)) = \mathbf{n}(\mathbf{r}(t)) \times \boldsymbol{\tau}$ å…¶ä¸­$\tau$æ˜¯éšæœºå•ä½å‘é‡
    - æ€»ç»“:
      - é‡‡ç”¨TUVRçš„ SDF-to-densityæ–¹æ³•ï¼Œä¿è¯æœ€å°åŒ–biasä¸”ä¿å­˜fineçš„ç‰©ä½“ç»†èŠ‚
        - $\sigma(t)=\begin{cases}\frac{1}{s(t)}\exp\left(\frac{-f(t)}{s(t)|f'(t)|}\right)&\text{if}f(t)\geq0,\\\frac{2}{s(t)}\left(1-\frac{1}{2}\exp\left(\frac{f(t)}{s(t)|f'(t)|}\right)\right)&\text{if}f(t)<0.\end{cases}$
      - $\mathcal{L}_{\mathrm{fine}}=\mathcal{L}_{\mathrm{color}}+\lambda_{\mathrm{eik}}\mathcal{L}_{\mathrm{eik}}(\nabla f)+\lambda_{\mathrm{smooth}}\mathcal{L}_{\mathrm{smooth}}.$


é™„å½•çš„ç†è®ºåˆ†æ:
1. æ ¹æ®æƒé‡æœ€å¤§çš„ç‚¹$t^*=\arg\max_{t\in(0,+\infty)}T(t)\sigma(\mathbf{r}(t))$
2. åœ¨è¯¥ç‚¹åº”è¯¥æ»¡è¶³$\frac{\partial w(t)}{\partial t}\Bigg|_{t=t^*}=0$
3. å¯ä»¥æ¨å¯¼å‡º$$\sigma^2(\mathbf{r}(t^*))=\left.\frac{\partial\sigma(\mathbf{r}(t))}{\partial t}\right|_{t=t^*}$$

å› æ­¤æ„å»ºçš„SDF-to-densityå‡½æ•°å¿…é¡»æ»¡è¶³ï¼š
- æ¡ä»¶1 $$\sigma^2(\mathbf{r}(t^*))=\left.\frac{\partial\sigma(\mathbf{r}(t))}{\partial t}\right|_{t=t^*}$$
- æ¡ä»¶2 (SDF=0çš„ç‚¹ï¼ŒåŒæ—¶æƒé‡æœ€å¤§) $$f(r(t^0))=0$$
ç„¶è€Œï¼š
- NeuSåªæœ‰åœ¨æ²¿ç€å…‰çº¿çš„SDFåˆ†å¸ƒçš„ä¸€é˜¶è¿‘ä¼¼æ¡ä»¶ä¸‹æ‰æ»¡è¶³æ­¤æ¡ä»¶
- TUVRæ‰©å±•åˆ°äº†ä»»æ„åˆ†å¸ƒï¼Œä½†æ˜¯ä»æœ‰é—®é¢˜å°±æ˜¯ä¸ä¸€å®š$t^{0} \neq t^{*}$ï¼Œ(åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæ²¿ç€å…‰çº¿çš„æƒé‡åˆ†å¸ƒæ˜¯ä¸€ä¸ªå¤æ‚çš„non-convexå‡½æ•°ï¼Œåªèƒ½æ‹…ä¿$t^{0}$æ˜¯åœ¨å±€éƒ¨æœ€å¤§å€¼ä¸Š)



### ReTR

> [ReTR: Modeling Rendering Via Transformer for Generalizable Neural Surface Reconstruction](https://arxiv.org/pdf/2305.18832)

**ä½¿ç”¨Transformer ä»£æ›¿æ¸²æŸ“è¿‡ç¨‹ï¼Œå¹¶ä¸”æ·»åŠ äº†æ·±åº¦ç›‘ç£**

![image.png|666](https://raw.githubusercontent.com/qiyun71/Blog_images/main/pictures20231109094904.png)


## Mixture of Experts (MoE)

### Boost Your NeRF

> [Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High Quality and Efficient Rendering](https://arxiv.org/pdf/2407.10389#page=20.42)

> [æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰è¯¦è§£](https://huggingface.co/blog/zh/moe#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B) MoE å±‚ç”±ä¸¤ä¸ªæ ¸å¿ƒéƒ¨åˆ†ç»„æˆ: ä¸€ä¸ªé—¨æ§ç½‘ç»œ(ç”¨äºå†³å®šå“ªäº›ä»¤ç‰Œ (token) è¢«å‘é€åˆ°å“ªä¸ªä¸“å®¶)å’Œè‹¥å¹²æ•°é‡çš„ä¸“å®¶(æ¯ä¸ªä¸“å®¶æœ¬èº«æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ç¥ç»ç½‘ç»œ)

- é—¨æ§ç½‘ç»œæ¥å†³å®šé‡‡æ ·ç‚¹è¢«è¾“å…¥å“ª Top-Kä¸ªä¸“å®¶ç½‘ç»œ(å¹¶ä¸”åœ¨filtering stepæŠ›å¼ƒlow-densityçš„ç‚¹ï¼Œå¯†åº¦å€¼æ ¹æ®lowest resolution model è¿›è¡Œè®¡ç®—)
- æ¯ä¸ªä¸“å®¶ç½‘ç»œä»¥é‡‡æ ·ç‚¹ä½ç½®å’Œæ–¹å‘ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºé¢œè‰²å’Œå¯†åº¦
- æ ¹æ®è¯¥ç‚¹åˆ°æ¯ä¸ªä¸“å®¶ç½‘ç»œçš„æƒé‡(æ¦‚ç‡Probability Field)å’Œé¢œè‰²å¯†åº¦å€¼ï¼Œè®¡ç®—è¯¥ç‚¹æœ€ç»ˆçš„é¢œè‰²å’Œå¯†åº¦å€¼ã€‚æœ€åé€šè¿‡ä½“æ¸²æŸ“å¾—åˆ°pixel colorï¼Œå¹¶è”åˆä¼˜åŒ–resolution-weighted auxiliary loss(ç”¨äºé€‰å–ä¸“å®¶ç½‘ç»œ)

ä¸»è¦æ€æƒ³æ˜¯åœ¨è®­ç»ƒäº†ä¸€æ‰¹ä¸åŒåˆ†è¾¨ç‡çš„NeRF modelsåï¼Œä¼˜å…ˆä½¿ç”¨low-resolution modelsï¼Œå‡å°‘high-resolution models çš„ä½¿ç”¨

![image.png|666](https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240806202919.png)


# Efficiency

1. å®Œå…¨æŠ›å¼ƒMLPï¼Œå­˜å‚¨æ˜¾ç¤ºçš„é¢œè‰²/å¯†åº¦
2. åŠ é€Ÿæ¸²æŸ“ï¼šå¯¹å‡ ä½•ä»£ç†è¿›è¡Œå…‰æ …åŒ–å¤„ç†
3. åŠ é€Ÿæ¸²æŸ“ï¼šä½¿ç”¨å‰ä¸€ä¸ªè§†å›¾çš„ä¿¡æ¯æ¥å‡å°‘æ¸²æŸ“åƒç´ çš„æ•°é‡

## Explicit Grids

**Explicit Grids with features(Efficiency of T&R)**


**(å‡è½»MLPæ¶æ„)**

| Explicit Grids     | Related Works                                                                                                                                  |
| ------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| Feature grids      | [Plenoxels](https://arxiv.org/pdf/2112.05131), [DirectVoxGo](https://arxiv.org/pdf/2111.11215), [InstantNGP](https://arxiv.org/pdf/2201.05989) |
| Tri-planes         | [Tri-MipRF](https://arxiv.org/pdf/2307.11335)                                                                                                  |
| Multi-plane images | [MMPI](https://arxiv.org/pdf/2310.00249), [fMPI](https://arxiv.org/pdf/2312.16109)                                                             |
| Tensorial vectors  | [TensoRF](https://arxiv.org/pdf/2203.09517), [Strivec](https://arxiv.org/pdf/2307.13226)                                                       |

### NGP-RT

> [NGP-RT: Fusing Multi-Level Hash Features with Lightweight Attention for Real-Time Novel View Synthesis](https://arxiv.org/pdf/2407.10482)

InstantNGP è™½ç„¶è®­ç»ƒé€Ÿåº¦(æŸ¥è¯¢ç½‘æ ¼)å¾ˆå¿«ï¼Œä½†æ˜¯æ¸²æŸ“çš„æ—¶å€™ä»ç„¶éœ€è¦å¤§é‡çš„3D PointæŸ¥è¯¢MLPï¼Œè€—è´¹å¤§é‡æ—¶é—´

(*Deferred neural rendering*) SNeRG [12] å’Œ MERF [32] é€šè¿‡å°†é¢œè‰²å’Œå¯†åº¦å­˜åœ¨æ˜¾ç¤ºç½‘æ ¼ä¸­ï¼Œ**åªå¯¹æ¯æ¡æŠ•å°„å…‰çº¿æ‰§è¡Œä¸€æ¬¡ MLP**ï¼Œä»è€Œå¤§å¤§åŠ å¿«äº†æ¸²æŸ“è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œä»–ä»¬å¯¹æ˜¾å¼ç‰¹å¾çš„å¤„ç†æ˜¾ç¤ºå‡ºæœ‰é™çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä¸é€‚åˆ Instant-NGP çš„å¤šçº§ç‰¹å¾ã€‚**å°†è¿™äº›ç‰¹å¾æ„å»ºæ–¹æ³•ç›´æ¥åº”ç”¨äº Instant-NGP ä¸­çš„å¤šçº§ç‰¹å¾å¯èƒ½ä¼šå¯¼è‡´å…¶è¡¨ç¤ºèƒ½åŠ›å—æŸï¼Œå¹¶å¯¼è‡´æ¸²æŸ“è´¨é‡ä¸‹é™**ã€‚

![image.png|666](https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240718142648.png)

æå‡ºäº† NGP-RTï¼Œä¸€ç§åˆ©ç”¨è½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶é«˜æ•ˆæ¸²æŸ“é«˜ä¿çœŸæ–°è§†å›¾çš„æ–°æ–¹æ³•ã€‚NGP-RT çš„æ³¨æ„åŠ›æœºåˆ¶é‡‡ç”¨äº†ç®€å•è€Œæœ‰æ•ˆçš„åŠ æƒå’Œè¿ç®—ï¼Œå¯å­¦ä¹ çš„æ³¨æ„åŠ›å‚æ•°å¯è‡ªé€‚åº”åœ°ä¼˜å…ˆå¤„ç†æ˜¾å¼å¤šçº§å“ˆå¸Œç‰¹å¾

![image.png|666](https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240718142838.png)


## MIMO MLP

**Multi input and Multi output MLP(Efficiency of T&R)**

**(å‡å°‘MLPè®¡ç®—æ¬¡æ•°)**

### MIMO-NeRF

> [MIMO-NeRF: Fast Neural Rendering with Multi-input Multi-output Neural Radiance Fields](https://arxiv.org/pdf/2310.01821)

å¤šè¾“å…¥å¤šè¾“å‡ºçš„MLPï¼ŒåŒæ—¶è¾“å…¥å¤šä¸ªä¸‰ç»´ç‚¹æ¥è¿›è¡Œè®¡ç®—


## Points Sampling

**Points Sampling(Efficiency of T&R)**

**(å‡å°‘é‡‡æ ·ç‚¹æ•°é‡ per ray)**

### [HashPoint](https://jiahao-ma.github.io/hashpoint/)

Primary surface point sampling

![image.png|666](https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240823163840.png)



## Pixel Sampling

**Pixel Sampling (Efficiency of Training)**

**(å‡å°‘MLPè®¡ç®—æ¬¡æ•°)**

ä¹‹å‰æ–¹æ³•å¯¹train_dataä¸­æ‰€æœ‰çš„åƒç´ rgbä¸‰ä¸ªå€¼ï¼Œè¿›è¡Œé¢„æµ‹+l1 loss+åå‘ä¼ æ’­ï¼Œè®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

é€šè¿‡é‡åŒ–rendering image ä¸ g.t. image ä¹‹é—´çš„å·®å¼‚ï¼Œæ¥æŒ‡å¯¼åœ¨å›¾åƒä¸Šé‡‡æ ·åƒç´ çš„ä½ç½®/æ•°é‡ï¼šæ ¹æ®error map between rendering image and g.t. imageï¼Œæ¶ˆé™¤lossæ¯”è¾ƒå°çš„åŒºåŸŸï¼Œå¯¹losså¤§çš„åŒºåŸŸè¿›è¡Œæ›´å¤šçš„é‡‡æ ·

**idea**ï¼š
- Other sampling method å¦‚æœè¦åŠ é€Ÿè®­ç»ƒçš„è¯ï¼Œå°±è¦ä½¿ç”¨æ›´é«˜æ•ˆçš„é‡‡æ ·æ–¹æ³•
  - LHS (Latin hypercube sampling)
- è¿™ç§æ–¹æ³•åªèƒ½å¯¹Trainè¿‡ç¨‹è¿›è¡ŒåŠ é€Ÿ
- ç”±äºä¼˜åŒ–çš„æ˜¯MLPæ•´ä½“çš„å‚æ•°ï¼Œå› æ­¤å¯èƒ½å‡ºç°å¯¹è¯¯å·®å¤§çš„åƒç´ ä¼˜åŒ–æ—¶ï¼Œé™ä½è¯¯å·®å°çš„åƒç´ çš„é¢„æµ‹ç²¾åº¦ã€‚
- å¯èƒ½ä¼šå¯¹errorå¤§ä½†æ˜¯éé‡è¦åŒºåŸŸçš„åƒç´ è¿›è¡Œå¤šæ¬¡é‡‡æ ·

> [é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—ç®—æ³•ï¼ˆMCMCï¼‰ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/37121528) Basic
> [è¯¦è§£Markov Chain Monte Carlo (MCMC): ä»æ‹’ç»-æ¥å—é‡‡æ ·åˆ°Gibbs Sampling | Lemon's Blog](https://coderlemon17.github.io/posts/2022/05-11-mcmc/) **æ›´æ¸…æ¥šä¸€ç‚¹**
> [MCMCè¯¦è§£2â€”â€”MCMCé‡‡æ ·ã€M-Hé‡‡æ ·ã€Gibbsé‡‡æ ·ï¼ˆé™„ä»£ç ï¼‰](https://blog.csdn.net/u012290039/article/details/105696097)
> [é‡‡æ ·ï¼ˆä¸‰ï¼‰ï¼šé‡è¦æ€§é‡‡æ ·ä¸æ¥å—æ‹’ç»é‡‡æ · | Erwin Feng Blog](https://allenwind.github.io/blog/10466/)
> [The Markov-chain Monte Carlo Interactive Gallery](https://chi-feng.github.io/mcmc-demo/) **å¤šç§MCMCé‡‡æ ·æ–¹æ³•**

Monte Carloé‡‡æ ·æ— æ³•å¾—åˆ°å¤æ‚çš„åˆ†å¸ƒ(äºŒç»´åˆ†å¸ƒ)ï¼ŒåŠ å…¥Markov Chainï¼Œé©¬å°”ç§‘å¤«é“¾æ¨¡å‹çš„çŠ¶æ€è½¬ç§»çŸ©é˜µæ”¶æ•›åˆ°çš„ç¨³å®šæ¦‚ç‡åˆ†å¸ƒä¸æˆ‘ä»¬çš„åˆå§‹çŠ¶æ€æ¦‚ç‡åˆ†å¸ƒæ— å…³$\pi(j)=\sum_{i=0}^\infty\pi(i)P_{ij}$(åªä¸çŠ¶æ€è½¬ç§»çŸ©é˜µæœ‰å…³)ï¼Œå¦‚æœå¯ä»¥å¾—åˆ°çŠ¶æ€è½¬ç§»çŸ©é˜µï¼Œå°±å¯ä»¥é‡‡æ ·å¾—åˆ°å¹³ç¨³åˆ†å¸ƒçš„æ ·æœ¬é›†ã€‚å¦‚ä½•å¾—åˆ°çŠ¶æ€è½¬ç§»çŸ©é˜µï¼Ÿ
--> MCMCæ–¹æ³•(ä¸æ‹’ç»-æ¥å—é‡‡æ ·çš„æ€è·¯ç±»ä¼¼ï¼Œå…¶é€šè¿‡æ‹’ç»-æ¥å—æ¦‚ç‡æ‹Ÿåˆä¸€ä¸ªå¤æ‚åˆ†å¸ƒ, MCMCæ–¹æ³•åˆ™é€šè¿‡æ‹’ç»-æ¥å—æ¦‚ç‡å¾—åˆ°ä¸€ä¸ªæ»¡è¶³ç»†è‡´å¹³ç¨³æ¡ä»¶çš„è½¬ç§»çŸ©é˜µ.)
- Metropolis-Hastings Samplingï¼šéœ€è¦è®¡ç®—æ¥å—ç‡, åœ¨é«˜ç»´æ—¶è®¡ç®—é‡å¤§, å¹¶ä¸”ç”±äºæ¥å—ç‡çš„åŸå› å¯¼è‡´ç®—æ³•æ”¶æ•›æ—¶é—´å˜é•¿. å¯¹äºé«˜ç»´æ•°æ®, å¾€å¾€æ•°æ®çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæ˜“å¾—, è€Œè”åˆæ¦‚ç‡åˆ†å¸ƒä¸æ˜“å¾—.
- Gibbs Samplingï¼š

### Soft Mining

> [Accelerating Neural Field Training via Soft Mining](https://arxiv.org/pdf/2312.00075)

> [EGRA-NeRF: Edge-Guided Ray Allocation for Neural Radiance Fields](https://www.sciencedirect.com/science/article/pii/S0262885623000446) NeRF çš„æ¸²æŸ“æ˜¾å¾—è¿‡äºæ¨¡ç³Šï¼Œå¹¶ä¸”åœ¨æŸäº›çº¹ç†æˆ–è¾¹ç¼˜ä¸­åŒ…å«é”¯é½¿ä¼ªå½±ï¼Œä¸ºæ­¤æå‡ºäº†è¾¹ç¼˜å¼•å¯¼å…‰çº¿åˆ†é…ï¼ˆEGRA-NeRFï¼‰æ¨¡å—ï¼Œä»¥**åœ¨è®­ç»ƒé˜¶æ®µå°†æ›´å¤šå…‰çº¿é›†ä¸­åœ¨åœºæ™¯çš„çº¹ç†å’Œè¾¹ç¼˜ä¸Š** **(æ²¡æœ‰åŠ é€Ÿ)**

To implement our idea we use Langevin Monte-Carlo sampling. We show that by doing so, regions with higher error are being selected more frequently, leading to more than 2x improvement in convergence speed.
$\mathcal{L}=\frac{1}{N}\sum_{n=1}^{N}\mathrm{err}(\mathbf{x}_{n})\approx\mathbb{E}_{\mathbf{x}\sim P(\mathbf{x})}\left[\mathrm{err}(\mathbf{x})\right]=\int\mathrm{err}(\mathbf{x})P(\mathbf{x})d\mathbf{x}$. P is the distribution of the sampled data points $x_{n}$. ä¹‹å‰æ–¹æ³•å¤§å¤šæ˜¯å‡åŒ€åˆ†å¸ƒ
å…·ä½“åšæ³•ï¼š
1. Soft mining with **importance sampling**. å¼•å…¥äº† importance distribution$Q(x)$

> è¡¥å……çŸ¥è¯† [[è’™ç‰¹å¡æ´›æ–¹æ³•] 02 é‡è¦æ€§é‡‡æ ·ï¼ˆimportance samplingï¼‰åŠ python å®ç°_å“”å“©å“”å“©_bilibili](https://www.bilibili.com/video/BV1SV4y1i7bW/?spm_id_from=333.788&vd_source=1dba7493016a36a32b27a14ed2891088) å¯ä»¥ä»ä¸€ä¸ªä»»æ„åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·

è¯¯å·®å¯ä»¥é‡å†™ä¸ºï¼š$\int\operatorname{err}(\mathbf{x})P(\mathbf{x})d\mathbf{x} =\int\frac{\mathrm{err}(\mathbf{x})P(\mathbf{x})}{Q(\mathbf{x})}Q(\mathbf{x})d\mathbf{x}  =\mathbb{E}_{\mathbf{x}\sim Q(\mathbf{x})}\left[\frac{\mathrm{err}(\mathbf{x})P(\mathbf{x})}{Q(\mathbf{x})}\right].$ ç”±äºå‡åŒ€åˆ†å¸ƒ$P(x)$çš„PDFé€šå¸¸ä¸ºå¸¸æ•°ï¼Œå› æ­¤$\mathcal{L}=\frac{1}{N}\sum_{n=1}^{N}\frac{\mathrm{err}(\mathbf{x}_{n})}{Q(\mathbf{x}_{n})}$ $\mathrm{where} \quad\mathbf{x}_{n}\sim Q(\mathbf{x}).$
ä½†æ˜¯$\mathcal{L}$æ— æ³•ç”¨äºè®­ç»ƒï¼Œé‡‡ç”¨[stop gradient operator](https://arxiv.org/pdf/1711.00937)(åœ¨æ­£å‘è®¡ç®—æ—¶å®šä¹‰ä¸ºåŒä¸€å€¼ï¼Œä¸”åå¯¼æ•°ä¸ºé›¶)
$\mathcal{L}\approx\mathbb{E}_{\mathbf{x}\sim\mathrm{sg}(Q(\mathbf{x}))}\left[\frac{\mathrm{err}(\mathbf{x})}{\mathrm{sg}(Q(\mathbf{x}))}\right].$
é€‰æ‹©çš„$Q(x)$å¿…é¡»ä¸$err(x)$æˆæ¯”ä¾‹å…³ç³»(æ¶ˆé™¤errorå°ºåº¦çš„å½±å“)ï¼š$\mathrm{err}(\mathbf{x})=\|f_{\boldsymbol{\psi}}(\mathbf{x})-f_{\mathrm{gt}}(\mathbf{x})\|_{2}^{2},\\Q(\mathbf{x})=\|f_{\boldsymbol{\psi}}(\mathbf{x})-f_{\mathrm{gt}}(\mathbf{x})\|_{1}.$
**æ ¹æ®**$Q(x)$çš„å®šä¹‰ï¼Œä¼šåœ¨errorå¤§çš„åœ°æ–¹å¤šé‡‡æ ·ä¸€äº›$x$å³åƒç´ ç‚¹
Soft mining. $\mathcal{L}=\frac1N\sum_{n=1}^N\left[\frac{\mathrm{err}(\mathbf{x}_n)}{\mathrm{sg}(Q(\mathbf{x}_n))^\alpha}\right],\quad\text{where }\alpha\in[0,1]$(åœ¨å…³æ³¨errorå¤§çš„åŒºåŸŸçš„åŒæ—¶ä¹Ÿè¦å…³æ³¨ä¸€ä¸‹å…¶ä»–åŒºåŸŸï¼Œä¸ç„¶å¯èƒ½ä¼šå­¦æ­ª)
- $\alpha = 0$ï¼šhard mining
- $\alpha = 1$ï¼š(pure) importance sampling

2. Sampling via **Langevin Monte Carlo**

> [The promises and pitfalls of Stochastic Gradient Langevin Dynamics](https://arxiv.org/pdf/1811.10072) æ•°å­¦åˆ†æLMC, SGLD, SGLDFP and SGD
> [MCMC using Hamiltonian dynamics](https://arxiv.org/pdf/1206.1901) 

ä»ä»»æ„åˆ†å¸ƒ$Q(x)$ä¸­é‡‡æ ·ï¼Œä½¿ç”¨MCMCæ–¹æ³•ä¸­çš„Langevin Monte Carlo (LMC)
$\mathbf{x}_{t+1}=\mathbf{x}_t+a\nabla\log Q(\mathbf{x}_t)+b\boldsymbol{\eta}_{t+1},$
- a>0 is a hyperparameter defining the step size for the gradient-based walks
- b>0 is a hyperparameter defining the step size for the random walk $\boldsymbol{\eta}_{t\boldsymbol{+}1}\boldsymbol{\sim}\mathcal{N}(0,\mathbf{1})$
- é‡‡æ ·æ˜¯å±€éƒ¨çš„ï¼Œå› æ­¤é‡‡æ ·çš„å¼€é”€å¾ˆå°
- logçš„ä½œç”¨åº”è¯¥æ˜¯æŠŠä¹˜é™¤è½¬æ¢æˆåŠ å‡, eg: $w_i=\frac{p(x_i)}{q(x_i)}, \log w_i=\log p(x_i)-\log q(x_i)$

Sample (re-)initialization.(é‡‡æ ·çš„åˆå§‹åŒ–å¾ˆé‡è¦)ï¼šWe first initialize the sampling distribution to be uniform over the domain of interest as $\mathbf{x}_{0}{\sim}\mathcal{U}(\mathcal{R})$. We further re-initialize samples that either move out of $\mathcal{R}$ or have too low error value causing samples to get â€˜stuckâ€™. We use uniform sampling as well as edge-based sampling for 2D workloads.
Warming up soft mining. Start with $\alpha=0$, i.e., no correction, then linearly increase it to the desired $\alpha$ value at 1k iterations.
Alternative: multinomial sampling. To use multinomial sampling, one needs to do a forward pass of all data points to build a probability density function, which is computationally expensive. Hence an alternative strategy, such as those based on Markov Chain Monte Carlo (MCMC) is required. ä¸ºäº†é˜²æ­¢å¯¹æ‰€æœ‰åƒç´ ç‚¹è¿›è¡Œå‰å‘è®¡ç®—ä»¥è®¡ç®—PDFçš„é«˜è€—è´¹ï¼Œä½¿ç”¨MCMCé‡‡æ ·

Ablation studiesä¸­: å³ä½¿LMCé‡‡æ ·çš„ç²¾åº¦æ¯”multinomial samplingä½ä¸€ç‚¹ï¼Œä½†ä»ç„¶æ¯”Uniform samplingæ›´é«˜ï¼Œä¸”æ›´effective(æ•ˆç‡ä¸ç²¾åº¦çš„æŠ˜ä¸­(compromise/trade-off))

![image.png|666](https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240708102314.png)


### Shooting Much Fewer Rays

> [Fast Learning Radiance Fields by Shooting Much Fewer Rays](https://arxiv.org/pdf/2208.06821)

![image.png|888](https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240620160134.png)
ä¹‹å‰çš„ä»å›¾ç‰‡ä¸­é‡‡æ ·åƒç´ orå…‰çº¿çš„æ–¹æ³•ï¼š$\mathbf{r}_i(u,v)\sim U(I_i),u\in[0,H_i],v\in[0,W_i],$ æŒ‰å‡åŒ€åˆ†å¸ƒéšæœºé‡‡æ ·
- trivial areasï¼šå¯¹äºèƒŒæ™¯(å¤§éƒ¨åˆ†é¢œè‰²ç›¸åŒ)ï¼Œè¿™æ ·å›¾ç‰‡çš„åƒç´ å°±æ˜¯éå‡åŒ€çš„åˆ†å¸ƒï¼Œå‡åŒ€é‡‡æ ·æ–¹å¼ä¼šå¯¼è‡´é‡‡æ ·åˆ°ä¸€äº›æ— æ„ä¹‰çš„åƒç´ ã€‚ **Therefore**, we only need to shoot fewer rays in the trivial areas where the color changes slightly to perceive the radiance fields
- nontrivial areasï¼šcolor changes greatly contain more information, so more rays are required to capture the detailed information and learn how to distinguish these pixelsâ€™ colors from its neighboring ones

trivial areasä¼šå¾ˆå¿«æ”¶æ•›ï¼Œè€Œnontrivial areasä¸å®¹æ˜“æ”¶æ•›
Based on the above observation, we propose two strategies to optimize ray distribution on input images. 
- The first one is to calculate a **prior probability distribution based on the image context** GTå›¾ç‰‡çš„å…ˆéªŒåˆ†å¸ƒ
- the second one is to apply an **adaptive quadtree subdivision algorithm** to dynamically adjust ray distribution. è‡ªé€‚åº”çš„QSA

å…·ä½“åšæ³•ï¼š
1. Context based Probability Distributionï¼šwe use the color variation of pixels relative to their surroundings to quantitatively identify the image context.

è®¡ç®—æ¯ä¸ªåƒç´ ç‚¹è·Ÿå‘¨å›´å…«ä¸ªç‚¹çš„stdï¼š$g(u,v)=\operatorname{std}(\mathbf{c}(u,v))=\sqrt{\frac19\sum_{x,y}[\mathbf{c}(x,y)-\overline{\mathbf{c}}]^2},\\x\in\{u-1,u,u+1\},y\in\{v-1,v,v+1\}.$ gè¶Šé«˜è¡¨ç¤ºåƒç´ é¢œè‰²/å¯†åº¦å˜åŒ–è¶Šå‰§çƒˆï¼Œé€šå¸¸åœ¨3Dç‰©ä½“çš„è¡¨é¢è¾¹ç•Œå¤„ã€‚ä¼˜åŠ¿ï¼šour image context based probability distribution function naturally helps to estimate where surfaces are located.
ä¸ºäº†å¹³è¡¡$g_{max}$ä¸$g_{min}$ä¹‹é—´çš„å·®å¼‚ï¼Œclampåè¿›è¡Œå½’ä¸€åŒ–ï¼š$g^{\prime}(u,v)=\frac{\mathrm{clamp}(s,\max(g(u,v)))}{\mathrm{max}(g(u,v))}$ï¼Œwe typically define threshold $\begin{aligned}s=0.01\times\text{mean}(g(u,v))\end{aligned}$. Values less than s will be clamped to s to avoid sampling too few rays at the corresponding positions.
**Sampling strategy**. In the lines of â€œSampled Rays Distributionâ€, we sample 50% rays according to the context based probability distribution and randomly sample the other 50% rays, where each red point represents a sampled ray(**ä¸ºä»€ä¹ˆè¦è®¾ç½®æˆ50%**)
2. Adaptive QuadTree Subdivisionï¼šå¯¹äºrendering errorï¼Œåªåœ¨errorå¤§çš„åœ°æ–¹ç»†åˆ†ï¼Œåœ¨errorå°çš„åœ°æ–¹ä¸åœ¨ç»†åˆ†(æ ¹æ®pre-defined threshold aæ¥åˆ¤æ–­å¤§å°)

![image.png|666](https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240620170649.png)
å›¾ç‰‡$I_{i}(H_{i} \times W_{i})$ä¸Šæ€»çš„é‡‡æ ·æ•°é‡ï¼š$M_i^l=Q_1^l\times\frac{H_i}{2^l}\times\frac{W_i}{2^l}+Q_2^l\times n_0,$ $l$ denote the times of subdivision, $Q_1^l$ and $Q_2^l$ denote the number of unmarked leaf nodes (error>a) and marked leaf nodes (error<a) separately. $n_{0} = 10(constant)$
> [iMAP: Implicit Mapping and Positioning in Real-Time](https://arxiv.org/pdf/2103.12352)

åŒæ—¶[iMap](https://arxiv.org/pdf/2103.12352)ä¹Ÿä½¿ç”¨rendering erroræ¥å¼•å¯¼é‡‡æ ·ï¼Œä¸åŒç‚¹ï¼š
- The applications of rendering loss are different. iMap uses the rendering loss distribution on image blocks to decide how many points should be sampled on each block, while we use the **rendering loss** on each leaf node to decide whether this node should be subdivided into 4 child nodes.
- The number of sampled points and image blocks are different. In our method, the number of sampled points in each leaf node is identical, but the number and area of the image blocks (i.e. leaf nodes) changes during training. In contrast, iMap samples different numbers of rays according to a render loss based distribution in each one of the same size blocks.
- The sampling strategies in image blocks are different. In each image block, iMap uniformly samples points for rendering, while we sample points according to the image context. More points are sampled in the nontrivial areas where color changes a lot, while fewer points are sampled in the trivial areas where color changes slightly. Our sampling strategy helps to capture the detailed information in the nontrivial areas and reduce the training burden in the trivial areas.

3. Implementation Details
- åœ¨æ¯ä¸ªepochç»“æŸåï¼Œä¹Ÿå°±æ˜¯å¯¹æ•°æ®é›†ä¸­æ‰€æœ‰å›¾åƒçš„åƒç´ éƒ½è¿›è¡Œä¸€æ¬¡renderingï¼Œç„¶åä¸g.t.è¿›è¡Œå¯¹æ¯”å¾—åˆ°rendering error
- In practice, we initially subdivide the quadtrees into 2 or 3 depths at the begin of training. This helps our method to distinguish the trivial and nontrivial areas faster among the quadtree leaf nodes

å­˜åœ¨çš„é—®é¢˜ï¼š
- All-Pixel Samplingä¸­ï¼Œä½œè€…ä¸ºäº†é˜²æ­¢MLPå¯¹unmarked leaf nodesè¿›è¡Œæ‹Ÿåˆçš„åŒæ—¶ä¼šæ”¹å˜å·²ç»æ‹Ÿåˆå¥½çš„marked leaf nodesï¼Œåœ¨æ¥è¿‘æœ€åçš„epochï¼Œä½¿ç”¨äº†randomly sample rays from the whole image instead of using quadtrees for sampling, where the number of sampled rays is equal to the total number of pixels. **å¦‚ä½•freezeå·²ç»æ‹Ÿåˆå¥½çš„marked leaf nodes???**

### iNeRF

> [iNeRF: Inverting Neural Radiance Fields for Pose Estimation](https://arxiv.org/pdf/2012.05877)

ä¸€ç§åŸºäºNeRFé¢„è®­ç»ƒæ¨¡å‹çš„å§¿æ€ä¼°è®¡çš„æ–¹æ³•. æœ‰äº†NeRF(MLP), å»refineä½å§¿
- Sampling Rays: è®¡ç®—æ‰€æœ‰pixelæ˜¯éå¸¸è€—æ—¶è€—åŠ›çš„ã€‚ç›®çš„æ˜¯æƒ³è¦é‡‡æ ·çš„ç‚¹å¯ä»¥æ›´å¥½çš„åŒ…å«åœ¨observed imageså’Œrendered imagesä¸Šã€‚ä¸‰ç§ç­–ç•¥ï¼š
  - Random Sampling
  - Interest Point Sampling: employ interest point detectors to localize a set of candidate pixel locations in the observed image, this strategy makes optimization converge faster since less stochasticity is introduced. **But** we found that it is prone to local minima as it only considers interest points on the observed image instead of interest points from both the observed and rendered images.
  - Interest Region Sampling: After the interest point detector localizes the interest points, we apply a 5 Ã— 5 **morphological dilation** for I iterations to enlarge the sampled region.

![image.png|666](https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240620151248.png)


### Expansive Supervision

>[Expansive Supervision for Neural Radiance Field | PDF](https://arxiv.org/pdf/2409.08056)


![image.png|333](https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240928172853.png)

æœ¬æ–‡æ€è·¯ï¼špixels within the same batch must derive from identical input views
- **Strict Sequential order** in imageï¼šthis approach results in a significant decrease in model performance due to the **reduced entropy of the training data** This reduction in entropy negatively impacts the learning performance during each iteration.
- **Permutation algorithm**(maximizes the entropy) å°†ä»åŒä¸€å¼ å›¾ç‰‡ä¸­é‡‡æ ·å¾—åˆ°çš„batchè¿›è¡Œæ‰“ä¹±é¡ºåºï¼š$P^*=\arg\max_PH(P(\mathcal{D}))$ æ‰¾åˆ°ç†µ$H(\cdot)$æœ€å¤§æ—¶çš„ permutation $P^{*}$
  - $\text{s.t.}C:B\cap I=B,\forall B\in\mathcal{B},\exists I\in I$ ç”¨æ•°å­¦å…¬å¼æè¿° Batch B of Batch set $\mathcal{B}$ ä¸­çš„æ‰€æœ‰åƒç´ åœ¨ image set $\mathcal{I}$ ä¸­çš„å›¾ç‰‡$I$ä¸­ 
  - $\mathcal{D} = g(\mathcal{B}) = g(\mathcal{I})$ å…¶ä¸­ $g(\cdot)$è¡¨ç¤ºreshape function å°†å¤šç»´é›†æ˜ å°„ä¸ºå•ä½é›†å¹¶ä¿å­˜element order: å°†ä»å•ä¸ªå›¾ç‰‡$I$ä¸­æŠ½å–å¾—åˆ°çš„$\mathcal{B}$ ä¸­çš„å¤šä¸ª$B$ å±•æˆä¸€ç»´æ•°æ®ï¼Œæœ€ç»ˆè·å¾—å•ç»´é›†$\mathcal{D}$ï¼Œç„¶åè¿›è¡ŒPæ’åˆ—ï¼Œå¾—åˆ°$P(\mathcal{D})$
  - æœ€ç»ˆå¾—åˆ°shuffledçš„batch set $\mathcal{B}=P^{*}(\mathcal{D})$ å’Œ the image set $\mathcal{I}$

**Section 3.2 Content-aware Permutation**

$\hat{P}_{\mathrm{intra}}^{I}(B)$è¡¨ç¤ºå¯¹ä»ç›¸åŒçš„è¾“å…¥è§†å›¾ä¸­æŠ½å–çš„ä¸åŒbatchçš„åƒç´ è¿›è¡Œæ’åº
$\hat{P}_{\mathrm{inter}}^{\mathcal{B}}(\mathcal{D})$è¡¨ç¤ºå¯¹åŒä¸€ä¸ªbatchçš„åƒç´ è¿›è¡Œæ’åº

**Section 3.3 Expansive Supervision**

![image.png|666](https://raw.githubusercontent.com/qiyun71/Blog_images/main/MyBlogPic/202403/20240928172804.png)

- The anchor area are computed by the light-wight edge detector to displays prominent error patterns. è¿™ä¸€åŒºåŸŸä»where patterns exhibit larger errorsè¿›è¡Œé€‰æ‹©
- And source area are sampled to expand its values to the reaming area. ğŸ¤”ä¸ºä»€ä¹ˆè¿˜è¦æœ‰sorce areaï¼Œé™¤äº†é‡è¦è¾¹ç¼˜åŒºåŸŸï¼Œè¿˜è€ƒè™‘ä¸€ä¸‹å…¶ä»–åŒºåŸŸï¼ŸIn my opinionï¼š
  - The source set is composed of sampled points, and the error is estimated based on these source points, which expand to cover all remaining areasã€‚**é€šè¿‡é‡‡æ ·ä¸€äº›é™¤äº†anchor setä¹‹å¤–çš„åƒç´ ï¼Œç”¨è¿™äº›åƒç´ çš„erroræ¥ä»£è¡¨å…¶ä»–åœ°æ–¹çš„errorï¼Œä»è€Œä¸éœ€è¦è®¡ç®—æ‰€æœ‰åƒç´ çš„errorï¼ŒèŠ‚çœäº†æ—¶é—´**

æœ€ç»ˆçš„æŸå¤±ï¼š
$$\begin{aligned}
\hat{L}=& \frac1{|A^*|}\sum_{r_A\in A^*}||\hat{C}(r_A)-C(r_A)||_2^2+ \\
&\frac{1}{|S|}(\frac{1}{\beta_{A}+\beta_{S}}-1)\sum_{r_{S}\in S}||\hat{C}(r_{S})-C(r_{S})||_{2}^{2},
\end{aligned}$$
- $\beta_{A}$å’Œ$\beta_{S}$åˆ†åˆ«ç”¨æ¥æ§åˆ¶anchor area å’Œ source areaçš„å¤§å°


# Uncertainty

## Sources of Uncertainty

[Sources of Uncertainty in 3D Scene Reconstruction](Sources%20of%20Uncertainty%20in%203D%20Scene%20Reconstruction.md)

- ç¯å¢ƒå…‰ç…§æ˜¯å¦å¯ä»¥é€šè¿‡ä¸ç¡®å®šæ€§è¿›è¡Œé‡åŒ–
- ä¸åŒçš„cuda/æ˜¾å¡ç¯å¢ƒæ˜¯å¦ä¹Ÿæ˜¯ä¸ç¡®å®šæ€§

## ActiveNeRF

[ActiveNeRF](ActiveNeRF.md)

## FisherRF

[FisherRF: Active View Selection and Uncertainty Quantification for Radiance Fields using Fisher Information](https://jiangwenpl.github.io/FisherRF/)

limited availability of 2D images poses uncertainties stemming from occlusions, depth ambiguities, and imaging errors

FisherRF computes the Fisher Information for the model and could select next best views or quantify pixel-wise uncertainty for its renderings.

![teaser-cropped.gif (1320Ã—1080)|333](https://jiangwenpl.github.io/FisherRF/static/images/teaser-cropped.gif)
![teaser.jpg (986Ã—863)|555](https://jiangwenpl.github.io/FisherRF/static/images/teaser.jpg)